{
  "research_topic": "Data-efficient image classification methods using novel regularization and augmentation techniques",
  "queries": [
    "data-efficient image classification",
    "sample-efficient CNN",
    "few-shot regularization",
    "novel augmentation methods",
    "mixup regularization",
    "autoaugment images",
    "randaugment classification",
    "manifold mixup",
    "small dataset regularization",
    "semi-supervised augmentation"
  ],
  "research_study_list": [
    {
      "title": "Explanation-based Data Augmentation for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Logarithmic Lenses: Exploring Log RGB Data for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Contextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification",
      "full_text": "Contextual Squeeze-and-Excitation for Efﬁcient Few-Shot Image Classiﬁcation Massimiliano Patacchiola University of Cambridge mp2008@cam.ac.uk John Bronskill University of Cambridge jfb54@cam.ac.uk Aliaksandra Shysheya University of Cambridge as2975@cam.ac.uk Katja Hofmann Microsoft Research kahofman@microsoft.com Sebastian Nowozin∗ nowozin@gmail.com Richard E. Turner University of Cambridge ret26@cam.ac.uk Abstract Recent years have seen a growth in user-centric applications that require effective knowledge transfer across tasks in the low-data regime. An example is personaliza- tion, where a pretrained system is adapted by learning on small amounts of labeled data belonging to a speciﬁc user. This setting requires high accuracy under low computational complexity, therefore the Pareto frontier of accuracy vs. adaptation cost plays a crucial role. In this paper we push this Pareto frontier in the few-shot image classiﬁcation setting with a key contribution: a new adaptive block called Contextual Squeeze-and-Excitation (CaSE) that adjusts a pretrained neural network on a new task to signiﬁcantly improve performance with a single forward pass of the user data (context). We use meta-trained CaSE blocks to conditionally adapt the body of a network and a ﬁne-tuning routine to adapt a linear head, deﬁning a method called UpperCaSE. UpperCaSE achieves a new state-of-the-art accuracy relative to meta-learners on the 26 datasets of VTAB+MD and on a challenging real-world personalization benchmark (ORBIT), narrowing the gap with leading ﬁne-tuning methods with the beneﬁt of orders of magnitude lower adaptation cost. 1 Introduction In recent years, the growth of industrial applications based on recommendation systems (Bennett et al., 2007), speech recognition (Xiong et al., 2018), and personalization (Massiceti et al., 2021) has sparked an interest in machine learning techniques that are able to adapt a model on small amounts of data belonging to a speciﬁc user. A key factor in many of these applications is the Pareto frontier of accuracy vs. computational complexity (cost to adapt). For example, in a real-time classiﬁcation task on a phone, a pretrained model must be personalized by exploiting small amounts of data on the user’s device (context). In these applications the goal is twofold: maximize the classiﬁcation accuracy on unseen data (target) while avoiding any latency and excessive use of computational resources. Methods developed to face these challenges in the few-shot classiﬁcation setting can be grouped in two categories: meta-learning and ﬁne-tuning. Meta-learning is based on the idea of learning-how-to- learn by improving the algorithm itself (Schmidhuber, 1987; Hospedales et al., 2020). Meta-learners are trained across multiple tasks to ingest a labeled context set, adapt the model, and predict the class membership of an unlabeled target point. Fine-tuning methods adjust the parameters of a pretrained neural network on the task at hand by iterative gradient-updates (Chen et al., 2019; Triantaﬁllou et al., 2019; Tian et al., 2020; Kolesnikov et al., 2020; Dumoulin et al., 2021). ∗Work done while the author was at Microsoft Research – Cambridge (UK) 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.09843v3  [cs.CV]  11 Jan 2023We can gain an insight on the differences between those two paradigms by comparing them in terms of accuracy and adaptation cost. Figure 1 illustrates this comparison by showing on the vertical axis the average classiﬁcation accuracy on the 18 datasets of the Visual Task Adaptation Benchmark (VTAB, Dumoulin et al. 2021), and on the horizontal axis the adaptation cost measured as the number of multiply–accumulate operations (MACs) required to adapt on a single task (see Appendix C.1 for details). Overall, ﬁne-tuners achieve a higher classiﬁcation accuracy than meta-learners but are more expensive to adapt. The comparison between two state-of-the-art methods for both categories, Big Transfer (BiT, Kolesnikov et al. 2020) and LITE (Bronskill et al., 2021), shows a substantial performance gap of 14% in favor of the ﬁne-tuner but at a much higher adaptation cost, with BiT requiring 526 ×1012 MACs and LITE only 0.2 ×1012 MACs. Figure 1: Accuracy and adaptation cost on VTAB for meta-learners (blue), ﬁne- tuners (red), and hybrids (blue-red). Black dotted-line is the previous Pareto front across categories. UpperCaSE nar- rows the gap with the leading ﬁne-tuning method and represents the best trade-off in terms of accuracy/adaptation-cost. It is crucial to ﬁnd solutions that retain the best of both worlds: the accuracy of ﬁne-tuners and low adaptation cost of meta-learners. The main bottleneck that hampers the adaptation of ﬁne-tuners is the need for multiple gradi- ent adjustments over the entire set of network parameters. Restricting those adjustments to the last linear layer (head) signiﬁcantly speeds up ﬁne-tuning, but it harms perfor- mance (e.g. see experiments in Section 5.1). Finding a way to rapidly adapt the feature extractor (body) is there- fore the main obstacle to bypass. In this paper we propose a hybrid solution to this issue, exploiting meta-learned adapters for rapidly adjusting the body and a ﬁne-tuning routine for optimizing the head. At the core of our approach is a novel extension of the popular Squeeze-and-Excitation block proposed by Hu et al. (2018) to the meta-learning setting that we call Contextual Squeeze-and-Excitation (CaSE). We exploit CaSE as building block of a hybrid training protocol called UpperCaSE which is based on the idea of adjusting the body of the network in a single forward pass over the context, and reserving the use of expensive ﬁne-tuning routines for the linear head, similarly to methods like MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). Figure 1 shows how UpperCaSE substantially improves the performance in the low-cost regime, outperforming meta-learners, ﬁne-tuners such as MD-Transfer (Triantaﬁllou et al., 2019), and reducing the gap with the current state of the art (BiT). When adaptation cost is critical, UpperCaSE is the best method currently available since it can provide substantial computation savings and compelling classiﬁcation performance. Our contributions can be summarized as follows: 1. We introduce a new adapter calledContextual Squeeze-and-Excitation (CaSE), based on the popular Squeeze-and-Excitation model proposed by Hu et al. (2018), that outperforms other adaptation mechanisms (e.g. the FiLM generators used in Bronskill et al. 2021) in terms of parameter efﬁciency (a 75% reduction in the number of adaptation parameters) and classiﬁcation accuracy (a 1.5% improvement on MetaDataset and VTAB). The code is released with an open-source license 1. 2. We use CaSE adaptive blocks in conjuction with a ﬁne-tuning routine for the linear head in a model called UpperCaSE, reporting an improved classiﬁcation accuracy compared to the SOTA meta-learner (Bronskill et al., 2021) on the 8 datasets of MDv2 (+2.5% on average) and the 18 datasets of VTAB (+6.8% on average), narrowing the gap with BiT (Kolesnikov et al., 2020) with the beneﬁt of orders of magnitude lower adaptation cost. 3. We showcase the potential of UpperCaSE in a real-world personalization task on the ORBIT dataset (Massiceti et al., 2021), where it compares favorably with the leading methods in the challenging cross-domain setting (training on MDv2, testing on ORBIT). 1https://github.com/mpatacchiola/contextual-squeeze-and-excitation 22 Contextual Squeeze-and-Excitation (CaSE) Problem formulation In this paragraph we introduce the few-shot learning notation, as this will be used to describe the functioning of a CaSE adaptive block. Let us deﬁne a collection of meta-training tasks as D= {τ1,...,τ D}where τi = (Ci,Ti) represents a generic task composed of a context set Ci = {(x,y)1,..., (x,y)M}and a target set Ti = {(x,y)1,..., (x,y)D}of input-output pairs. Following common practice we use the term shot to identify the number of samples per class (e.g. 5-shot is 5 samples per class) and the term way to identify the number of classes (e.g. 10-way is 10 classes per task). Given an evaluation task τ∗ = {C∗,x∗}the goal is to predict the true label y∗ of the unlabeled target point x∗ conditioned on the context set C∗. In ﬁne-tuning methods, we are given a neural network fθ(·), with parameters θ estimated via standard supervised-learning on a large labeled dataset (e.g. ImageNet). Given a test task τ∗ adaptation consists of minimizing the lossL(·) via gradient updates to ﬁnd the task-speciﬁc parameters θτ∗ ←G(ϵ,L,τ∗,fθ), where ϵis a learning rate, and G(·) is a functional representing an iterative routine that returns the adapted parameters θτ∗ (used for prediction). This procedure is particularly effective because it can exploit efﬁcient mini-batching, parallelization, and large pretrained models. In meta-learning methods training and evaluation are performed episodically (Vinyals et al., 2016), with training tasks sampled from a meta-train dataset and evaluation tasks sampled from an unseen meta-test dataset. The distinction in tasks is exploited to deﬁne a hierarchy. The parameters are divided in two groups: φtask-common parameters shared across all tasks (top of the hierarchy), and ψτ task-speciﬁc parameters estimated on the task at hand as part of an adaptive mechanism (bottom of the hierarchy). The way φand ψτ come into play is method dependent; they can be estimated via gradient updates (e.g. MAML, Finn et al. 2017), learned metrics (e.g. ProtoNets, Snell et al. 2017), or Bayesian methods (Gordon et al., 2018; Patacchiola et al., 2020; Sendera et al., 2021). Standard Squeeze-Excite (SE) We brieﬂy introduce standard SE (Hu et al., 2018), as we are going to build on top of this work. SE is an adaptive layer used in the supervised learning setting to perform instance based channel-wise feature adaptation, which is trained following a supervised protocol together with the parameters of the neural network backbone. Given a convolutional neural network, consider a subset of Llayers and associate to each one of them a Multi-Layer Perceptron (MLP), here represented as a function gφ(·). The number of hidden units in the MLP is deﬁned by the number of inputs divided by a reduction factor. Given a mini-batch of B input images, each convolution produces an output of size B×C×H×W where Cis the number of channels, Hthe height, and W the width of the resulting tensor. For simplicity we split this tensor into sub-tensors that are grouped into a set {H1,..., HB}with Hi ∈RC×H×W. To avoid clutter, we suppress the layer indexing when possible. SE perform a spatial pooling that produces a tensor of shape B×C×1 ×1; this can be interpreted as a set of vectors {h1,..., hB}with hi ∈RC. For each layer l, the set is passed to the associated MLP that will generate an individual scale vector γi ∈RC, where γ(l) 1 = g(l) φ ( h(l) 1 ) ··· γ(l) B = g(l) φ ( h(l) B ) . (1) An elementwise product is then performed between the scale vector and the original tensor ˆH(l) 1 = H(l) 1 ∗γ(l) 1 ··· ˆH(l) B = H(l) B ∗γ(l) B , (2) with the aim of modulating the activation along the channel dimension. This operation can be interpreted as a soft attention mechanism, with the MLP conditionally deciding which channel must be attended to. A graphical representation of SE is provided in Figure 2 (left). Contextual Squeeze-Excite (CaSE) Standard SE is an instance-based mechanism that is suited for i.i.d. data in the supervised setting. In a meta-learning setting we can exploit the distinction in tasks to deﬁne a new version of SE for task-based channel-wise feature adaptation. For a task τ = (C,T), consider the N images from the context set C, and the tensors produced by each convolution in the layers of interest {H1,..., HN}with Hi ∈RC×H×W. As in standard SE, we ﬁrst apply a spatial pooling to each tensor Hi which produces N vectors {h1,..., hN}of shape hi ∈RC. Then a context pooling is performed; this corresponds to an empirical mean over {h1,..., hN}(see Appendix A for more details about context pooling). The pooled representation is passed to the associated MLP to produce a single scale-vector for that layer γ(l) = g(l) φ ( ¯h(l) ) with ¯h(l) = 1 N ( h(l) 1 + ··· + h(l) N ) , (3) 3Figure 2: Comparison between the standard Squeeze-Excite (left) and the proposed Contextual Squeeze-Excite (right). Red frames highlight the two key differences between SE and CaSE: context pooling and scale transfer from context to target. B = mini-batch size, C = channels, H = height, W = width, N = context-set size, M = target-set size, ∗elementwise multiplication. which is then multiplied elementwise by the original tensors ˆH(l) 1 = H(l) 1 ∗γ(l) ··· ˆH(l) N = H(l) N ∗γ(l). (4) The scale vector is estimated in adaptive mode and transferred to the target points T in inference mode (no forward pass on the MLPs), as shown in the rightmost part of Figure 2. In synthesis, the three major differences between SE and CaSE are: (i) CaSE uses a contextual pooling with the aim of generating an adaptive vector per-task instead of per-instance as in SE; (ii) CaSE distinguishes between an adaptive mode and an inference mode that transfers the scale from context to target, while SE does not make such a distinction; and (iii) CaSE parameters are estimated via episodic meta-training while SE parameters via standard supervised-training. In Section 5.1 we show that those differences are fundamental to achieve superior performance in the few-shot setting. A representation of a CaSE block is reported in Figure 2 (right), additional technical details are provided in Appendix A. Comparison with other adapters Popular adaptation mechanisms for few-shot learning are based on Feature-wise Linear Modulation layers (FiLM, Perez et al. 2018). Those mechanisms perform adaptation using a separate convolutional set-encoder to produce an embedding of the context set. The embedding is forwarded to local MLPs to produce the scale and shift vectors of the FiLM layers that modulate a pretrained model. Variations of this adapter have been used in several methods, such as TADAM (Oreshkin et al., 2018), CNAPs (Requeima et al., 2019), SimpleCNAPs (Bateni et al., 2020), CA VIA (Zintgraf et al., 2019), and LITE (Bronskill et al., 2021). We will use the generic term FiLM generator to refer to these adapters and the term FiLM to refer to the scale and shift vectors used to modulate the activations. There are two key differences between FiLM and CaSE: (i) CaSE exploits context pooling to aggregate the activations of the backbone instead of a separate set-encoder as in FilM generators (see Appendix A for details) which is more efﬁcient in terms of parameter count and implementation overhead; and (ii) FiLM uses scale and shift to modulate the activations, CaSE only the scale, therefore 50% less parameters are stored in memory and transferred during inference. In Section 5.1 we compare CaSE and the FiLM generators used in a recent SOTA method (LITE, Bronskill et al. 2021), showing that CaSE is superior in terms of accuracy while using a fraction of the amortization parameters. 3 UpperCaSE: system description and optimization protocol We exploit CaSE blocks as part of UpperCaSE, a hybrid training protocol based on Coordinate- Descent (CD). We call this protocolhybrid because it combines a meta-training procedure to optimize the CaSE parameters (body) with a ﬁne-tuning routine to estimate the task-speciﬁc parameters (head). Preliminaries We are given a feature extractor (body) pretrained with supervised learning on a large dataset (e.g. ImageNet), deﬁned as bθ(·) where θare the pretrained parameters. CaSE blocks, 4parameterized by φ, are added to the model at speciﬁc locations to give bθ,φ(·) (see Appendix A for details about this step). We are interested in learning the CaSE parameters φkeeping constant the pretrained parameters θ(omitted from here to keep the notation uncluttered). At training time, we are given a series of tasks τ = {C,T}∼D , where Dis the training set. The number of classes (way) is calculated from the context set and used to deﬁne a linear classiﬁcation head hψτ (·) parameterized by ψτ. The complete model is obtained by nesting the two functions as hψτ (bφ(·)). We indicate a forward pass through the body over the context inputs with the shorthand bφ(Cx) →{z1,..., zN}, where zn is the context embedding for the input xn. All the context embeddings and the associated labels are stored in M= {(zn,yn)}N n=1. Optimization challenges We have two sets of learnable parameters,φthe CaSE parameters, and ψτ the parameters of the linear head for the taskτ. While φis shared across all tasks (task-common), ψτ must be inferred on the task at hand (task-speciﬁc). In both cases, the objective is the minimization of a classiﬁcation loss L. There are some challenges in optimizing the CaSE parameters in the body, as shown by the decomposition of the full gradient dL dφ = ∑ τ (∂Lτ ∂ψτ dψτ dφ + ∂Lτ ∂φ ) . (5) The ﬁrst term ∂Lτ/∂ψτ (sensitivity of the loss w.r.t. the head) and the direct gradient ∂L/∂φ (sensitivity of the loss w.r.t. the adaptation parameters with a ﬁxed head) can be obtained with auto-differentiation as usual. The second term dψτ/dφ(sensitivity of the head w.r.t. the adaptation parameters) is problematic because ψτ is obtained iteratively after a sequence of gradient updates. Backpropagating the gradients to φincludes a backpropagation through all the gradient steps per- formed to obtain the task-speciﬁc ψτ. Previous work has showed that this produces instability, vanishing gradients, and high memory consumption (Antoniou et al., 2018; Rajeswaran et al., 2019). Meta-training via Coordinate-Descent A potential solution to these issues is the use of implicit gradients (Chen et al., 2020; Rajeswaran et al., 2019; Chen et al., 2022). The main problem with implicit gradients is the computation and inversion of the Hessian matrix as part of Cauchy’s implicit function theorem, which is infeasible when the number of parameters in the linear head is large. Another possible solution is the use of an alternating-optimization scheme, similar to the one proposed in a number of recent methods such as MetaOptNet (Lee et al., 2019), R2D2 (Bertinetto et al., 2018), and ANIL (Raghu et al., 2019). These methods share the idea of inner-loop-head/outer- loop-body meta-training, and they ﬁnd the parameters of the linear head with closed form solutions or by stochastic optimization. Starting from similar assumptions we propose a simple yet effective alternating-optimization scheme, which we formalize using Coordinate-Descent (CD) (Wright, 2015). The idea behind CD is to consider the minimization of a complex multi-variate function as a set of simpler objectives that can be solved one at a time. In our case, we can consider the joined landscape w.r.t. φand ψτ as composed of two separate sets of coordinates (block CD, Wright 2015). By minimizing ψτ ﬁrst, we reach a local minimum where ∂Lτ/∂ψτ ≈0. Therefore CD induces a direct optimization objective w.r.t.φ, with Equation (5) reducing to ∂Lτ/∂φ(no red term). The time complexity of this method is only affected by the number of classes but is constant w.r.t. the number of training points due to the use of mini-batching, which scales well with large tasks (e.g. those in MetaDataset and VTAB). See Appendix B for more details. In practice, at each training iteration we sample a task τ = (C,T) ∼D, perform a forward pass on the body (with CaSE in adaptive mode) to get bφ(Cx) →{z1,..., zN}. (6) The context embeddings are temporarily stored in a buffer with their associated labels M = {(zn,yn)}N n=1 to avoid expensive calls to bφ(·). We then set the head parameters to zero, and solve the ﬁrst minimization problem (inner-loop), obtaining the task-speciﬁc parameters ψτ via ψτ ←G ( α,M,L,hψτ ) (7) where αis a learning rate, and G(·) is a functional representing an iterative gradient-descent routine for parameter estimation (e.g. maximum likelihood estimation or maximum a posteriori estimation). Note that the iterative routine in Equation(7) only relies on the headhψτ (·) and not on the bodybφ(·), which is the primary source of memory savings and the crucial difference with common ﬁne-tuning methods. Moreover, the inner-loop is agnostic to the choice of optimizer, it can handle many gradient steps without complications, exploit parallelization and efﬁcient mini-batching. 5We then turn our attention to the second coordinate: the task-common parameters of the CaSE blocks in the body. For a single task, the update consists of a single optimization step w.r.t.φ(outer-loop) given support/target points and the task-speciﬁc parameters ψτ identiﬁed previously. The ﬁnal form of the equation depends on the optimizer, for a generic SGD the update is given by φ←φ−β∇φL ( Cy ∪Qy,hψτ ,bφ ) , (8) where βis a learning rate. CaSE blocks must be in adaptive mode to allow the backpropagation of the gradients to the MLPs. The process repeats, alternating the minimization along the two sets of coordinates. The pseudo-code for train and test is provided in Appendix B. Inference on unseen tasks After the training phase, we are given an unseen task τ∗ = (C∗,x∗) where x∗ is a single target input and y∗ the associate true label to estimate. Inference consists of three steps: (i) forward pass on the body for all the context inputs with CaSE set to adaptive mode as in Equation (6) and embeddings/labels stored in M, (ii) estimation of the task-speciﬁc parameters ψ∗ via iterative updates as in Equation (7), and (iii) inference of the target-point membership via a forward pass over body and head ˆy∗ = hψ∗ (bφ(x∗)) with CaSE in inference mode. 4 Related work Meta-learning There has been a large volume of publications related to meta-learning. Here we focus on those methods that are the most related to our work, and refer the reader to a recent survey for additional details (Hospedales et al., 2020). LITE (Bronskill et al., 2021) is a protocol for training meta-learners on large images, that achieved SOTA accuracy on VTAB+MD. LITE is particularly relevant in this work, as its best performing method is based on Simple CNAPs (Bateni et al., 2020) that exploits FiLM for fast body adaptation. We compare against LITE in Section 5.2 showing that UpperCaSE is superior in terms of classiﬁcation accuracy and parameter efﬁciency. Fine-tuning Chen et al. (2019) were the ﬁrst to expose the potential of simple ﬁne-tuning baselines for transfer learning. MD-Transfer has been proposed in Triantaﬁllou et al. (2019) as an effective ﬁne-tuning baseline for the MetaDataset benchmark. More recently Kolesnikov et al. (2020) have presented Big Transfer (BiT), showing that large models pretrained on ILSVRC-2012 ImageNet and the full ImageNet-21k are very effective at transfer learning. MD-Transfer and BiT differ in terms of classiﬁcation head, learning schedule, normalization layers, and batching. Fine-tuning only the last linear layer can be effective (Bauer et al., 2017; Tian et al., 2020). We compare against this baseline in Section 5.1, showing that adapting the body via CaSE signiﬁcantly boosts the performance. Overall, ﬁne-tuners have consistently outperformed meta-learners in terms of classiﬁcation accuracy, only under particular conditions (e.g. strong class-imbalance) the trend is reversed (Ochal et al., 2021a,b). Hybrids Hybrid methods are trained episodically like meta-learners but rely on ﬁne-tuning routines for adaptation. Model Agnostic Meta-Learning (MAML, Finn et al. 2017) ﬁnds a set of parameters that is a good starting point for adaptation towards new tasks in a few gradient steps. MAML has been the inspiration for a series of other models such as MAML++ (Antoniou et al., 2018), ProtoMAML (Triantaﬁllou et al., 2019), and Reptile (Nichol et al., 2018). Dynamic networks CaSE blocks belong to the wider family of dynamic networks, models that can adapt their structure or parameters to different inputs (Han et al., 2021). Adaptive components have been used in a variety of applications, such as neural compression (Veit and Belongie, 2018; Wu et al., 2018), generation of artistic styles (Dumoulin et al., 2016; Huang and Belongie, 2017), or routing (Guo et al., 2019). Residual adapters (Rebufﬁ et al., 2017, 2018) have been used in transfer learning (non few-shot) but they rely on ﬁne-tuning routines which are signiﬁcantly slow during adaptation. More recently, Li et al. (2022) have used serial and residual adapters in the few-shot setting, with the task-speciﬁc weights being adapted from scratch on the context set. This approach has similar limitations, since it requires backpropagation to the task-speciﬁc weights in the body of the network which is costly. In Sun et al. (2019) the authors introduce a Meta-Transfer Learning (MTL) method for the few-shot setting. In MTL a series of scale and shift parameters are meta-learned across tasks and then dynamically adapted during the test phase via ﬁne-tuning. This method suffers of similar limitations, as the ﬁne-tuning stage is expensive during adaptation. Moreover, MTL relies on scale and shift vectors to perform adaptation whereas CaSE only relies on a scale vector, meaning that it needs to store and transfer 50% less parameters at test time. 6Figure 3: Left: CaSE vs Squeeze-and-Excitation (SE) (both methods use EfﬁcientNetB0, 84 × 84 inputs, Mahalanobis-distance head). CaSE outperforms SE in all conditions. Center: CaSE vs. FiLM generators (Bronskill et al., 2021) and a baseline with no body adaptation (all methods use EfﬁcientNetB0, 84 ×84 inputs, Mahalanobis-distance head). CaSE outperforms FiLM generators in all conditions. Right: boxplot of CaSE activations at different depth of an EfﬁcientNetB0 for 800 tasks sampled from the MDv2 test set (224 ×224 inputs, UpperCaSE). The modulation of CaSE is minimal at early stages for general-purpose ﬁlters and increases at deeper stages. 5 Experiments In this section we report on experiments on VTAB+MD (Dumoulin et al., 2021) and ORBIT (Massiceti et al., 2021). VTAB+MD has become the standard evaluation protocol for few-shot approaches, and it includes a large number of datasets (8 test dataset for MD, 18 for VTAB). For a description of ORBIT, see Section 5.2. In all experiments we used the following pretrained (on ImageNet) backbones: EfﬁcientNetB0 from the ofﬁcial Torchvision repository; ResNet50x1-S released with BiT (Kolesnikov et al., 2020). We used three workstations (CPU 6 cores, 110GB of RAM, and a Tesla V100 GPU), the meta-training protocol of Bronskill et al. (2021) ( 10K training tasks, updates every 16 tasks), the Adam optimizer with a linearly-decayed learning rate in [10−3,10−5] for both the CaSE and linear-head. The head is updated 500 times using a random mini-batch of size 128. MD test results are averaged over 1200 tasks per-dataset (conﬁdence intervals in appendix). We did not use data augmentation. Code to reproduce the experiments is available at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . 5.1 Analysis of CaSE blocks In this sub-section we report empirical results related to CaSE blocks in three directions: 1) we compared standard SE (Hu et al., 2018) and CaSE on MDv2 and VTAB, conﬁrming that a) adaptation helps over not adapting, b) contextual adaptation (CaSE) outperforms instance based adaptation (SE); 2) we compare CaSE against a SOTA FiLM generator (Bronskill et al., 2021), showing that CaSE is signiﬁcantly more efﬁcient using 75% fewer parameters while boosting the classiﬁcation accuracy on average by +1.5% on VTAB and MD2; and 3) we provide an insight on the effectiveness of CaSE blocks with a series of qualitative analysis. Comparing SE vs. CaSE We compare standard SE and the proposed CaSE on VTAB and MD- v2. For a fair comparison we keep constant all factors of variation (backbone, training schedule, hyperparameters, etc.) and use the same reduction of 32 (0.8M adaptive parameters). In order to compare the results with the other experiments in this section, we use a Mahalanobis-distance head as in Bronskill et al. (2021), reporting results with a linear head in the appendix. We summarize the results in Figure 3 (left) and add a tabular breakdown in Appendix C.2. CaSE outperforms SE in all conditions, conﬁrming that a contextual adaptation mechanism is fundamental to transfer knowledge effectively across tasks. Comparing adaptation mechanisms We perform a comparison on VTAB+MD of CaSE against FiLM generators (Bronskill et al., 2021), and a baseline that uses a pretrained model but no adaptation of the body. Methods are compared in identical conditions, using a Mahalanobis-distance head, an EfﬁcientNetB0 backbone, and same training schedule. We show a summary of the results in 7Figure 3 (center) and provide a full breakdown in the appendix. CaSE is able to outperform FiLM generators in all conditions. In Appendix C.3 we report the results for CaSE with reduction 64 (0.4M parameters) showing that it is able to outperform FiLM generators (1.7M parameters) using a fraction of the parameters. The comparison with the baseline with no adaptation, shows that in all but one condition (VTAB specialized) adaptation is beneﬁcial. This is likely due to the strong domain shift introduced by some of the specialized datasets. Role of CaSE blocks To examine the role of CaSE blocks we analyze the aggregated activations at different stages of the body for 800 tasks sampled from the MDv2 test set using an EfﬁcientNetB0 trained with UpperCaSE on224×224 images. In Figure 3 (right) we report the aggregated distribution as boxplots, and in Appendix C.5 we provide a per-dataset breakdown. Overall the median is close to 1.0 (identity) which is the expected behavior as on average we aim at exploiting the underlying pretrained model. The variance is small at early stages, indicating that CaSE has learned to take advantage of general-purpose ﬁlters that are useful across all tasks. In deeper layers the variance increases, showing a task-speciﬁc modulation effect. In Appendix C.5 we also include a plot with per-channel activations for all datasets at different depths, showing that the modulation is similar across datasets at early stages and it diverges later on. An ablation study of different factors (e.g. reduction, number of hidden layers, activation functions) is reported in Appendix C.4. 5.2 Performance evaluation of UpperCaSE In this sub-section we analyze the performance of UpperCaSE in two settings: 1) comparison on the VTAB+MD benchmark against SOTA ﬁne-tuners and meta-learners, where we show that UpperCaSE is able to outperform all the meta-learners, narrowing the gap with Big Transfer (BiT) on VTAB;2) we show an application of UpperCaSE in a real-world personalization task on the challenging ORBIT dataset (Massiceti et al., 2021) for the cross-domain condition MDv2→ORBIT, where we achieve the best average-score in most metrics, although these improvements are within the error bars. Table 1: UpperCaSE outperforms ﬁne-tuners on MDv2 and narrows the gap on VTAB with the leading method (BiT) with a much lower adaptation cost. Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (no adapters) in millions. Cost: MACs to adapt on a task (10-shot, 100-way), in Teras. Best results in bold. Cost↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. MACs all all natur. spec. struc. MD-Transfer ﬁne-tuning RN18 126 11.2 118.6 63.4 55.6 52.4 72.9 49.3 SUR ﬁne-tuning RN50 224 164.6 28.8 71.3 43.7 50.9 66.2 27.2 Big Transfer ﬁne-tuning RN50 224 23.5 526.3 73.3 65.4 69.4 81.0 54.5 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.2 76.1 58.4 69.1 80.3 39.4 Table 2: UpperCaSE outperforms all meta-learning/hybrid methods and uses the lowest num- ber of parameters per adaptive blocks . Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfﬁcientNet. Img: image size. Param.: total parameters (excluding adapters). Adapt.: total adaptive parameters in millions. Best results in bold. Adapt.↓ MDv2↑ VTAB↑ Method Protocol Net Img Param. count all all natur. spec. struc. ProtoMAML hybrid RN18 126 11.2 n/a 64.2 45.0 45.7 70.7 31.5 CTX meta-learning RN34 224 21.3 n/a 71.6 50.5 61.1 67.3 34.0 ProtoNet meta-learning ENB0 224 4.0 n/a 72.7 46.1 60.9 64.2 25.9 LITE meta-learning ENB0 224 4.0 1.7 73.8 51.4 65.2 71.9 30.8 UpperCaSE hybrid RN50 224 23.5 0.8 74.9 56.6 66.3 80.1 37.6 UpperCaSE hybrid ENB0 224 4.0 0.4 76.1 58.4 69.1 80.3 39.4 Comparison on VTAB+MDWe compare UpperCaSE against ﬁne-tuners, meta-learners, and hybrids on the 18 datasets of VTAB and the 8 datasets of MetaDataset-v2 (MDv2) and report the results 8Table 3: ORBIT: UpperCaSE obtains the best average-score in most metrics, being within error bars with leading methods. Average accuracy and 95% conﬁdence interval for frames, videos, and frames to recognition (FTR). Cost: average MACs over all tasks (Teras). Results and setup from Massiceti et al. (2021): meta-train on MetaDataset and test on ORBIT, image-size 84 ×84, ResNet18 backbone, 85 test tasks (17 test users, 5 tasks per user). Best results (within error bars) in bold. Cost Clean Video Evaluation (CLE-VE) Clutter Video Evaluation (CLU-VE) Method MACs ↓ frame acc.↑ FTR↓ video acc.↑ frame acc.↑ FTR↓ video acc.↑ ProtoNet 3.2 59.0 ±2.2 11.5 ±1.8 69.2 ±3.0 47.0 ±1.8 20.4 ±1.7 52.8 ±2.5 CNAPs 3.5 51.9 ±2.5 20.8 ±2.3 60.8 ±3.2 41.6 ±1.9 30.7 ±2.1 43.0 ±2.5 MAML 95.3 42.5 ±2.7 37.3 ±3.0 47.0 ±3.2 24.3 ±1.8 62.3 ±2.3 25.7 ±2.2 FineTuner 317.7 61.0±2.2 11.5 ±1.8 72.6 ±2.9 48.4 ±1.9 19.1 ±1.7 54.1 ±2.5 UpperCaSE 3.5 63.0±2.2 8.8 ±1.6 74.4 ±2.8 48.1 ±1.8 18.2 ±1.7 54.5 ±2.5 in Table 1 and Table 2. UpperCaSE outperforms all methods (including BiT) on MDv2 with an accuracy of 74.9% (ResNet50) and 76.1% (EfﬁcientNetB0). On VTAB, UpperCaSE outperforms most methods, narrowing the gap with BiT. A closer look at the differences in performance on VTAB between UpperCaSE and BiT (see Table 1) shows that the gap is narrower on the natural and specialized splits (+3.1% and +0.9%) but larger on structured (+16.9%). The breakdown by dataset reported in Appendix C.6 shows that the major performance drops are on tasks that require localization and counting (e.g. dSprites, SmallNORB). Similar issues are encountered by methods such as LITE (Bronskill et al., 2021) which are based on FiLM generators, suggesting that those tasks may introduce a strong domain shift w.r.t. the meta-training set that is difﬁcult to compensate without ﬁne-tuning the body. It is not clear whether transfer learning is beneﬁcial on these datasets in the ﬁrst place. The results in terms of adaptation cost (see Table 1) over a synthetic task (10-shot, 100 way) show that UpperCaSE is orders of magnitude more efﬁcient (0.2 ×1012 MACs) than all ﬁne-tuners, with BiT being the most expensive method overall (526.3 × 1012 MACs). The comparison against meta-learners in terms of number of adaptive parameters (see Table 2) shows that UpperCaSE requires a fraction of the parameters (0.4 vs 1.7 millions for an EfﬁcientNetB0) compared to LITE (Bronskill et al., 2021) which is based on FiLM generators. Comparison on ORBIT We compare UpperCaSE to other methods on ORBIT (Massiceti et al., 2021), a real-world dataset for teachable object recognizers. ORBIT consists of 3822 videos of 486 objects recorded by 77 blind/low-vision people on their mobile phones. The dataset is challenging because objects are poorly framed, occluded, blurred, and in a wide variation of backgrounds and lighting. The dataset includes two sets of target videos, one for clean video evaluation (CLE-VE) with well-centered objects, and another for clutter video evaluation (CLU-VE) with objects in complex, cluttered environments. We consider a hard transfer-learning condition where classiﬁers are meta-trained on MetaDataset and tested on ORBIT. Results are reported in Table 3. UpperCaSE outperforms all other methods (on average) on most metrics, being within error bars with the two leading methods. Comparing UpperCaSE with FineTuner, the gap in favor of UpperCaSE is marginal on CLU-VE but substantial on CLE-VE (frame accuracy +2%, video accuracy +1.8%, and FTR −2.7). Comparison in terms of adaptation cost (average MACs over all tasks) shows that UpperCaSE is orders of magnitude more efﬁcient than FineTuner and close to the leading method (ProtoNet). 6 Conclusions We have introduced a new adaptive block called CaSE, which is based on the popular Squeeze-and- Excitation (SE) block proposed by Hu et al. (2018). CaSE is effective at modulating a pretrained model in the few-shot setting, outperforming other adaptation mechanisms. Exploiting CaSE we have designed UpperCaSE, a hybrid method based on a Coordinate-Descent training protocol, that combines the performance of ﬁne-tuners with the low adaptation cost of meta-learners. UpperCaSE achieves SOTA accuracy w.r.t. meta-learners on the 26 datasets of VTAB+MD and it compares favorably with leading methods in the ORBIT personalization benchmark. 9Limitations There are two limitations that are worth mentioning: (i) UpperCaSE requires iterative gradient updates that are hardware-dependent and may be slow/unavailable in some portable devices; (ii) breakdown VTAB results per-dataset shows that the method falls short on structured datasets. This indicates that ﬁne-tuning the body may be necessary for high accuracy when the shift w.r.t. the meta-training set is large. Societal impact Applications based on CaSE and UpperCaSE could be deployed in few-shot classiﬁ- cation settings that can have a positive impact such as: medical diagnosis, recommendation systems, object detection, etc. The efﬁciency of our method can reduce energy consumption and beneﬁt the environment. Certain applications require careful consideration to avoid biases that can harm speciﬁc groups of people (e.g. surveillance, legal decision-making). Acknowledgments and Disclosure of Funding Funding in direct support of this work: Massimiliano Patacchiola, John Bronskill, Aliaksandra Shysheya, and Richard E. Turner are supported by an EPSRC Prosperity Partnership EP/T005386/1 between the EPSRC, Microsoft Research and the University of Cambridge. The authors would like to thank: anonymous reviewers for useful comments and suggestions; Aristeidis Panos, Daniela Massiceti, and Shoaib Ahmed Siddiqui for providing suggestions and feedback on the preliminary version of the manuscript. References Antoniou, A., Edwards, H., and Storkey, A. (2018). How to train your maml. arXiv preprint arXiv:1810.09502. Bateni, P., Goyal, R., Masrani, V ., Wood, F., and Sigal, L. (2020). Improved few-shot visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Bauer, M., Rojas-Carulla, M., ´Swi ˛ atkowski, J. B., Schölkopf, B., and Turner, R. E. (2017). Discrimi- native k-shot learning using probabilistic models. arXiv preprint arXiv:1706.00326. Bennett, J., Lanning, S., et al. (2007). The netﬂix prize. In Proceedings of KDD cup and workshop. Bertinetto, L., Henriques, J. F., Torr, P. H., and Vedaldi, A. (2018). Meta-learning with differentiable closed-form solvers. arXiv preprint arXiv:1805.08136. Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., and Turner, R. (2021). Memory efﬁcient meta-learning with large images. Advances in Neural Information Processing Systems. Chen, W., Tripp, A., and Hernández-Lobato, J. M. (2022). Meta-learning feature representations for adaptive gaussian processes via implicit differentiation. arXiv preprint arXiv:2205.02708. Chen, W.-Y ., Liu, Y .-C., Kira, Z., Wang, Y .-C. F., and Huang, J.-B. (2019). A closer look at few-shot classiﬁcation. arXiv preprint arXiv:1904.04232. Chen, Y ., Friesen, A. L., Behbahani, F., Doucet, A., Budden, D., Hoffman, M., and de Freitas, N. (2020). Modular meta-learning with shrinkage. Advances in Neural Information Processing Systems. Dumoulin, V ., Houlsby, N., Evci, U., Zhai, X., Goroshin, R., Gelly, S., and Larochelle, H. (2021). Comparing transfer and meta learning approaches on a uniﬁed few-shot classiﬁcation benchmark. arXiv preprint arXiv:2104.02638. Dumoulin, V ., Shlens, J., and Kudlur, M. (2016). A learned representation for artistic style.arXiv preprint arXiv:1610.07629. Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning. Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D. J., Eslami, S., and Teh, Y . W. (2018). Neural processes. arXiv preprint arXiv:1807.01622. 10Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R. E. (2018). Meta-learning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921. Guo, Y ., Shi, H., Kumar, A., Grauman, K., Rosing, T., and Feris, R. (2019). Spottune: transfer learning through adaptive ﬁne-tuning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Han, Y ., Huang, G., Song, S., Yang, L., Wang, H., and Wang, Y . (2021). Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence. Hospedales, T., Antoniou, A., Micaelli, P., and Storkey, A. (2020). Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439. Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Huang, X. and Belongie, S. (2017). Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. (2020). Big transfer (bit): General visual representation learning. In European conference on computer vision. Lee, K., Maji, S., Ravichandran, A., and Soatto, S. (2019). Meta-learning with differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Li, W.-H., Liu, X., and Bilen, H. (2022). Cross-domain few-shot learning with task-speciﬁc adapters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Massiceti, D., Zintgraf, L., Bronskill, J., Theodorou, L., Harris, M. T., Cutrell, E., Morrison, C., Hofmann, K., and Stumpf, S. (2021). Orbit: A real-world few-shot dataset for teachable object recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Nichol, A., Achiam, J., and Schulman, J. (2018). On ﬁrst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021a). Few-shot learning with class imbalance. arXiv preprint arXiv:2101.02523. Ochal, M., Patacchiola, M., Storkey, A., Vazquez, J., and Wang, S. (2021b). How sensitive are meta-learners to dataset imbalance? arXiv preprint arXiv:2104.05344. Oreshkin, B., Rodríguez López, P., and Lacoste, A. (2018). Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in neural information processing systems, 31. Patacchiola, M., Turner, J., Crowley, E. J., O’Boyle, M., and Storkey, A. J. (2020). Bayesian meta- learning for the few-shot setting via deep kernels. Advances in Neural Information Processing Systems. Perez, E., Strub, F., De Vries, H., Dumoulin, V ., and Courville, A. (2018). Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. (2019). Rapid learning or feature reuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157. Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. (2019). Meta-learning with implicit gradients. Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2017). Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems. Rebufﬁ, S.-A., Bilen, H., and Vedaldi, A. (2018). Efﬁcient parametrization of multi-domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition. 11Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. (2019). Fast and ﬂexible multi- task classiﬁcation using conditional neural adaptive processes. Advances in Neural Information Processing Systems. Schmidhuber, J. (1987). Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München. Sendera, M., Tabor, J., Nowak, A., Bedychaj, A., Patacchiola, M., Trzcinski, T., Spurek, P., and Zieba, M. (2021). Non-gaussian gaussian processes for few-shot regression. Advances in Neural Information Processing Systems. Snell, J., Swersky, K., and Zemel, R. (2017). Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems. Sun, Q., Liu, Y ., Chua, T.-S., and Schiele, B. (2019). Meta-transfer learning for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Tian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and Isola, P. (2020). Rethinking few-shot image classiﬁcation: a good embedding is all you need? In European Conference on Computer Vision. Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., et al. (2019). Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096. Veit, A. and Belongie, S. (2018). Convolutional networks with adaptive inference graphs. In Proceedings of the European Conference on Computer Vision (ECCV). Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. (2016). Matching networks for one shot learning. Advances in Neural Information Processing Systems. Wright, S. J. (2015). Coordinate descent algorithms. Mathematical Programming, 151. Wu, Z., Nagarajan, T., Kumar, A., Rennie, S., Davis, L. S., Grauman, K., and Feris, R. (2018). Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., and Stolcke, A. (2018). The microsoft 2017 conversational speech recognition system. In IEEE international conference on acoustics, speech and signal processing (ICASSP). Zintgraf, L., Shiarli, K., Kurin, V ., Hofmann, K., and Whiteson, S. (2019). Fast context adaptation via meta-learning. In International Conference on Machine Learning. 12A CaSE: additional details A.1 CaSE implementation Standardization Empirically we have observed that standardizing the pooled representations before passing them to the MLP improves the training stability in CaSE (but not in SE). Standardization is performed by taking the pooled representation at layer las showed in Equation (3), that is ¯h(l) ∈RC, subtracting the mean and dividing by the standard deviation. Activation function for the output layer Standard SE blocks usually rely on a sigmoid function in the last layer of the MLPs. This works well when the adaptive block is trained in parallel with the underlying neural network. However, in our case we use a pretrained model and learning can be speeded up considerably by enforcing the identity function as output of the MLPs. We achieve this by multiplying the output of the sigmoid by a constant scalar c= 2which extends the range to [0,2], and then set to zero the weights and bias of the layer. This has the effect of enforcing the identity function at the beginning of the training. We have also used a linear activation function instead of a sigmoid, with good results. When using a linear output the identity can be enforced by setting the weights of the last layer to zero, and the bias to one. An ablation over the activation function of SE and CaSE is provided in Appendix C.4 (Table 6). CaSE location For the choice of CaSE location in the feature extractor, we followed the same principles used in Bronskill et al. (2021) for FiLM generators. In EfﬁcientNetB0 we place CaSE at the beginning of each hyperblock and the last layer (excluding the ﬁrst layer). Differently from FiLM (placed after the BatchNorm) we place CaSE after the non-linearity (as done in standard SE) and before the Squeeze-and-Excitation block (included by default in EfﬁcientNet): Conv2d→BatchNorm2d→SiLU→CaSE→SqueezeExcitation→Conv2d→BatchNorm2d This results in a total of 18 CaSE blocks for EfﬁcientNetB0. Increasing the number of blocks did not provide a signiﬁcant beneﬁt. In ResNet18 we place two CaSE blocks per each basic block as: Conv2d→BatchNorm2d→ReLU→CaSE→Conv2d→BatchNorm2d→ReLU→CaSE Similarly we place two CaSE blocks inside a bottleneck block in ResNet50. See the code for more details. Based on the qualitative analysis reported in Section 5 we hypothesize that adaptive blocks are not needed in the initial layers of the network, since at those stages their activity is minimal. Identifying which layer needs adapters and which layer does not, can reduce even more the parameter count of adaptive blocks. Additional work is needed to fully understand this factor. CaSE reduction The number of parameters allocated to the CaSE blocks is regulated by a divider r that is used to compute the number of hidden units in the MLPs. Given the input sizeC(corresponding to the number of channels in that layer) the number of hidden units is given by C/r. We also use a clipping factor rmin that prevents the number of units to fall under a given threshold. This prevents the allocation of a low number of units for layers with a small number of channels. A.2 Context pooling In this section we provide additional details about the context pooling operation performed in a CaSE adaptive block (described in Section 2). Similarities with other methods Context pooling is a way to summarize a task with a permutation- invariant aggregation of the embeddings. A similar mechanism has been exploited in various meta-learning methods. For instance, in ProtoNets (Snell et al., 2017) a prototype for a single class is computed by taking the average over all the context embeddings associated to the inputs for that class. The embeddings are generated in the last layer of the feature extractor. In Simple-CNAPs (Bateni et al., 2020) a prototype is estimated as in ProtoNets but it is used to deﬁne a Gaussian distribution instead of a mean vector. Neural latent variable models, such as those derived from the Neural Processes family (Garnelo et al., 2018) also rely on similar permutation-invariant aggregations to deﬁne distributions over functions. 13Global vs. local context-pooling Comparing CaSE with the FiLM generators of Bronskill et al. (2021) it is possible to distinguish between two types of context pooling: global and local. The FiLM generators of Bronskill et al. (2021) rely on a global pooling strategy, meaning that the aggregation is performed once-for-all by using a dedicated convolutional set encoder. More speciﬁcally, the encoder takes as input all the context images and produces embeddings for each one of them, followed by an average-pooling of those embeddings. The aggregated embedding is then passed to MLPs in each layer that generates a scale and shift parameter. Crucially, each MLP receives the same embedding. CaSE exploits a local context-pooling at the layer level. The convolutional set encoder is discarded, and the feature maps produces by the backbone itself at each stage are used as context embeddings. Therefore, the MLPs responsible for generating the scale parameters receive a unique embedding. As showed in the experimental section (Section 5), local pooling improves performances and uses less parameters, as no convolutional encoder is needed. Additional details about the differences between CaSE and FiLM generators is also provided in the paper (Section 4). 14A.3 Pytorch code for CaSE Implementation of a CaSE adaptive block in Pytorch. The script is also available as case.py at https://github.com/mpatacchiola/contextual-squeeze-and-excitation . import torch from torch import nn class CaSE (nn. Module ): def __init__ (self , cin , reduction =32 , min_units =32 , standardize =True , out_mul =2.0, device =None , dtype = None ): \"\"\" Initialize a CaSE adaptive block . Parameters : cin ( int ): number of input channels . reduction ( int ): divider for computing number of hidden units . min_units ( int ): clip hidden units to this value (if lower ). standardize ( bool ): standardize the input for the MLP . out_mul ( float ): multiply the MLP output by this value . \"\"\" factory_kwargs = {’ device ’: device , ’dtype ’: dtype } super (CaSE , self ). __init__ () self . cin = cin self . standardize = standardize self . out_mul = out_mul hidden = max ( min_units , cin // reduction ) self . gamma_generator = nn. Sequential ( nn. Linear (cin , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , hidden , bias =True , ** factory_kwargs ), nn. SiLU () , nn. Linear ( hidden , cin , bias =True , ** factory_kwargs ), nn. Sigmoid () ) self . reset_parameters () def reset_parameters ( self ): nn. init . zeros_ ( self . gamma_generator [4]. weight ) nn. init . zeros_ ( self . gamma_generator [4]. bias ) self . gamma = torch . tensor ([1.0]) def forward (self , x): if( self . training ): # adaptive mode self . gamma = torch . mean (x, dim =[2,3]) # spatial pooling self . gamma = torch . mean ( self . gamma , dim =[0])# context pooling if( self . standardize ): self . gamma = ( self . gamma - torch . mean ( self . gamma )) / \\ torch . sqrt ( torch . var ( self . gamma , unbiased = False ) + 1e-5) self . gamma = self . gamma . unsqueeze (0) self . gamma = self . gamma_generator ( self . gamma ) * self . out_mul self . gamma = self . gamma . reshape ([1,-1,1,1]) return self . gamma * x else : # inference mode self . gamma = self . gamma .to(x. device ) return self . gamma * x def extra_repr ( self ): return ’cin ={}’. format ( self . cin ) 15B UppereCaSE: additional details B.1 Algorithm of UpperCaSE Algorithm 1 UpperCaSE: training function for the few-shot classiﬁcation setting. Require: D= {τ1,...,τ D}training dataset Require: bφ() pretrained feature extractor (body) with CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α, β: step-size hyperparameters for the optimizer. 1: Set φto random values ⊿optional: set φto enforce identity in CaSE output 2: while not done do 3: Sample task τ = (C,T) ∼D 4: Forward pass over context set bφ(Cx) → z1,..., zN ⊿CaSE in adaptive mode 5: Store context embeddings and associated labels M= {(zn,yn)}N n=1 ⊿temporary memory buffer 6: Deﬁne a linear model for the head hψτ () and set ψτ to zero 7: for total inner-steps do ⊿loop to estimate head params 8: Sample (with replacement) mini-batch of training pairs B∼M 9: Update the head parameters ψτ ←step(α,L,B,hψτ ) 10: end for 11: Update the CaSE parameters φ←step(β,L,C,T,bφ,hψτ ) ⊿CaSE in adaptive mode 12: end while Algorithm 2 UpperCaSE: test function for the few-shot classiﬁcation setting. Require: τ∗ = (C∗,x∗) unseen test task with target input x∗ an context C∗. Require: bφ() pretrained feature extractor (body) with meta-learned CaSE blocks parameterized by φ. Require: step(): gradient-step function; Lloss; α: step-size hyperparameter for the optimizer. 1: Forward pass over context set bφ(Cx ∗ ) → z1,..., zN ⊿CaSE in adaptive mode 2: Store context embeddings and associated labels M∗ = {(zn,yn)}N n=1 ⊿temporary memory buffer 3: Deﬁne a linear model for the head hψτ∗ () and set ψτ∗ to zero 4: for total inner-steps do ⊿loop to estimate head params 5: Sample (with replacement) mini-batch of training pairs B∗ ∼M∗ 6: Update the head parameters ψτ∗ ←step(α,L,B∗,hψτ∗ ) 7: end for 8: Return Prediction ˆy∗ = hψτ∗ (bφ(x∗)) ⊿CaSE in inference mode C Additional experimental details and results C.1 Additional details MACs counting MACs are proportional to the size of the task, size of the images, and number of classes. We can count MACs using synthetic tasks. In our case we used a synthetic task of 100-way, 10-shot with input images of size 224 ×224 ×3 generated via Gaussian noise (µ= 0,σ = 1), and labels generated as random integers. We used a mini-batch of size 128 and 500 update steps for UpperCaSE and BiT with an EfﬁcientNetB0 backbone for the ﬁrst and a ResNet50-S for the second. For MD-Transfer we used the same parameters reported in Dumoulin et al. (2021) with images of size 126 ×126 ×3 and ResNet18 backbone. For the ORBIT experiments we counted MACs by using the code in the original repository 2 and reporting the average MACs over all test tasks for both CLE-VE and CLU-VE using a ResNet18 backbone. VTAB+MD trainingWe follow the protocol reported in the original papers (Triantaﬁllou et al., 2019; Dumoulin et al., 2021) training UpperCaSE for 10K tasks on the training datasets and evaluating on the MD test set and on the VTAB datasets. At evaluation time we sample 1200 tasks from the MD test set, and report the mean and conﬁdence intervals. On VTAB we report the results of a single run on the test data (data points are given in advance and do not change across seeds). In all experiments we used the MetaDataset-v2 (MDv2) which does not include ImageNet in the test set. We used a pretrained EfﬁcientNetB0 from the ofﬁcial Torchvision repository 3, and a pretrained ResNet50-S 2https://github.com/microsoft/ORBIT-Dataset 3https://pytorch.org/vision 16from the BiT repository 4. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), for ResNet50-S we use the BiT normalization values (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]). ORBIT training For the ORBIT experiments we trained UpperCaSE on MDv2 using a pretrained ResNet18 taken from the ofﬁcial Torchvision repository. We normalized the inputs using the values reported in the Torchvision documentation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]). For the evaluation phase we followed the instructions reported in Massiceti et al. (2021). C.2 CaSE vs SE Table 4: Comparing CaSE against standard Squeeze-and-Excitation (SE) on VTAB+MD using different adaptation heads. MD: Mahalanobis distance head (Bronskill et al., 2021). Linear: linear head trained with UpperCaSE. All adaptive blocks use a reduction of 32. Best results in bold. Model SE CaSE SE CaSE Contextual pooling No Yes No Yes Adaptation head MD MD Linear Linear Image size 84 84 224 224 MetaDataset (all) 67.8 69.6 74.6 76.2 VTAB (all) 43.6 45.3 56.6 58.2 VTAB (natural) 47.5 50.2 65.3 68.1 VTAB (specialized) 63.6 64.9 79.8 79.6 VTAB (structured) 30.6 31.8 38.6 40.1 C.3 CaSE vs other adapters Table 5: Comparing CaSE adaptive blocks (with reduction 64, 32, 16) on VTAB+MD against the FiLM generators used in Bronskill et al. (2021), and a baseline with no body adaptation. CaSE blocks are more efﬁcient in terms of adaptive and amortization parameters while providing higher classiﬁcation accuracy. All models have been trained and tested on 84 ×84 images, using a Mahalanobis distance head. Best results in bold. Adaptation type None FiLM CaSE64 CaSE32 CaSE16 Adaptive Params (M) n/a 0.02 0.01 0.01 0.01 Amortiz. Params (M) n/a 1.7 0.4 0.8 1.6 MetaDataset (all) 53.4 68.4 69.8 69.6 70.4 VTAB (all) 43.5 44.7 46.2 45.3 46.4 VTAB (natural) 45.4 49.5 52.1 50.2 52.6 VTAB (specialized) 69.4 63.8 66.3 64.9 65.5 VTAB (structured) 29.1 31.7 31.8 31.8 32.1 C.4 Ablation studies In this section we provide additional experimental results focusing on ablation studies of the CaSE adaptive block. The results can be summarized as follows: • Ablation of the activation function for the output layer for both CaSE and SE. We have tested three activation funcitons: linear, sigmoid, sigmoid with multiplier. The sigmoid with multiplier uses a constant value set to 2 to center the sigmoid at 1 (this enforces the identity function). The empirical results reported in Table 6 show that the sigmoid with multiplier and the linear layer provide the best results. 4https://github.com/google-research/big_transfer 17• Ablation of the number of hidden units in the hidden layers of CaSE. The number of hidden units is controlled by the reduction and min-units parameters in the code and it depends on the number of inputs. See the paper for more details. The results reported in Table 8 show that blocks with more units provide marginal gains or no gains at all. This is probably due to overﬁtting issues affecting the models with more units. • Ablation of the number of hidden layers of CaSE. The results reported in Table 7 show that the best performance is obtained with 1 and 2 layers. The performance worsen when there are 3 or more layers which is likely due to overﬁtting issues affecting the models with more parameters. • Ablation of the activation function for the hidden layers. Results reported in Table 9 show that CaSE is quite robust against this factor when activations like ReLU and SiLU are used but the performance worsen with Tanh. We have chosen SiLU for the experiments as this is the same activation typically used in Squeeze-and-Excitation layers (e.g. in EfﬁcientNet backbones). Table 6: Performance on VTAB+MD for various activation functions used in the last layer of SE and CaSE. Sigmoid-2 indicates that the output of a standard Sigmoid is multiplied by 2. Both SE and CaSE use a reduction factor of 32 with min-clipping of 32. All model have been trained using an EfﬁcientNetB0 backbone with a linear head on images of size 224 ×224. Results for SE with linear activation have not been reported because the training was unstable (loss rapidly diverging at the ﬁrst iterations). Best results in bold. Adaptive block SE SE CaSE CaSE CaSE Activation (output) Sigmoid Sigmoid-2 Linear Sigmoid Sigmoid-2 MetaDataset (all) 74.2 74.6 75.8 74.9 76.2 VTAB (all) 56.8 56.6 58.4 56.8 58.2 VTAB (natural) 67.0 65.3 68.3 67.1 68.1 VTAB (specialized) 81.1 79.8 79.5 80.8 79.6 VTAB (structured) 36.9 38.6 40.3 37.1 40.1 Table 7: Comparing CaSE adaptive blocks with different number of hidden layers on VTAB+MD. All models have been trained and tested on 224 ×224 images, using CaSE with reduction 64 and clip factor (min-units) 16, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. # Hidden layers 1 2 3 4 Amortiz. Params (M) 0.420 0.426 0.432 0.438 MetaDataset (all) 76.0 76.1 75.5 75.2 VTAB (all) 58.2 58.4 58.2 58.0 VTAB (natural) 68.3 69.1 68.0 67.4 VTAB (specialized) 79.7 80.3 80.5 80.3 VTAB (structured) 40.0 39.4 39.7 39.7 18Table 8: Comparing CaSE adaptive blocks with different number of hidden units on VTAB+MD. The number of hidden units depends on the input size and is deﬁned by the reduction and the clip factor (min-units). All models have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Reduction factor 64 32 16 8 Clip factor 16 32 48 64 Amortiz. Params (M) 0.4 0.8 1.6 3.0 MetaDataset (all) 76.1 76.2 75.8 76.2 VTAB (all) 58.4 58.2 57.9 58.5 VTAB (natural) 69.1 68.1 67.9 68.3 VTAB (specialized) 80.3 79.6 79.4 79.0 VTAB (structured) 39.4 40.1 39.7 40.9 Table 9: Comparing CaSE adaptive blocks with different activation functions for the hidden layers on VTAB+MD. All models are based on a reduction factor of 64 and a clip factor of 16 (0.4M amortiza- tion parameters) and they have been trained and tested on 224 ×224 images, using UpperCaSE and EfﬁcientNetB0 backbone. Best results in bold. Activation (hidden) SiLU ReLU Tanh MetaDataset (all) 76.1 75.8 74.8 VTAB (all) 58.4 57.8 48.2 VTAB (natural) 69.1 69.8 67.0 VTAB (specialized) 80.3 79.7 80.8 VTAB (structured) 39.4 39.4 36.4 19C.5 Role of CaSE blocks Figure 4: Boxplots for all the MDv2 test datasets (100 tasks per dataset) reporting the CaSE activation (vertical axis) at different stages of an EfﬁcientNetB0 (horizontal axis, with early stages on the left). The box encloses ﬁrst to third quartile, with the median represented by the orange line. The whiskers extend from the box by 1.5 the inter-quartile range. Outlier (point past the end of the whiskers) are represented with black circles. 20Figure 5: CaSE activation values (vertical axis) for all channels (horizontal axis) at different stages (top plots are early stages) in EfﬁcientNetB0 for the MDv2 test dataset (one task per dataset). Values are similar and closer to one in the ﬁrst stages but diverge in the latest. The magnitude tends to increase with depth. 21C.6 UpperCaSE: results on VTAB+MD In this section we provide a full breakdown of the results for UpperCaSE vs. other methods on the VTAB+MD benchmark. Results for other methods are taken from Bronskill et al. (2021) and Dumoulin et al. (2021). UpperCaSE uses CaSE with reduction 64 (min-clip 16) for EfﬁcientNetB0 and reduction 32 (min-clip 32) for ResNet50-S. Results for UpperCaSE on MD are the average over 1200 test tasks. In Table 10 we report the results for UpperCaSE against ﬁne-tuning methods (BiT, MD-Trasnfer, SUR) and in Table 11 the results for UpperCaSE against meta-learning and hybrid methods (ProtoNet, ProtoMAML, Cross Transformer CTX, LITE). Overall UpperCaSE performs well on MD and the natural split of VTAB, this may be due to the fact that transfer learning is more beneﬁcial on those datasets as they are more similar to those used during meta-training. The largest difference in performance between UpperCaSE and ﬁne-tuning methods is on the structured split of VTAB, which includes tasks that require counting and pose estimation. This is likely due to the difference w.r.t. the meta-training set. In this case, ﬁne-tuning the entire network is more effective than body adaptation as the knowledge gap is wider and it requires more adjustments to the parameters. Table 10: Comparing UpperCaSE against ﬁne-tuning methods. Best result in bold. Model BiT MD-Transfer SUR UpperCaSE UpperCaSE Image Size 224 126 224 224 224 Network RN50-S RN18 RN50 ×7 ENB0 RN50-S Params (M) 23.5 11.2 164.5 4.0 23.5 Omniglot 68.0 ±4.5 82.0 ±1.3 92.8±0.5 90.7±0.4 89.1 ±0.5 Aircraft 77.4 ±3.5 76.8 ±1.2 84.4 ±0.6 89.4±0.4 87.5±0.4 Birds 90.8±1.5 61.2±1.3 75.8 ±1.0 90.4±0.4 89.6 ±0.4 DTD 85.0±2.5 66.0±1.1 74.3 ±0.7 83.4±0.4 84.8 ±0.5 QuickDraw 66.6 ±3.7 61.3 ±1.1 70.3 ±0.7 76.8±0.5 73.7±0.6 Fungi 59.4 ±4.2 35.5 ±1.1 81.7±0.6 59.3±0.8 56.8 ±0.8 Trafﬁc Sign 73.5 ±4.7 84.7±0.9 50.0±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 65.7±2.7 39.6±1.0 49.4 ±1.1 50.8 ±0.7 46.7 ±0.8 Caltech101 87.2 70.6 82.3 88.3 86.2 CIFAR100 54.4 31.3 33.7 52.7 47.0 Flowers102 83.3 66.1 55.7 85.3 83.0 Pets 87.9 49.1 76.3 89.9 89.3 Sun397 33.3 13.9 27.5 35.8 32.5 SVHN 70.4 83.2 18.7 62.7 59.8 EuroSAT 94.4 88.7 78.9 92.2 91.6 Resics45 76.1 63.7 62.4 75.5 74.4 Patch Camelyon 83.1 81.5 75.6 79.3 80.9 Retinopathy 70.2 57.6 27.9 74.3 73.7 CLEVR-count 74.0 40.3 30.0 40.3 42.0 CLEVR-dist 51.5 52.9 37.1 38.9 37.3 dSprites-loc 82.7 85.9 30.0 45.3 38.1 dSprites-ori 55.1 46.4 19.8 42.5 41.4 SmallNORB-azi 17.8 36.5 12.9 15.7 15.1 SmallNORB-elev 32.1 31.2 18.1 22.7 21.0 DMLab 43.2 37.9 33.3 38.7 36.1 KITTI-dist 79.9 58.7 52.3 71.0 69.6 MetaDataset (all) 73.3 63.4 71.0 76.1 74.9 VTAB (all) 65.4 55.6 42.9 58.4 56.6 VTAB (natural) 69.4 52.4 49.0 69.1 66.3 VTAB (specialized) 81.0 72.9 61.2 80.3 80.1 VTAB (structured) 54.5 49.4 29.2 39.4 37.6 22Table 11: Comparing UpperCaSE against meta-learning and hybrid methods. Best result in bold. Model ProtoNet ProtoMAML CTX LITE UpperCaSE UpperCaSE Image Size 224 126 224 224 224 224 Network ENB0 RN18 RN34 ENB0 ENB0 RN50-S Params (M) 4.0 11.2 21.3 4.0 4.0 23.5 Omniglot 88.3 ±0.8 90.2±0.7 84.6±0.9 86.5 ±0.8 90.7±0.4 89.1±0.5 Aircraft 85.0 ±0.7 82.1 ±0.6 85.3 ±0.8 83.6 ±0.7 89.4±0.4 87.5±0.4 Birds 90.2±0.5 73.4±0.9 72.9 ±1.1 88.6 ±0.7 90.4±0.4 89.6±0.4 DTD 81.4 ±0.6 66.3 ±0.8 77.3 ±0.7 84.1±0.7 83.4±0.4 84.8±0.5 QuickDraw 76.0±0.7 66.4±1.0 73.3 ±0.8 75.7±0.8 59.3±0.8 56.8 ±0.8 Fungi 57.4 ±1.1 46.3 ±1.1 48.0 ±1.2 56.9 ±1.2 59.3±0.8 56.8±0.8 Trafﬁc Sign 53.5 ±1.1 50.3 ±1.1 80.1±1.0 65.8±1.1 68.5 ±0.8 70.6 ±0.8 MSCOCO 49.8±1.1 39.0±1.0 51.4±1.1 50.0 ±1.0 50.8 ±0.7 46.7±0.8 Caltech101 87.4 73.1 84.2 87.7 88.3 86.2 CIFAR100 43.1 29.7 37.5 48.8 52.7 47.0 Flowers102 78.2 60.2 81.8 83.5 85.3 83.0 Pets 88.6 56.6 70.9 89.3 89.9 89.3 Sun397 32.9 8.1 24.8 30.9 35.8 32.5 SVHN 35.2 46.8 67.2 51.0 62.7 59.8 EuroSAT 83.3 80.1 86.4 89.3 92.2 91.6 Resics45 68.8 53.5 67.7 76.4 75.5 74.4 Patch Camelyon 73.3 75.9 79.8 81.4 79.3 80.9 Retinopathy 31.3 73.2 35.5 40.3 74.3 73.7 CLEVR-count 27.2 32.7 27.9 31.4 40.3 42.0 CLEVR-dist 28.5 35.4 29.6 32.8 38.9 37.3 dSprites-loc 13.4 42.0 23.2 12.3 45.3 38.1 dSprites-ori 19.6 23.0 46.9 31.1 42.5 41.4 SmallNORB-azi 9.4 13.4 37.0 14.5 15.7 15.1 SmallNORB-elev 17.0 18.8 21.6 21.0 22.7 21.0 DMLab 35.8 32.5 31.9 39.4 38.7 36.1 KITTI-dist 56.5 54.4 54.3 63.9 71.0 69.6 MetaDataset (all) 72.7 64.2 71.6 73.9 76.1 74.9 VTAB (all) 46.1 45.0 50.5 51.4 58.4 56.6 VTAB (natural) 60.9 45.7 61.1 65.2 69.1 66.3 VTAB (specialized) 64.2 70.7 67.3 71.9 80.3 80.1 VTAB (structured) 25.9 31.5 34.1 30.8 39.4 37.6 23",
      "references": [
        "How to train your maml",
        "Improved few-shot visual classification",
        "Discriminative k-shot learning using probabilistic models",
        "The netflix prize",
        "Meta-learning with differentiable closed-form solvers",
        "Memory efficient meta-learning with large images",
        "Meta-learning feature representations for adaptive gaussian processes via implicit differentiation",
        "A closer look at few-shot classification",
        "Modular meta-learning with shrinkage",
        "Comparing transfer and meta learning approaches on a unified few-shot classification benchmark",
        "A learned representation for artistic style",
        "Model-agnostic meta-learning for fast adaptation of deep networks",
        "Neural processes",
        "Meta-learning probabilistic inference for prediction",
        "Spottune: transfer learning through adaptive fine-tuning",
        "Dynamic neural networks: A survey",
        "Meta-learning in neural networks: A survey",
        "Squeeze-and-excitation networks",
        "Arbitrary style transfer in real-time with adaptive instance normalization",
        "Big transfer (bit): General visual representation learning",
        "Meta-learning with differentiable convex optimization",
        "Cross-domain few-shot learning with task-specific adapters",
        "Orbit: A real-world few-shot dataset for teachable object recognition",
        "On first-order meta-learning algorithms",
        "Few-shot learning with class imbalance",
        "How sensitive are meta-learners to dataset imbalance?",
        "Tadam: Task dependent adaptive metric for improved few-shot learning",
        "Bayesian meta-learning for the few-shot setting via deep kernels",
        "Film: Visual reasoning with a general conditioning layer",
        "Rapid learning or feature reuse? towards understanding the effectiveness of maml",
        "Meta-learning with implicit gradients",
        "Learning multiple visual domains with residual adapters",
        "Efficient parametrization of multi-domain deep neural networks",
        "Fast and flexible multi-task classification using conditional neural adaptive processes",
        "Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook",
        "Non-gaussian gaussian processes for few-shot regression",
        "Prototypical networks for few-shot learning",
        "Meta-transfer learning for few-shot learning",
        "Meta-dataset: A dataset of datasets for learning to learn from few examples",
        "Convolutional networks with adaptive inference graphs",
        "Matching networks for one shot learning",
        "Coordinate descent algorithms",
        "Blockdrop: Dynamic inference paths in residual networks",
        "The microsoft 2017 conversational speech recognition system",
        "Fast context adaptation via meta-learning",
        "Visual Task Adaptation Benchmark",
        "Big Transfer",
        "LITE",
        "MD-Transfer",
        "MetaOptNet",
        "R2D2",
        "ANIL",
        "Simple CNAPs",
        "TADAM",
        "CAVIA",
        "ProtoNets",
        "FiLM",
        "Model Agnostic Meta-Learning",
        "MAML++",
        "ProtoMAML",
        "Reptile",
        "Meta-Transfer Learning"
      ],
      "meta_data": {
        "arxiv_id": "2206.09843v3",
        "authors": [
          "Massimiliano Patacchiola",
          "John Bronskill",
          "Aliaksandra Shysheya",
          "Katja Hofmann",
          "Sebastian Nowozin",
          "Richard E. Turner"
        ],
        "published_date": "2022-06-20T15:25:08Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Contextual Squeeze-and-Excitation (CaSE), an adapter that meta-learns task-specific, per-layer channel-scales via context pooling, enabling single-forward-pass adaptation of a pretrained backbone. Combines CaSE with lightweight fine-tuning of a linear head in a hybrid approach called UpperCaSE, trained by a coordinate-descent meta-learning scheme. UpperCaSE pushes the accuracy/ adaptation-cost Pareto frontier: state-of-the-art on the 26 VTAB+MetaDataset tasks among meta-learners, narrows the gap with leading fine-tuners while requiring orders-of-magnitude fewer multiply–accumulate operations, and achieves top performance on the ORBIT personalization benchmark.",
        "methodology": "1) CaSE block: extends standard Squeeze-and-Excitation by (a) pooling activations across the task’s context set rather than per-image, (b) producing only a scale (no shift) vector, and (c) meta-training the associated MLPs episodically. 2) Integration: insert CaSE blocks at selected layers of a frozen ImageNet-pretrained network (EfficientNetB0 or ResNet50-S). 3) UpperCaSE learning: for each task, a) forward context once to compute CaSE scales and embeddings, b) optimize a task-specific linear classifier on those embeddings via gradient descent, c) update shared CaSE parameters with one outer-loop step; this coordinate-descent procedure avoids back-propagating through head updates and keeps memory low.",
        "experimental_setup": "Meta-training: 10k episodic tasks drawn from MetaDataset-v2 training split, images down-sampled to 84×84 or 224×224, Adam with decayed LR. Evaluation: • MetaDataset-v2 test (8 datasets, 1200 tasks each). • VTAB few-shot benchmark (18 datasets, natural/specialized/structured splits). • ORBIT personalization benchmark (train on MDv2, test on 85 user tasks). Baselines include fine-tuning methods (BiT, MD-Transfer, SUR), meta-learners (ProtoNet, CTX, LITE) and hybrids (ProtoMAML). Metrics: mean episode accuracy, 95% CI; adaptation cost measured as MACs on synthetic 100-way 10-shot tasks; ORBIT also reports frame accuracy, video accuracy, frames-to-recognition.",
        "limitations": "1) Still relies on iterative gradient updates for the head; speed depends on on-device automatic differentiation availability. 2) Body remains frozen; performance lags on tasks with large domain shift or requiring spatial reasoning (structured VTAB datasets). 3) Benefits demonstrated on ImageNet-pretrained CNNs; efficacy for other modalities or self-supervised pretraining untested. 4) Only channel-wise scaling is adapted; may limit expressiveness compared to full scale-and-shift or spatial adapters.",
        "future_research_directions": "• Incorporate spatial or shift modulation into CaSE while maintaining parameter efficiency. • Automatically learn where and how many CaSE blocks to insert to further cut memory. • Combine with self-supervised or multimodal pre-training to broaden applicability. • Develop acceleration schemes or closed-form solvers for the linear head to remove per-task optimization. • Explore domain-adaptive meta-training to close the gap on structured and localization tasks. • Apply the CaSE concept to other data types (audio, NLP) and continual-learning scenarios.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Efficient Generalized Spherical CNNs",
      "full_text": "Published as a conference paper at ICLR 2021 EFFICIENT GENERALIZED SPHERICAL CNN S Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. Mavor-Parker, Augustin Marignier, Matthew A. Price, Mayeul d’Avezac & Jason D. McEwen˚ Kagenova Limited, Guildford GU5 9LD, UK ABSTRACT Many problems across computer vision and the natural sciences require the anal- ysis of spherical data, for which representations may be learned efﬁciently by en- coding equivariance to rotational symmetries. We present a generalized spherical CNN framework that encompasses various existing approaches and allows them to be leveraged alongside each other. The only existing non-linear spherical CNN layer that is strictly equivariant has complexity OpC2L5q, where C is a measure of representational capacity and Lthe spherical harmonic bandlimit. Such a high computational cost often prohibits the use of strictly equivariant spherical CNNs. We develop two new strictly equivariant layers with reduced complexityOpCL4q and OpCL3 log Lq, making larger, more expressive models computationally fea- sible. Moreover, we adopt efﬁcient sampling theory to achieve further computa- tional savings. We show that these developments allow the construction of more expressive hybrid models that achieve state-of-the-art accuracy and parameter ef- ﬁciency on spherical benchmark problems. 1 I NTRODUCTION Many ﬁelds involve data that live inherently on spherical manifolds, e.g. 360˝photo and video con- tent in virtual reality and computer vision, the cosmic microwave background radiation from the Big Bang in cosmology, topographic and gravitational maps in planetary sciences, and molecular shape orientations in molecular chemistry, to name just a few. Convolutional neural networks (CNNs) have been tremendously effective for data deﬁned on Euclidean domains, such as the 1D line, 2D plane, or nD volumes, thanks in part to their translation invariance properties. However, these techniques are not effective for data deﬁned on spherical manifolds, which have a very different geometric structure to Euclidean spaces (see Appendix A). To transfer the remarkable success of deep learning to data deﬁned on spherical domains, deep learning techniques deﬁned inherently on the sphere are required. Recently, a number of spherical CNN constructions have been proposed. Existing CNN constructions on the sphere fall broadly into three categories: fully real (i.e. pixel) space approaches (e.g. Boomsma & Frellsen, 2017; Jiang et al., 2019; Perraudin et al., 2019; Co- hen et al., 2019); combined real and harmonic space approaches (Cohen et al., 2018; Esteves et al., 2018; 2020); and fully harmonic space approaches (Kondor et al., 2018). Real space approaches can often be computed efﬁciently but they necessarily provide an approximate representation of spherical signals and the connection to the underlying continuous symmetries of the sphere is lost. Consequently, such approaches cannot fully capture rotational equivariance. Other constructions take a combined real and harmonic space approach (Cohen et al., 2018; Esteves et al., 2018; 2020), where sampling theorems (Driscoll & Healy, 1994; Kostelec & Rockmore, 2008) are exploited to connect with underlying continuous signal representations to capture the continuous symmetries of the sphere. However, in these approaches non-linear activation functions are computed pointwise in real space, which induces aliasing errors that break strict rotational equivariance. Fully harmonic space spherical CNNs have been constructed by Kondor et al. (2018). A continual connection with underlying continuous signal representations is captured by using harmonic signal representations throughout. Consequently, this is the only approach exhibiting strict rotational equivariance. How- ever, strict equivariance comes at great computational cost, which can often prohibit usage. ˚Corresponding author: jason.mcewen@kagenova.com 1 arXiv:2010.11661v3  [cs.CV]  8 Mar 2021Published as a conference paper at ICLR 2021 In this article we present a generalized framework for CNNs on the sphere (and rotation group), which encompasses and builds on the inﬂuential approaches of Cohen et al. (2018), Esteves et al. (2018) and Kondor et al. (2018) and allows them to be leveraged alongside each other. We adopt a harmonic signal representation in order to retain the connection with underlying continuous repre- sentations and thus capture all symmetries and geometric properties of the sphere. We construct new fully harmonic (non-linear) spherical layers that are strictly rotationally equivariant, are parameter- efﬁcient, and dramatically reduce computational cost compared to similar approaches. This is achieved by a channel-wise structure, constrained generalized convolutions, and an optimized de- gree mixing set determined by a minimum spanning tree. Furthermore, we adopt efﬁcient sampling theorems on the sphere (McEwen & Wiaux, 2011) and rotation group (McEwen et al., 2015a) to im- prove efﬁciency compared to the sampling theorems used in existing approaches (Driscoll & Healy, 1994; Kostelec & Rockmore, 2008). We demonstrate state-of-the-art performance on all spherical benchmark problems considered, both in terms of accuracy and parameter efﬁciency. 2 G ENERALIZED SPHERICAL CNN S We ﬁrst overview the theoretical underpinnings of the spherical CNN frameworks introduced by Cohen et al. (2018), Esteves et al. (2018), and Kondor et al. (2018), which make a connection to underlying continuous signals through harmonic representations. For more in-depth treatments of the underlying harmonic analysis we recommend Esteves (2020), Kennedy & Sadeghi (2013) and Gallier & Quaintance (2019). We then present a generalized spherical layer in which these and other existing frameworks are encompassed, allowing existing frameworks to be easily integrated and leveraged alongside each other in hybrid networks. Throughout the following we consider a network composed of S rotationally equivariant layers Ap1q,...., ApSq, where the i-th layer Apiq maps an input activation fpi´1q P Hpi´1q onto an out- put activation fpiq PHpiq. We focus on the case where the network input space Hp0q consists of spherical signals (but note that input signals on the rotation group may also be considered). 2.1 S IGNALS ON THE SPHERE AND ROTATION GROUP Let L2pΩqdenote the space of square-integrable functions over domain Ω. A signal f P L2pΩq on the sphere ( Ω “S2) or rotation group ( Ω “SOp3q) can be rotated by ρ PSOp3qby deﬁning the action of rotation on signals by Rρfpωq “fpρ´1ωqfor ω PΩ. An operator A : L2pΩ1q Ñ L2pΩ2q, where Ω1,Ω2 P tS2,SOp3qu, is then equivariant to rotations if RρpApfqq “ApRρfq for all f P L2pΩ1qand ρ P SOp3q, i.e. rotating the function before application of the operator is equivalent to application of the operator ﬁrst, followed by a rotation. A spherical signal f PL2pS2qadmits a harmonic representation pˆf0, ˆf1,..., qwhere ˆfℓ PC2ℓ`1 are the harmonic coefﬁcients given by the inner product xf,Y ℓ my, where Yℓ m are the spherical harmonic functions of degree ℓand order |m|ď ℓ. Likewise a signal f PL2pSOp3qqon the rotation group ad- mits a harmonic representationpˆf0, ˆf1,...qwhere ˆfℓ PCp2ℓ`1qˆp2ℓ`1qare the harmonic coefﬁcients with pm,nq-th entry xf,Dℓ mnyfor integers |m|,|n| ďℓ, where Dℓ : SOp3qÑ Cp2ℓ`1qˆp2ℓ`1q is the unique 2ℓ`1 dimensional irreducible group representation of SOp3qon Cp2ℓ`1q. The rotation f ÞÑRρf of a signal f PL2pΩqcan be described in harmonic space by ˆfℓ ÞÑDℓpρqˆfℓ. A signal on the sphere or rotation group is said to be bandlimited at Lif, respectively, xf,Y ℓ my“ 0 or xf,Dℓ mny“ 0 for ℓ ěL. Furthermore, a signal on the rotation group is said to be azimuthally bandlimited at N if, additionally, xf,Dℓ mny “0 for |n| ěN. Bandlimited signals therefore ad- mit ﬁnite harmonic representations pˆf0,..., ˆfL´1q. In practice real-world signals can be accurately represented by suitably bandlimited signals; henceforth, we assume signals are bandlimited. 2.2 C ONVOLUTION ON THE SPHERE AND ROTATION GROUP A standard deﬁnition of convolution between two signals f,ψ PL2pΩqon either the sphere ( Ω “ S2) or rotation group (Ω “SOp3q) is given by pf ‹ψqpρq“x f,Rρψy“ ż Ω dµpωqfpωqψ˚pρ´1ωq, (1) 2Published as a conference paper at ICLR 2021 where dµpωqdenotes the Haar measure on Ω and ¨˚ complex conjugation (e.g. Wandelt & G ´orski, 2001; McEwen et al., 2007; 2013; 2015b; 2018; Cohen et al., 2018; Esteves et al., 2018). In partic- ular, the convolution satisﬁes ppRρfq‹ ψqpρ1q“x Rρf,Rρ1ψy“x f,Rρ´1ρ1ψy“p Rρpf ‹ψqqpρ1q (2) and is therefore a rotationally equivariant linear operation, which we shall denote by Lpψq. The convolution of bandlimited signals can be computed exactly and efﬁciently in harmonic space as {pf ‹ψqℓ “ ˆfℓˆψℓ˚, ℓ “0,...,L ´1, (3) which for each degree ℓis a vector outer product for signals on the sphere and a matrix product for signals on the rotation group (see Appendix B for further details). Convolving in this manner results in signals on the rotation group (for inputs on both the sphere and rotation group). However, in the spherical case, if the ﬁlter is invariant to azimuthal rotations the resultant convolved signal may be interpreted as a signal on the sphere (see Appendix B). 2.3 G ENERALIZED SIGNAL REPRESENTATIONS The harmonic representations and convolutions described above have proven useful for describing rotationally equivariant linear operators Lpψq. Cohen et al. (2018) and Esteves et al. (2018) deﬁne spherical CNNs that sequentially apply this operator, with intermediary representations taking the form of signals on SOp3qand S2 respectively. Alternatively, for intermediary representations we now consider the more general space of signals introduced by Kondor et al. (2018), to which the aforementioned notions of rotation and convolution naturally extend. In describing the generalization we ﬁrst note from Section 2.1 that all bandlimited signals on the sphere and rotation group can be represented as a set of variable length vectors of the form f “t ˆfℓ t PC2ℓ`1 : ℓ“0,..,L ´1; t“1,...,τ ℓ fu, (4) where τℓ f “1 for signals on the sphere and τℓ f “minp2ℓ`1,2N ´1qfor signals on the rotation group. The generalization is to let FL be the space of all such sets of variable length vectors, with τf unrestricted. This more general space contains the spaces of bandlimited signals on the sphere and rotation group as strict subspaces. For a generalized signalf PFL we adopt the terminology of Kondor et al. (2018) by referring to ˆfℓ t as the t-th fragment of degree ℓand to τf “pτ0 f,...,τ L´1 f q, specifying the number of fragments for each degree, as the type of f. The action of rotations upon FL can be naturally extended from their action upon L2pS2qand L2pSOp3qq. For f P FL we deﬁne the rotation operator f ÞÑRρf by ˆfℓ t ÞÑDℓpρqˆfℓ t, allowing us to extend the usual notion of equivariance to operators A : FL ÑFL. 2.4 G ENERALIZED CONVOLUTIONS The convolution described by Equation 1 provides a learnable linear operator Lpψqthat satisﬁes the desired property of equivariance. Nevertheless, given the generalized interpretation of signals onS2 and SOp3qas signals in FL, the notion of convolution can also be generalized (Kondor et al., 2018). In order to linearly and equivariantly transform a signalf PFL of type τf into a new signal f˚ψP FL of any desired type τpf˚ψq, we may specify a ﬁlter ψ “t ˆψℓ PCτℓ fˆτℓ pf˚ψq : ℓ“0,...,L ´1u, which in general is not an element of FL, and deﬁne a transformation f ÞÑf ˚ψby {pf ˚ψq ℓ t “ τℓ fÿ t1“1 ˆfℓ t1 ˆψℓ t,t1, ℓ “0,...,L ´1; t“1,...,τ ℓ pf˚ψq. (5) The degree-ℓ fragments of the transformed signal pf ˚ψqare simply linear combinations of the degree-ℓfragments of f, with no mixing between degrees. Equation 3 shows that this is precisely the form taken by convolution on the sphere and rotation group. In fact Kondor & Trivedi (2018) show that all equivariant linear operations take this general form; the standard notion of convolution is just a special case. One beneﬁt to the generalized notion is that the ﬁlter ψis not forced to occupy the same domain as the signal f, thus allowing control over the type τpf˚ψq of the transformed signal. We use Lpψq G to denote this generalized convolutional operator. 3Published as a conference paper at ICLR 2021 2.5 N ON-LINEAR ACTIVATION OPERATORS For FL to be a useful representational space, it must be possible to not only linearly but also non- linearly transform its elements in an equivariant manner. However, equivariance and non-linearity is not enough. Equivariant linear operators cannot mix information corresponding to different degrees. Therefore it is of crucial importance that degree mixing is achieved by the non-linear operator. 2.5.1 P OINTWISE ACTIVATIONS When the type τf of f PFL permits an interpretation as a signal on S2 or SOp3qwe may perform an inverse harmonic transform to map the function onto a sample-based representation (e.g. Driscoll & Healy, 1994; McEwen & Wiaux, 2011; Kostelec & Rockmore, 2008; McEwen et al., 2015a). A non-linear function σ : C Ñ C may then be applied pointwise, i.e. separately to each sample, before performing a harmonic transform to return to a representation in FL. We denote the corre- sponding non-linear operator as Nσpfq “FpσpF´1pfqqq, where F represents the harmonic (i.e. Fourier) transform on S2 or SOp3q. The computational cost of the non-linear operator is dominated by the harmonic transforms. While costly, fast algorithms can be leveraged (see Appendix A). While inverse and forward harmonic transforms onS2 or SOp3qthat are based on a sampling theory main- tain perfect equivariance for bandlimited signals, the pointwise application of σ (most commonly ReLU) is only equivariant in the continuous limit L Ñ 8. For any ﬁnite bandlimit L, aliasing effects are introduced such that equivariance becomes approximate only, as shown by the following experiments. We consider 100 random rotations ρ P SOp3q, for each of 100 random signal-ﬁlter pairs pf,ψq, and compute the mean equivariance error dpApRρfq,RρpAfqqfor operator A, where dpf,gq “ }f ´g}{}f}is the relative distance between signals. For convolutions the equivariance error is 4.4 ˆ10´7 for signals on S2 and 5.3 ˆ10´7 for signals on SOp3q(achieving ﬂoating point pre- cision). By comparison the equivariance error for a pointwise ReLU is 0.34 for signals on S2 and 0.37 for signals on SOp3q. Only approximate equivariance is achieved for the ReLU since the non- linear operation spreads information to higher degrees that are not captured at the original bandlimit, resulting in aliasing. To demonstrate this point we reduce aliasing error by oversampling the real- space signal. When oversampling by 2ˆor 8ˆfor signals on SOp3qthe equivariance error of the ReLU is reduced to 0.10 and 0.01, respectively. See Appendix D for further experimental details. Despite the high cost of repeated harmonic transforms and imperfect equivariance, this is never- theless the approach adopted by Cohen et al. (2018), Esteves et al. (2018) and others, who ﬁnd empirically that such models maintain a reasonable degree of equivariance. 2.5.2 T ENSOR -PRODUCT ACTIVATIONS In order to deﬁne a strictly equivariant non-linear operation that can be applied to a signalf PFLof any type τf the decomposability of tensor products between group representations may be leveraged, as ﬁrst considered by Thomas et al. (2018) in the context of neural networks. Given two group representations Dℓ1 and Dℓ2 of SOp3qon C2ℓ1`1 and C2ℓ2`1 respectively, the tensor-product group representation Dℓ1 bDℓ2 of SOp3qon C2ℓ1`1 bC2ℓ2`1 is deﬁned such that pDℓ1 bDℓ2 qpρq“ Dℓ1 pρqb Dℓ2 pρqfor all ρPSOp3q. Decomposing Dℓ1 bDℓ2 into a direct sum of irreducible group representations then constitutes ﬁnding a change of basis for C2ℓ1`1 bC2ℓ2`1 such that pDℓ1 bDℓ2 qpρqis block diagonal, where for each ℓthere is a block equal to Dℓpρq. The necessary change of basis for ˆuℓ1 bˆvℓ2 PC2ℓ1`1 bC2ℓ2`1 is given by pˆuℓ1 bˆvℓ2 qℓ m “ ℓ1ÿ m1“´ℓ1 ℓ2ÿ m2“´ℓ2 Cℓ1,ℓ2,ℓ m1,m2,mˆuℓ1 m1 ˆvℓ2 m2 , (6) where Cℓ1,ℓ2,ℓ m1,m2,m PC denote Clebsch-Gordan coefﬁcients whose symmetry properties are such that pˆuℓ1 bˆvℓ2 qℓ m is non-zero only for |ℓ1 ´ℓ2|ď ℓďℓ1 `ℓ2. The use of Equation 6 arises naturally in quantum mechanics when coupling angular momenta. This property is useful since if ˆfℓ1 PC2ℓ1`1 and ˆfℓ2 PC2ℓ2`1 are two fragments that are equivariant with respect to rotations of the network input, then a rotation ofρapplied to the network input results 4Published as a conference paper at ICLR 2021 in ˆfℓ1 b ˆfℓ2 transforming as rˆfℓ1 b ˆfℓ2 sℓ ÞÑrpDℓ1 pρqˆfℓ1 qbp Dℓ2 pρqˆfℓ2 qsℓ (7) “rpDℓ1 bDℓ2 qpρqpˆfℓ1 b ˆfℓ2 qsℓ (8) “Dℓpρqrˆfℓ1 b ˆfℓ2 sℓ, (9) where the ﬁnal equality follows by block diagonality with respect to the chosen basis. Therefore, if fragments ˆfℓ1 and ˆfℓ2 are equivariant with repsect to rotations of the network input, then so is the fragment pCℓ1,ℓ2,ℓqJpˆfℓ1 b ˆfℓ2 qP C2ℓ`1, where we have written Equation 6 more compactly. We now describe how Kondor et al. (2018) use this fact to deﬁne equivariant non-linear transformations of elements in FL. A signal f “ tˆfℓ t P C2ℓ`1 : ℓ “ 0,..,L ´1; t “ 1,...,τ ℓ fu PFL may be equivariantly and non-linearly transformed by an operator Nb : FL ÑFL deﬁned as Nbpfq“tp Cℓ1,ℓ2,ℓqJpˆfℓ1 t1 bˆfℓ2 t2 q: ℓ“0,...,L ´1; pℓ1,ℓ2qP Pℓ L; t1 “0,...,τ ℓ1 f ; t2 “0,...,τ ℓ2 f u, (10) where for each degree ℓPt0,...,L ´1uthe set Pℓ L “tpℓ1,ℓ2qPt 0,...,L ´1u2 : |ℓ1 ´ℓ2|ď ℓďℓ1 `ℓ2u (11) is deﬁned in order to avoid the computation of trivially equivariant all-zero fragments. We make the dependence on Pℓ L explicit since we redeﬁne it in Section 3. Unlike the pointwise activations discussed in the previous section this operator is strictly equivariant, with a mean relative equivariance error of 5.0 ˆ10´7 (see Appendix D). Note that applying this operator to signals on the sphere or rotation group results in generalized signals that are no longer on the sphere or rotation group. This is the rationale for the generalization to FL: to unlock the ability to introduce non-linearity in a strictly equivariant manner. Note, however, that g “Nbpfq has type τg “pτ0 g,...,τ L´1 g qwhere τℓ g “ř pℓ1,ℓ2qPPℓ L τℓ1 f τℓ2 f and therefore application of this non- linear operator results in a drastic expansion in representation size, which is problematic. 2.6 G ENERALIZED SPHERICAL CNN S Equipped with operators to both linearly and non-linearly transform elements of FL, with the latter also performing degree mixing, we may consider a network with representation spaces Hp0q “ ... “HpSq “FL. We consider the s-th layer of the network to take the form of a triple Apsq “ pL1,N,L2qsuch that Apsqpfps´1qq“ L2pNpL1pfps´1qqqq, where L1,L2 : FL ÑFL are linear operators and N : FL ÑFL is a non-linear activation operator. The approaches of Cohen et al. (2018) and Esteves et al. (2018) are encompassed in this framework as Apsq “ pLpψq,Nσ,Iq, where I denotes the identity operator and ﬁlters ψ may be deﬁned to encode real-space properties such as localization (see Appendix C). The framework of Kondor et al. (2018) is also encompassed as Apsq “pI,Nb,Lpψq G q. Here the generalized convolution Lpψq G comes last to counteract the representation-expanding effect of the tensor-product activation and prevent it from compounding as signals pass through the network. Appendix E lends intuition regarding rela- tionships that may be captured by tensor-product activations followed by generalized convolutions. For any intermediary representation fpiq PFL we may transition from equivariance with respect to the network input to invariance by discarding all but the scalar-valued fragments corresponding to ℓ“0 (equivalent to average pooling for signals on the sphere and rotation group). Finally, note that within this general framework we are free to consider hybrid approaches whereby layers proposed by Cohen et al. (2018); Esteves et al. (2018); Kondor et al. (2018) and others, and those presented subsequently, can be leveraged alongside each other within a single model. 3 E FFICIENT GENERALIZED SPHERICAL CNN S Existing approaches to spherical convolutional layers that are encompassed within the above frame- work are computationally demanding. They require the evaluation of costly harmonic transforms 5Published as a conference paper at ICLR 2021 on the sphere and rotation group. Furthermore, the only strictly rotationally equivariant non-linear layer is that of Kondor et al. (2018), which has an even greater computational cost, scaling with the ﬁfth power of bandlimit — thereby limiting spatial resolution — and quadratically with the number of fragments per degree — thereby limiting representational capacity. This often prohibits the use of strictly equivariant spherical networks. In this section we introduce a channel-wise structure, constrained generalized convolutions, and an optimized degree mixing set in order to construct new strictly equivariant layers that exhibit much improved scaling properties and parameter efﬁciency. Furthermore, we adopt efﬁcient sampling theory on the sphere and rotation group to achieve additional computational savings. 3.1 E FFICIENT GENERALIZED SPHERICAL LAYERS For an activation f P FL the value ¯τf “ 1 L řL´1 ℓ“1 τℓ f represents a resolution-independent proxy for its representational capacity. Kondor et al. (2018) consider the separate fragments contained within f to subsume the traditional role of separate channels and therefore control the capacity of intermediary network representations through speciﬁcation of τf. This is problematic because, whereas activation functions usually act on each channel separately and therefore have a cost that scales linearly with representational capacity (usually controlled by the number of channels), for the activation function Nb not only does the cost scale quadratically with representational capacity ¯τf, but so too does the size of Nbpfq. This feeds forward the quadratic dependence to the cost of, and number of parameters required by, the proceeding generalized convolution. More speciﬁcally, note that computation of g “Nbpfqrequires the computation of řL´1 ℓ“0 τℓ g frag- ments, where τℓ g “ř pℓ1,ℓ2qPPℓ L τℓ1 f τℓ2 f . The size of Pℓ L is OpLℓqfor each ℓ and therefore the ex- panded representation has size řL´1 ℓ“0 τℓ g, of order Op¯τ2 fL3q. By exploiting the sparsity of Clebsch- Gordan coefﬁcients (Cℓ1,ℓ2,ℓ m1,m2,m “0 if m1 `m2 ‰m) each fragment pCℓ1,ℓ2,ℓqJpˆfℓ1 b ˆfℓ2 qcan be computed in Opℓminpℓ1,ℓ2qq. Hence, the total cost of computing all necessary fragments has complexity OpC2L5q, where C “¯τf captures representational capacity. 3.1.1 C HANNEL -WISE TENSOR -PRODUCT ACTIVATIONS As is more standard for CNNs we maintain a separate channels axis, with network activations taking the form pf1,...,f KqP FK L where fi PFL all share the same type τf. The non-linearity Nb may then be applied to each channel separately at a cost that is reduced byK-times relative to its applica- tion on a single channel with the same total number of fragments. This saving arises since for each ℓwe need only compute Kř pℓ1,ℓ2qPPℓ L τℓ1 f τℓ2 f fragments rather than ř pℓ1,ℓ2qPPℓ L pKτℓ1 f qpKτℓ2 f q. Figure 1 visualizes this reduction for the case K “3. Note, however, that for practical applications K „ 100 is more typical. The K-times reduction in cost is therefore substantial and allows for intermediary activations with orders of magnitude more representational capacity. By introducing this multi-channel approach and using C “ K rather than C “ ¯τf to control representational capacity, we reduce the complexity ofNbwith respect to representational capacity from OpC2qto OpCq. N⊗ ℓ=0 ℓ=1 ℓ=2 (a) Prior approach to applying a tensor-product based non-linear operator N⊗ ℓ=0, 1, 2 (b) Ours Figure 1: Comparison (to scale) of the expansion caused by the tensor-product activation applied to inputs of equal representational capacity but different structure. With depth representing the number of channels and width the number of fragments for each degree, it is clear that by grouping fragments into K separate channels the expansion (and therefore cost) can be K-times reduced. Visualization corresponds to inputs with pL,Kqequal to p3,1qand p3,3qfor panel (a) and (b), respectively. 6Published as a conference paper at ICLR 2021 3.1.2 C ONSTRAINED GENERALIZED CONVOLUTION Although much reduced, for a signal f PFKin L the channel-wise application of Nb still results in a drastically expanded representation g “Nbpfq, to which a representation-contracting generalized convolution must be applied in order to project onto a new activation g1 “ Lpψq G pgq PFKout L of the desired type τg1 and number of channels Kout. However, under our multi-channel structure computational and parameter efﬁciency can be improved signiﬁcantly by decomposing Lpψq G into three separate linear operators, Lpψ1q G1 , Lpψ2q G2 and Lpψ3q G3 . The ﬁrst, Lpψ1q G1 , acts uniformly across channels, performing a linear projection down onto the desired type, and should be interpreted as a learned extension of Nb which undoes the drastic expansion. The second, Lpψ2q G2 , then acts channel-wise, taking linear combinations of the (con- tracted number of) fragments within each channel. The third, Lpψ3q G3 , acts across channels, tak- ing linear combinations to learn new features. More concretely, the three ﬁlters are of the form ψ1 “t ˆψℓ 1 PCτℓ gˆτℓ g1 : ℓ“0,...,L ´1u, ψ2 “t ˆψℓ,k 2 PCτℓ g1ˆτℓ g1 : ℓ“0,...,L ´1; k “1,...,K inu and ψ3 “ tˆψℓ 3 P CKinˆKout : ℓ “ 0,...,L ´ 1u, rather than a single ﬁlter of the form ψ“t ˆψℓ PCKinˆτℓ gˆKoutˆτℓ g1 : ℓ“0,...,L ´1u, leading to a large reduction in the number of pa- rameters as τℓ g is invariably very large. By applying the ﬁrst step uniformly across channels we minimize the parametric dependence on the expanded representation and allow new features to be subsequently learned much more efﬁciently. Together the second and third steps can be seen as analogous to depthwise separable convolutions often used in planar convolutional networks. 3.1.3 O PTIMIZED DEGREE MIXING SETS We now consider approaches to reduce theOpL5qcomplexity with respect to spatial resolutionL. In the deﬁnition of Nbeach element of Pℓ L independently deﬁnes an equivariant fragment. Therefore a restricted Nbin which only a subset ofPℓ Lis used for each degreeℓstill deﬁnes a strictly equivariant operator, while reducing computational complexity. In order to make savings whilst remaining at resolution Lit is necessary to consider subsets of Pℓ L that scale better than OpL2q. The challenge is to ﬁnd such subsets that do not hamper the ability of the resulting operator to inject non-linearity and mix information corresponding to different degrees ℓ. Whilst various subsetting approaches are possible, the following argument motivates an approach that we have found to be particularly effective. If pℓ1,ℓ3q PPℓ L, then representational space is designated to capture the relationship between ℓ1 and ℓ3-degree information. However, if resources have been designated already to capture the relationship between ℓ1 and ℓ2-degrees, as well as between ℓ2 and ℓ3-degrees, then some notion of the relationship betweenℓ1 and ℓ3-degrees has been captured already. Consequently, it is unnecessary to designate further resources for this purpose. More generally, consider the graphGℓ L “pNL,Pℓ Lqwith nodes NL “t0,...,L ´1uand edges Pℓ L. A restricted tensor-product activation can be constructed by using a subset ofPℓ L that corresponds to a subgraph of Gℓ L. The subgraph of Gℓ L captures some notion of the relationship between incomingℓ1 and ℓ2-degree information if it contains a path between nodesℓ1 and ℓ2. Therefore we are interested in subgraphs for which there exists a path between any two nodes if there exists such a path in the original graph, guaranteeing that any degree-mixing relationship captured by the original graph is also captured by the subgraph. The smallest subgraph satisfying this property is a minimum spanning tree (MST) of Gℓ L. The set of edges corresponding to any MST has at most L elements and we choose to consider its union with the set of loop-edges in Gℓ L (of the form pℓ1,ℓ1q), which proved particularly important for injecting non-linearity. We denote the resulting set as ¯Pℓ L and note that it satisﬁes |¯Pℓ L| ď2L. Therefore the tensor-product activation ¯Nb corresponding to Equation 11 with Pℓ L replaced by ¯Pℓ L has reduced spatial complexity OpL4q. Given that many minimal spanning trees of the unweighted graph Gℓ Lexist for eachℓ, we select the ones that minimize the cost of the resulting activation¯Nbby assigning to each edge pℓ1,ℓ2qin Gℓ L a weight equal to the cost of computingpCℓ1,ℓ2,ℓqJpˆfℓ1 bˆfℓ2 q and selecting the MST of the weighted graph. 7Published as a conference paper at ICLR 2021 0 1 2 3 4 5 6 1 0 1 2 3 4 5 62 (a) Full Pℓ L set of size OpL2q 0 1 2 3 4 5 6 1 0 1 2 3 4 5 62  (b) MST subset of size OpLq 0 1 2 3 4 5 6 1 0 1 2 3 4 5 62  (c) RMST subset of size Oplog Lq Figure 2: Visualization of the mixing set Pℓ L (for L“7 and ℓ“4) and the approaches to subsetting based on the minimum spanning tree (MST) and reduced minimum spanning tree (RMST) mixing polices, which reduces related computation costs from OpL2qto, respectively, OpLqor Oplog Lq. An example of Pℓ L and a MST subset ¯Pℓ L is shown in Figure 2, where the dashed line in Figure 2c shows the general form of the MST. Using this as a principled starting point we consider the further reduced MST (RMST) subset˜Pℓ Lcorresponding to centering the MST at the edgepℓ,ℓqand retaining only the edges that fall a distance of2i away on the dotted line for someiPN. We use ˜Nbto denote the corresponding operator and note that it has further reduced spatial complexity of OpL3 log Lq. We demonstrate in Section 4 that networks that make use of the MST tensor-product activation achieve state-of-the-art performance. Replacing the MST with RMST activation results in a small but insigniﬁcant degradation in performance, which is offset by the reduced computational cost. 3.1.4 R EDUCTION IN COMPUTATIONAL AND MEMORY FOOTPRINTS The three modiﬁcations proposed in Sections 3.1.1 to 3.1.3 are motivated by improved scaling prop- erties. Importantly, they also translate to large reductions in the computational and memory cost of strictly equivariant layers in practice, as detailed in Appendix F. Even at a modest bandlimit of L “ 64 and relatively small number of channels K “ 4, for example, the modiﬁcations together lead to a 51-times reduction in the number of ﬂops required for computations and 16-times reduction in the amount of memory required to store representations, weights and gradients for training. 3.2 E FFICIENT SAMPLING THEORY By adopting sampling theorems on the sphere we provide access to underlying continuous signal representations that fully capture the symmetries and geometric properties of the sphere, and allow standard convolutions to be computed exactly and efﬁciently through their harmonic representations, as discussed in greater detail in Appendices A and B. We adopt the efﬁcient sampling theorems on sphere and rotation group of McEwen & Wiaux (2011) and McEwen et al. (2015a), respectively, which reduce the Nyquist rate by a factor of two compared to those of Driscoll & Healy (1994) and Kostelec & Rockmore (2008), which have been adopted in other spherical CNN constructions (e.g. Cohen et al., 2018; Kondor et al., 2018; Esteves et al., 2018; 2020). The sampling theorems adopted are equipped with fast algorithms to compute harmonic transforms, with complexityOpL3q for the sphere andOpL4qfor the rotation group. When imposing an azimuthal bandlimitN !L, the complexity of transforms on the rotation group can be reduced to OpNL3q, which we often exploit in our standard (non-generalized) convolutional layers. 4 E XPERIMENTS Using our efﬁcient generalized spherical CNN framework (implemented in the fourpiAI 1 code) we construct networks that we apply to numerous spherical benchmark problems. We achieve state- of-the-art performance, demonstrating enhanced equivariance without compromising representa- tional capacity or parameter efﬁciency. In all experiments we use a similar architecture, consisting of 2–3 standard convolutional layers (e.g.S2 or SOp3qconvolutions proceeded by ReLUs), followed by 2–3 of our efﬁcient generalized layers. We adopt the efﬁcient sampling theory described in Sec- tion 3.2 and encode localization of spatial ﬁlters as discussed in Appendix C. Full experimental details may be found in Appendix G. 1https://www.kagenova.com/products/fourpiAI/ 8Published as a conference paper at ICLR 2021 Table 1: Test accuracy for spherical MNIST digits clas- siﬁcation problem NR/NR R/R NR/R Params Planar CNN 99.32 90.74 11.36 58k Cohen et al. (2018) 95.59 94.62 93.40 58k Kondor et al. (2018) 96.40 96.60 96.00 286k Esteves et al. (2020) 99.37 99.37 99.08 58k Ours (MST) 99.35 99.38 99.34 58k Ours (RMST) 99.29 99.17 99.18 57k Table 2: Test root mean squared (RMS) error for QM7 regression problem RMS Params Montavon et al. (2012) 5.96 - Cohen et al. (2018) 8.47 1.4M Kondor et al. (2018) 7.97 ą1.1M Ours (MST) 3.16 337k Ours (RMST) 3.46 335k Table 3: SHREC’17 object retrieval competition metrics (perturbed micro-all) P@N R@N F1@N mAP NDCG Params Kondor et al. (2018) 0.707 0.722 0.701 0.683 0.756 ą1M Cohen et al. (2018) 0.701 0.711 0.699 0.676 0.756 1.4M Esteves et al. (2018) 0.717 0.737 - 0.685 - 500k Ours 0.719 0.710 0.708 0.679 0.758 250k 4.1 R OTATED MNIST ON THE SPHERE We consider the now standard benchmark problem of classifying MNIST digits projected onto the sphere. Three experimental modes NR/NR, R/R and NR/R are considered, indicating whether the training/test sets have been randomly rotated (R) or not (NR). Results are presented in Table 1, which shows that we closely match the prior state-of-the-art performance obtained by Esteves et al. (2020) on the NR/NR and R/R modes, whilst outperforming all previous spherical CNNs on the NR/R mode, demonstrating the increased degree of equivariance achieved by our model. Results are shown for models using both the MST-based and RMST-based mixing sets within the tensor-product activation. The results obtained when using the full sets Pℓ L are very similar to those obtained when using the MST-based sets (e.g. full sets achieved an accuracy of99.39 for R/R). 4.2 A TOMIZATION ENERGY PREDICTION We consider the problem of regressing the atomization energy of molecules given the molecule’s Coulomb matrix and the positions of the atoms in space, using the QM7 dataset (Blum & Rey- mond, 2009; Rupp et al., 2012). Results are presented in Table 2, which shows that we dramatically outperform other approaches, whilst using signiﬁcantly fewer parameters. 4.3 3D S HAPE RETRIEVAL We consider the 3D shape retrieval problem on the SHREC’17 (Savva et al., 2017) competition dataset, containing 51k 3D object meshes. We follow the pre-processing step of Cohen et al. (2018), where several spherical projections of each mesh are computed, and use the ofﬁcial SHREC’17 data splits. Results are presented in Table 3 for the standard SHREC precision and recall metrics, which shows that we achieve state-of-the-art performance compared to other spherical CNN approaches, achieving the highest three of ﬁve performance metrics, whilst using signiﬁcantly fewer parameters. 5 C ONCLUSIONS We have presented a generalized framework for CNNs on the sphere that encompasses various ex- isting approaches. We developed new efﬁcient layers to be used as primary building blocks in this framework by introducing a channel-wise structure, constrained generalized convolutions, and opti- mized degree mixing sets determined by minimum spanning trees. These new efﬁcient layers exhibit strict rotational equivariance, without compromising on representational capacity or parameter efﬁ- ciency. When combined with the ﬂexibility of the generalized framework to leverage the strengths of alternative layers, powerful hybrid models can be constructed. On all spherical benchmark prob- lems considered we achieve state-of-the-art performance, both in terms of accuracy and parameter efﬁciency. In future work we intend to improve the scalability of our generalized framework further still. In particular, we plan to introduce additional highly scalable layers, for example by extending scattering transforms (Mallat, 2012) to the sphere, to further realize the potential of deep learning on a host of new applications where spherical data are prevalent. 9Published as a conference paper at ICLR 2021 REFERENCES Lorenz Blum and Jean-Louis Reymond. 970 million druglike small molecules for virtual screening in the chemical universe database GDB-13.Journal of the American Chemical Society, 131:8732, 2009. Wouter Boomsma and Jes Frellsen. Spherical convolutions and their application in molecular mod- elling. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 3433–3443. Cur- ran Associates, Inc., 2017. Taco Cohen, Mario Geiger, Jonas K ¨ohler, and Max Welling. Spherical CNNs. In International Conference on Learning Representations, 2018. URL https://arxiv.org/abs/1801.10130. Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convo- lutional networks and the icosahedral CNN. arXiv preprint arXiv:1902.04615, 2019. URL https://arxiv.org/abs/1902.04615. James Driscoll and Dennis Healy. Computing Fourier transforms and convolutions on the sphere. Advances in Applied Mathematics, 15:202–250, 1994. Carlos Esteves. Theoretical aspects of group equivariant neural networks. arXiv preprint arXiv:2004.05154, 2020. URL https://arxiv.org/abs/2004.05154. Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning SO(3) equivariant representations with spherical CNNs. In Proceedings of the European Con- ference on Computer Vision (ECCV), pp. 52–68, 2018. URL https://arxiv.org/abs/1711. 06721. Carlos Esteves, Ameesh Makadia, and Kostas Daniilidis. Spin-weighted spherical CNNs. arXiv preprint arXiv:2006.10731, 2020. URL https://arxiv.org/abs/2006.10731. Jean Gallier and Jocelyn Quaintance. Aspects of Harmonic Analysis and Representation Theory. 2019. URL https://www.seas.upenn.edu/„jean/nc-harmonic.pdf. Dennis Healy, Daniel Rockmore, Peter Kostelec, and S. Moore. FFTs for the 2-sphere – improve- ments and variations. Journal of Fourier Analysis and Applications, 9(4):341–385, 2003. Chiyu Jiang, Jingwei Huang, Karthik Kashinath, Philip Marcus, Matthias Niessner, et al. Spherical CNNs on unstructured grids. arXiv preprint arXiv:1901.02039, 2019. URL https://arxiv. org/abs/1901.02039. Rodney A Kennedy and Parastoo Sadeghi. Hilbert space methods in signal processing. Cambridge University Press, 2013. Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic gradient descent. In ICLR: International Conference on Learning Representations, 2015. URL https://arxiv.org/abs/ 1412.6980. Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International Conference on Machine Learning, pp. 2747–2755, 2018. URL https://arxiv.org/abs/1802.03690. Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-Gordan nets: a fully fourier space spherical convolutional neural network. InAdvances in Neural Information Processing Systems, pp. 10117– 10126, 2018. URL https://arxiv.org/abs/1806.09231. Peter Kostelec and Daniel Rockmore. FFTs on the rotation group. Journal of Fourier Analysis and Applications, 14:145–179, 2008. St´ephane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):1331–1398, 2012. URL https://arxiv.org/abs/1101.2286. Domenico Marinucci and Giovanni Peccati. Random Fields on the Sphere: Representation, Limit Theorem and Cosmological Applications. Cambridge University Press, 2011. 10Published as a conference paper at ICLR 2021 Jason McEwen and Yves Wiaux. A novel sampling theorem on the sphere. IEEE Transactions on Signal Processing, 59(12):5876–5887, 2011. URL https://arxiv.org/abs/1110.6298. Jason McEwen, Michael P. Hobson, Daniel J. Mortlock, and Anthony N. Lasenby. Fast directional continuous spherical wavelet transform algorithms.IEEE Trans. Sig. Proc., 55(2):520–529, 2007. URL https://arxiv.org/abs/astro-ph/0506308. Jason McEwen, Pierre Vandergheynst, and Yves Wiaux. On the computation of directional scale- discretized wavelet transforms on the sphere. In Wavelets and Sparsity XV , SPIE international symposium on optics and photonics, invited contribution, volume 8858, 2013. URL https: //arxiv.org/abs/1308.5706. Jason McEwen, Martin B ¨uttner, Boris Leistedt, Hiranya V Peiris, and Yves Wiaux. A novel sam- pling theorem on the rotation group. IEEE Signal Processing Letters, 22(12):2425–2429, 2015a. URL https://arxiv.org/abs/1508.03101. Jason McEwen, Boris Leistedt, Martin B ¨uttner, Hiranya Peiris, and Yves Wiaux. Directional spin wavelets on the sphere. IEEE Trans. Sig. Proc., submitted, 2015b. URL https://arxiv.org/ abs/1509.06749. Jason McEwen, Claudio Durastanti, and Yves Wiaux. Localisation of directional scale-discretised wavelets on the sphere.Applied Comput. Harm. Anal., 44(1):59–88, 2018. URLhttps://arxiv. org/abs/1509.06767. Gr´egoire Montavon, Katja Hansen, Siamac Fazli, Matthias Rupp, Franziska Biegler, Andreas Ziehe, Alexandre Tkatchenko, Anatole V . Lilienfeld, and Klaus-Robert M¨uller. Learning invariant repre- sentations of molecules for atomization energy prediction. In F. Pereira, C. J. C. Burges, L. Bot- tou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 440–448. Curran Associates, Inc., 2012. Nathana¨el Perraudin, Micha ¨el Defferrard, Tomasz Kacprzak, and Raphael Sgier. Deepsphere: Ef- ﬁcient spherical convolutional neural network with HEALPix sampling for cosmological appli- cations. Astronomy and Computing, 27:130–146, 2019. URL https://arxiv.org/abs/1810. 12186. Matthias Rupp, Alexandre Tkatchenko, Klaus-Robert M ¨uller, and O. Anatole von Lilienfeld. Fast and accurate modeling of molecular atomization energies with machine learning.Physical Review Letters, 108:058301, 2012. URL https://arxiv.org/abs/1109.2618. Manolis Savva, Fisher Yu, Hao Su, Asako Kanezaki, Takahiko Furuya, Ryutarou Ohbuchi, Zhichao Zhou, Rui Yu, Song Bai, Xiang Bai, et al. Large-scale 3d shape retrieval from shapenet core55: Shrec’17 track. InProceedings of the Workshop on 3D Object Retrieval, pp. 39–50. Eurographics Association, 2017. Max Tegmark. An Icosahedron-Based method for pixelizing the celestial sphere.Astrophys. J. Lett., 470:L81, October 1996. URL https://arxiv.org/abs/astro-ph/9610094. Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor ﬁeld networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018. URL https://arxiv.org/abs/1802.08219. Stefano Trapani and Jorge Navaza. Calculation of spherical harmonics and Wigner d functions by FFT. Applications to fast rotational matching in molecular replacement and implementation into AMoRe. Acta Crystallographica Section A, 62(4):262–269, 2006. Benjamin Wandelt and Krzysztof G ´orski. Fast convolution on the sphere. Phys. Rev. D., 63(12): 123002, 2001. URL https://arxiv.org/abs/astro-ph/0008227. 11Published as a conference paper at ICLR 2021 A R EPRESENTATIONS OF SIGNALS ON THE SPHERE AND ROTATION GROUP To provide further context for the discussion presented in the introduction and to elucidate the prop- erties of different sampling theory on the sphere and rotation group, we concisely review represen- tations of signals on the sphere and rotation group. A.1 D ISCRETIZATION It is well-known that a completely regular point distribution on the sphere does in general not exist (e.g. Tegmark, 1996). Consequently, while a variety of spherical discretization schemes exists (e.g. icosahedron, HEALPix, graph, and other representations), it is not possible to discretize (i.e. to sample or pixelize) the sphere in a manner that is invariant to rotations, i.e. a discrete sampling of rotations of the samples on the sphere will in general not map onto the same set of sample positions. This differs to the Euclidean setting and has important implications when constructing convolution operators on the sphere, which clearly are a critical component of CNNs. Since convolution operators are in general built using a translation operator – equivalently a rotation operator when on the sphere – it is thus not possible to construct a convolution operator directly on a discretized representation of the sphere that captures all of the symmetries of the underlying spher- ical manifold. While approximate discrete representations can be considered, and are nevertheless useful, such representations cannot capture all underlying spherical symmetries. A.2 S AMPLING THEORY Alternative representations, however, can capture all underlying spherical symmetries. Sampling theories on the sphere (e.g. Driscoll & Healy, 1994; McEwen & Wiaux, 2011) provide a mechanism to capture all information content of an underlying continuous function on the sphere from a ﬁnite set of samples (and similarly on the rotation group; Kostelec & Rockmore 2008; McEwen et al. 2015a). A sampling theory on the sphere is equivalent to a cubature (i.e. quadrature) rule for the exact integration of a bandlimited functions on the sphere. While optimal cubature on the sphere remains an open problem, the most efﬁcient sampling theory on the sphere and rotation group is that developed by McEwen & Wiaux (2011) and McEwen et al. (2015a), respectively. On a compact manifold like the sphere (and rotation group), harmonic (i.e. Fourier) space is discrete. Hence, a ﬁnite set of harmonic coefﬁcients captures all information content of an underlying contin- uous bandlimited signal. Since such a representation provides access to the underlying continuous signal, all symmetries and geometric properties of the sphere are captured perfectly. Such repre- sentations have been employed extensively in the construction of wavelet transforms on the sphere, where the use of sampling theorems on the sphere and rotation group yield wavelet transforms of discretized continuous signals that are theoretically exact (e.g. McEwen et al., 2013; 2015b; 2018). Harmonic signal representations have also been exploited in spherical CNNs to access all underlying spherical symmetries and develop equivariance network layers (Cohen et al., 2018; Kondor et al., 2018; Esteves et al., 2018; 2020). A.3 E XACT AND EFFICIENT COMPUTATION Signals on the sphere f PL2pS2qmay be decomposed into their harmonic representations as fpωq“ 8ÿ ℓ“0 ℓÿ m“´ℓ fℓ mYℓ mpωq, (12) where their spherical harmonic coefﬁcients are given by fℓ m “xf,Y ℓ my“ ż S2 dµpωqfpωqYℓ˚ m pωq, (13) for ω PS2. Similarly, signals on the rotation group g PL2pSOp3qqmay be decomposed into their harmonic representations as gpρq“ 8ÿ ℓ“0 2ℓ`1 8π2 ℓÿ m“´ℓ ℓÿ n“´ℓ gℓ mnDℓ˚ mnpρq (14) 12Published as a conference paper at ICLR 2021 where their harmonic (Wigner) coefﬁcients are given by gℓ mn “xg,Dℓ˚ mny“ ż SOp3q dµpρqgpρqDℓ mnpρq, (15) for ρ PSOp3q. Note that we adopt the convention where the conjugate of the Wigner D-function is used in Equation 14 since this leads to a convenient harmonic representation when considering convolutions (cf. McEwen et al., 2015a; 2018). As mentioned above, sampling theory pertains to strategies to capture all of the information content of band limited signals from a ﬁnite set of samples. Since the harmonic space of the sphere and rota- tion group is discrete, this is equivalent to an exact quadrature rule for the computation of harmonic coefﬁcients by Equation 13 and Equation 15 from sampled signals. The canonical equiangular sampling theory on the sphere was that developed by Driscoll & Healy (1994), and subsequently extended to the rotation group by Kostelec & Rockmore (2008). More recently, novel sampling theorems on the sphere and rotation group were developed by McEwen & Wiaux (2011) and McEwen et al. (2015a), respectively, that reduce the Nyquist rate by a factor of two. Previous CNN constructions on the sphere (e.g. Cohen et al., 2018; Kondor et al., 2018; Esteves et al., 2018; 2020) have adopted the more well-known sampling theories of Driscoll & Healy (1994) and Kostelec & Rockmore (2008). In contrast, we adopt the more efﬁcient sampling theories of McEwen & Wiaux (2011) and McEwen et al. (2015a) to provide additional efﬁciency savings, implemented in the open source ssht 2 and so3 3 software packages (we also make use of a TensorFlow implementation of these algorithms in our private tensossht 4 code). Note also that the sampling schemes associated with the theory of McEwen & Wiaux (2011) (and other minor variants implemented in ssht ) align more closely with the one-to-two aspect ratio of common spherical data, such as 360˝photos and videos. All of the sampling theories discussed are equipped with fast algorithms to compute harmonic trans- forms, with complexity OpL3qfor transforms on the sphere (Driscoll & Healy, 1994; McEwen & Wiaux, 2011) and complexity OpL4qfor transforms on the rotation group (Kostelec & Rockmore, 2008; McEwen et al., 2015a). Note that algorithms that achieve slightly lower complexity have been developed (Driscoll & Healy, 1994; Healy et al., 2003; Kostelec & Rockmore, 2008) but these are known to suffer stability issues (Healy et al., 2003; Kostelec & Rockmore, 2008). By imposing an azimuthally bandlimit N, where typically N !L, the complexity of transforms on the rotation group can be reduced to OpNL3q(McEwen et al., 2015a), which we exploit in our networks. These fast algorithms to compute harmonic transforms on the sphere and rotation group can be lever- aged to yield the exact and efﬁcient computation of convolutions through their harmonic representa- tions (see Appendix B). By computing convolutions in harmonic space, pixelization and quadrature errors are avoided and computational complexity is reduced to the cost of the respective harmonic transforms. B C ONVOLUTION ON THE SPHERE AND ROTATION GROUP For completeness we make explicit the standard (non-generalized) convolution operations on the sphere and rotation group that we adopt. The general form of convolution for signals f P L2pΩq either on the sphere (Ω “S2) or rotation group (Ω “SOp3q) is speciﬁed by Equation 1, with har- monic representation given by Equation 3. Here we provide speciﬁc expressions for the convolution for a variety of cases, describe the normalization constants that arise and may be absorbed into learn- able ﬁlters, and derive the corresponding harmonic forms. In practice all convolutions are computed in harmonic space since the computation is then exact, avoiding pixelisation or quadrature errors, and efﬁcient when fast algorithms to compute harmonic transforms are exploited (see Appendix A). 2http://www.spinsht.org/ 3http://www.sothree.org/ 4Available on request from https://www.kagenova.com/. 13Published as a conference paper at ICLR 2021 B.1 C ONVOLUTION ON THE SPHERE Given two spherical signals f,ψ P L2pS2qtheir convolution, which in general is a signal on the rotation group, may be decomposed as pf ‹ψqpρq“x f,Rρψy (16) “ ż S2 dΩpωqfpωqψ˚pρ´1ωq (17) “ ÿ ℓm ÿ ℓ1m1 ÿ n fℓ mDℓ1˚ m1npρqψℓ1˚ n ż S2 dΩpωqYℓ mpωqYℓ1˚ m1 pωq (18) “ ÿ ℓm ÿ ℓ1m1 ÿ n fℓ mDℓ1˚ m1npρqψℓ1˚ n δℓℓ1δmm1 (19) “ ÿ ℓmn ` fℓ mψℓ˚ n ˘ Dℓ˚ mnpρq, (20) yielding harmonic coefﬁcients pf ˚ψqℓ mn “ 8π2 2ℓ`1fℓ mψℓ˚ n . (21) The constants 8π2{p2ℓ`1qmay be absorbed into learnable parameters. B.2 C ONVOLUTION ON THE SPHERE WITH AXISYMMETRIC FILTERS When convolving a spherical signal f PL2pS2qwith an axisymmetric spherical ﬁlter ψ PL2pS2q that is invariant to azimuthal rotations, the resultant pf ‹ψqmay be interpreted as a signal on the sphere. To see this note that an axisymmetric ﬁlter ψ has harmonic coefﬁcients ψℓ n “ψℓ 0δn0 that are non-zero only for m “ 0. Denoting rotations by their zyz-Euler angles ρ “ pα,β,γ qand substituting into Equation 20 we see that the convolution may be decomposed as pf ‹ψqpα,β,γ q“ ÿ ℓmn ` fℓ mψℓ˚ 0 δn0 ˘ Dℓ˚ mnpα,β,γ q (22) “ ÿ ℓm fℓ mψℓ˚ 0 Dℓ˚ m0pα,β, 0q (23) “ ÿ ℓm fℓ mψℓ˚ 0 c 4π 2ℓ`1Yℓ mpβ,αq. (24) We may therefore interpret pf ‹ψqas a signal on the sphere with spherical harmonic coefﬁcients pf ‹ψqℓ m “ c 4π 2ℓ`1fℓ mψℓ˚ 0 . (25) The constants a 4π{p2ℓ`1qmay be absorbed into learnable parameters. 14Published as a conference paper at ICLR 2021 B.3 C ONVOLUTION ON THE ROTATION GROUP Given two signals f,ψ PL2pSOp3qqon the rotation group their convolution may then be decom- posed as pf ‹ψqpρq“x f,Rρψy (26) “ ż SOp3q dµpρ1qfpρ1qψ˚pρ´1ρ1q (27) “ ż SOp3q dµpρ1q „ÿ ℓ 2ℓ`1 8π2 ÿ mn fℓ mnDℓ˚ mnpρ1q „ÿ ℓ1 2ℓ1`1 8π2 ÿ m1n1 ψℓ1˚ m1n1Dℓ1 m1n1pρ´1ρq  (28) “ ÿ ℓ 2ℓ`1 8π2 ÿ mn fℓ mn ÿ ℓ1 2ℓ1`1 8π2 ÿ m1n1 ψℓ1˚ m1n1 ż SOp3q dµpρ1qDℓ˚ mnpρ1qDℓ1 m1n1pρ´1ρ1q (29) “ ÿ ℓ 2ℓ`1 8π2 ÿ mn fℓ mn ÿ ℓ1 2ℓ1`1 8π2 ÿ m1n1 ψℓ1˚ m1n1 ż SOp3q dµpρ1qDℓ˚ mnpρ1q ÿ k Dℓ1˚ km1pρqDℓ1 kn1pρ1q (30) “ ÿ ℓ 2ℓ`1 8π2 ÿ mn fℓ mn ÿ ℓ1 2ℓ1`1 8π2 ÿ m1n1 ψℓ1˚ m1n1 ÿ k Dℓ1˚ km1pρq 8π2 2ℓ`1δℓℓ1δmkδnn1 (31) “ ÿ ℓmm1 2ℓ`1 8π2 Dℓ˚ mm1pρq ˆÿ n fℓ mnψℓ˚ m1n ˙ , (32) where for Equation 30 we make use of the relation (e.g. Marinucci & Peccati, 2011; McEwen et al., 2018) Dℓ mnpρ´1ρ1q“ ÿ k Dℓ˚ kmpρqDℓ knpρ1q. (33) This decomposition yields harmonic coefﬁcients pf ˚ψqℓ mn “ ÿ m1 fℓ mm1ψℓ˚ nm1. (34) C F ILTERS ON THE SPHERE AND ROTATION GROUP When deﬁning ﬁlters we look to encode desirable real-space properties, such as locality and regular- ity. However, in practice considerable computation may be saved by deﬁning the ﬁlters in harmonic space and saving the cost of harmonic transforming ahead of harmonic space convolutions. We de- scribe here how ﬁlters motivated by their real space properties may be deﬁned directly in harmonic space. C.1 D IRAC DELTA FILTERS ON THE SPHERE Spherical ﬁlters may be constructed as a weighted sum of Dirac delta functions on the sphere. This construction is useful as the harmonic representation has an analytic form that may be computed efﬁciently. Furthermore, various real space properties can be encoded through sensible placement of the Dirac delta functions. The spherical Dirac delta function δω1 centered at ω1 “pθ1,φ1qP S2 is deﬁned as δω1pωq“ 1 sin θδRpcos θ´cos θ1qδRpφ´φ1q, (35) where δR is the familiar Dirac delta function on the reals centered at0. The Dirac delta on the sphere may be represented in harmonic space by pδω1qℓ m “Yℓ˚ m pω1q“ Nℓ mPℓ mpcos θ1qe´imφ1 , (36) 15Published as a conference paper at ICLR 2021 which follows form the sifting property of the Dirac delta, and where Yℓ m denote the spherical harmonic functions, Pℓ mpxqare associated Legendre functions and Nℓ m “ d 2ℓ`1 4π pl´mq! pl`mq! (37) is a normalizing constant. This representation may then be used to deﬁne a ﬁlter ψ P L2pS2qas a weighted sum of spher- ical Dirac delta functions, with weights wij assigned to Dirac delta functions centered at points tpθi,φjq: i“1,...,N θ; j “1,...,N φu. The associated harmonic space representation is given by ψℓ m “ ÿ i,j wijNℓ mPℓ mpcos θiqe´imφj (38) “ ÿ i Nℓ mPℓ mpcos θiq ÿ j wije´imφj, (39) where fast Fourier transforms may be leveraged to compute the inner sum if the Dirac deltas are spaced evenly azimuthally (e.g. if φj “2πj{Nφ). Alternative arbitrary samplings can of course be considered if useful for a problem at hand. When deﬁning ﬁlters in this manner one should be careful not to over-parametrize by assigning more weights than needed to deﬁne a ﬁlter at the harmonic bandlimit of the signal with which we wish to convolve. For example, if the ﬁlter is to be convolved with a signal bandlimited at Lthen a maximum of 2L´1 Dirac deltas should be placed along each ring of constant θ. One may also choose to interpolate the weights from a smaller number of learnable parameters acting as anchor points, allowing higher resolution ﬁlters to be deﬁned with fewer learnable parameters. C.2 D IRAC DELTA FILTERS ON THE ROTATION GROUP Similarly a Dirac delta function δρ1 on the rotation group SOp3q centered at position ρ1 “pα1,β1,γ1qP SOp3qis deﬁned as δρ1pρq“ 1 sin βδRpα´α1qδRpcos β´cos β1qδRpγ´γ1q, (40) with harmonic form pδρ1qℓ mn “Dℓ mnpρ1q“ e´imα1 dℓ mnpβ1qe´inγ1 , (41) where dℓ mn are Wigner (small) d-matrices. The ﬁlter ψ PL2pSOp3qqcorresponding to a weighted sum of Dirac deltas with weights wijk as- signed to Dirac delta functions centered at points tpαi,βj,γkq: i“1,...,N α; j “1,...,N β; k “ 1,...,N γuhas harmonic form ψℓ mn “ ÿ i,j,k wijk e´imαjdℓ mnpβiqe´inγk (42) “ ÿ j dℓ mnpβiq ÿ i e´imαj ÿ k wijke´inγk, (43) where again fast Fourier transforms may be leveraged to compute the inner two sums assuming the Dirac deltas are spaced evenly in αand γ. The outer sums of Equation 39 and Equation 43 can also be computed by fast Fourier transforms by decomposing the Wigner d-matrices into their Fourier representation (cf. Trapani & Navaza, 2006; McEwen & Wiaux, 2011). One should again be careful not to over-parametrize. D E QUIVARIANCE TESTS To test rotational equivariance of operators we consider Nf “ 100 random signals tfiuNf i“1 in L2pΩ1qwith harmonic coefﬁcients sampled from the standard normal distribution and Nρ “ 100 16Published as a conference paper at ICLR 2021 random rotations tρjuNρ j“1 sampled uniformly on SOp3q. In order to measure the extent to which an operator A : L2pΩ1qÑ L2pΩ2qis equivariant we evaluate the mean relative error dpApRρjfiq,RρjpAfiqq“ 1 Nf 1 Nρ Nfÿ i“1 Nρÿ j“1 }ApRρjfiq´ RρjpAfiqq} }ApRρjfiq} (44) resulting from pre-rotation of the signal, followed by application of A, as opposed to post-rotation after application of A, where the operator norm }¨} is deﬁned using the inner product x¨,¨yL2pΩ2q. Table 4 presents the mean relative equivariance errors computed. We consider the three standard convolutions described in Appendix B (with a random ﬁlter ψi for each signal fi, generated in the same manner as fi), the pointwise ReLU activation described in Section 2.5.1 for signals on the sphere ( Ω1 “ S2) and rotation group ( Ω1 “ SOp3q), and the composition of tensor-product activation with a generalized convolution, described in Sections 2.5.2 and 2.4, respectively. We follow the tensor-product activation with a generalized convolution in order to project down onto the sphere to allow the same notion of error to be adopted as for the other operators. For consistency with the context in which we leverage these operators, all experiments are performed using single- precision arithmetic. We see that all three standard notions of convolution and the composition of the tensor-product activation and generalized convolution are all strictly equivariant to ﬂoating point machine precision, with errors on the order of 10´7. The pointwise ReLU operator is not strictly equivariant, with a mean relative error of0.37 for signals on the rotation group and0.34 for signals on the sphere. These errors reduce when the signals are oversampled before application of the ReLU, indicating that the error is due to aliasing induced by the spreading of information to higher degrees not captured at the original bandlimit. For example, for the pointwise ReLU operator on the rotation group oversampling by factors of 2ˆ,4ˆand 8ˆresults in a reduction in the mean relative equivariance error from 0.37 to 0.098,0.032 and 0.0096, respectively. Table 4: Layer equivariance tests Layer Mean Relative Error S2 to S2 conv. 4.4 ˆ10´7 S2 to SOp3qconv. 5.3 ˆ10´7 SOp3qto SOp3qconv. 9.3 ˆ10´7 Tensor-product activationÑGeneralized conv. 5.0 ˆ10´7 S2 ReLU 3.4 ˆ10´1 S2 ReLU (2ˆoversampling) 8.9 ˆ10´2 S2 ReLU (4ˆoversampling) 2.9 ˆ10´2 S2 ReLU (8ˆoversampling) 1.3 ˆ10´2 SOp3qReLU 3.7 ˆ10´1 SOp3qReLU (2ˆoversampling) 9.8 ˆ10´2 SOp3qReLU (4ˆoversampling) 3.2 ˆ10´2 SOp3qReLU (8ˆoversampling) 9.6 ˆ10´3 E C ONNECTION BETWEEN THE TENSOR PRODUCT ACTIVATION AND POINTWISE SQUARING To provide some intuition on the manner in which the tensor-product based activation introduces non-linearity into representations we describe its relationship to pointwise squaring for signals on the sphere. Here we consider the operator N : L2pS2q ÑL2pS2qsatisfying pNfqpxq “f2pxq for all x P S2, which differs subtly to Nσ with σpxq “x2 (using notation from Section 2.5.1), which corresponds to obtaining a sample-based representation at a ﬁnite bandlimit Land applying the squaring at the sample positions. For the special case σpxq“ x2 we can directly compute the harmonic representation corresponding to the equivariance-preserving continuous limit LÑ8. 17Published as a conference paper at ICLR 2021 106 108 1010 1012 1014 FLOPs Kondor et al. (2018) Ours 0 20 40 60 80 100 120 Bandlimit L 0x 50x 100xReduction (a) Computational cost 10 1 101 103 105 Memory (MB)  Kondor et al. (2018) Ours 0 20 40 60 80 100 120 Bandlimit L 0x 10x 20x 30xReduction  (b) Memory requirements Figure 3: Comparison of computation and memory footprints between the generalized spherical CNN layers of Kondor et al. (2018) and our efﬁcient generalized layers. The reduction in cost due to our efﬁcient layers is given as multiplicative factors in the lower plot of each panel. Given a spherical signal f P L2pS2qwith generalized representation f “ tˆfℓ 0 P C2ℓ`1 : ℓ “ 0,..,L ´1u, the generalized representation of the signal f2 PL2pS2qis given as f2 “t ÿ pℓ1,ℓ2qPPℓ,L pGℓ1,ℓ2,ℓqJpˆfℓ1 0 b ˆfℓ2 0 q : ℓ“0,1,...,L ´1u, (45) where Gℓ1,ℓ2,ℓ PCp2ℓ1`1qˆp2ℓ2`1qˆp2ℓ`1qare Gaunt coefﬁcients deﬁned as Gj1j2j3 m1m2m3 “ ż S2 dµpωqYj1 m1 pωqYj2 m2 pωqYj3˚ m3 pωq. (46) Gaunt coefﬁcients are related to the Clebsch-Gordan coefﬁcients by Gj1j2j3 m1m2m3 “wj1j2j3 Cj1j2j3 m1m2m3 , (47) where wj1j2j3 “ p´1qm3 b p2j1`1qp2j2`1q 4πp2j3`1q Cj1j2j3 0 0 0 . Therefore, the continuous squaring operation corresponds to passing f through a tensor-product activation Nb followed by a generalized con- volution back down onto the sphere (single fragment per degree) with weight assigned to the pℓ1,ℓ2qP Pℓ L fragment in degree-ℓgiven by wl1l2l3 . This demonstrates that activations that are learnable within our framework can have very simple real- space interpretations. Even when conﬁning outputs to the sphere we found it to be beneﬁcial to allow the down-projection to be learnable rather than enforcing the weights given above for pointwise squaring. Learned activations will remain quadratic, however, given that output fragments are linear combinations of products between input fragments. F C OMPARISON OF COMPUTATIONAL AND MEMORY FOOTPRINTS We perform a quantitative analysis of the computational cost and memory requirements of our pro- posed layers, and comparisons to prior approaches, to demonstrate how the complexity savings made through our proposals in Section 3 translate to tangible efﬁciency improvements. We consider the simplest comparison, between a multi-channel generalized signal f “ pf1,...,f KqP FK L where each channel fi has type τfi “p1,..., 1q(and therefore corresponds to a signal on the sphere) and a uni-channel signal g PFL of type τg “pK,...,K q. The setting corre- sponding to signal f captures the efﬁciency improvements of our proposed layers, while the setting corresponding to grepresents the case without these improvements. Notice that the total number of fragments is the same in bothf and g. We compare the number of ﬂoating point operations (FLOPs) and amount of memory required to perform a tensor-product based activation followed by a general- ized convolution projecting back down onto a signal of the same type as the input. When applied to f, MST mixing sets ¯Pℓ L (Section 3.1.3) and constrained generalized convolutions (Section 3.1.2) are 18Published as a conference paper at ICLR 2021 ReLU S2 Layer S2 Conv. I SO(3) Conv. SO(3) Layer ReLU I Constrained Gen. Conv. Eﬃcient Gen. Layer Constrained Gen. Conv. Tensor Products I Constrained Gen. Conv. Eﬃcient Gen. Layer Tensor Products I Constrained Gen. Conv. Eﬃcient Gen. Layer Tensor Products Figure 4: Visualization of the architecture used for the convolutional base in our hybrid models. The input to the ﬁrst convolutional layer is a signal on the sphere. The output from the ﬁnal convolutional layer are scalar values corresponding to fragments of degree ℓ“0, which are then mapped through some fully connected layers to give the model output. used. When applied to g full mixing sets Pℓ Land unconstrained generalized convolutions are used, as in Kondor et al. (2018). Considered are the costs for a single training instance (batch size of 1). Figure 3 shows the computational costs and memory requirements, in terms of ﬂoating point op- erations and megabytes respectively, for K “ 4 and various spatial bandlimits L. We adopt the convention whereby complex addition and multiplication require 2 and 6 ﬂoating point operations respectively. At low bandlimits we see the saving arising from the channel-wise structure. Note that the saving illustrated here is relatively small since K “4 for these experiments (so that the L scaling is apparent), whereas in practice typically K „100. The saving then increases linearly in response to increases in the bandlimit of the input, as expected given the OpL5qand OpL4qspa- tial complexities. We see that even in this simple case, with a relatively small number of channels (K “4), both the computational and memory footprints are reduced by orders of magnitude. At a bandlimit of L “128 the computational cost is 101-times reduced and the memory requirement is 29-times reduced. G A DDITIONAL INFORMATION ON EXPERIMENTS G.1 R OTATED MNIST ON THE SPHERE For our MNIST experiments we used a hybrid model with the architecture shown in Figure 4. The ﬁrst block includes a directional convolution on the sphere that lifts the spherical input ( τℓ fp0q “1) onto the rotation group (τℓ fp1q “minp2ℓ`1,2N1 ´1q). The second block includes a convolution on the rotation group, hence its input and output both live on the rotation group. We then apply a restricted generalized convolution to map to typeτℓ fp3q “rτmax{ ? 2ℓ`1s, where τmax“5. The same type is used for the following three channel-wise tensor-product activations and two restricted gen- eralized convolutions until the ﬁnal restricted generalized convolution maps down to a rotationally invariant representation (τℓ fp5q “ δℓ0). As is traditional in convolution networks we gradually de- crease the resolution, with pL0,L1,L2,L3,L4,L5q“p 20,10,10,6,3,1q, and increase the number of channels, with pK0,K1,K2,K3,K4,K5q “ p1,20,22,24,26,28q. We proceed these convolu- tional layers with a single dense layer of size 30, sandwiched between two dropout layers (keep probability 0.5), and then fully connect to the output of size 10. We train the network for50 epochs on batches of size 32, using the Adam optimizer (Kingma & Ba, 2015) with a decaying learning rate starting at 0.001. For the restricted generalized convolutions we follow the approach of Kondor et al. (2018) by using L1 regularization (regularization strength 10´5) and applying a restricted batch normalization across fragments, where the fragments are only scaled by their average and not translated (to preserve equivariance). 19Published as a conference paper at ICLR 2021 G.2 A TOMIZATION ENERGY PREDICTION When regressing the atomization energy of molecules there are two inputs to the model: the number of atoms of each element contained in the molecule; and spherical cross-sections of the potential energy around each atom. We adopt the high-level QM7-speciﬁc architecture of Cohen et al. (2018) which contains a spherical CNN as a sub-model, for which we substitute our own. This results in an overall model that is invariant to both rotations of the molecule around each constituent atom and to permutations of the ordering of the atoms. The ﬁrst (non-spherical) input is mapped onto a scalar output using a multi-layer perceptron (MLP) with three hidden layers of sizes100, 100 and 10 (and ReLU activations). The second input, multiple spherical cross sections for each atom, are separately projected using a shared spherical CNN (of architecture described below) onto lower dimensional vectors of size 64. The mean vector is then taken across atoms (ensuring invariance w.r.t. permutations of the atoms) and mapped onto a scalar output using an MLP with a single hidden layer of size512 (with a ReLU activation). The predicted energy is then taken to be the sum of the two scalar outputs. As a starting point we train the ﬁrst MLP to regress the atomization energies alone (achieving RMS „20), before pairing it with the spherical model (and its connected MLP). We then train the joint model for 60 epochs, again with the Adam optimizer, a decaying learning rate (starting at 2.5 ˆ 10´4), regularizing the efﬁcient generalized layers with L2 regularization (strength 2.5 ˆ10´6) and batch sizes of 32. For the spherical component we again adopt the convolutional architecture shown in Figure 4 except with one fewer efﬁcient generalized layer. We use bandlimits of pL0,L1,L2,L3,L4q“p10,6,6,3,1q, channels of pK0,K1,K2,K3,K4q“p5,16,24,32,40q and τmax “6. One minor difference is that this time we include a skip connection between the ℓ “0 components of the fourth and ﬁfth layer. We proceed the convolutional layers with two dense layers of size p256,64qand use batch normalization between each layer. G.3 3D S HAPE RETRIEVAL To project the 3D meshes of the SHREC’17 data onto spherical representations (bandlimited at L “128) we adopt the preprocessing approach of Cohen et al. (2018) and augment the data with random rotations and translations. We construct a model with an architecture that is again similar to that described in Appendix G.1 but with an additional axisymmetric convolutional layer prepended to the start of the net- work and one fewer efﬁcient generalized layers. We use bandlimits pL0,L1,L2,L3,L4,L5q “ p128,32,16,16,6,1q, channels pK0,K1,K2,K3,K4,K5q “ p6,20,30,40,60,70qand τmax “ 6 for the efﬁcient generalized layers. The convolutional layers are followed by a dense layer of size 128 which is fully connected to the output (of size 55). We again train with the Adam optimizer, a decaying learning rate (starting at 5 ˆ10´4) and batch sizes of 8, this time until performance on the validation set showed no improvement for at least 4 epochs (36 epochs in total). We perform batch normalization between convolutional layers and dropout preceding the dense layer. We regularize the efﬁcient generalized layers with L2 regu- larization (strength 10´5). When testing our model we average the output probabilities over 15 augmentations of the data. 20",
      "references": [
        "970 million druglike small molecules for virtual screening in the chemical universe database GDB-13",
        "Spherical convolutions and their application in molecular modelling",
        "Spherical CNNs",
        "Gauge equivariant convolutional networks and the icosahedral CNN",
        "Computing Fourier transforms and convolutions on the sphere",
        "Theoretical aspects of group equivariant neural networks",
        "Learning SO(3) equivariant representations with spherical CNNs",
        "Spin-weighted spherical CNNs",
        "Aspects of Harmonic Analysis and Representation Theory",
        "FFTs for the 2-sphere – improvements and variations",
        "Spherical CNNs on unstructured grids",
        "Hilbert space methods in signal processing",
        "Adam: A method for stochastic gradient descent",
        "On the generalization of equivariance and convolution in neural networks to the action of compact groups",
        "Clebsch-Gordan nets: a fully fourier space spherical convolutional neural network",
        "FFTs on the rotation group",
        "Group invariant scattering",
        "Random Fields on the Sphere: Representation, Limit Theorem and Cosmological Applications",
        "A novel sampling theorem on the sphere",
        "Fast directional continuous spherical wavelet transform algorithms",
        "On the computation of directional scale-discretized wavelet transforms on the sphere",
        "A novel sampling theorem on the rotation group",
        "Directional spin wavelets on the sphere",
        "Localisation of directional scale-discretised wavelets on the sphere",
        "Learning invariant representations of molecules for atomization energy prediction",
        "Deepsphere: Efﬁcient spherical convolutional neural network with HEALPix sampling for cosmological applications",
        "Fast and accurate modeling of molecular atomization energies with machine learning",
        "Large-scale 3d shape retrieval from shapenet core55: Shrec’17 track",
        "An Icosahedron-Based method for pixelizing the celestial sphere",
        "Tensor ﬁeld networks: Rotation-and translation-equivariant neural networks for 3d point clouds",
        "Calculation of spherical harmonics and Wigner d functions by FFT. Applications to fast rotational matching in molecular replacement and implementation into AMoRe",
        "Fast convolution on the sphere"
      ],
      "meta_data": {
        "arxiv_id": "2010.11661v3",
        "authors": [
          "Oliver J. Cobb",
          "Christopher G. R. Wallis",
          "Augustine N. Mavor-Parker",
          "Augustin Marignier",
          "Matthew A. Price",
          "Mayeul d'Avezac",
          "Jason D. McEwen"
        ],
        "published_date": "2020-10-09T18:00:05Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces a unified \"generalized spherical CNN\" framework that subsumes prior harmonic-space, real-space, and hybrid approaches and allows them to be combined. Proposes two new strictly SO(3)-equivariant nonlinear layers whose computational complexity drops from the previous worst-case O(C^2 L^5) to O(C L^4) and O(C L^3 log L) by (i) using a multi-channel (channel-wise) tensor-product activation, (ii) decomposing generalized convolutions into constrained, depth-wise-separable projections, and (iii) selecting degree-mixing connections through minimum spanning trees (MST/RMST). Adopts more efficient sampling theorems on S^2 and SO(3) to halve sample counts. Together these advances enable deeper, more expressive yet tractable strictly equivariant models that achieve state-of-the-art accuracy and parameter efficiency on multiple spherical benchmarks.",
        "methodology": "1. Formulates signals as sets of harmonic \"fragments\" in a generalized space F_L, enabling linear (generalized convolution) and nonlinear (tensor-product) equivariant operators. 2. Designs channel-wise tensor-product activation: applies Clebsch–Gordan mixing per channel to curb quadratic growth in fragments. 3. Introduces constrained generalized convolutions split into three stages (shared projection, depthwise per-channel mixing, pointwise channel mixing) to cut parameters and FLOPs. 4. Defines optimized degree-mixing sets via MST or logarithmically sparse RMST to reduce spatial complexity. 5. Replaces traditional Driscoll–Healy/Kostelec–Rockmore sampling with McEwen–Wiaux (S^2) and McEwen et al. (SO(3)) sampling; leverages fast O(L^3) / O(N L^3) harmonic transforms. 6. Builds hybrid networks that combine standard S^2/SO(3) convolutions with the proposed efficient generalized layers.",
        "experimental_setup": "Benchmarks:\n• Spherical MNIST: digits projected to S^2; three protocols – non-rotated train/test (NR/NR), rotated train/test (R/R), non-rotated train vs rotated test (NR/R). Accuracy compared to planar CNN and prior spherical CNNs.\n• QM7 molecular regression: predicts atomization energies from Coulomb matrices; reports test RMS error.\n• SHREC’17 3D shape retrieval (perturbed, micro-all split): evaluates P@N, R@N, F1@N, mAP, NDCG.\n\nImplementation details: models composed of 2–3 standard equivariant convolutions followed by 2–3 efficient generalized layers; bandlimits up to L=128, τ_max≈6, channels up to 70. Training with Adam, learning-rate decay, batch-normalization and dropout; L1/L2 regularization on generalized convolutions. Equivariance validated numerically on 100 random signals and rotations, yielding ~10^-7 relative error for proposed layers. Hardware savings quantified: at L=64,K=4 the new layers cut FLOPs by 51× and memory by 16× versus Kondor et al. 2018.",
        "limitations": "• Despite reductions, harmonic transforms (O(L^3)) and generalized operations remain heavy for very high resolutions; scalability to ultra-high L or real-time applications needs assessment.\n• Implementation requires specialized spherical/SO(3) FFT libraries, raising engineering barrier.\n• Nonlinearity relies on quadratic tensor products; expressiveness vs traditional activations (e.g., ReLU) not fully explored.\n• Experiments cover synthetic or medium-scale datasets; performance on large real-world spherical video or climate data untested.\n• Strict equivariance is achieved only for rotations; translation, scaling, and other group actions are not addressed.",
        "future_research_directions": "1. Develop even more scalable or approximate equivariant layers (e.g., scattering transforms on S^2, low-rank approximations) to handle L≫128.\n2. Explore alternative, learnable sparse degree-mixing graphs beyond MST/RMST to balance accuracy vs cost dynamically.\n3. Investigate richer nonlinearities that preserve equivariance but extend beyond quadratic tensor products.\n4. Optimize implementations for GPUs/TPUs and edge devices, potentially with mixed precision or custom kernels.\n5. Extend framework to other manifolds/groups (e.g., SE(3), hyperbolic spaces) and to spatio-temporal spherical data (video, climate).\n6. Apply models to larger, real-world tasks such as autonomous driving with 360° cameras, cosmological inference, or molecular property prediction at scale.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "PENNI: Pruned Kernel Sharing for Efficient CNN Inference",
      "full_text": "PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference Shiyu Li 1 Edward Hanson 1 Hai Li 1 Yiran Chen 1 Abstract Although state-of-the-art (SOTA) CNNs achieve outstanding performance on various tasks, their high computation demand and massive number of parameters make it difﬁcult to deploy these SOTA CNNs onto resource-constrained devices. Previous works on CNN acceleration utilize low- rank approximation of the original convolution layers to reduce computation cost. However, these methods are very difﬁcult to conduct upon sparse models, which limits execution speedup since re- dundancies within the CNN model are not fully exploited. We argue that kernel granularity de- composition can be conducted with low-rank as- sumption while exploiting the redundancy within the remaining compact coefﬁcients. Based on this observation, we propose PENNI, a CNN model compression framework that is able to achieve model compactness and hardware efﬁciency si- multaneously by (1) implementing kernel sharing in convolution layers via a small number of basis kernels and (2) alternately adjusting bases and coefﬁcients with sparse constraints. Experiments show that we can prune 97% parameters and 92% FLOPs on ResNet18 CIFAR10 with no accuracy loss, and achieve 44% reduction in run-time mem- ory consumption and a 53% reduction in inference latency. 1. Introduction One of the greatest strengths of Deep Neural Net- works (DNNs), speciﬁcally Convolutional Neural Networks (CNNs), is their large design space, which innately height- ens ﬂexibility and potential for accuracy. Improving model accuracy conventionally involves increasing its size, given sufﬁcient training data. This increase in size can come in the form of more layers (He et al., 2016), more channels per 1Department of Electrical and Computer Engineering, Duke University, Durham NC, United States. Correspondence to: Shiyu Li <shiyu.li@duke.edu>. Proceedings of the37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). layer (Zagoruyko & Komodakis, 2016), or more branches (Szegedy et al., 2015). A major drawback of na ¨ıvely in- creasing model size is the substantial computational power and memory bandwidth required to train and run inference tasks. To address this issue, multiple methods have been introduced to compress CNN models and increase sparsity (Han et al., 2015; Wen et al., 2016). Model compression can come in the form of weight quantization (Ullrich et al., 2017) or Low Rank Approximation (LRA) (Denton et al., 2014). LRA utilizes matrix factorization to decompose weight ma- trices into the product of two low rank matrices, thus reduc- ing computation cost. Some works (Lebedev et al., 2014; Tai et al., 2016) use tensor decomposition to represent the original weight with the outer product of one-dimensional tensors (i.e., vectors) . The speedup and parameter reduction of these methods are notable; however, current approaches are limited because they do not consider redundancies in CNN parameters. Model sparsity can be induced via various pruning tech- niques, most of which are categorized under structured or unstructured. On one hand, unstructured pruning aims to remove unimportant weights of a network, irrespective of its location. By targeting the least important weights in a model, unstructured pruning has minimal impact on overall accuracy while achieving a high sparsity level. However, the undeﬁned distribution of pruned weights makes it chal- lenging to compress the model’s representation in memory. On the other hand, structured pruning achieves sparsity by removing entire DNN structures (e.g. ﬁlter-channels, ﬁlters, layers, etc.) that are deemed unimportant, which may impact a model’s performance by inadvertently removing sensitive parameters. Such predictable pruning patterns open the avenue for efﬁcient model storage and computation. It is important to note that merely applying structured pruning is not enough to fully reap hardware efﬁciency beneﬁts. Without additional changes to the underlying representa- tion in memory or the model’s training or inference stage algorithms, conventional DNN platforms still fall victim to inefﬁcient memory transfers and computations. In this paper, we propose Pruned kernel sharing for Efﬁcient CNN Inference (PENNI), a CNN model compression frame- work that overcomes these challenges by decomposing layer arXiv:2005.07133v2  [cs.CV]  25 Jun 2020PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference parameters into tiny sets of basis kernels and accompanying coefﬁcient matrices. This method can beneﬁt inference efﬁ- ciency by organizing the involved coefﬁcients and computa- tion ﬂow in a hardware-friendly manner. High compression rate is achieved by applying l1-regularization to the coefﬁ- cients. The structural redundancies are further explored in a model shrinkage procedure. We evaluate our method on CIFAR10 and ImageNet with VGG16, ResNet and AlexNet. Results show that we can achieve a 98.3% reduction on parameters and a 93.3% reduction on FLOPs with less than 0.4% accuracy drop. PENNI outperforms state-of-the-art (SOTA) pruning schemes in addition to being more efﬁ- cient for hardware implementation. Our code is avaliable at: https://github.com/timlee0212/PENNI. Our main contributions are listed as follows: • We propose a hardware-friendly CNN model compres- sion framework, PENNI. We apply ﬁlter decompo- sition to generate a limited set of basis kernels and corresponding coefﬁcient matrix. Sparsity is achieved by applying l1-regularization to coefﬁcient matrices in the retraining process. Structural redundancies are then explored via a model shrinking procedure. • Hardware inference efﬁciency is directly beneﬁted through model shrinking with no modiﬁcations to in- ference algorithm. Further speedup can be brought by computation reorganization of convolutional lay- ers. To avoid restoring original ﬁlter tensors, we can separate basis kernel convolutions from their weighted sum computation. Keeping the two computation steps distinct opens the avenue for exposing all pruned coefﬁ- cients, thus leveraging coefﬁcient sparsity and avoiding wasteful zero-computations. • Evaluation on CIFAR-10 and ImageNet with various network architectures proves the effectiveness of the proposed method with signiﬁcant reduction in both FLOPs and number of parameters with negligible ac- curacy loss. 2. Related Work Various methods have been proposed to accelerate CNN inference. These methods either exploit redundancies of CNN models to reduce the number of parameters and com- putations or introduce lightweight model structures for a given task. Compact Model Design Previous works aim to develop resource-efﬁcient model structures to reduce computation requirements and improve latency. Lin et al. (2013) pro- pose global average pooling and 1x1 convolution, which are widely adopted in the later compact architectures. SqueezeNet (Iandola et al., 2016) utilizes both structures to reduce the number of channels and remove fully-connected layers. A similar idea appears in InceptionNet (Szegedy et al., 2015), while a later version (Szegedy et al., 2016) extends the idea by spatially separating the convolutional layers. MobileNet (Howard et al., 2017) uses depthwise separable convolution to reduce the computation cost by splitting the original convolutional layer channel-wise. Its following version, MobileNet V2 (Sandler et al., 2018), adopts residual connections and introduces the inverted bot- tleneck module to improve efﬁciency. Xie et al. (2017) enhance the expressiveness of the depthwise convolution by allowing limited connectivity within groups, while later ShufﬂeNet (Zhang et al., 2018b) adopts the grouped con- volution. In addition to the manually designed compact ar- chitectures listed above, Neural Architecture Search (NAS) methods aim to automatically ﬁnd architectures with opti- mal balances of compactness and performance. Multiple such works (Tan et al., 2019; Cai et al., 2019; Liu et al., 2019a; Tan & Le, 2019) generate architectures that outper- form manually designed ones. Low Rank Approximation Low Rank Approximation (LRA) method decomposes the original weights into sev- eral low rank matrices (Denton et al., 2014; Zhang et al., 2015) or low dimension tensors (Lebedev et al., 2014; Kim et al., 2015). Denton et al. (2014) utilize Singular Value De- composition (SVD) to conduct the decomposition, whereas Zhang et al. (2015) take nonlinear activations into account to obtain the decomposition while minimizing error of the response. Kim et al. (2015) adopt Tucker Decomposition to compress the kernel tensor. Lebedev et al. (2014) use canon- ical polyadic (CP) decomposition. In addition, Learning Structured Sparsity (Wen et al., 2016) and Centripetal-SGD (Ding et al., 2019a) directly train the DNN with low rank constraints. These tensor decomposition methods rely on the rank selection, which is an ill-posed problem, while the matrix factorization methods have limited speedup since redundancies in the standalone weight values are not con- sidered. Model Pruning The idea of weight pruning dates back to the last century. Optimal Brain Damage (LeCun et al., 1990) proposes pruning weight based on their impact on the loss function. A later work, Optimal Brain Surgeon (Hassibi & Stork, 1993) improves this method by replac- ing the diagonal Hessian Matrix with an approximated full covariance matrix. However, due to the giant size of the modern DNNs, these methods incur unacceptable compu- tation cost. Han et al. (2015) propose pruning weights by comparing the magnitude with a threshold, and achieve the optimal result by iterative pruning and ﬁne-tuning. Guo et al. (2016) further improve the sparsity level by maintaining a mask instead of directly pruning the redundant weights.PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference 𝑐𝑙+1 …… 𝑐𝑙+1 𝑘𝑙 𝑑 … 𝑐𝑙 𝑑 𝑐𝑙 𝑘𝑙 … … A B …  … C D Figure 1.Overview of PENNI. There are four phases in the proposed framework: A. Decompose the ﬁlters into d-dimension basis and the corresponding coefﬁcient matrix; B. Recover the model performance by alternatively training basis and coefﬁcients with sparsity regularization applied to coefﬁcients; C. Prune coefﬁcient by magnitude; D. Explore the structure redundancies and shrink the model. Beyond conventional unstructured pruning methods, various structured pruning methodologies have been proposed to ease translation from sparsity to inference speedup. Wen et al. (2016) and Yang et al. (2019) apply group regularizer in the training process to obtain structured sparsity. Liu et al. (2017) apply l1-regularization to the scaling factors of batch normalization layers to identify insigniﬁcant channels. ThiNet (Luo et al., 2017) utilizes a data-driven method to prune the channel with the smallest impact on the following layer. In recent works (He et al., 2018a; Zhang et al., 2018a; He et al., 2019; Ding et al., 2019b), different criteria are adopted to rank the importance of the ﬁlter. Louizos et al. (2017) use stochastic gates to apply l0-regularization to the ﬁlters. NAS methods also incorporate ﬁlter pruning (He et al., 2018b; Liu et al., 2019b). Although structured prun- ing can directly beneﬁt the inference efﬁciency, its pruning granularity limits the compression rate or accuracy of CNN models. 3. Proposed Method 3.1. Overview Fig.1 presents the overview of PENNI framework. We ﬁrst decompose each layer’s convolution ﬁlters into a few basis kernels and a coefﬁcient matrix. Then, we retrain the decomposed network with sparsity regularization applied to coefﬁcient matrix to recover any lost accuracy. Finally, we prune the redundant coefﬁcients based on magnitude and obtain a compact CNN model. Before the discussion on the method, we ﬁrst deﬁne the notations that will be used in this paper. We denote the pa- rameters of convolutional layer las θ(l) ∈Rcl×cl+1×kl w×kl h , where cl is the number of the input channels, cl+1 is the number of the output channels, and k(l) w and k(l) h are the ker- nel dimensions. Since most CNN architectures implement a square kernel shape, i.e., k(l) w = k(l) h , we denote the kernel shape as k(l) ×k(l) for simplicity; the shape of the kernel does not affect this framework. Θ = {θ(l)}is the set of all parameters of convolutional layers of a CNN model. We denote the output features of layer las S(l), and the input features as I(l). (X,Y ) represents the data pairs, while Y is the given label or unknown ground-truth. ˆY represents the network model’s prediction. With these notations, the i-th channel of a layer’s output feature map S(l) can be computed by: S(l) i = σ(l) ((cl−1∑ j=1 I(l−1) j ∗θ(l) i,j ) + b(l) j ) , (1) where θ(l) i,j is the j-th kernel of the i-th ﬁlter, b(l) j is the bias term of the j-th ﬁlter and the σ(l) is the non-linear function of the layer l. 3.2. Filter Decomposition The convolution operation dominates computation cost of CNN inference. Irregular data access and compute patterns make it extremely difﬁcult to efﬁciently map the operation onto parallel hardware and further improve inference efﬁ- ciency. We address this issue by reducing the number of convolution operations and ofﬂoading the irregular compu- tation to a sequential and simple pattern. Previous work (Zhang et al., 2015) on accelerating CNN inference utilizes a low rank assumption of output feature subspace to represent the original weight matrix with the multiplication of two low rank matrices, thus reducing the computation required. Low rank assumption is reasonable in this case because the number of output features is compa- rable with the dimension of the feature space. Recent work (Ding et al., 2019a) indicates that in most CNNs, regular- ization on convolutional kernels can push the kernels to be alike one another. Based on this observation, we argue that the low rank assumption can also be applied to the subspace that each convolutional kernel lies in. With this assumption, we approximate the original convolutional layer by sharing a tiny set of basis kernels and representing original kernelsPENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference with coefﬁcients. Decomposition at a kernel granularity is done to obtain an approximated layer. This process applies to a single layer a time, so the superscript lis omitted for readability. We ﬁrst reshape the original weight tensor into a 2D matrix θ′ ∈Rclcl+1×k2 l ; thus, each kernel can be seen as its row vector w ∈Rk2 l . Suppose U ⊂Rk2 l is a subspace with basis B= {u1,u2,...,u d}where d≤k2 l. The objective of decomposition process is to ﬁnd the subspace that minimizes the error between the projected and original vectors, shown in Equation 2. min αw∈Rd ∑ w∈θ′ ||w−αwBT||2. (2) B = [u1 u2 ...ud] is the basis matrix where each column vector is a basis of the subspace and αw is the coefﬁcient vector corresponding to the row vector w. With this decom- position, the output of each layer is computed by: S(l) i = σ(l) ((cl−1∑ j=1 I(l−1) j ∗(α(l) i,jB(l)T) + b(l) j ) , (3) where α(l) i,j is the row vector in the coefﬁcient matrix corre- sponding to the j-th kernel of the i-th ﬁlter. The decomposition problem can be formulated as best ap- proximation and is perfectly solved using singular value decomposition (SVD). We ﬁrst obtain ¯θ′ by subtracting each row vector with the mean vector, and then compute the covariance matrix W = θ′Tθ′. Conducting SVD on W, and organizing the singular value by their magnitude, we’ll have: W = UΣVT. (4) The basis matrix B is then derived by selecting the ﬁrst d columns from matrix U and obtaining the corresponding coefﬁcients by multiplying the θ′by the projection matrix BBT. Normally, k2 l <<clcl+1 and parameter matrices of a pretrained model are dense, soW is a full rank matrix with the rank k2 l. Thus, the low rank approximation on kernel space makes the SVD computation faster than conducting decomposition at the ﬁlter granularity. A singular value may represent the portion of the basis vector contributing to the original vectors; but, rather than selecting dbased on it, we leave it as a hyper-parameter providing a trade-off between computational cost and model accuracy. 3.3. Retraining Although the discussed ﬁlter decomposition scheme gives the best approximation of the original parameters in low- rank subspace, the model accuracy may greatly degrade due to varying sensitivity of affected weights. Zhang et al. (2015) address this issue by considering the non-linear block and minimizing response of the layer. However, innate redundancies in the models are not exploited, which limits the compression rate and the speedup. Thus, we incorporate a retraining process for twofold beneﬁts: recover the model accuracy and exploit redundancy within the CNN structure through coefﬁcient sparsity regularization. The objective of retraining phase is to minimize the loss: L′= L(Θ,X,Y ) + γ ∑ l clcl+1∑ i ||α(l) i ||1, (5) where the ﬁrst term is the original loss of the model (i.e., cross entropy loss), the second term is the sum of the co- efﬁcients magnitude and γ is the strength of the sparsity regularization. If we visualize these two parameter sets as separate lay- ers, it conceptually increases the depth of the model and makes it harder to converge. Thus, in the training process, we generate the reconstructed parameter ˆθfrom the basis and coefﬁcients and compute the gradients as the original convolutional layer. The chain rule can then be applied to derive the gradients of the basis and the coefﬁcients from the original convolutional layer’s gradients. Speciﬁcally, ∂L′ ∂B = (∂L′ ∂ˆθ )TA, ∂L′ ∂A = ∂L′ ∂ˆθ BT + γ, (6) where the ˆθ= ABT and A ∈Rclcl+1×d is the coefﬁcient matrix. Again, we omit the superscript lfor readability. The gradient of coefﬁcient matrix consists of two terms. The ﬁrst term pushes the coefﬁcient towards the direction that decreases the error, while the second term coerces the reconstructed kernels to be close to the basis kernels. If we jointly train the basis and coefﬁcients, the coefﬁcients will be updated based on the old basis and vice versa. Jointly training both makes it very difﬁcult for the model to con- verge, producing further accuracy drop. We avoid this issue by conducting retraining in an alternating fashion, i.e., freez- ing the coefﬁcients and train the basis for several epochs and then freezing the basis and train coefﬁcients. The decomposed manner can also beneﬁt the sparsity regu- larization. Since l1-regularizer is non-smooth, it equates to adding a constant to the gradient, which dominates the gra- dient in the later stage of training. This causes the training process to be very unstable or unable to converge. Regu- larization on the decomposed ﬁlter state avoids this issue. Examining the gradient from the original weight’s perspec- tive, the regularization constant is essentially scaled as a consequence of being applied only to the coefﬁcients. This scaling factor decreases the constant proportional to the di- minishing gradients. The constant is still within the same magnitude of the gradient of the loss term, thus stabilizing the process of converging to a sparse parameter set.PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference 3.4. Model Shrinking Retraining the ﬁlter-decomposed model with sparsity reg- ularization results in predominantly near-zero coefﬁcients. As shown in (Han et al., 2015), we can select a threshold based on the standard deviation of each coefﬁcient matrix and prune all weight values with a magnitude lower than the threshold. Only a few epochs of coefﬁcient ﬁne-tuning is required to recover accuracy lost by pruning. A combination of high sparsity level and low accuracy loss can be achieved without any additional iterations. The sparse coefﬁcients expose redundancies in CNN struc- tures that can be utilized to shrink the model. Model shrink- age begins with reshaping the coefﬁcient matrix θ(l) into the shape cl ×cl+1 ×k′. By selecting the ﬁrst dimension (i.e., the input channels) and summing the number of the non-zero elements of the remaining two dimensions, we can obtain a vector p(l) i with cl elements. Zeros in p(l) i indicate that corresponding input channels are redundant since no output channels are connected. Indices of these channels can be represented by the set P(l) i . Selecting the second dimension (i.e., the output channels) and conducting the same procedure, we can get p(l) o and P(l) o , which indicate re- dundant output channels. The redundancies in basis kernels can also be derived with the same procedure. Note that it is possible for redundancies of a layer’s input and output channels to not match. We can exploit this fea- ture by considering the connections between input channels and redundant output channels of the same layer. If some input channels only have connections to redundant output channels, these inputs consequentially become redundant. Thus, we iteratively update the redundancy sets by applying the following steps. First, we take the union of the current layer’s output channels with the next layer’s input chan- nels, i.e., P(l) o ←−P(l+1) i ←−P(l) i ∪P(l+1) o . Then, we update θ(l) by setting all corresponding coefﬁcients in Po and Pi to zero and deriving new redundancy vectors and sets. This procedure, depicted in Fig. 2, is repeated until no modiﬁcation is made in an iteration. Layer  Layer + 1 ( + ) ( ) Layer  Layer + 1 Update Coeﬃcients Figure 2.Model shrinking procedure. The blank items in P(l) o and P(l+1) i represents the redundant channels, while the shaded items denote the difference of the two redundant sets. A potential problem with the iterative θ(l) update procedure is when it is applied to CNN architectures with skip con- nections, such as ResNet (He et al., 2016). Speciﬁcally, dimensions of pruned output feature maps might be incon- sistent with corresponding skip connections. The solution to this issue is straightforward. If the shortcut path has a dimension-matching operation (i.e., 1×1 convolution), we update the output channel of the 1×1 convolution and the current layer by taking the intersection of their redundancy sets. If the shortcut path has no such operation, we will need to update the redundancy sets of the start and the end of the skip connection before updating the coefﬁcients. Layer 𝑙𝑙 Layer 𝑛𝑛+ 1 …1x1 Conv Layer 𝑛𝑛 Layer 𝑙𝑙 Layer 𝑛𝑛+ 1 …1x1 Conv Layer 𝑛𝑛 Layer 𝑙𝑙 Layer 𝑛𝑛+ 1 … 1x1 Conv Layer 𝑛𝑛Intersection Union (a) With dimension matching component. Layer 𝑙𝑙 Layer 𝑛𝑛+ 1 … Layer 𝑙𝑙+ 1 Layer 𝑛𝑛 Layer 𝑙𝑙 Layer 𝑛𝑛+ 1 … Layer 𝑙𝑙+ 1 Layer 𝑛𝑛 Layer 𝑙𝑙 Layer 𝑛𝑛+ 1 … Layer 𝑙𝑙+ 1 Layer 𝑛𝑛Union Intersection (b) Without dimension matching component. Figure 3.Shrinking a model containing skip connections. The shaded items represent the difference of the redundant sets in each step. The corresponding items will be eliminated (added) in the intersection (union) step. 3.5. Hardware Beneﬁt The decisive advantage of PENNI over previous CNN prun- ing, compression, or ﬁlter decomposition methods is its potential for synergistic reduction of memory and compu- tational footprints. PENNI directly leverages ﬁlter decom- position by enabling a partition of the convolution step into two distinct stages. The ﬁrst stage involves channel-by-channel convolutions with each of the dtwo-dimensional basis kernels, producing cld intermediate feature maps; this stage is analogous to to depthwise separable convolution (Chollet, 2017) with d branches. Each branch duplicates one of the basis kernels across the cl input channels. Applying such a technique greatly reduces the number of multiply-and-accumulates (MACs) in the convolution step, which is the bottleneck inPENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference convolutional layers. The second stage is a weighted sum to produce the con- volutional layer’s output feature map. Speciﬁcally, cldin- termediate feature maps are multiplied element-wise with the coefﬁcient matrix and then accumulated at the output. As described in Section 3.4, the coefﬁcient matrices are incredibly sparse; therefore, we reduce the model’s memory footprint and prevent redundant zero-multiply computations by representing the coefﬁcients through a sparse matrix for- mat. Although this stage introduces additional computations that offset the reduction in MACs from the ﬁrst stage, the overall number of computations is dramatically reduced, thus improving inference latency. Beyond the aforementioned straightforward beneﬁts of the proposed two-stage convolutional layer scheme, PENNI also offers a unique attribute that can be leveraged for current and future hardware accelerator designs. The determinis- tic convolutional kernel structure means that the number of basis kernels can be altered to ﬁt nicely with the num- ber of processing elements (PEs) in accelerators such as DaDianNao (Chen et al., 2014) without forcing the model to conform to the hardware (e.g. reducing layer width). Meanwhile, the weighted sum stage can be computed in a streaming manner, much favored by single-instruction, multiple-data (SIMD) processors. Also, because data ac- cess patterns of convolutional layers conventionally require hardware-speciﬁc data-reuse algorithms to minimize costly cache evictions, removing interactions of the input channels at the convolution step via depthwise separation alleviates hardware complexity. Lastly, partitioning the convolution step to two stages opens the avenue for further accelerator- based throughput optimizations such as pipelining. 4. Experiments In this section, we demonstrate the effectiveness of the pro- posed framework. Experiments were held on CIFAR10 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009) datasets. Experiment settings are detailed before comparing compression results between PENNI and both state-of-the- art channel pruning and weight pruning methods. Finally, we conduct an ablation study to show the contribution of each component in the framework. 4.1. Experiment Settings CIFAR10 On CIFAR-10, we chose VGG16 (Simonyan & Zisserman, 2014), ResNet18 and ResNet56 (He et al., 2016) for experimentation. VGG16 is a small version tailored for CIFAR10. We use ResNet56 to test the performance on compact models. Model training involved the following data preprocessing steps: random ﬂipping, random crop- ping with 4 pixels padding, and normalization. The VGG16 and ResNet18 models were ﬁrst pretrained for 100 epochs with 0.1 initial learning rate; then, the learning rate was multiplied by 0.1 at 50% and 75% epochs, while ResNet56 was pretrained for 250 epochs with the same learning rate scheduling. All pretraining, retraining and ﬁne-tuning pro- cedures implemented Stochastic Gradient Descent (SGD) as the optimizer with 10−4 weight decay, 0.9 momentum, and batch size set to 128. We selected d = 5 for the de- composition stage and retrained for 100 epochs with 0.01 initial learning rate and the same scheduling. Regularization strength was set to γ = 10−4. The interval between training basis and coefﬁcients was set to 5 epochs. The ﬁnal ﬁne- tuning procedure took 30 epochs with 0.01 initial learning rate and the same scheduling scheme. ImageNet On ImageNet, we used AlexNet (Krizhevsky et al., 2012) and ResNet50 for the experiment, incorporating the pretrained models provided by PyTorch (PyTorch, 2019). Since AlexNet has different kernel sizes across layers, we selected d= 64 and 14 for the ﬁrst two convolutional lay- ers, and d = 5 for the rest 3×3 convolutional layers. For ResNet50, we use 4 sets of parameter settings, withd= 5,6 and regularization strength set to 5e−5 and 1e−4. The retraining procedure lasted 50 epochs with the same hyper- parameters as CIFAR10 but set batch size to 256 and cosine annealing. For AlexNet, we warmed up with a learning rate of 0.0001 for ﬁve epochs; then, the learning rate was set to 0.001 for the remaining 45 epochs. The ﬁne-tune procedure took 30 epochs with learning rate set to 0.01 for ResNet50 and 0.0001 for AlexNet. 4.2. CIFAR10 Results We selected channel pruning methods PFEC (Li et al., 2016), Slimming (Liu et al., 2017), SFP (He et al., 2018a), AOFP (Ding et al., 2019b), C-SGD (Ding et al., 2019a), FPGM (He et al., 2019) and Group-HoyerSquare (Yang et al., 2019) for comparison. For the works providing parameter trade- offs, we use results with similar accuracy drop. The results are shown in Table 1. ‘Ours-D’ denotes the compression result with only decomposition and retraining phases, while ‘Ours-P’ incorporates all four phases. We only consider the parameters of the convolutional and linear layers, and the FLOP count is taken by calculating the number of Multiply- Accumulation (MAC) operations. Based on the computation ﬂow described in Section 3.5, we consider that the sparsity of coefﬁcient matrix can be converted to reduction in FLOPs. Thus, we ignore the zeros in the coefﬁcient matrices when counting FLOPs. On VGG16, we outperformed channel pruning methods by achieving a reduction over 98% on parameters and 93.26% on FLOPs. Although there is a slightly higher accuracy drop, it is only 0.15% behind AOFP with 10% extra reduction on FLOPs and 0.42% behind Slimming with almost double reduction on FLOPs, whichPENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference Table 1.Compression Result on CIFAR10. ‘Ours-D’denotes the result with only the decomposition and retraining (i.e., phase A and phase B in Figure 1), while ‘Ours-P’incorporates the pruning and model shrinkage based on the ‘Ours-D’model. ‘-’ denotes unavailable data from the original paper. Arch Method Base Acc. Pruned Acc. ∆Acc Param. RParam. FLOPs RFLOPs VGG16 Baseline 93.49% - - 14.71M - 314.26M - PFEC 93.25% 93.40% -0.15% 5.4M 64% 206M 34.2% Slimming 93.62% 93.56% -0.06% 1.77M 87.97% 127M 43.50% AOFP 93.38% 93.28% -0.10% - - 77M 75.27% Ours-D 93.49% 93.14% -0.35% 183.4M 44.44% 183.4M 41.65% Ours-P 93.49% 93.12% -0.37% 0.135M 98.33% 21.19M 93.26% ResNet18 Baseline 93.77% - - 11.16M - 555.43M - Ours-D 93.77% 93.89% +0.12% 6.28M 56.27% 332.34M 40.17% Ours-P 93.77% 94.01% +0.24% 0.341M 96.94% 44.98M 91.90 % ResNet56 Baseline 93.57% - - 0.848M - 125.49M - PFEC 93.04% 93.06% +0.02% 0.73M 13.7% 90.9M 27.6% SFP 93.59% 93.35% -0.24% - - 59.4M 52.67% C-SGD 93.39% 93.44% +0.05% - - - 60.85% FPGM 93.59% 93.49% -0.10% - - 59.4M 52.67% Group-HS 93.14% 93.45% +0.31% - - - 68.43% Ours-D 93.57% 94.00% +0.43% 0.471M 44.46% 92.80M 26.15% Ours-P 93.57% 93.38% -0.19% 39.37K 95.36% 28.98M 79.40% is acceptable. Since ResNet18 is originally designed for the ImageNet dataset, no previous work has provided result for comparison. We include it in this paper to show that PENNI is able to shrink over-parameterized models and may improve accuracy. On ResNet56, which is a compact model specially tailored for CIFAR10, we can still prune 94.52% parameters and 76.9% FLOPS with 0.2% accuracy drop. Our method outperformed previous channel pruning methods by a nearly 20% extra FLOPs reduction, and a 10% extra reduction over the group regularization method. 4.3. ImageNet Results Table 2.Compression Result of AlexNet on ImageNet. Method Top-1 Top-5 FLOPs RFLOPs Baseline 56.51% 79.07% 773M - AOFP 56.17% 79.53% 492M 41.33% Ours-D 55.41% 78.30% 573M 25.88% Ours-P 55.57% 78.32% 232M 70.04% On ImageNet, we chose Slimming, ThiNet (Luo et al., 2017), SFP, AOFP, C-SGD and FPGM for comparison. The result of AlexNet compression is shown in Table 2. We can prune 70.04% FLOPs with 1% loss on top-1 accuracy. For ResNet50, we observe the 1×1 convolutional layer of the bottleneck block as the coefﬁcient matrix with 1-D basis and apply regularization to it. Table 3 shows the result on ResNet50 compression. We use multiple parameter settings 1Computed based on the reduction percentage reported by orig- inal paper. Table 3.Compression Result of ResNet50 on ImageNet. We cate- gorize the results by accuracy. Method Top-1 Top-5 FLOPs RFLOPs Baseline 76.13% 92.86% 4.09G - Ours-D 76.20% 92.85% 3.23G 21.10% ThiNet-70 72.02% 90.67% 2.58G 1 36.80% Ours-R1 73.87% 91.79% 220M 94.73% SFP 74.61% 92.06% 2.38G 1 41.80% C-SGD-50 74.54% 92.09% 1.81G 1 55.76% Ours-R2 74.74% 92.27% 527M 87.12% C-SGD-60 74.93% 92.27% 2.20G 1 46.24% FPGM-40% 74.83% 92.32% 1.90G1 53.50% Ours-R3 75.00% 92.21% 576M 85.92% AOFP-C1 75.63% 92.69% 2.58G 32.88% AOFP-C2 75.11% 92.28% 1.66G 56.73% C-SGD-70 75.27% 92.46% 2.59G 1 36.75% FPGM-30% 75.59% 92.63% 2.36G 1 42.20% Ours-R4 75.66% 92.79% 768M 81.23% to justify the trade-off between accuracy and compression rate. ‘Ours-D’ only involves decomposition and retraining step with d= 5, while ‘R1’ and ‘R2’ incorprate pruning and shrinking phases with regularization strength set to 1e−4 and 5 −e5. ‘R3’ and ‘R4’ has the same parameter apart from setting d= 6 in the decomposition phase. The results show that the decomposition step can reduce more than 20% of the FLOPs with no accuracy drop. With the pruning and shrinking procedures, 94.73% of the FLOPS can be pruned with 2.4% top-1 accuracy loss. When we relax the regular- ization, we can still prune 81.23% of the FLOPs with only 0.5% accuracy loss. The FLOPs reduction is nearly twoPENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference times the reduction of previous channel pruning methods. A even larger compression rate can be achieved by combining the 1×1 convolutional layer with the coefﬁcient matrices. 4.4. Inference Acceleration Table 4.Measured inference performance of VGG16-CIFAR10 on different devices. Device Variation Latency(ms) Memory(MB) CPU Baseline 12.9 137 PENNI 5.96 77.6 GPU Baseline 10.8 487 PENNI 7.26 424 Hardware Settings We used Intel Xeon Gold 6136 to test the inference performance for CPU platform and NVIDIA Titan X for the GPU platform. For software, we used Py- Torch 1.4 (Paszke et al., 2019) to implement the inference test. Batch size was set to 128 (1) for inference testing on the GPU (CPU). GPU inference batch size is higher than CPU to increase utilization and emphasize the latency impact of our method on the highly parallel platform. We indicate these settings as latency and peak memory consumption values vary across platforms or library versions. Table 4 displays inference latencies and memory consump- tion recorded for the baseline and PENNI framework. As mentioned in 3.5, one of PENNI’s deﬁning strengths is its impact on computational and memory footprints. Results shown in Table 4 reveal a 1.5 ×(2.2×) reduction in mea- sured inference latency on the GPU (CPU). Peak memory consumption also beneﬁted from a 1.1×(1.8×) reduction. It is important to note that these metrics were taken with- out applying the convolution computation reorganization described in 3.5; this is done intentionally to reveal the ef- fectiveness of our model shrinkage with zero changes to the hardware and inference-time computation. The reduction in memory is a straightforward consequence of the decom- position and shrinking stages of the PENNI framework. Al- though our method is successful at dramatically decreasing model size in memory, intermediate feature maps seems to dominate on-device memory consumption, especially with a batch size of 128 on the GPU. 4.5. Subspace Dimension To justify the selection of the parameter d, we conduct an experiment with different decompose dimensions. We used the same VGG16 baseline model and hyper-parameters as 4.2. The result is shown in Fig.4 indicates that the remaining FLOPs scales linearly with the number of basis kernels. This is expected since the number of convolutional operations is determined by d. The parameters scale linearly before 1 2 3 4 5 6 7 8 Number Of Basis Kernels 0.0 0.1 0.2 0.3 0.4 0.5 0.6Remain Parameters\\% 0 2 4 6 8 Remain FLOPs\\% Parameters FLOPs 1 2 3 4 5 6 7 8 Number Of Basis Kernels 0.82 0.84 0.86 0.88 0.90 0.92 0.94Test Accuracy Decomposed baseline Figure 4.Test accuracy, parameters and computation reduction with different number of basis kernels d. 6-D basis and have minor difference with the increasing dimension. This is because even though more basis vector requires more coefﬁcients, it also adds ﬂexibility and thus leads to sparser coefﬁcients. The test accuracy reveals the same trend, with d≥4, minor improvement on the accuracy can be brought by increasing d. Thus, we select d= 5 for the balance between parameter and FLOPs reduction and accuracy drops. 4.6. Model Shrinking We show the effectiveness of model shrinking by compar- ing layer widths. As shown in Figure 5, on VGG16, the model shrinking procedure effectively removes redundant channels in the second half of all layers. Meanwhile, on ResNet56, the shrinking is limited by the dimension match- ing requirement of the skip connections. The oscillation pattern of the layer width indicates that redundancies of the inner-block layer can be effectively exploited. These results show that PENNI can beneﬁt unmodiﬁed inference software and hardware by exploiting structural redundancies. 5. Conclusion This work proposes the PENNI framework for hardware- friendly CNN model compression. Our method improves inference latency with no changes to inference algorithms and hardware via model shrinking, thus translating model sparsity to speedup. A low rank assumption is used to de- compose CNN ﬁlters into basis kernels and prune the result- ing coefﬁcient matrices, which results in structured sparsity.PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference (a) VGG16-CIFAR10 (b) ResNet56 Figure 5.Layer width after the model shrinking. A novel alternating ﬁne-tuning method is used to further increase sparsity and improve model performance. Unique characteristics generated by the decomposition step may be leveraged for hardware efﬁciency via convolution com- putation reorganization, directly beneﬁting modern DNN platforms. Acknowledgment This work was supported in part by NSF-1937435, NSF- 1822085, NSF-1725456, ARO W911NF-19-2-0107, and NSF IUCRC for ASIC memberships from Cadence etc. References Cai, H., Zhu, L., and Han, S. ProxylessNAS: Direct neu- ral architecture search on target task and hardware. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=HylVB3AqYm. Chen, Y ., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu, Z., Sun, N., et al. Dadiannao: A machine- learning supercomputer. In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, pp. 609– 622. IEEE, 2014. Chollet, F. Xception: Deep learning with depthwise separa- ble convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1251– 1258, 2017. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Denton, E. L., Zaremba, W., Bruna, J., LeCun, Y ., and Fer- gus, R. Exploiting linear structure within convolutional networks for efﬁcient evaluation. In Advances in neural information processing systems, pp. 1269–1277, 2014. Ding, X., Ding, G., Guo, Y ., and Han, J. Centripetal sgd for pruning very deep convolutional networks with compli- cated structure. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4943– 4953, 2019a. Ding, X., Ding, G., Guo, Y ., Han, J., and Yan, C. Ap- proximated oracle ﬁlter pruning for destructive cnn width optimization. In International Conference on Machine Learning, pp. 1607–1616, 2019b. Guo, Y ., Yao, A., and Chen, Y . Dynamic network surgery for efﬁcient dnns. In Advances in neural information processing systems, pp. 1379–1387, 2016. Han, S., Mao, H., and Dally, W. J. Deep compres- sion: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. Hassibi, B. and Stork, D. G. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in neural information processing systems, pp. 164–171, 1993. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. He, Y ., Kang, G., Dong, X., Fu, Y ., and Yang, Y . Soft ﬁlter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018a. He, Y ., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S. Amc: Automl for model compression and acceleration on mo- bile devices. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 784–800, 2018b. He, Y ., Liu, P., Wang, Z., Hu, Z., and Yang, Y . Filter pruning via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4340–4349, 2019. Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., and Keutzer, K. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016. Kim, Y .-D., Park, E., Yoo, S., Choi, T., Yang, L., and Shin, D. Compression of deep convolutional neural networks for fast and low power mobile applications.arXiv preprint arXiv:1511.06530, 2015. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012. Lebedev, V ., Ganin, Y ., Rakhuba, M., Oseledets, I., and Lempitsky, V . Speeding-up convolutional neural net- works using ﬁne-tuned cp-decomposition. arXiv preprint arXiv:1412.6553, 2014. LeCun, Y ., Denker, J. S., and Solla, S. A. Optimal brain damage. In Advances in neural information processing systems, pp. 598–605, 1990. Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H. P. Pruning ﬁlters for efﬁcient convnets. arXiv preprint arXiv:1608.08710, 2016. Lin, M., Chen, Q., and Yan, S. Network in network. arXiv preprint arXiv:1312.4400, 2013. Liu, H., Simonyan, K., and Yang, Y . DARTS: Differ- entiable architecture search. In International Confer- ence on Learning Representations, 2019a. URL https: //openreview.net/forum?id=S1eYHoC5FX. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C. Learning efﬁcient convolutional networks through net- work slimming. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2736–2744, 2017. Liu, Z., Mu, H., Zhang, X., Guo, Z., Yang, X., Cheng, K.-T., and Sun, J. Metapruning: Meta learning for automatic neural network channel pruning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3296–3305, 2019b. Louizos, C., Welling, M., and Kingma, D. P. Learning sparse neural networks through l 0 regularization. arXiv preprint arXiv:1712.01312, 2017. Luo, J.-H., Wu, J., and Lin, W. Thinet: A ﬁlter level pruning method for deep neural network compression. InProceed- ings of the IEEE international conference on computer vision, pp. 5058–5066, 2017. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024–8035, 2019. PyTorch. Torchvision models, 2019. URL https://github.com/pytorch/vision/ tree/master/torchvision/models. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510–4520, 2018. Simonyan, K. and Zisserman, A. Very deep convolu- tional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., and Rabinovich, A. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015. Szegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vi- sion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. Tai, C., Xiao, T., Zhang, Y ., Wang, X., and Weinan, E. Con- volutional neural networks with low-rank regularization. In 4th International Conference on Learning Representa- tions, ICLR 2016, 2016. Tan, M. and Le, Q. Efﬁcientnet: Rethinking model scal- ing for convolutional neural networks. In International Conference on Machine Learning, pp. 6105–6114, 2019. Tan, M., Chen, B., Pang, R., Vasudevan, V ., Sandler, M., Howard, A., and Le, Q. V . Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2820–2828, 2019. Ullrich, K., Meeds, E., and Welling, M. Soft weight- sharing for neural network compression. arXiv preprint arXiv:1702.04008, 2017. Wen, W., Wu, C., Wang, Y ., Chen, Y ., and Li, H. Learning structured sparsity in deep neural networks. In Advances in neural information processing systems, pp. 2074–2082, 2016. Xie, S., Girshick, R., Doll´ar, P., Tu, Z., and He, K. Aggre- gated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, pp. 1492–1500, 2017.PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference Yang, H., Wen, W., and Li, H. Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures. arXiv preprint arXiv:1908.09979, 2019. Zagoruyko, S. and Komodakis, N. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. Zhang, D., Wang, H., Figueiredo, M., and Balzano, L. Learning to Share: Simultaneous Parameter Tying and Sparsiﬁcation in Deep Learning. In International Confer- ence on Learning Representations, 2018a. URL https: //openreview.net/forum?id=rypT3fb0b. Zhang, X., Zou, J., He, K., and Sun, J. Accelerating very deep convolutional networks for classiﬁcation and detec- tion. IEEE transactions on pattern analysis and machine intelligence, 38(10):1943–1955, 2015. Zhang, X., Zhou, X., Lin, M., and Sun, J. Shufﬂenet: An extremely efﬁcient convolutional neural network for mo- bile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6848–6856, 2018b.PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference Appendix Visualization of coefﬁcient matrix The coefﬁcient matrix is shown in Fig.6. By conducting retraining, pruning and ﬁne-tuning process, both weight and structured redundancies are explored. 0 100 200 300 400 500 600 0 20 40 60 80 100 120 (a) Decomposed 0 100 200 300 400 500 600 0 20 40 60 80 100 120 (b) Pruned Figure 6.The visualization of the coefﬁcient matrix. This is the third convolution layer of VGG16-CIFAR10 model with 128 input channels and 128 convolutional ﬁlters. The y-axis represents the output channels while the x-axis is the basis of each input channel. To compare with the baseline model, we reconstruct the weight matrix by multiplying the coefﬁcient matrix with the basis. The reconstructed weight matrix is shown in Fig.7. Although the sparsity is also shown in the reconstructed weight, more computation can be saved by using the decomposed convolution. Pruned Model Structure In this section, we list the detailed structure of the pruned model, including the number of basis kernels, the layer width before/after the shrinking process and the sparsity level of the coefﬁcient matrix. We only consider the convolutional layers. VGG16 on CIFAR10 We list the detailed structure of the VGG16 before and after the shrinking process on Table.5. We only consider all the convolutional layers. The ﬁrst index of the name represents the downsampling stage. The width represents the number of the ﬁlters (i.e., the number of output channels).PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference 0 200 400 600 800 1000 0 20 40 60 80 100 120 (a) Baseline 0 200 400 600 800 1000 0 20 40 60 80 100 120 (b) Decomposed 0 200 400 600 800 1000 0 20 40 60 80 100 120 (c) Pruned Figure 7.The visualization of the reconstructed weight matrix comparing with the baseline model. This is the third convolution layer of VGG16-CIFAR10 model with 128 input channels and 128 convolutional ﬁlters. The y-axis represents the output channels. Table 5.Detailed Structure of VGG16-CIFAR10 Name Size Before shrinking After shrinking Width Basis Coefﬁcients (Non-zero/Total) Sparsity/% Width basis Coefﬁcients (Non-zero/Total) Sparsity/% conv1 1 3 64 5 196/960 79.58 55 5 196/825 76.24 conv1 2 3 64 5 2313/20480 88.71 64 5 2238/17600 87.28 conv2 1 3 128 5 5862/40960 85.69 128 5 5862/40960 85.68 conv2 2 3 128 5 12052/81920 85.29 128 5 12052/81920 85.28 conv3 1 3 256 5 23169/163840 85.86 256 5 23169/163840 85.85 conv3 2 3 256 5 36870/327680 88.75 256 5 36870/327680 88.74 conv3 3 3 256 5 27719/327680 91.54 237 5 27716/303360 90.86 conv4 1 3 512 5 15688/65536 97.61 158 5 15665/187230 91.63 conv4 2 3 512 5 4534/1310720 99.65 62 5 4530/48980 90.75 conv4 3 3 512 5 2668/1310720 99.79 48 5 2666/14880 82.08 conv5 1 3 512 5 1318/1310720 99.89 36 5 1315/8640 84.78 conv5 2 3 512 5 874/1310720 99.93 26 5 874/4680 81.32 conv5 3 3 512 5 2554/1310720 99.80 486 5 2554/63180 95.95 Total 135817/8172480 98.34 135707/1263775 89.26 ResNet50 on ImageNet We list the detailed structure of ResNet50-R1 after shrinking process in Table.6. We only consider all the convolutional layers including the 1×1 convolution. ’L’ represents the downsampling stages while the ’B’ stands for the residual blocks. This table reveals that, although for some layer the reduction of the layer width is not signiﬁcant, the model can still beneﬁt from pruning basis kernels.PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference Table 6: Detailed Structure of ResNet50 after the shrinking process. Name Size width # of basis coefﬁcients (non-zero/total) sparsity/% conv1 7 59 5 128/885 85.53 L1B1.conv1 1 22 - 500/1289 61.48 L1B1.conv2 3 29 4 125/2552 95.1 L1B1.conv3 1 256 - 2223/7424 70.06 L1B1.downsample 1 256 - 2589/15104 82.86 L1B2.conv1 1 32 - 2636/8192 67.82 L1B2.conv2 3 34 5 349/5440 93.58 L1B2.con3 1 256 - 2301/8704 73.56 L1B3.conv1 1 31 - 2978/7936 62.47 L1B3.conv2 3 63 5 807/9756 91.74 L1B3.conv3 1 256 - 3395/16128 78.95 L2B1.conv1 1 88 - 5973/22528 73.49 L2B1.conv2 3 115 3 581/30360 98.09 L2B1.conv3 1 512 - 11939/58880 79.72 L2B1.downsample 1 512 - 23134/131072 82.35 L2B2.conv1 1 20 - 3778/10240 63.11 L2B2.conv2 3 76 4 334/6080 94.51 L2B2.conv3 1 512 - 8154/38912 79.05 L2B3.conv1 1 66 - 9898/33792 70.71 L2B3.conv2 3 98 4 497/25872 98.08 L2B3.conv3 1 512 - 10702/50176 78.67 L2B4.conv1 1 78 - 12508/39936 68.68 L2B4.conv2 3 95 4 1006/29640 96.61 L2B4.conv3 1 512 - 12680/48640 73.93 L3B1.conv1 1 240 - 29283/122880 76.17 L3B1.conv2 3 229 2 712/100920 99.35 L3B1.conv3 1 1024 - 57763/234496 75.37 L3B1.downsample 1 1024 - 117775/524288 77.54 L3B2.conv1 1 103 - 29283/122880 76.17 L3B2.conv2 3 182 5 866/93730 99.08 L3B2.conv3 1 1024 - 57763/234496 75.37 L3B3.conv1 1 94 - 35956/96256 62.65 L3B3.conv2 3 180 5 1222/84600 98.56 L3B3.conv3 1 1024 - 53268/184320 71.1 L3B4.conv1 1 135 - 49843/138240 63.94 L3B4.conv2 3 193 5 925/130275 99.29 L3B4.conv3 1 1024 - 58240/197632 70.53 L3B5.conv1 1 155 - 56279/158720 64.54 L3B5.conv2 3 197 4 1138/122140 99.07 L3B5.conv3 1 1024 - 60387/201728 70.07 L3B6.conv1 1 206 - 65496/210944 68.95 L3B6.conv2 3 217 4 1156/178808 99.35 L3B6.conv3 1 2024 - 65842/222208 70.37 L4B1.conv1 1 495 - 148203/506880 70.76 L4B1.conv2 3 484 1 546/239580 99.77 L4B1.conv3 1 2048 - 279619/991232 91.79 L4B1.downsample 1 2048 - 580155/2097152 72.34 L4B2.conv1 1 411 - 276731/841728 67.12PENNI: Pruned Kernel Sharing for Efﬁcient CNN Inference Table 6 continued from previous page Nam Size width # of basis coefﬁcients (non-zero/total) sparsity/% L4B2.conv2 3 445 4 1334/731580 99.82 L4B2.conv3 1 2048 - 275688/911360 69.75 L4B3.conv1 1 501 - 307600/1026048 70.02 L4B3.conv2 3 484 3 1076/727452 99.85 L4B3.conv3 1 2048 - 245290/991232 75.25 total 2.98M/12.98M 77.0 Learning Curves We show the learning curve of both the retraining and ﬁne-tuning phase of the ResNet-50 model on ImageNet dataset. The curve here belongs to the ‘R4’ setting, which isd= 6 and regularization strength λ= 5e−5 0 10 20 30 40 50 Epochs 70 72 74 76 78 80 82Accuracy train acc test acc 0 5 10 15 20 25 30 Epochs 70 72 74 76 78Accuracy train acc test acc Figure 8.The learning curve of the (left) retraining phase and (right) ﬁne-tuning phase. The model is ResNet-50 with parameter setting R4.",
      "references": [
        "ProxylessNAS: Direct neural architecture search on target task and hardware.",
        "Dadiannao: A machine-learning supercomputer.",
        "Xception: Deep learning with depthwise separable convolutions.",
        "Imagenet: A large-scale hierarchical image database.",
        "Exploiting linear structure within convolutional networks for efficient evaluation.",
        "Centripetal sgd for pruning very deep convolutional networks with complicated structure.",
        "Approximated oracle filter pruning for destructive cnn width optimization.",
        "Dynamic network surgery for efficient dnns.",
        "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.",
        "Second order derivatives for network pruning: Optimal brain surgeon.",
        "Soft filter pruning for accelerating deep convolutional neural networks.",
        "Amc: Automl for model compression and acceleration on mobile devices.",
        "Filter pruning via geometric median for deep convolutional neural networks acceleration.",
        "Mobilenets: Efficient convolutional neural networks for mobile vision applications.",
        "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size.",
        "Compression of deep convolutional neural networks for fast and low power mobile applications.",
        "Learning multiple layers of features from tiny images.",
        "Imagenet classification with deep convolutional neural networks.",
        "Speeding-up convolutional neural networks using fine-tuned cp-decomposition.",
        "Optimal brain damage.",
        "Pruning filters for efficient convnets.",
        "Network in network.",
        "DARTS: Differentiable architecture search.",
        "Learning efficient convolutional networks through network slimming.",
        "Metapruning: Meta learning for automatic neural network channel pruning.",
        "Learning sparse neural networks through l 0 regularization.",
        "Thinet: A filter level pruning method for deep neural network compression.",
        "Pytorch: An imperative style, high-performance deep learning library.",
        "Torchvision models",
        "Mobilenetv2: Inverted residuals and linear bottlenecks.",
        "Very deep convolutional networks for large-scale image recognition.",
        "Going deeper with convolutions.",
        "Rethinking the inception architecture for computer vision.",
        "Convolutional neural networks with low-rank regularization.",
        "Efficientnet: Rethinking model scaling for convolutional neural networks.",
        "Mnasnet: Platform-aware neural architecture search for mobile.",
        "Soft weight-sharing for neural network compression.",
        "Learning structured sparsity in deep neural networks.",
        "Aggregated residual transformations for deep neural networks.",
        "Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures.",
        "Wide residual networks.",
        "Accelerating very deep convolutional networks for classification and detection.",
        "Learning to Share: Simultaneous Parameter Tying and Sparsification in Deep Learning.",
        "Shufflenet: An extremely efficient convolutional neural network for mobile devices."
      ],
      "meta_data": {
        "arxiv_id": "2005.07133v2",
        "authors": [
          "Shiyu Li",
          "Edward Hanson",
          "Hai Li",
          "Yiran Chen"
        ],
        "published_date": "2020-05-14T16:57:41Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes PENNI, a hardware-friendly CNN compression framework that shares a small set of basis kernels across filters, enforces sparsity on the associated coefficients, and performs iterative model shrinking, achieving >90% reduction in parameters, FLOPs, memory, and inference latency with negligible accuracy loss.",
        "methodology": "1) Kernel-granularity low-rank decomposition via SVD to obtain d shared basis kernels and a coefficient matrix per layer; 2) Alternating retraining of basis and coefficients with l1 regularization on coefficients to induce sparsity; 3) Magnitude-based pruning of near-zero coefficients; 4) Iterative channel-level model shrinking that propagates redundancy across layers (including handling skip connections); 5) Hardware-oriented execution that separates depthwise convolutions on bases from sparse weighted sums, enabling efficient parallel or streaming computation.",
        "experimental_setup": "Datasets: CIFAR-10 (VGG16, ResNet18, ResNet56) and ImageNet (AlexNet, ResNet50). Pretrained models fine-tuned using SGD with standard data augmentation. Hyper-parameters: d=5 (CIFAR); d={5,6}, γ∈{1e-4,5e-5} (ImageNet). Metrics: top-1/ top-5 accuracy, parameter count, FLOPs (MACs), peak memory, wall-clock latency on Intel Xeon 6136 CPU and NVIDIA Titan X GPU. Benchmarks include comparisons with pruning baselines such as PFEC, Slimming, SFP, FPGM, AOFP, C-SGD, ThiNet. Key results: 98.3% parameter and 93.3% FLOP reduction on VGG16-CIFAR10 with 0.37% accuracy drop; 94.7% FLOP reduction on ResNet50-ImageNet with 2.4% top-1 drop; 1.5×–2.2× latency and 1.1×–1.8× memory reduction without specialized kernels.",
        "limitations": "Requires SVD and alternating retraining, introducing extra complexity and compute during compression. Choice of basis dimension d and sparsity weight γ needs manual tuning per model. Gains rely on effective sparse matrix support in deployment backend; two-stage convolution may incur overhead if not optimized. Evaluation limited to standard CNNs on image classification; other architectures or tasks untested. No ASIC implementation—hardware benefits are estimated or measured only on general-purpose CPU/GPU.",
        "future_research_directions": "Automate hyper-parameter selection (d, γ) via reinforcement or Bayesian search; extend method to other layer types (depthwise, dilated, transformer attention) and tasks (detection, NLP). Combine with quantization or knowledge distillation for further compression. Develop custom accelerators that exploit the fixed shared-basis structure. Investigate dynamic or adaptive basis selection at runtime for on-device learning. Explore joint network architecture search that incorporates shared-kernel decomposition during design.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Efficient Learning of CNNs using Patch Based Features",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Laplacian Regularized Few-Shot Learning",
      "full_text": "Laplacian Regularized Few-Shot Learning Imtiaz Masud Ziko 1 Jose Dolz 1 Eric Granger 1 Ismail Ben Ayed1 Abstract We propose a transductive Laplacian-regularized inference for few-shot tasks. Given any feature embedding learned from the base classes, we minimize a quadratic binary-assignment function containing two terms: (1) a unary term assign- ing query samples to the nearest class prototype, and (2) a pairwise Laplacian term encouraging nearby query samples to have consistent label as- signments. Our transductive inference does not re-train the base model, and can be viewed as a graph clustering of the query set, subject to super- vision constraints from the support set. We derive a computationally efﬁcient bound optimizer of a relaxation of our function, which computes inde- pendent (parallel) updates for each query sample, while guaranteeing convergence. Following a sim- ple cross-entropy training on the base classes, and without complex meta-learning strategies, we con- ducted comprehensive experiments over ﬁve few- shot learning benchmarks. Our LaplacianShot consistently outperforms state-of-the-art methods by signiﬁcant margins across different models, settings, and data sets. Furthermore, our trans- ductive inference is very fast, with computational times that are close to inductive inference, and can be used for large-scale few-shot tasks. 1. Introduction Deep learning models have achieved human-level perfor- mances in various tasks. The success of these models rely considerably on exhaustive learning from large-scale labeled data sets. Nevertheless, they still have difﬁculty general- izing to novel classes unseen during training, given only a few labeled instances for these new classes. In contrast, humans can learn new tasks easily from a handful of ex- amples, by leveraging prior experience and related context. 1 ´ETS Montreal, Canada. Correspondence to: Imtiaz Masud Ziko <imtiaz-masud.ziko.1@etsmtl.ca>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Few-shot learning (Fei-Fei et al., 2006; Miller et al., 2000; Vinyals et al., 2016) has emerged as an appealing paradigm to bridge this gap. Under standard few-shot learning scenar- ios, a model is ﬁrst trained on substantial labeled data over an initial set of classes, often referred to as the base classes. Then, supervision for novel classes, which are unseen during base training, is limited to just one or few labeled exam- ples per class. The model is evaluated over few-shot tasks, each one supervised by a few labeled examples per novel class (the support set) and containing unlabeled samples for evaluation (the query set). The problem has recently received substantial research in- terests, with a large body of work based on complex meta- learning and episodic-training strategies. The meta-learning setting uses the base training data to create a set of few-shot tasks (or episodes), with support and query samples that simulate generalization difﬁculties during test times, and train the model to generalize well on these artiﬁcial tasks. For example, (Vinyals et al., 2016) introduced matching network, which employs an attention mechanism to pre- dict the unknown query samples as a linear combination of the support labels, while using episodic training and memory architectures. Prototypical networks (Snell et al., 2017) maintain a single prototype representation for each class in the embedding space, and minimize the negative log-probability of the query features with episodic training. Ravi & Larochelle (2017) viewed optimization as a model for few-shot learning, and used an LSTM meta-learner to update classiﬁer parameters. Finn et al. (2017) proposed MAML, a meta-learning strategy that attempts to make a model “easy” to ﬁne-tune. These widely adopted works were recently followed by an abundant meta-learning litera- ture, for instance, (Sung et al., 2018; Oreshkin et al., 2018; Mishra et al., 2018; Rusu et al., 2019; Liu et al., 2019b; Hou et al., 2019; Ye et al., 2020), among many others. Several recent studies explored transductive inference for few-shot tasks, e.g., (Liu et al., 2019b; Hou et al., 2019; Dhillon et al., 2020; Hu et al., 2020; Kim et al., 2019; Qiao et al., 2019), among others. Given a few-shot task at test time, transductive inference performs class predic- tions jointly for all the unlabeled query samples of the task, rather than one sample at a time as in inductive inference. For instance, TPN (Liu et al., 2019b) used label propaga- tion (Zhou et al., 2004) along with episodic training and a arXiv:2006.15486v3  [cs.LG]  28 Apr 2021Laplacian Regularized Few-Shot Learning speciﬁc network architecture, so as to learn how to prop- agate labels from labeled to unlabeled samples. CAN-T (Hou et al., 2019) is another meta-learning based transduc- tive method, which uses attention mechanisms to propagate labels to unlabeled query samples. The transductive ﬁne- tuning method by (Dhillon et al., 2020) re-train the network by minimizing an additional entropy loss, which encour- ages peaked (conﬁdent) class predictions at unlabeled query points, in conjunction with a standard cross-entropy loss deﬁned on the labeled support set. Transductive few-shot methods typically perform better than their inductive counterparts. However, this may come at the price of a much heavier computational complexity during inference. For example, the entropy ﬁne-tuning in (Dhillon et al., 2020) re-trains the network, performing gradient up- dates over all the parameters during inference. Also, the label propagation in (Liu et al., 2019b) requires a matrix inversion, which has a computational overhead that is cubic with respect to the number of query samples. This may be an impediment for deployment for large-scale few-shot tasks. We propose a transductive Laplacian-regularized inference for few-shot tasks. Given any feature embedding learned from the base data, our method minimizes a quadratic binary-assignment function integrating two types of poten- tials: (1) unary potentials assigning query samples to the nearest class prototype, and (2) pairwise potentials favor- ing consistent label assignments for nearby query samples. Our transductive inference can be viewed as a graph clus- tering of the query set, subject to supervision constraints from the support set, and does not re-train the base model. Following a relaxation of our function, we derive a compu- tationally efﬁcient bound optimizer, which computes inde- pendent (parallel) label-assignment updates for each query point, with guaranteed convergence. We conducted compre- hensive experiments on ﬁve few-shot learning benchmarks, with different levels of difﬁculties. Using a simple cross- entropy training on the base classes, and without complex meta-learning strategies, our LaplacianShot outperforms state-of-the-art methods by signiﬁcant margins, consistently providing improvements across different settings, data sets, and training models. Furthermore, our transductive infer- ence is very fast, with computational times that are close to inductive inference, and can be used for large-scale tasks. 2. Laplacian Regularized Few-Shot Learning 2.1. Proposed Formulation In the few-shot setting, we are given a labeled support set Xs = ⋃C c=1 Xc s with Ctest classes, where each novel classc has |Xc s |labeled examples, for instance, |Xc s |= 1 for 1-shot and |Xc s |= 5 for 5-shot. The objective of few-shot learn- ing is, therefore, to accurately classify unlabeled unseen query sample set Xq = ⋃C c=1 Xc q from these Ctest classes. This setting is referred to as the |Xc s |-shot C-way few-shot learning. Let fθ denotes the embedding function of a deep convolu- tional neural network, with parametersθand xq = fθ(zq) ∈ RM encoding the features of a given data point zq. Embed- ding fθ is learned from a labeled training set Xbase, with base classes that are different from the few-shot classes of Xs and Xq. In our work, parameters θare learned through a basic network training with the standard cross-entropy loss deﬁned over Xbase, without resorting to any complex episodic-training or meta-learning strategy. For each query feature point xq in a few-shot task, we deﬁne a latent bi- nary assignment vector yq = [yq,1,...,y q,C]t ∈{0,1}C, which is within the C-dimensional probability simplex ∇C = {y ∈ [0,1]C | 1ty = 1 }: binary yq,c is equal to 1 if xq belongs to class c, and equal to 0 otherwise. tis used as the transpose operator. Let Y denotes the N ×C matrix whose rows are formed by yq, where N is the num- ber of query points in Xq. We propose a transductive few- shot inference, which minimizes a Laplacian-regularization objective for few-shot tasks w.r.t assignment variables Y, subject to simplex and integer constraints yq ∈∇C and yq ∈{0,1}C, ∀q: E(Y) = N(Y) + λ 2 L(Y) (1) N(Y) = N∑ q=1 C∑ c=1 yq,cd(xq −mc) L(Y) = 1 2 ∑ q,p w(xq,xp)∥yq −yp∥2 In (1), the ﬁrst term N(Y) is minimized globally when each query point is assigned to the class of the nearest prototype mc from the support set, using a distance metric d(xq,mc), such as the Euclidean distance. In the 1-shot setting, prototype mc is the support example of class c, whereas in multi-shot, mc can be the mean of the support examples. In fact, mc can be further rectiﬁed by integrating information from the query features, as we will detail later in our experiments. The second term L(Y) is the well-known Laplacian regular- izer, which can be equivalently written astr(YtLY), where L is the Laplacian matrix1 corresponding to afﬁnity matrix W = [w(xq,xp)], and tr denotes the trace operator. Pair- wise potential w(xq,xp) evaluates the similarity between feature vectors xq and xp, and can be computed using some kernel function. The Laplacian term encourages nearby 1The Laplacian matrix corresponding to afﬁnity matrix W = [w(xq,xp)] is L = D −W, with D the diagonal matrix whose diagonal elements are given by: Dq = ∑ p w(xq,xp).Laplacian Regularized Few-Shot Learning Algorithm 1 Proposed Algorithm for LaplacianShot Input: Xs, Xq, λ, fθ Output: Labels∈{1,..,C }N for Xq Get prototypes mc. Compute aq using (8a) ∀xq ∈Xq. Initialize i= 1. Initialize yi q = exp(−aq) 1t exp(−aq) . repeat Compute yi+1 q using (12) yi q ←yi+1 q . Y = [yi q]; ∀q. i= i+ 1. until Bi(Y) in (7) does not change lq = arg max c yq; ∀yq ∈Y. Labels= {lq}N q=1 points (xq, xp) in the feature space to have the same latent label assignment, thereby regularizing predictions at query samples for few-shot tasks. As we will show later in our comprehensive experiments, the pairwise Laplacian term complements the unary potentials in N(Y), substantially increasing the predictive performance of few-shot learning across different networks, and various benchmark datasets with different levels of difﬁculty. More generally, Laplacian regularization is widely used in the contexts of graph clustering (V on Luxburg, 2007; Shi & Malik, 2000; Ziko et al., 2018; Wang & Carreira- Perpin´an, 2014) and semi-supervised learning (Weston et al., 2012; Belkin et al., 2006). For instance, popular spectral graph clustering techniques (V on Luxburg, 2007; Shi & Ma- lik, 2000) optimize the Laplacian term subject to partition- balance constraints. In this connection, our transductive inference can be viewed as a graph clustering of the query set, subject to supervision constraints from the support set. Regularization parameter λcontrols the trade-off between the two terms. It is worth noting that the recent nearest- prototype classiﬁcation in (Wang et al., 2019) corresponds to the particular case of λ = 0 of our model in (1). It assigns a query sample xq to the label of the closest support prototype in the feature space, thereby minimizing N(Y): yq,c∗ = 1 if c∗= arg min c∈{1,...,C} d(xq,mc) (2) 2.2. Optimization In this section, we propose an efﬁcient bound-optimization technique for solving a relaxed version of our objective in (1), which guarantees convergence, while computing in- dependent closed-form updates for each query sample in few-shot tasks. It is well known that minimizing pairwise functions over binary variables is NP-hard (Tian et al., 2014), and a standard approach in the context of clustering algo- rithms is to relax the integer constraints, for instance, using a convex (Wang & Carreira-Perpin´an, 2014) or a concave relaxation (Ziko et al., 2018). In fact, by relaxing integer constraints yq ∈{0,1}C, our objective in (1) becomes a convex quadratic problem. However, this would require solving for the N ×C assignment variables all together, with additional projections steps for handling the simplex constraints. In this work, we use a concave relaxation of the Laplacian-regularized objective in (1), which, as we will later show, yields fast independent and closed-form updates for each assignment variable, with convergence guarantee. Furthermore, it enables us to draw interesting connections between Laplacian regularization and attention mechanisms in few-shot learning (Vinyals et al., 2016). It is easy to verify that, for binary (integer) simplex variables, the Laplacian term in (1) can be written as follows, after some simple manipulations: L(Y) = ∑ q Dq − ∑ q,p w(xq,xp)yt qyp (3) where Dq = ∑ pw(xq,xp) denotes the degree of query sample xq. By relaxing integer constraints yq ∈{0,1}C, the expression in Eq. (3) can be viewed as a concave relax- ation2 for Laplacian term L(Y) when symmetric afﬁnity matrix W = [w(xq,xp)] is positive semi-deﬁnite. As we will see in the next paragraph, concavity is important to derive an efﬁcient bound optimizer for our model, with independent and closed-form updates for each query sam- ple. Notice that the ﬁrst term in relaxation (3) is a constant independent of the soft (relaxed) assignment variables. We further augment relaxation (3) with a convex negative- entropy barrier function yt qlog yq, which avoids expensive projection steps and Lagrangian-dual inner iterations for the simplex constraints of each query point. Such a barrier 3 removes the need for extra dual variables for constraints yq ≥0 by restricting the domain of each assignment vari- able to non-negative values, and yields closed-form updates for the dual variables of constraints 1tyq = 1 . Notice that this barrier function is null at the vertices of the sim- plex. Putting all together, and omitting the additive constant∑ qDq in (3), we minimize the following concave-convex relaxation of our objective in (1) w.r.t soft assignment vari- ables Y, subject to simplex constraints yq ∈∇C,∀q: R(Y) = Ytlog Y + N(Y) + λ 2 ˜L(Y) (4) 2Equality (3) holds in for points on the vertices of the simplex, i.e., yq ∈{0,1}C, but is an approximation for points within the simplex (soft assignments), i.e., yq ∈]0,1[C. 3Note that entropy-like barriers are known in the context of Bregman-proximal optimization (Yuan et al., 2017), and have well-known computational beneﬁts when dealing with simplex constraints.Laplacian Regularized Few-Shot Learning where ˜L(Y) = −∑ q,pw(xq,xp)yt qyp. Bound optimization: In the following, we detail an iterative bound-optimization solution for relaxation (4). Bound optimization, often referred to as MM (Majorize- Minimization) framework (Lange et al., 2000; Zhang et al., 2007), is a general optimization principle4. At each iteration, it updates the variable as the minimum of a surrogate func- tion, i.e., an upper bound on the original objective, which is tight at the current iteration. This guarantees that the original objective does not increase at each iteration. Re-arranging the soft assignment matrix Y in vector form Y = [yq] ∈RNC, relaxation ˜L(Y) can be written conve- niently in the following form: ˜L(Y) = − ∑ q,p w(xq,xp)yt qyp = YtΨY (5) with Ψ = −W⊗I, where ⊗denotes the Kronecker product and I is the N ×N identity matrix. Note that Ψ is negative semi-deﬁnite for a positive semi-deﬁnite W. Therefore, YtΨY is a concave function, and the ﬁrst-order approxima- tion of (5) at a current solution Yi (iis the iteration index) gives the following tight upper bound on ˜L(Y): ˜L(Y) = YtΨY ≤(Yi)tΨYi+ 2 (ΨYi)t(Y −Yi) (6) Therefore, using unary potentials N(Y) and the negative entropy barrier in conjunction with the upper bound in (6), we obtain the following surrogate function Bi(Y) for relax- ation R(Y) at current solution Yi: R(Y) ≤Bi(Y) c= N∑ q=1 yt q(log(yq) + aq −λbi q) (7) where c= means equality up to an additive constant5 that is independent of variable Y, and aq and bi q are the following C-dimensional vectors: aq = [aq,1,...,a q,C]t; aq,c = d(xq,mc) (8a) bi q = [bi q,1,...,b i q,C]t; bi q,c = ∑ p w(xq,xp)yi p,c (8b) It is straightforward to verify that upper bound Bi(Y) is tight at the current iteration, i.e., Bi(Yi) = R(Yi). This 4The general MM principle is widely used in machine learn- ing in various problems as it enables to replace a difﬁcult opti- mization problem with a sequence of easier sub-problems (Zhang et al., 2007). Examples of well-known bound optimizers include expectation-maximization (EM) algorithms, the concave-convex procedure (CCCP) (Yuille & Rangarajan, 2001) and submodular- supermodular procedures (SSP) (Narasimhan & Bilmes, 2005), among many others. 5The additive constant in Bi(Y) is a term that depends only on Yi. This term comes from the Laplacian upper bound in (6). can be seen easily from the ﬁrst-order approximation in (6). We iteratively optimize the surrogate function at each iteration i: Yi+1 = arg min Y Bi(Y) (9) Because of upper-bound condition R(Y) ≤Bi(Y),∀Y, tightness condition Bi(Yi) = R(Yi) at the current solu- tion, and the fact that Bi(Yi+1) ≤Bi(Yi) due to minimiza- tion (9), it is easy to verify that updates (9) guarantee that relaxation R(Y) does not increase at each iteration: R(Yi+1) ≤Bi(Yi+1) ≤Bi(Yi) = R(Yi) Closed-form solutions of the surrogate functions: No- tice that Bi(Y) is a sum of independent functions of each assignment variable. Therefore, we can solve (9) for each yq independently, while satisfying the simplex constraint: min yq∈∇C yt q(log(yq) + aq −λbi q), ∀q (10) The negative entropy barrier term yt qlog yq in (10) restricts yq to be non-negative, removing the need of extra dual vari- ables for the constraints yq >0. Also, simplex constraint 1tyq = 1 is afﬁne. Thus, the solution of the following Karush-Kuhn-Tucker (KKT) condition provide the mini- mum of (10): log yq + aq −λbi q + β1 = 0 (11) with β the Lagrange multiplier for the simplex constraint. This provides, for each q, closed-form solutions for both the primal and dual variables, yielding the following indepen- dent updates of the assignment variables: yi+1 q = exp(−ai q + λbi q) 1texp(−aiq + λbiq) ∀q (12) 2.3. Proposed Algorithm The overall proposed algorithm is simpliﬁed in Algorithm 1. Once the network function fθ is learned using the base dataset Xbase, our algorithm proceeds with the extracted features xq. Before the iterative bound updates, each soft assignment y1 q is initialized as a softmax probability of aq, which is based on the distances to prototypes mc. The itera- tive bound optimization is guaranteed to converge, typically less than 15 iterations in our experiments (Figure 2). Also the independent point-wise bound updates yield a parallel structure of the algorithm, which makes it very efﬁcient (and convenient for large-scale few-shot tasks). We refer to our method as LaplacianShot in the experiments. Link to attention mechanisms: Our Laplacian-regularized model has interesting connection to the popular attention mechanism in (Vaswani et al., 2017). In fact, MatchingNetLaplacian Regularized Few-Shot Learning Table 1.Average accuracy (in %) in miniImageNet and tieredImageNet. The best results are reported in bold font. miniImageNet tieredImageNet Methods Network 1-shot 5-shot 1-shot 5-shot MAML (Finn et al., 2017) ResNet-18 49.61 ±0.92 65.72 ±0.77 - - Chen (Chen et al., 2019) ResNet-18 51.87 ±0.77 75.68 ±0.63 - - RelationNet (Sung et al., 2018) ResNet-18 52.48 ±0.86 69.83 ±0.68 - - MatchingNet (Vinyals et al., 2016) ResNet-18 52.91 ±0.88 68.88 ±0.69 - - ProtoNet (Snell et al., 2017) ResNet-18 54.16 ±0.82 73.68 ±0.65 - - Gidaris (Gidaris & Komodakis, 2018) ResNet-15 55.45 ±0.89 70.13 ±0.68 - - SNAIL (Mishra et al., 2018) ResNet-15 55.71 ±0.99 68.88 ±0.92 - - AdaCNN (Munkhdalai et al., 2018) ResNet-15 56.88 ±0.62 71.94 ±0.57 - - TADAM (Oreshkin et al., 2018) ResNet-15 58.50 ±0.30 76.70 ±0.30 - - CAML (Jiang et al., 2019) ResNet-12 59.23 ±0.99 72.35 ±0.71 - - TPN (Liu et al., 2019b) ResNet-12 59.46 75.64 - - TEAM (Qiao et al., 2019) ResNet-18 60.07 75.90 - - MTL (Sun et al., 2019) ResNet-18 61.20 ±1.80 75.50 ±0.80 - - VariationalFSL (Zhang et al., 2019) ResNet-18 61.23 ±0.26 77.69 ±0.17 - - Transductive tuning (Dhillon et al., 2020) ResNet-12 62.35 ±0.66 74.53 ±0.54 - - MetaoptNet (Lee et al., 2019) ResNet-18 62.64 ±0.61 78.63 ±0.46 65.99 ±0.72 81.56 ±0.53 SimpleShot (Wang et al., 2019) ResNet-18 63.10 ±0.20 79.92 ±0.14 69.68 ±0.22 84.56 ±0.16 CAN+T (Hou et al., 2019) ResNet-12 67.19 ±0.55 80.64 ±0.35 73.21 ±0.58 84.93 ±0.38 LaplacianShot (ours) ResNet-18 72.11 ±0.19 82.31 ±0.14 78.98 ±0.21 86.39 ±0.16 Qiao (Qiao et al., 2018) WRN 59.60 ±0.41 73.74 ±0.19 - - LEO (Rusu et al., 2019) WRN 61.76 ±0.08 77.59 ±0.12 66.33 ±0.05 81.44 ±0.09 ProtoNet (Snell et al., 2017) WRN 62.60 ±0.20 79.97 ±0.14 - - CC+rot (Gidaris et al., 2019) WRN 62.93 ±0.45 79.87 ±0.33 70.53 ±0.51 84.98 ±0.36 MatchingNet (Vinyals et al., 2016) WRN 64.03 ±0.20 76.32 ±0.16 - - FEAT (Ye et al., 2020) WRN 65.10 ±0.20 81.11 ±0.14 70.41 ±0.23 84.38 ±0.16 Transductive tuning (Dhillon et al., 2020) WRN 65.73 ±0.68 78.40 ±0.52 73.34 ±0.71 85.50 ±0.50 SimpleShot (Wang et al., 2019) WRN 65.87±0.20 82.09 ±0.14 70.90 ±0.22 85.76 ±0.15 SIB (Hu et al., 2020) WRN 70.0 ±0.6 79.2 ±0.4 - - BD-CSPN (Liu et al., 2019a) WRN 70.31 ±0.93 81.89 ±0.60 78.74 ±0.95 86.92 ±0.63 LaplacianShot (ours) WRN 74.86 ±0.19 84.13 ±0.14 80.18 ±0.21 87.56±0.15 SimpleShot (Wang et al., 2019) MobileNet 61.55 ±0.20 77.70 ±0.15 69.50 ±0.22 84.91 ±0.15 LaplacianShot (ours) MobileNet 70.27 ±0.19 80.10 ±0.15 79.13 ±0.21 86.75 ±0.15 SimpleShot (Wang et al., 2019) DenseNet 65.77 ±0.19 82.23 ±0.13 71.20 ±0.22 86.33 ±0.15 LaplacianShot (ours) DenseNet 75.57 ±0.19 84.72 ±0.13 80.30 ±0.22 87.93 ±0.15 (Vinyals et al., 2016) predicted the labels of the query sam- ples xq as a linear combination of the support labels. The ex- pression of bi q,c that we obtained in (8b), which stems from our bound optimizer and the concave relaxation of the Lapla- cian, also takes the form of a combination of labels at each it- eration iin our model: bi q,c = ∑ pw(xq,xp)yi p,c. However, there are important differences with (Vinyals et al., 2016): First, the attention in our formulation is non-parametric as it considers only the feature relationships among the query samples in Xq, not the support examples. Second, unlike our approach, the attention mechanism in (Vinyals et al., 2016) is employed during training for learning embedding function fθ with a meta-learning approach. 3. Experiments In this section, we describe our experimental setup. An implementation of our LaplacianShot is publicly available6. 6https://github.com/imtiazziko/LaplacianShot 3.1. Datasets We used ﬁve benchmarks for few-shot classiﬁcation: miniImageNet, tieredImageNet, CUB, cross-domain CUB (with base training on miniImageNet) and iNat. The miniImageNet benchmark is a subset of the larger ILSVRC-12 dataset (Russakovsky et al., 2015). It has a total of 60,000 color images with 100 classes, where each class has 600 images of size 84 ×84, following (Vinyals et al., 2016). We use the standard split of 64 base, 16 vali- dation and 20 test classes (Ravi & Larochelle, 2017; Wang et al., 2019). The tieredImageNet benchmark (Ren et al., 2018) is also a subset of ILSVRC-12 dataset but with 608 classes instead. We follow standard splits with 351 base, 97 validation and 160 test classes for the experiments. The images are also resized to 84 ×84 pixels. CUB-200-2011 (Wah et al., 2011) is a ﬁne-grained image classiﬁcation dataset. We follow (Chen et al., 2019) for few-shot classi- ﬁcation on CUB, which splits into 100 base, 50 validationLaplacian Regularized Few-Shot Learning Table 2.Results for CUB and cross-domain results on miniImagenet →CUB. Methods Network CUB miniImagenet →CUB 1-shot 5-shot 1-shot 5-shot MatchingNet (Vinyals et al., 2016) ResNet-18 73.49 84.45 - 53.07 MAML (Finn et al., 2017) ResNet-18 68.42 83.47 - 51.34 ProtoNet (Snell et al., 2017) ResNet-18 72.99 86.64 - 62.02 RelationNet (Sung et al., 2018) ResNet-18 68.58 84.05 - 57.71 Chen (Chen et al., 2019) ResNet-18 67.02 83.58 - 65.57 SimpleShot (Wang et al., 2019) ResNet-18 70.28 86.37 48.56 65.63 LaplacianShot(ours) ResNet-18 80.96 88.68 55.46 66.33 Table 3.Average accuracy (in %) in iNat benchmark for SimpleShot (Wang et al., 2019) and the proposed LaplacianShot. The best results are reported in bold font. Note that, for iNat, we do not utilize the rectiﬁed prototypes. [The best reported result of (Wertheimer & Hariharan, 2019) with ResNet50 is: Per Class: 46.04%, Mean: 51.25%.] Methods Network UN L2 CL2 Per Class Mean Per Class Mean Per Class Mean SimpleShot ResNet-18 55.80 58.56 57.15 59.56 56.35 58.63 LaplacianShot ResNet-18 62.80 66.40 58.72 61.14 58.49 60.81 SimpleShot ResNet-50 58.45 61.07 59.68 61.99 58.83 60.98 LaplacianShot ResNet-50 65.96 69.13 61.40 63.66 61.08 63.18 SimpleShot WRN 62.44 65.08 64.26 66.25 63.03 65.17 LaplacianShot WRN 71.55 74.97 65.78 67.82 65.32 67.43 and 50 test classes for the experiments. The images are also resized to 84 ×84 pixels, as in miniImageNet. The iNat benchmark, introduced recently for few-shot classiﬁca- tion in (Wertheimer & Hariharan, 2019), contains images of 1,135 animal species. It introduces a more challenging few- shot scenario, with different numbers of support examples per class, which simulates more realistic class-imbalance scenarios, and with semantically related classes that are not easily separable. Following (Wertheimer & Hariharan, 2019), the dataset is split into 908 base classes and 227 test classes, with images of size 84 ×84. 3.2. Evaluation Protocol In the case of miniImageNet, CUB and tieredImageNet, we evaluate 10,000 ﬁve-way 1-shot and ﬁve-way 5-shot classiﬁcation tasks, randomly sampled from the test classes, following standard few-shot evaluation settings (Wang et al., 2019; Rusu et al., 2019). This means that, for each of the ﬁve-way few-shot tasks, C = 5 classes are randomly selected, with |Xc s |= 1 (1-shot) and |Xc s |= 5 (5-shot) ex- amples selected per class, to serve as support set Xs. Query set Xq contains 15 images per class. Therefore, the evalua- tion is performed over N = 75 query images per task. The average accuracy of these 10,000 few shot tasks are reported along with the 95% conﬁdence interval. For the iNat bench- mark, the number of support examples |Xc s |per class varies. We performed 227-way multi-shot evaluation, and report the top-1 accuracy averaged over the test images per class (Per Class in Table 3), as well as the average over all test images (Mean in Table 3), following the same procedure as in (Wertheimer & Hariharan, 2019; Wang et al., 2019). 3.3. Network Models We evaluate LaplacianShot on four different backbone net- work models to learn feature extractor fθ: ResNet-18/50 is based on the deep residual network archi- tecture (He et al., 2016), where the ﬁrst two down-sampling layers are removed, setting the stride to 1 in the ﬁrst convolu- tional layer and removing the ﬁrst max-pool layer. The ﬁrst convolutional layer is used with a kernel of size3×3 instead of 7×7. ResNet-18 has 8 basic residual blocks, and ResNet- 50 has 16 bottleneck blocks. For all the networks, the dimen- sion of the extracted features is 512. MobileNet (Howard et al., 2017) was initially proposed as a light-weight con- volutional network for mobile-vision applications. In our setting, we remove the ﬁrst two down-sampling operations, which results in a feature embedding of size 1024. WRN (Zagoruyko & Komodakis, 2016) widens the residual blocks by adding more convolutional layers and feature planes. In our case, we used 28 convolutional layers, with a widen- ing factor of 10 and an extracted-feature dimension of 640. Finally, we used the standard 121-layer DenseNet (Huang et al., 2017), omitting the ﬁrst two down-sampling layers and setting the stride to 1. We changed the kernel size of the ﬁrst convolutional layer to 3 ×3. The extracted feature vector is of dimension 1024.Laplacian Regularized Few-Shot Learning Figure 1. We tune regularization parameter λover values ranging from 0.1 to 1.5. In the above plots, we show the impact of choosing λ on both validation and test accuracies. The values of λbased on the best validation accuracies correspond to good accuracies in the test classes. The results are shown for different networks on miniImageNet dataset, for both 1-shot (top row) and 5-shot (bottom row). Figure 2.Convergence of Algorithm 1: Bounds Bi(Y) vs. iteration numbers for features from different networks. Here, the plots are produced by setting λ= 1.0, for a single 5-way 5 shot task from the miniImageNet test set. 3.4. Implementation Details Network model training: We trained the network models using the standard cross-entropy loss on the base classes, with a label-smoothing (Szegedy et al., 2016) parameter set to 0.1. Note that the base training did not involve any meta- learning or episodic-training strategy. We used the SGD optimizer to train the models, with mini-batch size set to 256 for all the networks, except for WRN and DenseNet, where we used mini-batch sizes of 128 and 100, respectively. We used two 16GB P100 GPUs for network training with base classes. For miniImageNet, CUB and tieredImageNet, we used early stopping by evaluating the the nearest-prototype classiﬁcation accuracy on the validation classes, with L2 normalized features. Prototype estimation and feature transformation: Dur- ing the inference on test classes, SimpleShot (Wang et al., 2019) performs the following feature transformations: L2 normalization, xq := xq/∥xq∥2 and CL2, which computes the mean of the base class features ¯x = 1 |Xbase| ∑ x∈Xbase x and centers the extracted features as xq := xq −¯x, which is followed by an L2 normalization. We report the results in Table 1 and 2 with CL2 normalized features. In Table 3 for the iNat dataset, we provide the results with both nor- malized and unnormalized (UN) features for a comparative analysis. We reproduced the results of SimpleShot with our trained network models. In the 1-shot setting, prototype mc is just the support example xq ∈Xc s of class c, whereas in multi-shot, mc is the simple mean of the support examples of class c. Another option is to use rectiﬁed prototypes, i.e., a weighted combination of features from both the support examples in Xc s and query samples in Xc q, which are initiallyLaplacian Regularized Few-Shot Learning predicted as belonging to class cusing Eq. (2): ˜mc = 1 |Xcs |+ |Xcq| ∑ xp∈{Xcs ,Xcq } exp(cos(xp,mc))∑C c=1 exp(cos(xp,mc)) xp, where cosdenotes the cosine similarity. And, for a given few-shot task, we compute the cross-domain shift ∆ as the difference between the mean of features within the support set and the mean of features within the query set: ∆ = 1 |Xs| ∑ xp∈Xs xp − 1 |Xq| ∑ xq∈Xq xq. Then, we rectify each query point xp ∈Xq in the few-shot task as follows: xp = xp + ∆. This shift correction is similar to the pro- totype rectiﬁcation in (Liu et al., 2019a). Note that our LaplacianShot model in Eq. (1) is agnostic to the way of estimating the prototypes: It can be used either with the standard prototypes (mc) or with the rectiﬁed ones ( ˜mc). We report the results of LaplacianShot with the rectiﬁed pro- totypes in Table 1 and 2, for miniImagenet, tieredImagenet and CUB. We do not report the results with the rectiﬁed prototypes in Table 3 for iNat, as rectiﬁcation drastically worsen the performance. For W, we used the k-nearest neighbor afﬁnities as follows: w(xq,xp) = 1 if xp is within the k nearest neighbor of xq, and w(xq,xp) = 0 otherwise. In our experiments, kis simply chosen from three typical values (3, 5 or 10) tuned over 500 few-shot tasks from the base training classes (i.e., we did not use test data for choosing k). We used k = 3 for miniImageNet, CUB and tieredImageNet and k = 10 for iNat benchmark. Regularization parameter λis chosen based on the validation class accuracy for miniImageNet, CUB and tieredImageNet. This will be discussed in more details in section 3.6. For the iNat experiments, we simply ﬁx λ= 1.0, as there is no validation set for this benchmark. 3.5. Results We evaluated LaplacianShot over ﬁve different bench- marks, with different scenarios and difﬁculties: Generic image classiﬁcation, ﬁne-grained image classiﬁcation, cross- domain adaptation, and imbalanced class distributions. We report the results of LaplacianShot for miniImageNet, tieredImageNet, CUB and iNat datasets, in Tables 1, 2 and 3, along with comparisons with state-of-the-art methods. Generic image classiﬁcation: Table 1 reports the results of generic image classiﬁcation for the standard miniImageNet and tieredImageNet few-shot benchmarks. We can clearly observe that LaplacianShot outperforms state-of-the-art methods by large margins, with gains that are consistent across different settings and network models. It is worth mentioning that, for challenging scenarios, e.g., 1-shot with low-capacity models, LaplacianShot outperforms complex meta-learning methods by more than 9%. For instance, compared to well-known MAML (Finn et al., 2017) and ProtoNet (Snell et al., 2017), and to the recent MetaoptNet (Lee et al., 2019), LaplacianShot brings improvements of nearly 22%, 17%, and 9%, respectively, under the same evaluation conditions. Furthermore, it outperforms the very recent transductive approaches in (Dhillon et al., 2020; Liu et al., 2019a;b) by signiﬁcant margins. With better learned features with WRN and DenseNet, LaplacianShot brings sig- niﬁcant performance boosts, yielding state-of-the art results in few-shot classiﬁcation, without meta-learning. Fine-grained image classiﬁcation: Table 2 reports the re- sults of ﬁne-grained few-shot classiﬁcation on CUB, with Resnet-18 network. LaplacianShot outperforms the best performing method in this setting by a 7% margin. Cross-domain (mini-ImageNet →CUB): We perform the very interesting few-shot experiment, with a cross-domain scenario, following the setting in (Chen et al., 2019). We used the ResNet-18 model trained on the miniImagenet base classes, while evaluation is performed on CUB few-shot tasks, with 50 test classes. Table 2 (rightmost column) reports the results. In this cross-domain setting, and consis- tently with the standard settings, LaplacianShot outperforms complex meta-learning methods by substantial margins. Imbalanced class distribution: Table 3 reports the results for the more challenging, class-imbalanced iNat benchmark, with different numbers of support examples per class and, also, with high visual similarities between the different classes, making class separation difﬁcult. To our knowledge, only (Wertheimer & Hariharan, 2019; Wang et al., 2019) report performances on this benchmark, and SimpleShot (Wang et al., 2019) represents the state-of-the-art. We com- pared with SimpleShot using unnormalized extracted fea- tures (UN), L2 and CL2 normalized features. Our Laplacian regularization yields signiﬁcant improvements, regardless of the network model and feature normalization. However, unlike SimpleShot, our method reaches its best performance with the unnormalized features. Note that, for iNat, we did not use the rectiﬁed prototypes. These results clearly highlight the beneﬁt Laplacian regularization brings in chal- lenging class-imbalance scenarios. 3.6. Ablation Study Choosing the Value of λ: In LaplacianShot, we need to choose the value of regularization parameter λ, which con- trols the trade-off between the nearest-prototype classiﬁer term aq and Laplacian regularizer bi q. We tuned this param- eter using the validation classes by sampling 500 few-shot tasks. LaplacianShot is used in each few-shot task with the following values of λ: [0.1,0.3,0.5,0.7,0.8,1.0,1.2,1.5]. The best λ corresponding to the best average 1-shot and 5-shot accuracy over validation classes/data is selected for inference over the test classes/data. To examine experi- mentally whether the chosen values of λbased on the best validation accuracies correspond to good accuracies in theLaplacian Regularized Few-Shot Learning Table 4.Ablation study on the effect of each term corresponding to nearest prototype N(Y), Laplacian L(Y) and rectiﬁed prototype ˜mc. Results are reported with ResNet-18 network. Note that, the Laplacian regularization L(Y) improve the results consistently. mini-ImageNet tiered-ImageNet CUB N(Y) L(Y) ˜mc 1-shot 5-shot 1-shot 5-shot 1shot 5-shot \u0013 \u0017 \u0017 63.10 79.92 69.68 84.56 70.28 86.37 \u0013 \u0013 \u0017 66.20 80.75 72.89 85.25 74.46 86.86 \u0013 \u0017 \u0013 69.74 82.01 76.73 85.74 78.76 88.55 \u0013 \u0013 \u0013 72.11 82.31 78.98 86.39 80.96 88.68 Table 5.Average inference time (in seconds) for the 5-shot tasks in miniImagenet dataset. Methods inference time SimpleShot (Wang et al., 2019) 0.009 Transductive tuning (Dhillon et al., 2020) 20.7 LaplacianShot (ours) 0.012 test classes, we plotted both the validation and test class ac- curacies vs. different values of λfor miniImageNet (Figure 1). The results are intuitive, with a consistent trend in both 1-shot and 5-shot settings. Particularly, for 1-shot tasks, λ= 0.7 provides the best results in both validation and test accuracies. In 5-shot tasks, the best test results are obtained mostly with λ= 0.1, while the best validation accuracies were reached with higher values of λ. Nevertheless, we re- port the results of LaplacianShot with the values ofλchosen based on the best validation accuracies. Effects of Laplacian regularization: We conducted an ab- lation study on the effect of each term in our model, i.e., nearest-prototype classiﬁer N(Y) and Laplacian regular- izer L(Y). We also examined the effect of using prototype rectiﬁcation, i.e., ˜mc instead of mc. Table 4 reports the results, using the ResNet-18 network. The ﬁrst row corre- sponds to the prediction of the nearest neighbor classiﬁer (λ= 0), and the second shows the effect of adding Lapla- cian regularization. In the 1-shot case, the latter boosts the performances by at least 3%. Prototype rectiﬁcation (third and fourth rows) also boosts the performances. Again, in this case, the improvement that the Laplacian term brings is signiﬁcant, particularly in the 1-shot case (2 to 3%). Convergence of transductive LaplacianShot inference : The proposed algorithm belongs to the family of bound op- timizers or MM algorithms. In fact, the MM principle can be viewed as a generalization of expectation-maximization (EM). Therefore, in general, MM algorithms inherit the monotonicity and convergence properties of EM algorithms (Vaida, 2005), which are well-studied in the literature. In fact, Theorem 3 in (Vaida, 2005) states a simple condition for convergence of the general MM procedure, which is al- most always satisﬁed in practice: The surrogate function has a unique global minimum. In Fig. 2, we plotted surrogates Bi(Y), up to a constant, i.e., Eq. (7), as functions of the iter- ation numbers, for different networks. One can see that the value of Bi(Y) decreases monotonically at each iteration, and converges, typically, within less than 15 iterations. Inference time: We computed the average inference time required for each 5-shot task. Table 5 reports these infer- ence times for miniImageNet with the WRN network. The purpose of this is to check whether there exist a signiﬁcant computational overhead added by our Laplacian-regularized transductive inference, in comparison to inductive inference. Note that the computational complexity of the proposed inference is O(NkC) for a few-shot task, where k is the neighborhood size for afﬁnity matrix W. The inference time per few-shot task for LaplacianShot is close to induc- tive SimpleShot run-time (LaplacianShot is only 1-order of magnitude slower), and is 3-order-of-magnitude faster than the transductive ﬁne-tuning in (Dhillon et al., 2020). 4. Conclusion Without meta-learning, we provide state-of-the-art results, outperforming signiﬁcantly a large number of sophisticated few-shot learning methods, in all benchmarks. Our trans- ductive inference is a simple constrained graph clustering of the query features. It can be used in conjunction with any base-class training model, consistently yielding improve- ments. Our results are in line with several recent baselines (Dhillon et al., 2020; Chen et al., 2019; Wang et al., 2019) that reported competitive performances, without resorting to complex meta-learning strategies. This recent line of sim- ple methods emphasizes the limitations of current few-shot benchmarks, and questions the viability of a large body of convoluted few-shot learning techniques in the recent lit- erature. As pointed out in Fig. 1 in (Dhillon et al., 2020), the progress made by an abundant recent few-shot literature, mostly based on meta-learning, may be illusory. Classical and simple regularizers, such as the entropy in (Dhillon et al., 2020) or our Laplacian term, well-established in semi- supervised learning and clustering, achieve outstanding per- formances. We do not claim to hold the ultimate solution for few-shot learning, but we believe that our model-agnostic transductive inference should be used as a strong baseline for future few-shot learning research.Laplacian Regularized Few-Shot Learning References Belkin, M., Niyogi, P., and Sindhwani, V . Manifold regular- ization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399–2434, 2006. Chen, W.-Y ., Liu, Y .-C., Kira, Z., Wang, Y .-C. F., and Huang, J.-B. A closer look at few-shot classiﬁcation. In Interna- tional Conference on Learning Representations (ICLR), 2019. Dhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto, S. A baseline for few-shot image classiﬁcation. In Inter- national Conference on Learning Representations (ICLR), 2020. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28:594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Interna- tional Conference on Machine Learning (ICML), 2017. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018. Gidaris, S., Bursuc, A., Komodakis, N., P ´erez, P., and Cord, M. Boosting few-shot visual learning with self- supervision. In International Conference on Computer Vision (ICCV), 2019. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Hou, R., Chang, H., Bingpeng, M., Shan, S., and Chen, X. Cross attention network for few-shot classiﬁcation. In Neural Information Processing Systems (NeurIPS), 2019. Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. Preprint arXiv:1704.04861, 2017. Hu, S. X., Moreno, P. G., Xiao, Y ., Shen, X., Obozinski, G., Lawrence, N. D., and Damianou, A. Empirical bayes transductive meta-learning with synthetic gradients. In International Conference on Learning Representations (ICLR), 2020. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Jiang, X., Havaei, M., Varno, F., Chartrand, G., Chapa- dos, N., and Matwin, S. Learning to learn with condi- tional class dependencies. In International Conference on Learning Representations (ICLR), 2019. Kim, J., Kim, T., Kim, S., and Yoo, C. D. Edge-labeling graph neural network for few-shot learning. In Con- ference on Computer Vision and Pattern Recognition (CVPR), 2019. Lange, K., Hunter, D. R., and Yang, I. Optimization transfer using surrogate objective functions. Journal of computa- tional and graphical statistics, 9(1):1–20, 2000. Lee, K., Maji, S., Ravichandran, A., and Soatto, S. Meta- learning with differentiable convex optimization. In Con- ference on Computer Vision and Pattern Recognition (CVPR), 2019. Liu, J., Song, L., and Qin, Y . Prototype rectiﬁcation for few-shot learning. Preprint arXiv:1911.10713, 2019a. Liu, Y ., Lee, J., Park, M., Kim, S., Yang, E., Hwang, S., and Yang, Y . Learning to propagate labels: Transductive prop- agation network for few-shot learning. In International Conference on Learning Representations (ICLR), 2019b. Miller, E., Matsakis, N., and Viola, P. Learning from one example through shared densities on transforms. Con- ference on Computer Vision and Pattern Recognition (CVPR), 2000. Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. A simple neural attentive meta-learner. In International Conference on Learning Representations (ICLR), 2018. Munkhdalai, T., Yuan, X., Mehri, S., and Trischler, A. Rapid adaptation with conditionally shifted neurons. In Interna- tional Conference on Machine Learning (ICML), 2018. Narasimhan, M. and Bilmes, J. A submodular-supermodular procedure with applications to discriminative structure learning. In Conference on Uncertainty in Artiﬁcial Intel- ligence (UAI), 2005. Oreshkin, B., L´opez, P. R., and Lacoste, A. Tadam: Task de- pendent adaptive metric for improved few-shot learning. In Neural Information Processing Systems (NeurIPS) , 2018. Qiao, L., Shi, Y ., Li, J., Wang, Y ., Huang, T., and Tian, Y . Transductive episodic-wise adaptive metric for few- shot learning. In International Conference on Computer Vision (ICCV), 2019. Qiao, S., Liu, C., Shen, W., and Yuille, A. L. Few-shot image recognition by predicting parameters from activa- tions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.Laplacian Regularized Few-Shot Learning Ravi, S. and Larochelle, H. Optimization as a model for few- shot learning. In International Conference on Learning Representations (ICLR), 2017. Ren, M., Triantaﬁllou, E., Ravi, S., Snell, J., Swersky, K., Tenenbaum, J. B., Larochelle, H., and Zemel, R. S. Meta- learning for semi-supervised few-shot classiﬁcation. In International Conference on Learning Representations ICLR, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. Rusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S., and Hadsell, R. Meta-learning with latent embedding optimization. In International Confer- ence on Learning Representations (ICLR), 2019. Shi, J. and Malik, J. Normalized cuts and image segmenta- tion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Neural Information Processing Systems (NeurIPS), 2017. Sun, Q., Liu, Y ., Chua, T., and Schiele, B. Meta-transfer learning for few-shot learning. In Conference on Com- puter Vision and Pattern Recognition (CVPR), June 2019. Sung, F., Yang, Y ., Zhang, L., Xiang, T., Torr, P. H., and Hospedales, T. M. Learning to compare: Relation net- work for few-shot learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018. Szegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In Conference on Computer Vision and Pattern Recognition, 2016. Tian, F., Gao, B., Cui, Q., Chen, E., and Liu, T.-Y . Learn- ing deep representations for graph clustering. In AAAI Conference on Artiﬁcial Intelligence, 2014. Vaida, F. Parameter convergence for em and mm algorithms. Statistica Sinica, 15:831–840, 2005. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, u., and Polosukhin, I. Attention is all you need. InNeural Information Processing Systems (NeurIPS), 2017. Vinyals, O., Blundell, C., Lillicrap, T. P., Kavukcuoglu, K., and Wierstra, D. Matching networks for one shot learning. In Neural Information Processing Systems (NeurIPS) , 2016. V on Luxburg, U. A tutorial on spectral clustering.Statistics and computing, 17(4):395–416, 2007. Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S. The caltech-ucsd birds-200-2011 dataset. 2011. Wang, W. and Carreira-Perpin ´an, M. A. The lapla- cian k-modes algorithm for clustering. Preprint arXiv:1406.3895, 2014. Wang, Y ., Chao, W.-L., Weinberger, K. Q., and van der Maaten, L. Simpleshot: Revisiting nearest- neighbor classiﬁcation for few-shot learning. Preprint arXiv:1911.04623, 2019. Wertheimer, D. and Hariharan, B. Few-shot learning with localization in realistic settings. In Conference on Com- puter Vision and Pattern Recognition (CVPR), 2019. Weston, J., Ratle, F., Mobahi, H., and Collobert, R. Deep learning via semi-supervised embedding. In Neural net- works: Tricks of the trade, pp. 639–655. Springer, 2012. Ye, H.-J., Hu, H., Zhan, D.-C., and Sha, F. Few-shot learning via embedding adaptation with set-to-set functions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Yuan, J., Yin, K., Bai, Y ., Feng, X., and Tai, X. Bregman- proximal augmented lagrangian approach to multiphase image segmentation. In Scale Space and Variational Methods in Computer Vision (SSVM), 2017. Yuille, A. L. and Rangarajan, A. The concave-convex proce- dure (CCCP). In Neural Information Processing Systems (NeurIPS), 2001. Zagoruyko, S. and Komodakis, N. Wide residual networks. In British Machine Vision Conference (BMVC), 2016. Zhang, J., Zhao, C., Ni, B., Xu, M., and Yang, X. Varia- tional few-shot learning. In International Conference on Computer Vision (ICCV), 2019. Zhang, Z., Kwok, J. T., and Yeung, D.-Y . Surrogate max- imization/minimization algorithms and extensions. Ma- chine Learning, 69:1–33, 2007. Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and Sch¨olkopf, B. Learning with local and global consistency. In Neural Information Processing Systems (NeurIPS) , 2004. Ziko, I., Granger, E., and Ben Ayed, I. Scalable lapla- cian k-modes. In Neural Information Processing Systems (NeurIPS), 2018.",
      "references": [
        "Manifold regular- ization: A geometric framework for learning from labeled and unlabeled examples",
        "A closer look at few-shot classiﬁcation",
        "A baseline for few-shot image classiﬁcation",
        "One-shot learning of object categories",
        "Model-agnostic meta- learning for fast adaptation of deep networks",
        "Dynamic few-shot visual learning without forgetting",
        "Boosting few-shot visual learning with self- supervision",
        "Deep residual learn- ing for image recognition",
        "Cross attention network for few-shot classiﬁcation",
        "Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications",
        "Empirical bayes transductive meta-learning with synthetic gradients",
        "Densely connected convolutional networks",
        "Learning to learn with condi- tional class dependencies",
        "Edge-labeling graph neural network for few-shot learning",
        "Optimization transfer using surrogate objective functions",
        "Meta- learning with differentiable convex optimization",
        "Prototype rectiﬁcation for few-shot learning",
        "Learning to propagate labels: Transductive prop- agation network for few-shot learning",
        "Learning from one example through shared densities on transforms",
        "A simple neural attentive meta-learner",
        "Rapid adaptation with conditionally shifted neurons",
        "A submodular-supermodular procedure with applications to discriminative structure learning",
        "Tadam: Task de- pendent adaptive metric for improved few-shot learning",
        "Transductive episodic-wise adaptive metric for few- shot learning",
        "Few-shot image recognition by predicting parameters from activations",
        "Optimization as a model for few- shot learning",
        "Meta- learning for semi-supervised few-shot classiﬁcation",
        "ImageNet Large Scale Visual Recognition Challenge",
        "Meta-learning with latent embedding optimization",
        "Normalized cuts and image segmentation",
        "Prototypical networks for few-shot learning",
        "Meta-transfer learning for few-shot learning",
        "Learning to compare: Relation net- work for few-shot learning",
        "Rethinking the inception architecture for computer vision",
        "Learn- ing deep representations for graph clustering",
        "Parameter convergence for em and mm algorithms",
        "Attention is all you need",
        "Matching networks for one shot learning",
        "A tutorial on spectral clustering",
        "The caltech-ucsd birds-200-2011 dataset",
        "The lapla- cian k-modes algorithm for clustering",
        "Simpleshot: Revisiting nearest- neighbor classiﬁcation for few-shot learning",
        "Few-shot learning with localization in realistic settings",
        "Deep learning via semi-supervised embedding",
        "Few-shot learning via embedding adaptation with set-to-set functions",
        "Bregman- proximal augmented lagrangian approach to multiphase image segmentation",
        "The concave-convex proce- dure (CCCP)",
        "Wide residual networks",
        "Varia- tional few-shot learning",
        "Surrogate max- imization/minimization algorithms and extensions",
        "Learning with local and global consistency",
        "Scalable lapla- cian k-modes"
      ],
      "meta_data": {
        "arxiv_id": "2006.15486v3",
        "authors": [
          "Imtiaz Masud Ziko",
          "Jose Dolz",
          "Eric Granger",
          "Ismail Ben Ayed"
        ],
        "published_date": "2020-06-28T02:17:52Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces LaplacianShot, a transductive inference scheme for few-shot image classification that adds a graph-based Laplacian regularizer to a nearest-prototype classifier. The method requires no meta-learning or model fine-tuning, provides a bound-optimization algorithm with closed-form, parallel updates and provable convergence, and achieves state-of-the-art accuracy and speed across five standard benchmarks.",
        "methodology": "1) Formulate a quadratic assignment objective E(Y)=N(Y)+λL(Y) where N(Y) assigns each query feature to the closest support prototype and L(Y) enforces label smoothness via a graph Laplacian built on k-NN affinities among query samples. 2) Relax the binary simplex constraints, add an entropy barrier, and derive a concave-convex surrogate. 3) Apply a Majorize–Minimize (MM) bound optimizer that yields independent soft-label updates y_{q}^{i+1}=softmax(−a_q+λb_q) with guaranteed monotone decrease; complexity O(NkC). 4) Optional prototype rectification integrates query statistics. 5) Uses pre-trained feature extractors (ResNet-18/50, WRN-28-10, DenseNet-121, MobileNet) trained with plain cross-entropy on base classes.",
        "experimental_setup": "• Datasets: miniImageNet, tieredImageNet, CUB-200-2011, cross-domain miniImageNet→CUB, and iNat2017. • Episodes: 5-way 1-shot & 5-shot with 15 queries/class (10k episodes) for first three datasets; 227-way variable-shot for iNat. • Metrics: mean classification accuracy with 95% CI; per-class and overall accuracy for iNat. • Hyper-parameters: λ∈{0.1…1.5} selected on 500 validation episodes; k∈{3,5,10}. • Feature preprocessing: L2 and CL2 normalization; early stopping on validation accuracy. • Baselines: 30+ inductive, meta-learning and transductive methods; inference time compared to SimpleShot and Transductive Fine-Tuning.",
        "limitations": "1) Requires tuning λ and k on validation tasks; values may not transfer across domains. 2) Performance depends on quality of fixed embeddings; no mechanism to improve poor features. 3) Assumes that nearby query samples share labels; may fail when class manifolds overlap heavily. 4) Graph constructed only over query set; ignores support–query relations beyond prototypes. 5) Heuristic prototype rectification needed for best results and can hurt on some datasets (iNat). 6) Evaluated only on image classification benchmarks; generalization to other modalities/tasks untested.",
        "future_research_directions": "• Learn adaptive or data-driven λ, k, and affinity functions, possibly with meta-learning. • Jointly train embedding networks end-to-end with the Laplacian objective. • Extend to semi-supervised or active few-shot settings with large unlabeled pools. • Apply the framework to other tasks such as object detection, segmentation, or text classification. • Incorporate support–query and support–support graphs to exploit full transductive structure. • Provide theoretical guarantees on optimality gap of the relaxation and tighter error bounds.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Subspace Regularizers for Few-Shot Class Incremental Learning",
      "full_text": "Published as a conference paper at ICLR 2022 SUBSPACE REGULARIZERS FOR FEW-SHOT CLASS INCREMENTAL LEARNING Afra Feyza Aky¨urek Boston University akyurek@bu.edu Ekin Aky¨urek MIT CSAIL akyurek@mit.edu Derry Tanti Wijaya Boston University wijaya@bu.edu Jacob Andreas MIT CSAIL jda@mit.edu ABSTRACT Few-shot class incremental learning—the problem of updating a trained classiﬁer to discriminate among an expanded set of classes with limited labeled data—is a key challenge for machine learning systems deployed in non-stationary environ- ments. Existing approaches to the problem rely on complex model architectures and training procedures that are difﬁcult to tune and re-use. In this paper, we present an extremely simple approach that enables the use of ordinary logistic re- gression classiﬁers for few-shot incremental learning. The key to this approach is a new family of subspace regularization schemes that encourage weight vectors for new classes to lie close to the subspace spanned by the weights of existing classes. When combined with pretrained convolutional feature extractors, logis- tic regression models trained with subspace regularization outperform specialized, state-of-the-art approaches to few-shot incremental image classiﬁcation by up to 22% on the miniImageNet dataset. Because of its simplicity, subspace regular- ization can be straightforwardly extended to incorporate additional background information about the new classes (including class names and descriptions speci- ﬁed in natural language); these further improve accuracy by up to 2%. Our results show that simple geometric regularization of class representations offers an effec- tive tool for continual learning.1 1 I NTRODUCTION Standard approaches to classiﬁcation in machine learning assume a ﬁxed training dataset and a ﬁxed set of class labels. But for many real-world classiﬁcation problems, these assumptions are unreal- istic. Classiﬁers must sometimes be updated on-the-ﬂy to recognize new concepts (e.g. new skills in personal assistants or new road signs in self-driving vehicles), while training data is sometimes unavailable for reuse (e.g. due to privacy regulations, Lesort et al. 2019; McClure et al. 2018; or storage and retraining costs, Bender et al. 2021). Development of models that support few-shot class-incremental learning (FSCIL), in which classiﬁers’ label sets can be easily extended with small numbers of new examples and no retraining, is a key challenge for machine learning systems deployed in the real world (Masana et al., 2020). As a concrete example, consider the classiﬁcation problem depicted in Fig. 1. A model, initially trained on a large set of examples from several base classes (snorkel, arctic fox, meerkat; Fig. 1a), must subsequently be updated to additionally recognize two novel classes (white wolf and poncho; Fig. 1b), and ultimately distinguish among all ﬁve classes (Fig. 1c). Training a model to recognize the base classes is straightforward: for example, we can jointly optimize the parameters of a feature extractor (perhaps a convolutional network parameterized by θ) and a linear classiﬁcation layer ( η) to maximize the regularized likelihood of (image, label) pairs from the dataset in Fig. 1a: L(θ,η) = 1 n ∑ (x,y) log exp(η⊤ y fθ(x))∑ y′exp(η⊤ y′fθ(x)) + α ( ∥η∥2 + ∥θ∥2) (1) But how can this model be updated to additionally recognize the classes in Fig. 1b, with only a few examples of each new class and no access to the original training data? 1Code for the experiments is released under https://github.com/feyzaakyurek/subspace-reg. 1 arXiv:2110.07059v2  [cs.CV]  20 Feb 2022Published as a conference paper at ICLR 2022 Figure 1: Few-shot class incremental learning: (a) A base classiﬁer is trained on a large dataset (D(0)). (b) This classiﬁer is extended to also discriminate among a set of new classes with a small number of labeled examples (D(1)). (c) Models are evaluated on a test set that includes all seen classes (Q(1)). This paper focuses on extremely simple, regularization-based approaches to FSCIL, with and without side information from natural language: (i) We regularize novel classiﬁer weights toward the shortest direction to the subspace spanned by base classiﬁer weights. (ii) We regularize novel classiﬁers pulling them toward the weighted average of base classiﬁers where weights are calculated using label/description similarity between novel and base class names or one-sentence descriptions. (iii) We learn a linear mapping Lbetween word labels and classiﬁer weights of the base classes. Later, we project the novel label white wolf and regularize the novel classiﬁer weight ηwhite wolf towards the projection. Na¨ıvely continuing to optimize Eq. 1 on (x, y) pairs drawn from the new dataset will cause several problems. In the absence of any positive examples of those classes, performance on base classes will suffer due to catastrophic forgetting (Goodfellow et al., 2013), while performance on novel classes will likely be poor as a result of overﬁtting (Anderson & Burnham, 2004). As a consequence, most past work on FSCIL has focused on alternative approaches that use non- standard prediction architectures (e.g., Tao et al., 2020b) or optimize non-likelihood objectives (e.g., Yoon et al., 2020; Ren et al., 2019). This divergence between approaches to standard and incremen- tal classiﬁcation has its own costs—state-of-the-art approaches to FSCIL are complicated, requiring nested optimizers, complex data structures, and numerous hyperparameters. When improved repre- sentation learning and optimization techniques are developed for standard classiﬁcation problems, it is often unclear to how to apply these to the incremental setting. In this paper, we turn the standard approach to classiﬁcation into a surprisingly effective tool for FSCIL. Speciﬁcally, we show that both catastrophic forgetting and overﬁtting can be reduced by introducing an additional subspace regularizer (related to one studied by Agarwal et al. 2010 and Kirkpatrick et al. 2017) that encourages novel η to lie close to the subspace spanned by the base classes. On its own, the proposed subspace regularizer produces ordinary linear classiﬁers that achieve state-of-the-art results on FSCIL, improving over existing work in multiple tasks and datasets. Because of its simplicity, this regularization approach can be easily extended to incorporate ad- ditional information about relationships between base and novel classes. Using language data as a source of background knowledge about classes, we describe a variation of our approach, which we term semantic subspace regularization , that pulls weight vectors toward particular convex combinations of base classes that capture their semantic similarity to existing classes. This further improves accuracy by up to 2% over simple subspace regularization across multiple tasks. These results suggest that FSCIL and related problems may not require specialized machinery to solve, and that simple regularization approaches can solve the problems that result from limited access to training data for both base and novel classes. 2Published as a conference paper at ICLR 2022 2 B ACKGROUND A long line of research has focused on the development of automated decision-making systems that support online expansion of the set of concepts they can recognize and generate. An early example (closely related to our learning-from-deﬁnitions experiment in Section 5) appears in the classic SHRDLU language grounding environment (Winograd, 1972): given the deﬁnition a steeple is a small triangle on top of a tall rectangle , SHRDLU acquires the ability to answer questions containing the novel concept steeple. Recent work in machine learning describes several versions of this problem in featuring more complex perception or control: Few-shot and incremental learning Few-shot classiﬁcation problems test learners’ ability to dis- tinguish among a ﬁxed set of classes using only a handful of labeled examples per class (Scheirer et al., 2012). Most effective approaches to few-shot learning rely on additional data for pre-training (Tian et al., 2020) or meta-learning (Vinyals et al., 2016; Finn et al., 2017; Snell et al., 2017; Yoon et al., 2019). One peculiarity of this evaluation paradigm is that, even when pre-trained, models are evaluated only on new (few-shot) classes, and free to update their parameters in ways that cause them to perform poorly on pre-training tasks. As noted by past work, a more realistic evaluation of models’ ability to rapidly acquire new concepts should consider their ability to discriminate among both new concepts and old ones, a problem usually referred to asfew-shot class-incremental learning (FSCIL)2 (Tao et al., 2020b). FSCIL requires learners to incrementally acquire novel classes with few labeled examples while retaining high accuracy on previously learned classes. It combines the most challenging aspects of class-incremental learning (Rebufﬁ et al., 2017) task-incremental learning (Delange et al., 2021), and rehearsal-based learning (Rolnick et al., 2019; Chaudhry et al., 2019), three related problems with much stronger assumptions about the kind of information available to learners. Existing ap- proaches to this problem either prioritize novel class adaptation (Ren et al., 2019; Yoon et al., 2020; Chen & Lee, 2021; Cheraghian et al., 2021) or reducing forgetting in old classes (Tao et al., 2020b). Learning class representations Even prior to the widespread use of deep representation learn- ing approaches, the view of classiﬁcation as problem of learning class representations motivated a number of approaches to multi-class and multi-task learning (Argyriou et al., 2007a; Agarwal et al., 2010). In few-shot and incremental learning settings, many recent approaches have also focused on the space of class representations (Tao et al., 2020a). Qi et al. (2018) initialize novel class repre- sentations using the average features from few-shot samples. Others (Gidaris & Komodakis, 2018; Yoon et al., 2020; Zhang et al., 2021) train a class representation predictor via meta-learning, and Tao et al. (2020b) impose topological constraints on the manifold of class representations as new representations are added. Alternatively, Chen & Lee (2021) models the visual feature space as a Gaussian mixture and use the cluster centers in a similarity-based classiﬁcation scheme. Lastly, two concurrent works condition both old and new class representations at each session according to an auxiliary scheme; graph attention network in Zhang et al. (2021) and relation projection in Zhu et al. (2021). Our approach is related to Ren et al. (2019), who proposes a nested optimization framework tolearn auxiliary parameters for every base and novel class to inﬂuence the novel weights via regularization; we show that these regularization targets can be derived geometrically without the need for an inner optimization step. Also related is the work of Barzilai & Crammer (2015), which synthesizes the novel weights as linear combinations of base weights; we adopt a regularization approach that allows learning of class representations that are not strict linear combinations of base classes. Learning with side information from language The use of background information from other modalities (especially language) to bootstrap learning of new classes is widely studied (Frome et al., 2013; Radford et al., 2021; Reed et al., 2016; Yan et al., 2021)—particularly in the zero-shot learn- ing and generalized zero-shot learning where side information is the only source of information about the novel class (Chang et al., 2008; Larochelle et al., 2008; Akata et al., 2013; Pourpanah et al., 2Variants of this problem have gone by numerous names in past work, includinggeneralized few-shot learn- ing (Sch¨onfeld et al., 2019), dynamic few-shot learning (Gidaris & Komodakis, 2018) or simply incremental few-shot learning (Ren et al., 2019; Chen & Lee, 2021). 3Published as a conference paper at ICLR 2022 2020). Specialized approaches exist for integrating side information in few-shot learning settings (Schwartz et al., 2019; Cheraghian et al., 2021). 3 P ROBLEM FORMULATION We follow the notation in Tao et al. (2020b) for FSCIL: assume a stream of T learning sessions, each associated with a labeled dataset D(0),D(1),...,D (T). Every D(t) consists of a support set S(t) (used for training) and a query set Q(t) (used for evaluation). We will refer to the classes represented in D(0) as base classes; as in Fig. 1a, we will assume that it contains a large number of examples for every class. D(1) (and subsequent datasets) introduce novel classes (Fig. 1b). Let C(S) = {y : (x,y) ∈S}denote the set of classes expressed in a set of examples S; we will write C(t) = C(S(t)) and C(≤t) := ⋃ j≤tC(S(j)) for convenience. The learning problem we study is incremental in the sense that each support set contains only new classes (C(t) ∩C(<t) = ∅)3, while each query set evaluates models on both novel classes and previously seen ones (C(Q(t)) = C(≤t)). It is few-shot in the sense that for t >0, |S(t)|is small (containing 1–5 examples for all datasets studied in this paper). Given an incremental learning session t >0 the goal is to ﬁne-tune existing classiﬁer with the limited training data from novel classes such that the classiﬁer performs well in classifying all classes learned thus far. FSCIL with a single session Prior to Tao et al. (2020b), a simpler version of the multi-session FSCIL was proposed by Qi et al. (2018) where there is only single incremental learning session after the pre-training stage i.e. T = 1. This version, which we call single-session FSCIL, has been extensively studied by previous work (Qi et al., 2018; Gidaris & Komodakis, 2018; Ren et al., 2019; Yoon et al., 2020). This problem formulation is the same as above withT = 1: a feature extractor is trained on the samples from D(0), then D(1), then evaluated on samples with classes inC(0) ∪C(1). 4 A PPROACH Our approach to FSCIL consists of two steps. In the base session, we jointly train a feature extractor and classiﬁcation layer on base classes (Section 4.1). In subsequent (incremental learning) sessions, we freeze the feature extractor and update only the classiﬁcation layer using regularizers that (1) stabilize representations of base classes, and (2) bring the representations of new classes close to existing ones (Sections 4.2-4.4). 4.1 F EATURE EXTRACTOR TRAINING As in Eq. 1, we begin by training an ordinary classiﬁer comprising a non-linear feature extractor fθ and a linear decision layer with parameters η. We choose ηand θto maximize: L(η,θ) = 1 |S(0)| ∑ (x,y)∈S(0) log exp(η⊤ y fθ(x))∑ c∈C(0) exp(η⊤c fθ(x)) −α ( ∥η∥2 + ∥θ∥2) (2) As discussed in Section 5, all experiments in this paper implement fθ as a convolutional neural network. In subsequent loss formulations we refer to ∥η∥2 + ∥θ∥2 as Rprior(η,θ). 4.2 F INE -TUNING Along with the estimated ˆθ, feature extractor training yields parameters only for base classes ηy∈C(0) . Given an incremental learning dataset D(t), we introduce new weight vectors ηc∈C(t) and optimize L(η) = 1 |S(t)| ∑ (x,y)∈S(t) log exp(η⊤ y fˆθ(x))∑ c∈C(≤t) exp(η⊤c fˆθ(x)) −αRprior(η,0) −βR(t) old (η) −γR(t) new(η) . (3) 3This is the original setup established by Tao et al. (2020b). We will also present experiments in which we retain one example per class for memory replay following Chen & Lee (2021). 4Published as a conference paper at ICLR 2022 with respect to ηalone. Eq. 3 features two new regularization terms, R(t) old and R(t) new. Rt old limits the extent to which ﬁne-tuning can change parameters for classes that have already been learned: R(t) old (η) = ∑ t′<t ∑ c∈C(t′) ∥ηt′ c −ηc∥2 (4) where ηt′ c denotes the value of the corresponding variable at the end of session t′. (For example, η0 c refers to the weights for the base class cprior to ﬁne tuning, i.e. after session t′= 0.) As shown in Section 5.2, using Rold alone, and setting Rnew = 0, is a surprisingly effective baseline; however, performance can be improved by appropriately regularizing new parameters as described below. Variant: Memory Following past work (Chen & Lee, 2021) which performs incremental learn- ing while retaining a small “memory” of previous samples M, we explore an alternative baseline approach in which we append append S(t) in Eq. 3 with M(t). We deﬁne the memory at session tas M(t) = ⋃ (t′<t) M(t′) where M(t′) ⊆S(t′) and |M(t′)|= |C(t′)|. We sample only 1 example per previous class and we reuse the same example in subsequent sessions. 4.3 M ETHOD 1: S UBSPACE REGULARIZATION Past work on other multitask learning problems has demonstrated the effectiveness of constraining parameters for related tasks to be similar (Jacob et al., 2008), lie on the same manifold (Agarwal et al., 2010) or even on the same linear subspace (Argyriou et al., 2007a). Moreover, Sch ¨onfeld et al. (2019) showed that a shared latent feature space for all classes is useful for class-incremental classiﬁcation. Features independently learned for novel classes from small numbers of examples are likely to capture spurious correlations (unrelated to the true causal structure of the prediction problem) as a result of dataset biases (Arjovsky et al., 2019). In contrast, we expect most informative semantic features to be shared across multiple classes: indeed, cognitive research suggests that in humans’ early visual cortex, representations of different objects occupy a common feature space (Kriegeskorte et al., 2008). Therefore, regularizing toward the space spanned by base class weight vectors encourages new class representations to depend on semantic rather than spurious features and features for all tasks to lie in the same universal subspace. We apply this intuition to FSCIL via a simple subspace regularization approach. Given a parameter for an incremental class ηc and base class parameters {ηj∈C(0) }, we ﬁrst compute the subspace target mc for each class. We then compute the distance between ηc from mc and deﬁne: R(t) new(η) = ∑ c∈C(t) ∥ηc −mc∥2 (5) where mc is the projection of ηc onto the space spanned by {ηj∈C(0) }: mc = P⊤ C(0) ηc (6) and PC(0) contains the orthogonal basis vectors of the subspace spanned by the initial set of base weights ηj∈C(0) . (PC(0) can be found using a QR decomposition of the matrix of base class vectors, as described in the appendix.) Previous work that leverages subspace regularization for multitask learning assume that data from all tasks are available from the beginning (Argyriou et al., 2007b; Agarwal et al., 2010; Argyriou et al., 2007a). Our approach to subspace regularization removes these assumptions, enabling tasks (in this case, novel classes) to arrive incrementally and predictions to be made cumulatively over all classes seen thus far without any further information on which task that a query belongs to. Agarwal et al. (2010) is similar to ours in encouraging all task parameters to lie on the same manifold; it is different in that they learn the manifold and the task parameters alternately. Also related Simon et al. (2020) and Devos & Grossglauser (2019) model class representations over a set of subspaces (disjoint in the latter) for non-incremental few-shot learning. 4.4 M ETHOD 2: S EMANTIC SUBSPACE REGULARIZATION The constraint in Eq. 5 makes explicit use of geometric information about base classes, pulling novel weights toward the base subspace. However, it provides no information about where within that 5Published as a conference paper at ICLR 2022 subspace the weights for a new class should lie. In most classiﬁcation problems, classes have names consisting of natural language words or phrases; these names often contain a signiﬁcant amount of information relevant to the classiﬁcation problem of interest. (Even without having ever seen awhite wolf, a typical English speaker can guess that a white wolf is more likely to resemble an arctic fox than a snorkel.) These kinds of relations are often captured by embeddings of class labels (or more detailed class descriptions) (Pennington et al., 2014). When available, this kind of information about class semantics can be used to construct an improved subspace regularizer by encouraging new class representations to lie close to a convex combination of base classes weighted by their semantic similarity. We replace the subspace projectionP⊤ C(0) ηc in Eq. 5 with a semantic target lc for each class. Letting ec denote a semantic embedding of the class c, we compute: R(t) new(η) = ∑ c∈C(t) ∥ηc −lc∥2 (7) where lc = ∑ j∈C(0) exp (ej ·ec/τ)∑ j∈C(0) exp (ej ·ec/τ)ηj (8) and τ is a hyper-parameter. Embeddings ec can be derived from multiple sources: in addition to the class names discussed above, a popular source of side information for zero-shot and few-shot learning problems is detailed textual descriptions of classes; we evaluate both label and description embeddings in Section 5. Sch¨onfeld et al. (2019) also leverages label information on a shared subspace for few-shot incre- mental learning where they project both visual and semantic features onto a shared latent space for prediction in the single-session setting. In comparison, we re-use the base visual space for joint projection for multiple incremental sessions. Baseline: Linear Mapping While the approach described in Eq. 7 combines semantic informa- tion and label subspace information, a number of previous studies in vision and language have also investigated the effectiveness of directly learning a mapping from the space of semantic embeddings to the space of class weights (Das & Lee, 2019; Socher et al., 2013; Pourpanah et al., 2020; Romera- Paredes & Torr, 2015). Despite pervasiveness of the idea in other domains, this is the ﬁrst time we are aware of it being explored for FSCIL. We extend our approach to incorporate this past work by learning a linear map Lbetween the embedding space ej ∈E and the weight space containing ηC(0) : L∗= min L ∑ j∈C(0) ∥ηj −L(ej)∥2 (9) then set R(t) new = ∑ c∈C(t) ∥ηc −L∗(ec)∥2 . (10) Concurrent work by (Cheraghian et al., 2021) also leverages side information for FSCIL where they learn a mapping from image space onto the label space to directly produce predictions in the label space. We provide comparisons in Section 5. 5 E XPERIMENTS Given a classiﬁer trained on an initial set of base classes, our experiments aim to evaluate the effect of subspace regularization (1) on the learning of new classes, and (2) on the retention of base classes. To evaluate the generality of our method, we evaluate using two different experimental paradigms that have been used in past work: a multi-session experiment in which new classes are continuously added and the classiﬁer must be repeatedly updated, and a single-session setup (T = 1) in which new classes arrive only once. We use SGD as our optimizer to train all models. Details about experiment setups and results are discussed below. Additional details may be found in the appendix. 6Published as a conference paper at ICLR 2022 0 1 2 3 4 5 6 7 8 Session 0 10 20 30 40 50 60 70 80Accuracy (%) a) Weighted Average Model Chen & Lee (2021) Fine-tuning Linear Mapping Subspace Reg. Semantic Subspace Reg. 0 1 2 3 4 5 6 7 8 Session 0 10 20 30 40 50 60 70 80 b) Base Samples Model Chen & Lee (2021) Fine-tuning Linear Mapping Subspace Reg. Semantic Subspace Reg. 0 1 2 3 4 5 6 7 8 Session 0 10 20 30 40 50 60 70 80 c) Novel Samples Model Chen & Lee (2021) Fine-tuning Linear Mapping Subspace Reg. Semantic Subspace Reg. Figure 2: Multi-Session FSCIL accuracy (%) results on miniImageNet. In the ﬁrst session 0, there are a total of 60 classes (base). Every session following the ﬁrst one introduces 5 novel classes with 5 labeled samples from each. Each session provides accuracy over all classes that were seen thus far. Weighted average is the weighted combination of novel and base accuracies with respect to the number of classes in each category. Error bars are standard deviation (95% CI). In accordance with Chen & Lee (2021) we preserve only one sample per class from previous classes and append them to the support set during ﬁne-tuning (+M variant). Regularization based approaches i.e. subspace regularization, semantic subspace regularization and linear mapping consistently outperform previous benchmarks on average. 5.1 M ULTI -SESSION We follow the same setup established in Tao et al. (2020b) as well as Section 3: we ﬁrst train a ResNet (He et al., 2016) network from scratch as the feature extractor on a large number of exam- ples from base classes C(0) to obtain an initial classiﬁer ηj∈C(0) . We then observe a new batch of examples S(t) and produce a new classiﬁer deﬁned by ηc∈C(≤t) . Finally, we evaluate the classiﬁer according to top-1 accuracy in base and novel samples as well as their weighted average (Tao et al., 2020b; Chen & Lee, 2021). We use the miniImageNet dataset (Vinyals et al., 2016; Russakovsky et al., 2015) for our multi-session evaluation. miniImageNet contains 100 classes with 600 samples per class. Table 1: Multi-Session FSCIL weighted average of accuracy (%) results on miniImageNet using an identical setup to Fig. 2 with memory distinction. We report the average results over 10 random splits of the data for incremental sessions 1, 2 and 8. ±M indicates 1 sample per class is kept (or not) in the memory to further regularize forgetting. Our Fine-tuning baseline is already superior to previous results. In both memory settings, our regularizers substantially outperform respective benchmarks for all 1-8 sessions. ∗Results are only estimates from the plot in the respective work. Bold indicates the highest. Session 1 2 8Model −M +M −M +M −M +M Tao et al. (2020b) 50.1 45.2 24.4Chen & Lee (2021) 59.9 55.9 41.8Fine-tuning 61.8 67.7 49.9 62.9 26.5 48.1Subspace Reg. 71.7 72.9 66.9 67.8 46.8 48.8 +languageCheraghian et al. (2021)* 58.0 53.0 39.0Linear Mapping 72.6 73.2 67.1 68.0 46.9 50.0Semantic Subspace Reg.73.8 73.9 68.4 69.0 47.6 49.7 Feature extractor training In session t= 0, we randomly select 60 classes as base classes ( |C(0)|= 60 ) and use the remaining 40 classes as novel classes. Reported results are averaged across 10 random splits of the data (Fig. 2). We use a ResNet-18 model that is identical to the one described in Tian et al. (2020). Following Tao et al. (2020b), we use 500 labeled samples per base class to train our feature extractor and 100 for testing. Incremental evaluation Again fol- lowing Tao et al. (2020b), we evaluate for a total of 8 incremental sessions 1 ≤ t ≤8 for miniImageNet. In each ses- sion, for S(t), we sample 5 novel classes for training and 5 samples from each class. Hence, at the last session t = 8, evaluation involves the entire set of 100 miniImageNet classes. We use GloVe embeddings (Pen- nington et al., 2014) for label embeddings in Eq. 7. Results Fig. 2 and Table 1 show the results of multi-session experiments with and without mem- ory. Session 0 indicates base class accuracy after feature extractor training. We compare subspace and language-guided regularization (linear mapping and semantic subspace reg.) to simple ﬁne- 7Published as a conference paper at ICLR 2022 Table 2: miniImageNet 64+5-way and tieredImageNet 200+5-way single-session results. We follow previous work in reporting the average of accuracies of base and novel samples over all classes rather than weighted average. In addition to accuracy, we report a quantity labeled∆ by Ren et al. (2019), which is the gap between individual accuracies and joint accuracies of both base and novel samples averaged. Lower values of ∆ are better. Bold numbers are not signiﬁcantly different from the best result in each column under a paired t-test (p< 0.05 after Bonferroni correction). All results are averaged across 2000 runs. miniImageNet tieredImageNet Model 1-shot 5-shot 1-shot 5-shot Acc. ∆ Acc. ∆ Acc. ∆ Acc. ∆ Imprinted Networks (Qi et al., 2018) 41.34±0.54 -23.79% 46.34±0.54 -25.25% 40.83±0.45 -22.29% 53.87±0.48 -17.18%LwoF (Gidaris & Komodakis, 2018) 49.65±0.64 -14.47% 59.66±0.55 -12.35% 53.42±0.56 -9.59% 63.22±0.52 -7.27%Attention Attractor Networks (Ren et al., 2019) 54.95±0.30 -11.84% 63.04±0.30 -10.66% 56.11±0.33 -6.11% 65.52±0.31 -4.48%XtarNet (Yoon et al., 2020) 56.12 ±0.17 -13.62% 69.51±0.15 -9.76% 61.37±0.36 -1.85% 69.58±0.32 -1.79% Fine-tuning 58.56 ±0.33 -12.14% 66.54±0.33 -13.77%64.42±0.38 -7.23% 72.59±0.34 -6.88%Subspace Regularization 58.38 ±0.32 -12.30% 68.88±0.32 -10.74%64.39±0.38 -7.23%73.03±0.34 -6.16% +languageLinear Mapping 58.87±0.33 -12.83%69.68±0.31 -10.40%64.55±0.38 -7.31%73.10±0.33 -6.16%Semantic Subspace Reg. (w/ description)59.09±0.32 -12.38% 68.46±0.32 -11.70%64.49±0.38 -7.14% 72.94±0.34 -6.29%Semantic Subspace Reg. (w/ label) 58.70±0.32 -12.24%69.75±0.32 -10.48%64.75±0.38 -7.22%73.51±0.33 -6.08% tuning (a surprisingly strong baseline). We also compare our results to three recent benchmarks: Tao et al. (2020b), Chen & Lee (2021) and Cheraghian et al. (2021).4 When models are evaluated on combined base and novel accuracy, subspace regularization outper- forms previous approaches (by 22% (-M) and 7% (+M) at session 8); when semantic information about labels is available, linear mapping and semantic subspace regularization outperform Cher- aghian et al. (2021) (Table 1). Evaluating only base sample accuracies (Fig. 2b), semantic subspace reg. outperforms others; compared to regularization based approaches ﬁne-tuning is subject to catas- trophic forgetting. Rold is still useful in regulating forgetting (Table 3 in appendix). The method of Chen & Lee (2021) follows a similar trajectory to our regularizers, but at a much lower accu- racy (Fig. 2a). In Fig. 2c, a high degree of forgetting in base classes with ﬁne-tuning allows higher accuracy for novel classes—though not enough to improve average performance (Fig. 2a). By con- trast, subspace regularizers enable a good balance between plasticity and stability (Mermillod et al., 2013). In Table 1, storing as few as a single example per an old class substantially helps to re- duce forgetting. Results from linear mapping and semantic subspace regularization are close, with semantic subspace regularization performing roughly 1% better on average. The two approaches offer different trade-offs between base and novel accuracies: the latter is more competitive for base classes and vice-versa. 5.2 S INGLE SESSION In this section we describe the experiment setup for the single-session evaluation, (T = 1) , and compare our approach to state-of-the-art XtarNet (Yoon et al., 2020), as well as Ren et al. (2019), Gidaris & Komodakis (2018) and Qi et al. (2018). We evaluate our models on 1-shot and 5-shot settings.5 miniImageNet and tieredImageNet For miniImageNet single-session experiments, we follow the the splits provided by Yoon et al. (2020). Out of 100, 64 classes are used in session t = 0, 20 in session t= 1 and the remaining for development. Following Yoon et al. (2020), we use ResNet-12 (a smaller version of the model described in Section 5.1).tieredImageNet (Ren et al., 2018) contains a total of 608 classes out of which 351 are used in sessiont= 0 and 160 are reserved for t= 1. The remaining 97 are used for development. While previous work (Ren et al., 2019; Yoon et al., 2020) 4Chen & Lee (2021) and Cheraghian et al. (2021) do not provide a codebase and Tao et al. (2020b) does not provide an implementation for the main TOPIC algorithm in their released code. Therefore, we report published results rather than a reproduction. This comparison is inexact: our feature extractor performs substantially better than Tao et al. (2020b) and Chen & Lee (2021). Despite extensive experiments (see appendix) on various versions of ResNet-18 (He et al., 2016), we were unable to identify a training procedure that reproduced the reported accuracy for session 0: all model variants investigated achieved 80%+ validation accuracy. 5Unlike in the preceding section, we were able to successfully reproduce the XtarNet model. Our version gives better results on the miniImageNet dataset but worse results on the tieredImageNet datasets; for fair- ness, we thus report results for our version of XtarNet on miniImageNet and previously reported numbers on tieredImageNet. For other models, we show accuracies reported in previous work. 8Published as a conference paper at ICLR 2022 Figure 3: Simple ﬁne-tuning (top) vs. subspace regularization (bottom) predictions without memory across the ﬁrst four incremental sessions of miniImageNet. In the x- and y-axes, we present predictions and gold labels ranging from 0 to 79 where the ﬁrst 60 are base classes. The number of classes grows by 5 every session starting from 60 up to 80. Brighter colors indicate more frequent predictions. Note that simple fune-tuning entails bias towards the most recently learned classes (top row) whereas addition of subspace regularization on the novel weights remedies the aforementioned bias; resulting in a fairer prediction performance for all classes. separate 151 classes out of the 351 for meta training, we pool all 351 for feature extractor training. We train the same ResNet-18 described in Section 5.1. Additional details regarding learning rate scheduling, optimizer parameters and other training conﬁgurations may be found in the appendix. Incremental evaluation We follow Yoon et al. (2020) for evaluation. Yoon et al. independently sample 2000 D(1) incremental datasets (“episodes”) from the testing classesC(1). They report aver- age accuracies over all episodes with 95% conﬁdence intervals. At every episode,Q(1) is resampled from both base and novel classes, C(0) and C(1), with equal probability for both miniImageNet and tieredImageNet. We again ﬁne-tune the weights until convergence. We do not reserve samples from base classes, thus the only training samples during incremental evaluation is from the novel classes C(1). We use the same resources for label embeddings and Sentence-BERT embeddings (Reimers & Gurevych, 2019) for descriptions which are retrieved from WordNet (Miller, 1995). Results We report aggregate results for 1-shot and 5-shots settings of miniImageNet and tieredImageNet (Table 2). Compared to previous work specialized for the single-session setup with- out a straightforward way to expand into multi-session (Ren et al., 2019; Yoon et al., 2020), even our simple ﬁne-tuning baseline perform well on both datasets—outperforming the previous state- of-the-art in three out of four settings in Table 2. Addition of subspace and semantic regularization improves performance overall but tieredImageNet 1-shot setting. Semantic subspace regularizers match or outperform linear label mapping. Subspace regularization outperforms ﬁne-tuning in 5- shot settings and matches it in 1-shot. In addition to accuracy, we report a quantity labeled ∆ by Ren et al. (2019). ∆ serves as a measure of catastrophic forgetting, with the caveat that it can be minimized by a model that achieves a classiﬁcation accuracy of 0 on both base and novel classes. We ﬁnd that our approaches result in approximately the same ∆ in miniImageNet and worse in tieredImageNet than previous work. 6 A NALYSIS AND LIMITATIONS What does regularization actually do? Fine-tuning results in prediction biased towards the most recently learned classes (top of Fig. 3) when no subspace regularization is imposed. Our experiments show that preserving the base weights while regularizing novel weights gives signiﬁcant improve- ments over ordinary ﬁne-tuning (bottom of Fig. 3)—resulting a fairer prediction over all classes and reducing catastrophic forgetting. In Table 1, Semantic Subspace Reg. results in 73.8% and 47.6% accuracies in the 1st and 8th sessions whereas, ﬁne-tuning results in 61.8% and 26.5%, respectively, 9Published as a conference paper at ICLR 2022 even without any memory—suggesting that regularization ensures a better retention of accuracy. While the trade-off between accuracies of base and novel classes is inevitable due to the nature of the classiﬁcation problem, the proposed regularizers provide a good balance between the two. What are the limitations of the proposed regularization scheme? Our approach targets only errors that originate in the ﬁnal layer of the model—while a convolutional feature extractor is used, the parameters of this feature extractor are ﬁxed, and we have focused on FSCIL as a linear clas- siﬁcation problem. Future work might extend these approaches to incorporate ﬁne-tuning of the (nonlinear) feature extractor itself while preserving performance on all classes in the longer term. 7 C ONCLUSIONS We have described a family of regularization-based approaches to few-shot class-incremental learn- ing, drawing connections between incremental learning and the general multi-task and zero-shot learning literature. The proposed regularizers are extremely simple—they involve only one extra hyperparameter, require no additional training steps or model parameters, and are easy to understand and implement. Despite this simplicity, our approach enables ordinary classiﬁcation architectures to achieve state-of-the-art results on the doubly challenging few-shot incremental image classiﬁcation across multiple datasets and problem formulations. REFERENCES Arvind Agarwal, Samuel Gerber, and Hal Daume. Learning multiple tasks using manifold regular- ization. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta (eds.), Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc., 2010. 2, 3, 5 Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for attribute-based classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 819–826, 2013. 3 D Anderson and K Burnham. Model selection and multi-model inference. Second. NY: Springer- Verlag, 63(2020):10, 2004. 2 Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. In B. Sch ¨olkopf, J. Platt, and T. Hoffman (eds.), Advances in Neural Information Processing Systems, volume 19. MIT Press, 2007a. URL https://proceedings.neurips.cc/paper/ 2006/file/0afa92fc0f8a9cf051bf2961b06ac56b-Paper.pdf. 3, 5 Andreas Argyriou, Charles A Micchelli, Massimiliano Pontil, and Yiming Ying. A spectral regu- larization framework for multi-task structure learning. In NIPS, volume 1290, pp. 1296. Citeseer, 2007b. 5 Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5 Aviad Barzilai and Koby Crammer. Convex multi-task learning by clustering. In Artiﬁcial Intelli- gence and Statistics, pp. 65–73. PMLR, 2015. 3 Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 610–623, 2021. 1 Ming-Wei Chang, Lev-Arie Ratinov, Dan Roth, and Vivek Srikumar. Importance of semantic repre- sentation: Dataless classiﬁcation. In AAAI, volume 2, pp. 830–835, 2008. 3 Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019. 3 Kuilin Chen and Chi-Guhn Lee. Incremental few-shot learning via vector quantization in deep embedded space. In International Conference on Learning Representations, 2021. 3, 4, 5, 7, 8, 14 10Published as a conference paper at ICLR 2022 Ali Cheraghian, Shaﬁn Rahman, Pengfei Fang, Soumava Kumar Roy, Lars Petersson, and Mehrtash Harandi. Semantic-aware knowledge distillation for few-shot class-incremental learning. arXiv preprint arXiv:2103.04059, 2021. 3, 4, 6, 7, 8, 14 Debasmit Das and CS George Lee. Zero-shot image recognition using relational matching, adap- tation and calibration. In 2019 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2019. 6 Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 3 Arnout Devos and Matthias Grossglauser. Regression networks for meta-learning few-shot classiﬁ- cation. arXiv preprint arXiv:1905.13613, 2019. 5 Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning , pp. 1126–1135. PMLR, 2017. 3 Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc' Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In C. J. C. Burges, L. Bot- tou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. 3 Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 4367– 4375, 2018. 3, 4, 8 Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empiri- cal investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. 2 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016. 7, 8, 14, 15 Laurent Jacob, Francis Bach, and Jean-Philippe Vert. Clustered multi-task learning: A convex formulation. In Advances in Neural Information Processing Systems, 2008. 5 James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom- ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. 2 Nikolaus Kriegeskorte, Marieke Mur, Douglas A Ruff, Roozbeh Kiani, Jerzy Bodurka, Hossein Esteky, Keiji Tanaka, and Peter A Bandettini. Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron, 60(6):1126–1141, 2008. 5 Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zero-data learning of new tasks. In AAAI, volume 1, pp. 3, 2008. 3 Timoth´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia Diaz Rodriguez. Continual learning for robotics: Deﬁnition, framework, learning strategies, op- portunities and challenges. Information Fusion, 58, 12 2019. doi: 10.1016/j.inffus.2019.12.004. 1 Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost van de Weijer. Class-incremental learning: survey and performance evaluation. arXiv preprint arXiv:2010.15277, 2020. 1 Patrick McClure, Charles Y . Zheng, J. Kaczmarzyk, John Rogers-Lee, S. Ghosh, D. Nielson, P. Ban- dettini, and Francisco Pereira. Distributed weight consolidation: A brain segmentation case study. In NeurIPS, 2018. 1 11Published as a conference paper at ICLR 2022 Martial Mermillod, Aur ´elia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Inves- tigating the continuum from catastrophic forgetting to age-limited learning effects. Frontiers in psychology, 4:504, 2013. 8 George A Miller. Wordnet: a lexical database for english. Communications of the ACM , 38(11): 39–41, 1995. 9 Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta- learner. arXiv preprint arXiv:1707.03141, 2017. 15 Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543, 2014. 6, 7 Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou, Ran Wang, Chee Peng Lim, and Xi-Zhao Wang. A review of generalized zero-shot learning methods. arXiv preprint arXiv:2011.08641, 2020. 3, 6 Hang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5822–5830, 2018. 3, 4, 8 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 3 S Rebufﬁ, Alexander Kolesnikov, and Christoph H Lampert. icarl: Incremental classiﬁer and repre- sentation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2001–2010, 2017. 3 Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning deep representations of ﬁne-grained visual descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 49–58, 2016. 3 Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert- networks. arXiv preprint arXiv:1908.10084, 2019. 9 Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁca- tion. arXiv preprint arXiv:1803.00676, 2018. 8, 13, 15 Mengye Ren, Renjie Liao, Ethan Fetaya, and Richard Zemel. Incremental few-shot learning with attention attractor networks. In Advances in Neural Information Processing Systems, 2019. 2, 3, 4, 8, 9, 15 David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. 3 Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learn- ing. In International conference on machine learning, pp. 2152–2161. PMLR, 2015. 6 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. 7, 13 Walter J Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E Boult. Toward open set recognition. IEEE transactions on pattern analysis and machine intelligence , 35(7): 1757–1772, 2012. 3 12Published as a conference paper at ICLR 2022 Edgar Sch ¨onfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. General- ized zero-shot learning via aligned variational autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 3, 5, 6 Eli Schwartz, Leonid Karlinsky, Rogerio Feris, Raja Giryes, and Alex M Bronstein. Baby steps towards few-shot learning with multiple semantics. arXiv preprint arXiv:1906.01905, 2019. 4 Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash Harandi. Adaptive subspaces for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4136–4145, 2020. 5 Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. 3 Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani, Christopher D Manning, and An- drew Y Ng. Zero-shot learning through cross-modal transfer. In Advances in Neural Information Processing Systems, 2013. 6 Xiaoyu Tao, Xinyuan Chang, Xiaopeng Hong, Xing Wei, and Yihong Gong. Topology-preserving class-incremental learning. In European Conference on Computer Vision, pp. 254–270. Springer, 2020a. 3 Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, and Yihong Gong. Few- shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12183–12192, 2020b. 2, 3, 4, 7, 8 Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few- shot image classiﬁcation: a good embedding is all you need? arXiv preprint arXiv:2003.11539, 2020. 3, 7, 14, 15 Lloyd N Trefethen and David Bau III. Numerical linear algebra, volume 50. Siam, 1997. 16 Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match- ing networks for one shot learning. arXiv preprint arXiv:1606.04080, 2016. 3, 7, 13 Terry Winograd. Understanding natural language. Cognitive psychology, 3(1):1–191, 1972. 3 Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel, and Steven Schockaert. Aligning visual pro- totypes with bert embeddings for few-shot learning. In ICMR, 2021. 3 Sung Whan Yoon, Jun Seo, and Jaekyun Moon. Tapnet: Neural network augmented with task- adaptive projection for few-shot learning. In International Conference on Machine Learning, pp. 7115–7123. PMLR, 2019. 3 Sung Whan Yoon, Do-Yeon Kim, Jun Seo, and Jaekyun Moon. Xtarnet: Learning to extract task- adaptive representation for incremental few-shot learning. In International Conference on Ma- chine Learning, pp. 10852–10860. PMLR, 2020. 2, 3, 4, 8, 9, 15, 16 Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, and Yinghui Xu. Few-shot incremental learning with continually evolved classiﬁers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12455–12464, 2021. 3 Kai Zhu, Yang Cao, Wei Zhai, Jie Cheng, and Zheng-Jun Zha. Self-promoted prototype reﬁne- ment for few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6801–6810, 2021. 3 A C ODE AND DATASETS Code will be made publicly available. We use miniImageNet (Vinyals et al., 2016) and tieredImageNet (Ren et al., 2018) datasets both are subsets of ImageNet dataset (Russakovsky et al., 2015). Use of terms and licenses are available through the respective sources. 13Published as a conference paper at ICLR 2022 B A NALYSIS OF CATASTROPHIC FORGETTING In Table 3, we demonstrate the effectiveness of the regularization term Rold in mitigating catas- trophic forgetting. Table 3: miniImageNet weighted average results across multiple sessions showcasing the usefulness of Rold in reducing catastrophic forgetting across multiple sessions for -M setting. Higher accuracies are highlighted and results are averages over 10 random splits. Rold is useful in combination with Semantic Subspace Reg. for all sessions while it is more helpful in the long-run for Fine-tuning. Note that Semantic Subspace Reg. consistently outperforms Fine-tuning regardless the use of Rold. Model 0 1 2 3 4 5 6 7 8 Semantic Subspace Reg. 80.37 73.76 68.36 64.07 60.36 56.27 53.10 50.45 47.55 Semantic Subspace Reg. noRold 80.37 71.69 63.81 55.99 49.99 44.06 38.83 36.76 33.25 Fine-tuning 80.37 61.77 49.93 40.45 34.04 31.63 28.43 27.91 26.54 Fine-tuning noRold 80.37 62.39 53.89 46.56 39.73 32.92 27.00 23.95 20.39 C R ESULTS IN TABULAR FORM In Table 4 and Table 5, we present the multi-session results in the main paper in the tabular form. D D ETAILS OF FEATURE EXTRACTOR TRAINING We use the exact ResNet described in Tian et al. (2020), the differences compared to the standard ResNet (He et al., 2016): (1) Each block (collection of convolutional blocks) is composed of three convolutional layers instead of two. (2) Number of blocks for ResNet-12 is 4 instead of 6 of the standard version, thus the total number of convolutional layers are the same. (3) Filter sizes are Table 4: miniImageNet +M results across multiple sessions in tabular form. Initial number of base classes is 60 and 5 new classes are introduced at every session. Results are on the test set that grows with the increasing number of classes. In the last session we evaluate over all 100 classes. Session 0 1 2 3 4 5 6 7 8 Model Chen & Lee (2021) 64.77 59.87 55.93 52.62 49.88 47.55 44.83 43.14 41.84 Fine-tuning 80.37 67.69 62.91 59.52 56.87 54.37 51.92 50.26 48.13 Subspace Reg. 80.37 72.90 67.81 63.26 60.18 56.74 53.94 51.29 48.83 +language Linear Mapping 80.37 73.24 67.96 64.50 61.28 57.68 54.64 52.25 50.00 Semantic Subspace Reg. 80.37 73.92 69.00 65.10 61.73 58.12 54.98 52.21 49.65 Table 5: miniImageNet -M results across multiple sessions in tabular form. Initial number of base classes is 60 and 5 new classes are introduced at every session. Results are on the test set that grows with the increasing number of classes. The last session is evaluated over all 100 classes. *Note that the entries for Cheraghian et al. (2021) are only rough estimates from the visual plot provided in their published work. Model 0 1 2 3 4 5 6 7 8 Tao et al. (2020) 61.31 50.09 45.17 41.16 37.48 35.52 32.19 29.46 24.42 Cheraghian et al. (2021)* 62.00 58.00 52.00 49.00 48.00 45.00 42.00 40.00 39.00 Fine-tuning 80.37 61.77 49.93 40.45 34.04 31.63 28.43 27.91 26.54 Subspace Regularization 80.37 71.69 66.94 62.53 58.90 55.00 51.94 49.76 46.79 +language Linear Mapping 80.37 72.65 67.11 63.47 59.82 55.44 51.42 49.64 46.90 Semantic Subspace Reg. 80.37 73.76 68.36 64.07 60.36 56.27 53.10 50.45 47.55 14Published as a conference paper at ICLR 2022 Table 6: miniImageNet validation set accuracy with two ResNet-18 architectures with slight differences as listed in Appendix D. Overall performances are comparable. Seed 1 Seed 2 Seed 3 Seed 4 Seed 5 Seed 6 Seed 7 Seed 8 Seed 9 Seed 10 Mean Our ResNet-18 (Tian et al., 2020) 84.833 79.167 83.200 81.300 81.267 78.933 82.033 82.067 81.800 82.367 81.6967Standard ResNet-18 (He et al., 2016) 83.333 80.100 83.867 81.333 80.967 79.100 81.833 82.500 81.167 81.567 81.5767 [64,160,320,640] rather than [64,128,256,512], though the total number of ﬁlters is comparable since Tian et al. (2020) has less blocks. (4) There is Dropblock at the end of the last blocks. Tian et al. (2020) provides a full visualization in Appendix and their code repository 6 is easy to browse on which we base our own codebase. We observe that the previous work oftentimes use their slightly modiﬁed version of the standard ResNet. Ren et al. (2019) uses the ResNet-10 (Mishra et al., 2017) and ResNet-18 for for miniImageNet and tieredImageNet, respectively. XtarNet(Yoon et al., 2020) is originally based on a slightly modiﬁed version of ResNet-12 and ResNet-18 which we replaced with our version, improving their results for miniImageNet but not in tieredImageNet, thus we report improved results for miniImageNet and their results for tieredImageNet in the main paper. D.1 D EFAULT SETTINGS Unless otherwise indicated we use the following default settings of Tian et al. (2020) in our feature extractor training. We use SGD optimizer with learning starting at 0.05 with decays by 0.1 at epochs 60 and 80. We train for a total of 100 epochs. Weight decay is 5e-4, momentum is 0.9 and batch size is 64. As per transformations on training images, we use random crop of 84x84 with padding 8. We also use color jitter ( brightness=0.4, contrast=0.4, saturation=0.4) and horizontal ﬂips. For each run, we sample 1000 images from base classes and 25 images from each of novel classes. Our classiﬁer does not have bias. D.2 M ULTI -SESSION miniIMAGE NET We re-sample the set of base classes (60 classes) 10 times across different seeds and train ten ResNet- 18 architectures. Each class has 500 training images. We follow the default settings for training. D.3 M ULTI -SESSION COMPARISON TO STANDARD RESNET-18 In Table 6 we provide validation set results for two types of ResNet-18’s: Tian et al. (2020) and He et al. (2016) across ten different seeds. Results show that use of Tian et al. (2020) does not incur unfair advantage over those who used He et al. (2016). D.4 S INGLE -SESSION miniIMAGE NET We follow the default hyperparameters parameters Appendix D.1 except that for training, validation and testing we use the exact splits provided by Ren et al. (2019) also used by Yoon et al. (2020). There are 64 base, 16 validation and 20 testing classes provided (totaling 100). Training data consists of 600 images per base class. Dataset statistics are delineated in the Appendix of Ren et al. (2019) and downloadable splits are available here, courtesy of Ren et al. (2019). D.5 S INGLE -SESSION tieredIMAGE NET tieredImageNet is ﬁrst introduced by Ren et al. (2018). Same as above, we use the default parameters except that we train for a total of 60 epochs decaying the initial learning rate of 0.05 by 0.1 at epochs 30 and 45. Again, we use the same data as previous work available at the same link above. tieredImageNet is split into 351, 97 and 160 classes. Past work that use meta-learning Ren et al. (2019); Yoon et al. (2020) split 351 training classes into further 200 and 151 clases where the latter is used for meta learning. We pool all 351 for feature extractor training. At the end of feature extractor 6https://github.com/WangYueFt/rfs 15Published as a conference paper at ICLR 2022 training, we only keep the classiﬁer weights for the ﬁrst 200 classes to adhere to the evaluation scheme of 200+5 classes as past work. E D ETAILS OF INCREMENTAL EVALUATION E.1 QR D ECOMPOSITION FOR SUBSPACE REGULARIZATION To compute the orthogonal basisPC(0) for the subspace spanned by base classiﬁer weights ηC(0) we use QR decomposition(Trefethen & Bau III, 1997): [ PC(0) Q ′][ R 0 ] = η⊤ C(0) (11) E.2 M ULTI -SESSION For testing, we sample 1000 images from base classes and 25 images from each of novel classes. Testing images from a given class stay the same across sessions. Harmonic mean results take into account the ratio of base classes to novel classes in a given session. In this setting, there is no explicit development set (with disjoint classes than train and test) deﬁned by previous work thus we use the ﬁrst incremental learning session (containing 5 novel classes) as our development set. Default settings We use the same transformations as in Appendix D.1 on the training images. We stop ﬁne-tuning when loss does not change more than 0.0001 for at least 10 epochs. We use SGD optimizer. We repeat the experiments 10 times and report the average accuracy with standard deviation (95% conﬁdence interval) in the paper. Simple Fine-tuning We use learning rate of 0.002 and do not use learning decay. Weight-decay αis set at 5e-3. In order to limit the change in weights, we use different β’s for base and previously learned novel classes, where the former is 0.2 and the latter 0.1. We rely on the default settings otherwise. Subspace Regularization Different from simple ﬁne-tuning we use a weight decay of 5e-4. There is an additional parameter called γin this setting controlling the degree of pulling of novel weights toward the subspace which we set to 1.0. Semantic Subspace Regularization Different than simple subspace regularization, there is a tem- perature parameter used in the Softmax operation used in computation of lc’s which we set to 3.0. Linear mapping regularization Same parameters as in subspace regularization are used except γ = 0 .1. We formulate L as a linear layer with bias and we use gradient descent to train the parameters. E.3 S INGLE -SESSION In our 1-shot experiments unless ﬁne-tuning converges by then, we stop at the maximum number of epochs at 1000. We sample 2000 episodes which includes 5 novels classes and 1-5 samples from each and report average accuracy. For testing, base and novel samples have equal weight in average per previous work (Yoon et al., 2020). SGD optimizer is used. For description similarity we use Sentence-BERT’sstsb-roberta-large. miniImageNet Settings We use the same set of transformations on the training images as de- scribed in Appendix D.1. We ﬁrst describe details of 1-shot setting. In 1-shot experiments we set the maximum epochs to 1000. For simple ﬁne-tuning we use learning rate of 0.003, and weight de- cay of 5e-3. In Semantic Subspace Reg., we set temperature to 1.5. Both in Semantic Subspace Reg. and linear mapping γ = 0.005 and weight-decay is 5e-4. In subspace regularization, γ = 0.005 and weight-decay is set to 5e-5. Description similarity follows the same setup as Semantic Subspace Reg.. 16Published as a conference paper at ICLR 2022 Figure 4: Clasiﬁer weight space when subspace regularization is applied for miniImageNet single-session 5-shot setting. First two principal components are shown according to PCA. Red labels indicate novel classes while the black indicates base. The green crosses indicate the projection of the respective novel class weight to the base subspace. Note that unlike label/description similarity and linear mapping, subspace target is dynamic: it changes according to its corresponding novel weights and vice versa. In 5-shot setting, we set β = 0.03 weight-decay to 5e-3 and learning rate to 0.002. For subspace regularization, Semantic Subspace Reg. and linear mapping we use γ = 0.03 and for description similarity we use γ = 0.01. tieredImageNet Settings In 1-shot setting, ﬁne-tuning uses learning rate of 0.003. Semantic Sub- space Reg. has learning rate of 0.005, weight-decay 5e-3 and γ = 0.005. Subspace reg. and linear mapping use γ = 0.001. In 5-shot setting, for simple ﬁne-tuning we set lr = 0.001, weight-decay=5e-3, β = 0.3. For Seman- tic Subspace Reg. γ = 0.05 and β = 0.2 while others have γ = 0.03. F V ISUALIZATIONS In Fig. 4 and Fig. 5 we depict principal components of classiﬁer weights as well as semantic or subspace targets for novel weights. G C OMPUTE We use a single 32 GB V100 NVIDIA GPU for all our experiments. 17Published as a conference paper at ICLR 2022 Figure 5: Clasiﬁer weight space when Semantic Subspace Reg. is applied for miniImageNet single-session 5-shot setting. First two principal components are shown according to PCA. Red labels indicate novel classes while the black indicates base. The green crosses indicate the semantic target lof the respective novel class. Note that semantic targets are static: they don’t change during ﬁne-tuning. Notably, the semantic target for theater curtain falls closely to the class representation of the base class stage, dragging novel weight for theater curtain towards there. Same dynamic is visible for novel class crate and base barrel. 18",
      "references": [],
      "meta_data": {
        "arxiv_id": "2110.07059v2",
        "authors": [
          "Afra Feyza Akyürek",
          "Ekin Akyürek",
          "Derry Tanti Wijaya",
          "Jacob Andreas"
        ],
        "published_date": "2021-10-13T22:19:53Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes extremely simple subspace-based regularization methods enabling standard logistic regression classifiers (with frozen pretrained CNN features) to handle few-shot class-incremental learning (FSCIL). Achieves state-of-the-art performance on miniImageNet and tieredImageNet, outperforming complex specialized approaches by up to 22% accuracy. Introduces (1) geometric subspace regularizer that pulls new class weight vectors toward subspace spanned by base class weights; (2) semantic subspace regularizer that further guides new weights using language embeddings of class labels/descriptions; (3) linear mapping baseline from language space to weight space.",
        "methodology": "Step 1: Train a CNN feature extractor and linear classifier on base classes using standard cross-entropy with L2 regularization. Step 2 (each incremental session): Freeze CNN; create new weight vectors for novel classes and fine-tune only classifier weights by minimizing cross-entropy on few labeled novel examples plus prior regularization terms: (a) R_old – L2 penalty keeping previously learned weights close to their past values to reduce forgetting; (b) R_new – for each novel class c, add ||η_c − m_c||². For geometric subspace regularization, m_c is the projection of η_c onto the orthogonal basis of base weight subspace (computed via QR decomposition). For semantic subspace, m_c is a convex combination of base weights weighted by softmax of cosine similarities between language embeddings of novel and base classes (optionally learned linear mapping L from embeddings to weight space). Hyper-parameters α,β,γ and temperature τ control strengths. No changes to architecture, no nested optimization.",
        "experimental_setup": "Multi-session FSCIL: miniImageNet split into 60 base + eight sessions of 5 novel classes (5 shots each). ResNet-18 feature extractor trained on 500 images per base class; evaluation on cumulative 1000 base test images plus novel test samples. Comparisons with Tao et al. 2020, Chen & Lee 2021, Cheraghian et al. 2021; ± memory (one replay example per old class) variants. Single-session FSCIL: miniImageNet (64 base + 5 novel classes) and tieredImageNet (200 base + 5 novel) in 1-shot and 5-shot settings; 2000 randomly sampled episodes; ResNet-12 or ResNet-18 extractors. Language embeddings: GloVe for labels, Sentence-BERT for WordNet descriptions. Metrics: top-1 accuracy on base, novel, and weighted average; ∆ gap for catastrophic forgetting.",
        "limitations": "Regularization only adjusts final linear layer; CNN feature extractor is frozen, so representation shift for novel classes is limited. Performance depends on quality of pretrained features and assumes sufficient overlap between base and novel feature subspace. Hyperparameters (α,β,γ,τ) need tuning per dataset/session. Semantic regularization relies on availability and quality of class names/descriptions and external word embeddings. Method tested only on image classification; effectiveness for other modalities/tasks remains unverified. Does not address memory/computation scaling if many incremental sessions accumulate large classifier weight matrices.",
        "future_research_directions": "1) Extend regularization to allow limited, stability-preserving fine-tuning of feature extractor layers. 2) Explore adaptive or learned weighting of subspace versus semantic targets and automated hyperparameter selection. 3) Apply framework to other domains (e.g., NLP, audio) and multimodal incremental learning. 4) Investigate dynamic subspace updates or low-rank factorization to manage classifier growth with many classes. 5) Integrate stronger, context-aware language models (e.g., CLIP, GPT embeddings) or ontological hierarchies to enhance semantic guidance. 6) Study robustness to distribution shifts and adversarial new classes, including scenarios with class splits overlapping or ambiguous labels.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "FreeNeRF: Improving Few-Shot Neural Rendering With Free Frequency Regularization",
      "full_text": "FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization Jiawei Yang UC, Los Angeles jiawei118@ucla.edu Marco Pavone Nvidia Research, Stanford University pavone@stanford.edu Yue Wang Nvidia Research yuewang@nvidia.com NeRF Our base Free-NeRF Turning the left to the right by adding one line of code: pos enc[int(t/T*L)+3:]=0 Figure 1. Example novel view synthesis results from sparse inputs. The only difference between NeRF (left) and FreeNeRF (right) is the use of our frequency regularization, which can be implemented as few as, approximately, one line of code (bottom, where t and T denote the current training iteration and regularization duration, respectively; L is the length of the input positional encoding). Abstract Novel view synthesis with sparse inputs is a challeng- ing problem for neural radiance ﬁelds (NeRF). Recent ef- forts alleviate this challenge by introducing external super- vision, such as pre-trained models and extra depth signals, or by using non-trivial patch-based rendering. In this pa- per, we present Frequency regularized NeRF (FreeNeRF), a surprisingly simple baseline that outperforms previous methods with minimal modiﬁcations to plain NeRF . We an- alyze the key challenges in few-shot neural rendering and ﬁnd that frequency plays an important role in NeRF’s train- ing. Based on this analysis, we propose two regularization terms: one to regularize the frequency range of NeRF’s inputs, and the other to penalize the near-camera density ﬁelds. Both techniques are “free lunches” that come at no additional computational cost. We demonstrate that even with just one line of code change, the original NeRF can achieve similar performance to other complicated methods in the few-shot setting. FreeNeRF achieves state-of-the- art performance across diverse datasets, including Blender, DTU, and LLFF . We hope that this simple baseline will mo- tivate a rethinking of the fundamental role of frequency in NeRF’s training, under both the low-data regime and be- yond. This project is released at FreeNeRF. 1. Introduction Neural Radiance Field (NeRF) [21] has gained tremen- dous attention in 3D computer vision and computer graph- ics due to its ability to render high-ﬁdelity novel views. However, NeRF is prone to overﬁtting to training views and struggles with novel view synthesis when only a few inputs are available. We term this view synthesis from sparse in- puts problem as a few-shot neural rendering problem. 1 arXiv:2303.07418v1  [cs.CV]  13 Mar 2023Existing methods address this challenge using differ- ent strategies. Transfer learning methods, e.g., PixelNerf [37] and MVSNeRF [4], pre-train on large-scale curated multi-view datasets and further incorporate per-scene op- timization at test time. Depth-supervised methods [6, 29] introduce estimated depth as an external supervisory sig- nal, leading to a complex training pipeline. Patch-based regularization methods impose regularization from differ- ent sources on rendered patches, e.g., semantic consistency regularization [11], geometry regularization [22, 8], and ap- pearance regularization [22], all at the cost of computation overhead since an additional, non-trivial number of patches must be rendered during training [11, 22, 8]. In this work, we ﬁnd that a plain NeRF can work sur- prisingly well with none of the above strategies in the few- shot setting by adding (approximately) as few as one line of code (see Fig. 1). Concretely, we analyze the common failure modes in training NeRF under a low-data regime. Drawing on this analysis, we propose two regularization terms. One is frequency regularization, which directly reg- ularizes the visible frequency bands of NeRF’s inputs to stabilize the learning process and avoid catastrophic over- ﬁtting at the start of training. The other is occlusion reg- ularization, which penalizes the near-camera density ﬁelds that cause “ﬂoaters,” another failure mode in the few-shot neural rendering problem. Combined, we call our method Frequency regularized NeRF (FreeNeRF), which is “free” in two ways. First, it is dependency-free because it requires neither costly pre-training [37, 4, 11, 22] nor extra super- visory signals [6, 29]. Second, it is overhead-free as it re- quires no additional training-time rendering for patch-based regularization [11, 22, 8]. We consider FreeNeRF a simple baseline (with mini- mal modiﬁcations to a plain NeRF) in the few-shot neural rendering problem, although it already outperforms exist- ing state-of-the-art methods on multiple datasets, including Blender, DTU, and LLFF, at almost no additional computa- tion cost. Our contributions can be summarized as follows: • We reveal the link between the failure of few-shot neu- ral rendering and the frequency of positional encoding, which is further veriﬁed by an empirical study and ad- dressed by our proposed method. To our knowledge, our method is the ﬁrst attempt to address few-shot neural ren- dering from a frequency perspective. • We identify another common failure pattern in learning NeRF from sparse inputs and alleviate it with a new oc- clusion regularizer. This regularizer effectively improves performance and generalizes across datasets. • Combined, we introduce a simple baseline, FreeNeRF, that can be implemented with a few lines of code mod- iﬁcation while outperforming previous state-of-the-art methods. Our method is dependency-free and overhead- free, making it a practical and efﬁcient solution to this problem. We hope the observations and discussions in this paper will motivate people to rethink the fundamental role of fre- quency in NeRF’s positional encoding. 2. Related Work Neural ﬁelds. Neural ﬁelds [36] use deep neural networks to represent 2D images or 3D scenes as continuous func- tions. The seminal work, Neural Radiance Fields (NeRF) [21], has been widely studied and advanced in a variety of applications [2, 3, 32, 19, 23, 13, 25], including novel view synthesis [21, 18], 3D generation [25, 10], deforma- tion [23, 26, 28], video [15, 35, 7, 24, 14]. Despite tremen- dous progress, NeRF still requires hundreds of input images to learn high-quality scene representations; it fails to syn- thesize novel views with a few input views, e.g., 3, 6, and 9 views, limiting its potential applications in the real world. Few-shot Neural Rendering. Many works have attempted to address the challenging few-shot neural rendering prob- lem by leveraging extra information. For instance, external models can be used to acquire normalization-ﬂow regular- ization [22], perceptual regularization [38], depth supervi- sion [29, 6, 34], and cross-view semantic consistency [11]. Another thread of works [5, 37, 4] attempts to learn transfer- able models by training on a large, curated dataset instead of using an external model. Recent works argue that ge- ometry is the most important factor in few-shot neural ren- dering and propose geometry regularization [22, 1, 8] for better performance. However, these methods require expen- sive pre-training on tailored multi-view datasets [5, 37, 4] or costly training-time patch rendering [11, 22, 1, 8], introduc- ing signiﬁcant overhead in methodology, engineering im- plementation, and training budgets. In this work, we show that a plain NeRF can work surprisingly well with minimal modiﬁcations (a few lines of code) by incorporating our fre- quency regularization and occlusion regularization. Unlike most previous methods, our approach maintains the same computational efﬁciency as the original NeRF. Frequency in neural representations.Positional encoding lies at the heart of NeRF’s success [21, 31]. Previous stud- ies [31, 30] have shown that neural networks often struggle to learn high-frequency functions from low-dimensional in- puts. Encoding inputs with sinusoidal functions of differ- ent frequencies can alleviate this issue. Recent works show the beneﬁts of gradually increasing the input frequency in different applications, such as non-rigid scene deformation [23], bundle adjustment [16], surface reconstruction [33], and ﬁtting functions with a wider frequency band [9]. Our work leverages frequency curriculum to tackle the few-shot neural rendering problem. Notably, our approach not only demonstrates the surprising effectiveness of frequency reg- ularization in learning from sparse inputs, but also reveals 2the failure modes behind this problem and why frequency regularization helps. 3. Method 3.1. Preliminaries Neural radiance ﬁelds. A neural radiance ﬁeld (NeRF) [21] uses a multi-layer perceptron (MLP) to represent a scene as a volumetric density ﬁeld σ and associated RGB values c at each point in the scene. It takes as input a 3D coordinate x ∈ R3 and a viewing directional unit vector d ∈S2, and outputs the corresponding density and color. In its most basic form, NeRF learns a continuous function fθ(x,d) = (σ,c) where θdenotes MLP parameters. Positional encoding. Directly optimizing NeRF over raw inputs (x,d) often leads to difﬁculties in synthesizing high- frequency details [31, 21]. To address this issue, recent work has used sinusoidal functions with different frequen- cies to map the inputs into a higher-dimensional space [21]: γL(x) = [ sin(x),cos(x),..., sin(2L−1x),cos(2L−1x) ] , (1) where Lis a hyperparameter that controls the maximum en- coded frequency and may differ for coordinatesx and direc- tional vectors d. A common practice is to concatenate the raw inputs with the frequency-encoded inputs as follows: x′= [x,γL(x)] (2) This concatenation is applied to both coordinate inputs and view direction inputs. Rendering. To render a pixel in NeRF, a rayr(t) =o + td is cast from the camera’s origin o along the direction d to pass through the pixel, where tis the distance to the origin. Within the near and far bounds [tnear,tfar] of the cast ray, NeRF computes the color of that ray using the quadrature of Ksampled points tK = {t1,...,t K}: ˆc(r; θ,tK) = ∑ K Tk(1 −exp(−σk(tk+1 −tk)))ck, with Tk = exp ( − ∑ k′<k σ′ k(tk′+1 −tk′ ) ) , (3) where ˆc(r; θ,tK) is the ﬁnal integrated color. Note that the sampled points tK are in a near-to-far order, i.e., a point with a smaller index kis closer to the camera’s origin. 3.2. Frequency Regularization The most common failure mode of few-shot neural ren- dering is overﬁtting. NeRF learns 3D scene representations from a set of 2D images without explicit 3D geometry. 3D geometry is implicitly learned by optimizing appearance in its 2D projected views. However, given only a few input Object PSNR 8 10 12 14 16 18 Visible positional encoding ratio x 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 9.01 8.93 8.79 9.30 9.83 9.13 8.90 8.74 10.80 17.62 Ground Truthx=10% x=100% Figure 2. Masking high-frequency inputs helps few-shot neu- ral rendering. We investigate how NeRF performs with positional encodings under different masking ratios on the DTU dataset us- ing 3 input views. Despite its over-smoothness, the plain NeRF succeeds in the few-shot setting when only low-frequency inputs are visible. views, NeRF is prone to overﬁtting to these 2D images with small loss while not explaining 3D geometry in a multi- view consistent way. Synthesizing novel views from such models leads to systematic failure. As shown on the left of Figure 1, no NeRF model can successfully recover the scene geometry when synthesizing novel views. The overﬁtting issue in few-shot neural rendering is pre- sumably exacerbated by high-frequency inputs. [31] shows that higher-frequency mappings enable faster convergence for high-frequency components. However, the over-fast convergence on high-frequency impedes NeRF from ex- ploring low-frequency information and signiﬁcantly biases NeRF towards undesired high-frequency artifacts (horns and room examples in Fig. 1). In the few-shot scenario, NeRF is even more sensitive to susceptible noise as there are fewer images to learn coherent geometry. Thus, we hypoth- esize that high-frequency components are a major cause of the failure modes observed in few-shot neural rendering. We provide empirical evidence below. We investigate how a plain NeRF performs when inputs are encoded by different numbers of frequency bands. To achieve this, we train mipNeRF [2] using masked (integrated) positional encoding. Speciﬁcally, we set pos enc[int(L*x%]):]=0, where L denotes the length of frequency encoded coordinates after the positional encoding (Eq. (1)), and x is the visible ratio. We brieﬂy demonstrate our observation here and defer the experiment details to §4.1. Figure 2 shows the results for the DTU dataset under the 3 input-view setting. As anticipated, we observe a signiﬁcant drop in mipNeRF’s performance as higher-frequency inputs are presented to the model. When 10% of total embedding bits are used, mipNeRF achieves a high PSNR of 17.62, while the plain mipNeRF achieves only 9.01 PSNR on its own (at 100% visible ratio). Theonly difference between these two models is whether masked po- sitional encodings are used. Although removing a signif- icant portion of high-frequency components avoids catas- 3trophic failure at the start of training, it does not result in competitive scene representations, as the rendered im- ages are usually oversmoothed (as seen in Fig. 2 zoom-in patches). Nonetheless, it is noteworthy that in few-shot scenarios, models using low-frequency inputs may produce signiﬁcantly better representations than those using high- frequency inputs. Building on this empirical ﬁnding, we propose a fre- quency regularization method. Given a positional encoding of length L+ 3(Eq. (2)), we use a linearly increasing fre- quency mask α to regulate the visible frequency spectrum based on the training time steps, as follows: γ′ L(t,T; x) =γL(x) ⊙α(t,T,L ), (4) withαi(t,T,L) =   1 if i≤t·L T + 3 t·L T −⌊t·L T ⌋ if t·L T + 3<i≤t·L T + 6 0 if i>t·L T + 6 (5) where αi(t,T,L ) denotes the i-th bit value of α(t,T,L ); tand T are the current training iteration and the ﬁnal iter- ation of frequency regularization, respectively. Concretely, we start with raw inputs without positional encoding and linearly increase the visible frequency by 3-bit each time as training progresses. This schedule can also be simpliﬁed as one line of code, as shown in Figure 1. Our frequency reg- ularization circumvents the unstable and susceptible high- frequency signals at the beginning of training and gradually provides NeRF high-frequency information to avoid over- smoothness. We note that our frequency regularization shares some similarities with the coarse-to-ﬁne frequency schedules used in other works [23, 16]. Different from theirs, our work focuses on the few-shot neural rendering problem and reveals the catastrophic failure patterns caused by high- frequency inputs and their implication to this problem. 3.3. Occlusion Regularization Frequency regularization does not solve all problems in few-shot neural rendering. Due to the limited number of training views and the ill-posed nature of the problem, cer- tain characteristic artifacts may still exist in novel views. These failure modes often manifest as “walls” or “ﬂoaters” that are located extremely close to the camera, as seen in the bottom of Figure 3. Such artifacts can still be observed even with a sufﬁcient number of training views [3]. To ad- dress these issues, [3] proposed a distortion loss. However, our experiments show that this regularization does not help in the few-shot setting and may even exacerbate the issue. We ﬁnd most of these failure patterns originate from the least overlapped regions in the training views. Figure 3 shows an example of 3 training views and 2 novel views with “white walls”. We manually annotate the least over- lapped regions in the training views for demonstration ((a) a b a b is likely caused by  is likely caused by b a Training views Novel views /f.shortloaters a a b Before After Figure 3. Illustration of occlusion regularization. We show 3 training views (solid rectangles) and 2 novel views (dashed rect- angles) rendered by a frequency-regularized NeRF. The ﬂoaters in the novel views appear to benear-camera dense ﬁelds in the train- ing views (dashed circles) so that we can penalize them directly without the need for the costly novel-view rendering in [11, 22]. and (b) in Fig. 3). These regions are difﬁcult to estimate in terms of geometry due to the extremely limited infor- mation available (one-shot). Consequently, a NeRF model would interpret these unexplored areas as dense volumetric ﬂoaters located near the camera. We suspect that the ﬂoaters observed in [3] also come from these least overlapped re- gions. As discussed above, the presence of ﬂoaters and walls in novel views is caused by the imperfect training views, and thus can be addressed directly at training time without the need for novel-pose sampling [22, 11, 37]. To this end, we propose a simple yet effective “occlusion” regularization that penalizes the dense ﬁelds near the camera. We deﬁne: Locc = σ⊺ K ·mK K = 1 K ∑ K σk ·mk, (6) where mk is a binary mask vector that determines whether a point will be penalized, and σK denotes the density values of the K points sampled along the ray in the order of prox- imity to the origin (near to far). To reduce solid ﬂoaters near the camera, we set the values of mk up to index M, termed as regularization range, to 1 and the rest to 0. The occlusion regularization loss is easy to implement and compute. 44. Experiments 4.1. Setups Datasets & metrics. We evaluate our method on three datasets under few-shot settings: the NeRF Blender Syn- thetic dataset (Blender) [21], the DTU dataset [12], and the LLFF dataset [20]. For Blender, we follow DietNeRF [11] to train on 8 views and test on 25 test images. For DTU and LLFF, we adhere to RegNeRF’s [22] protocol. On DTU, we use objects’ masks to remove the background when computing metrics, as full-image evaluation is biased towards the background, as reported by [37, 22]. We report PSNR, SSIM, and LPIPS scores as quantitative results. We also report the geometric mean of MSE = 10−PSNR/10,√ 1 −SSIM, and LPIPS, following [22]. More details on the experimental setup can be found in the appendix. Implementations. Our FreeNeRF can directly improve NeRF [21] and mipNeRF [2]. To demonstrate this, we use DietNeRF’s codebase1 for NeRF on the Blender dataset and RegNeRF’s codebase2 for mipNeRF on the DTU dataset and the LLFF dataset. We disable the proposed compo- nents in those papers and implement our two regularization terms on top of their baselines. We make one modiﬁcation to mipNeRF [2], which is to concatenate positional encod- ings with the original Euclidean coordinates (Eq. (2)). This is a default step in NeRF but not in mipNeRF, and it helps unify our experiments’ initial visible frequency range. We follow their training schedules for optimization. Please re- fer to the Appendix for full training recipes. Hyper-parameters. We set the end iteration of frequency regularization as T = ⌊90% ∗total iters⌋for the 3-view setting and 70% for the 6-view setting and 20% for the 9- view setting. We regularize both coordinates x and view directions d. For Locc, we use a weight of 0.01 in all exper- iments and set the regularization range M = 20for LLFF and Blender and M = 10for DTU. For DTU in particular, we ﬁnd that the “walls” are mostly caused by the white desk and black background, so we use this information to penal- ize more points in a slightly wider range ( M = 15) if their colors are black or white. Comparing methods. Unless otherwise speciﬁed, we di- rectly use the results reported in DietNeRF [11] and Reg- NeRF [22] for comparisons, as our method is implemented using their codebases. We also include our reproduced re- sults for reference. 4.2. Comparison We compare with state-of-the-art methods in terms of novel view synthesis quality and computation overhead. We 1https://github.com/ajayjain/DietNeRF 2https : / / github . com / google - research / google - research/tree/master/regnerf Method PSNR ↑ SSIM ↑ LPIPS ↓ NeRF [21] 14.934 0.687 0.318 NV [17] 17.859 0.741 0.245 Simpliﬁed NeRF [11] 20.092 0.822 0.179 DietNeRF [11] 23.147 0.866 0.109 DietNeRF + LMSE ft 50k 23.591 0.874 0.097 NeRF (repro.) 13.931 0.689 0.320 DietNeRF (repro.) 22.503 0.823 0.124 Our FreeNeRF 24.259 0.883 0.098 Table 1. Quantitative comparison on Blender. “LMSE ft 50k”: ﬁne-tune for another 50k iterations with LMSE. The top row section includes results from [11], while the bottom row section shows our reproduced results (repro.). Gray: our baseline. Red, orange, and yellow: the best, second-best, and third-best. Imagined   Ketchup Imagined   track pads &   driver seat DietNeRF Ours Ground Truth Figure 4. Novel view synthesis examples on Blender. Our re- sults are qualitatively better than DietNeRF’s. DietNeRF renders “imaginary” components that do not exist in the original images. show that FreeNeRF outperforms others in synthesis quality while maintaining a much lower cost. Blender dataset. Table 1 shows the image synthesis met- rics on the Blender dataset [21]. Our approach outperforms all other methods in the PSNR and SSIM scores, with a comparable LPIPS score to the best one. The improved DietNeRF with ﬁne-tuning still underperforms ours. Note that our direct baseline is “NeRF (repro.)” as we do not use any techniques from DietNeRF [11]. Figure 4 shows two examples for qualitative comparison (see Fig. 1 for plain NeRF’s results). Interestingly, we observe that DietNeRF implicitly distills semantic information from a pre-trained CLIP model [27] into NeRF, which leads to unrealistic and “imaginary” patches that do not exist in the original scenes, such as “ketchup” in the hotdog and rubber-like track-pads in the bulldozer. This behavior is highly correlated to fea- ture distillation [13] and recent developments in 3D ob- ject generation that combine NeRF with large pre-trained vision-language models [10, 25]. Although this potentially 5Setting Object PSNR ↑ Object SSIM ↑ Full-image PSNR ↑ Full-image SSIM ↑ 3-view 6-view 9-view 3-view 6-view 9-view 3-view 6-view 9-view 3-view 6-view 9-view SRF [5] Trained on DTU 15.32 17.54 18.35 0.671 0.730 0.752 15.84 17.77 18.56 0.532 0.616 0.652 PixelNeRF [37] 16.82 19.11 20.40 0.695 0.745 0.768 18.74 21.02 22.23 0.618 0.684 0.714 MVSNeRF [4] 18.63 20.70 22.40 0.769 0.823 0.853 16.33 18.26 20.32 0.602 0.695 0.735 SRF ft [5] Trained on DTU and Optimized per Scene 15.68 18.87 20.75 0.698 0.757 0.785 16.06 18.69 19.97 0.550 0.657 0.678 PixelNeRF ft [37] 18.95 20.56 21.83 0.710 0.753 0.781 17.38 21.52 21.67 0.548 0.670 9.680 MVSNeRF ft [4] 18.54 20.49 22.22 0.769 0.822 0.853 16.26 18.22 20.32 0.601 0.694 0.736 mip-NeRF [2] Optimized per Scene 8.68 16.54 23.58 0.571 0.741 0.879 7.64 14.33 20.71 0.227 0.568 0.799 DietNeRF [11] 11.85 20.63 23.83 0.633 0.778 0.823 10.01 18.70 22.16 0.354 0.668 0.740 RegNeRF [22] 18.89 22.20 24.93 0.745 0.841 0.884 15.33 19.10 22.30 0.621 0.757 0.823 mip-NeRF concat. (repro.) Optimized per Scene 9.10 16.84 23.56 0.578 0.754 0.877 7.94 14.15 20.97 0.235 0.560 0.794 †RegNeRF concat. (repro.) 18.50 22.18 24.88 0.744 0.844 0.890 15.00 19.12 22.41 0.606 0.754 0.826 Our FreeNeRF 19.92 23.25 25.38 0.787 0.844 0.888 18.02 22.39 24.2 0.680 0.779 0.833 Table 2. Quantitative comparison on DTU.We present the PSNR and SSIM scores of foreground objects and full images. Our FreeNeRF synthesizes better foreground objects and full images than most of the others. Our direct baseline is mipNeRF [2] (marked in gray). Results in the bottom row section are our reproductions, and others come from [22]. “concat.”: inputs concatenation (Eq. (2)). †ReNeRF: w/o. appearance regularization. The best, second-best, and third-best entries are marked in red, orange, and yellow, respectively. Ours Ground TruthRegNeRFOurs Ground TruthRegNeRF (a) 3 Input Views (b) 6 Input Views (c) 9 Input Views Figure 5. Qualitative comparison on DTU. We show novel views rendered by RegNeRF and ours in 3 and 6 input-view settings. For the Buddha example, the piece-wise geometry regularization used by RegNeRF [22] hurts the ﬁne-grained geometry, erasing the details of eyes, ﬁngers and wrinkles. RegNeRF’s results are rendered by our reproduced†RegNeRF concat. (c.f. Tab. 2). could be an interesting application, such behavior is unde- sired in our task and will hamper outputs’ ﬁdelity. In con- trast, our method does not require semantics regularization while achieving better performance. DTU dataset. Table 2 shows the quantitative results on the DTU dataset. Transfer learning-based methods that require expensive pre-training (SRF [5], PixelNeRF[37], and MVS- NeRF [4]) underperform ours in almost all settings, except the full-image PSNR score under 3-view setting. This may be due to the bias introduced by the white table and black background present in many scenes in the DTU dataset, which can be learned as a prior through pre-training. Com- pared to per-scene optimization methods (mipNeRF [2], DietNeRF [11], and RegNeRF [22]), our approach achieves the best results. Figure 5 shows example novel views ren- dered by RegNeRF and ours. In the Buddha scene, for in- stance, piece-wise smoothness imposed by RegNeRF’s ge- ometry regularization [22] leads to the loss of ﬁne-grained details, such as eyes, ﬁngers, and wrinkles. In contrast, our frequency regularization, which can be seen as an im- plicit geometry regularization, forces smooth geometry at the beginning (due to the limited frequency spectrum) and gradually relaxes the constraint to facilitate the details. In the more challenging scenes (e.g., buildings and the bronze statue in Fig. 5), FreeNeRF produces higher-quality results. LLFF dataset. Table 3 and Figure 6 show quantitative and 6Setting PSNR ↑ SSIM ↑ LPIPS ↓ Average ↓ 3-view 6-view 9-view 3-view 6-view 9-view 3-view 6-view 9-view 3-view 6-view 9-view SRF [5] Trained on DTU 12.34 13.10 13.00 0.250 0.293 0.297 0.591 0.594 0.605 0.313 0.293 0.296 PixelNeRF [37] 7.93 8.74 8.61 0.272 0.280 0.274 0.682 0.676 0.665 0.461 0.433 0.432 MVSNeRF [4] 17.25 19.79 20.47 0.557 0.656 0.689 0.356 0.269 0.242 0.171 0.125 0.111 SRF ft [5] Trained on DTU and Optimized per Scene 17.07 16.75 17.39 0.436 0.438 0.465 0.529 0.521 0.503 0.203 0.207 0.193 PixelNeRF ft [37] 16.17 17.03 18.92 0.438 0.473 0.535 0.512 0.477 0.430 0.217 0.196 0.163 MVSNeRF ft [4] 17.88 19.99 20.47 0.584 0.660 0.695 0.327 0.264 0.244 0.157 0.122 0.111 mip-NeRF [2] Optimized per Scene 14.62 20.87 24.26 0.351 0.692 0.805 0.495 0.255 0.172 0.246 0.114 0.073 DietNeRF [11] 14.94 21.75 24.28 0.370 0.717 0.801 0.496 0.248 0.183 0.240 0.105 0.073 RegNeRF [22] 19.08 23.10 24.86 0.587 0.760 0.820 0.336 0.206 0.161 0.149 0.086 0.067 mip-NeRF concat. (repro.) Optimized per Scene 16.11 22.91 24.88 0.401 0.756 0.826 0.460 0.213 0.160 0.215 0.090 0.066 †RegNeRF concat. (repro.) 18.84 23.22 24.88 0.573 0.770 0.826 0.345 0.203 0.159 0.150 0.085 0.065 Our FreeNeRF 19.63 23.73 25.13 0.612 0.779 0.827 0.308 0.195 0.160 0.134 0.075 0.064 Table 3. Quantitative comparison on LLFF. Our FreeNeRF achieves the best results in most metrics under different input-view settings. Our direct baseline is mipNeRF [2] (marked in gray). Results in the bottom row section are our reproductions, and others come from [22]. “concat.”: inputs concatenation (Eq. (2)). †ReNeRF: w/o. appearance regularization. The best, second-best, and third-best entries are marked in red, orange, and yellow, respectively. OursGround Truth RegNeRF (a) 3 Input Views (b) 6 Input Views (c) 9 Input Views Figure 6. Qualitative comparison on LLFFF. RegNeRF [22] fails to estimate the accurate depth though it renders visually satisfactory RGB images (a). It also suffers from near-camera ﬂoaters (b). In contrast, our method reconstructs less noisy occupancy ﬁelds with fewer ﬂoaters. RegNeRF’s results are rendered by our reproduced†RegNeRF concat. (c.f. Tab. 3). qualitative results, respectively, on the LLFF dataset. We re- produce mipNeRF [2] and obtain better results. Our FreeN- eRF is generally the best. Transfer learning-based meth- ods [5, 4, 37] perform much worse than ours on the LLFF dataset due to the non-trivial domain gap between DTU and LLFF. Compared to RegNeRF [22], our approach predicts more precise geometry and exhibits fewer artifacts. For in- stance, RegNeRF’s rendered “horns” example (Fig. 6-a) is perceptually acceptable but has poor depth map quality, in- dicating its incorrect geometry estimation. FreeNeRF, in contrast, renders a less noisy and smoother occupancy ﬁeld. Also, our approach suffers less from “ﬂoaters” than ReN- eRF (Fig. 6-b), further demonstrating the efﬁcacy of our occlusion regularization. Training overhead. In Table 4, we include the training time of different methods under the same setting. Our method only introduces negligible training overhead (1.02−1.04×) compared to the other approaches (1.62−2.8×). Both Diet- NeRF [11] and RegNeRF [22] render unobserved patches from novel poses for regularization, which signiﬁcantly sets back the training efﬁciency. DietNeRF requires additional forward evaluation of a large model (CLIP ViT B/32,2242, [27]), and RegNeRF also experiences increased computa- tion due to the use of a normalizing ﬂow model (this part is not open-sourced and therefore not available for our exper- iments). In contrast, FreeNeRF does not require such addi- tional steps, making it a lightweight and efﬁcient solution for addressing few-shot neural rendering problems. 4.3. Ablation Study In this section, we ablate our design choices on the DTU dataset and the LLFF dataset under the 3-view setting. We use a batch size of 1024 for faster training instead of 4096 for the main experiments in Tables 2 and 3. Frequency curriculum. We investigate the impact of fre- quency regularization duration T in Figure 7. Our FreeN- 7Dataset # views Training time multiplier w.r.t. baseline NeRF [21] +Ours DietNeRF [11] Blender 8 1.0× 1.02× 2.8× Dataset # views mipNeRF [2] +Ours †RegNeRF [22] DTU 3 1.0× 1.04× 1.69× LLFF 3 1.0× 1.04× 1.98× Table 4. Training time comparison. We run experiments under a fair setting and report the training time multipliers relative to the baselines. Our FreeNeRF has negligible training overhead com- pared to baselines (gray), while DietNeRF and RegNeRF do not. †: w/o. appearance regularization. Note that using appearance regularization will further increase training budgets. 20% 40% 60% 80% 100%17 18 19 20Object PSNR 17.30 18.54 18.97 19.25 19.27 19.47 19.30 19.69 19.81 19.41 w./o. frequency regularization: 14.50 DTU-3 0.20 0.21 0.22 0.23 Object LPIPS 0.219 0.205 0.204 0.205 0.204 0.209 0.213 0.214 0.219 0.230 20% 40% 60% 80% 100% Maximum frequency reached at x% of total training iterations 19 20Object PSNR 19.15 19.23 19.44 19.31 19.48 19.31 19.60 19.61 19.65 19.59 w./o. frequency regularization: 18.12 LLFF-3 0.30 0.31 0.32 0.33 0.34 Object LPIPS 0.328 0.315 0.313 0.310 0.309 0.307 0.315 0.313 0.312 0.318 Figure 7. Effect of frequency regularization duration. We set the end of frequency regularization as T = ⌊total iters ∗x%⌋. FreeNeRF achieves reasonably well performance across a wide range of curriculum choices. All entries use the occlusion regular- ization, including “w/o. frequency regularization”. eRF beneﬁts more from a longer curriculum in terms of PSNR score across two datasets, with the 90%-schedule being the best. We thus adopt it as our default schedule. However, we notice a trade-off between PSNR and LPIPS where a longer frequency regularization duration can result in higher PSNR but lower LPIPS scores. Fine-tuning the trained model can address this issue and yield better LPIPS scores. More details and discussions are provided in the Appendix. Occlusion regularization. Table 5-(a) studies the effect of occlusion regularization. We observe consistent improve- ments in both datasets when occlusion regularization is in- cluded, conﬁrming its efﬁcacy. In contrast, the distortion loss Ldistort in [3] worsens the results. Additionally, we ﬁnd the performance of DTU-3 drops signiﬁcantly if a large M is chosen since a large portion of real radiance ﬁelds falls in those ranges. The hyper-parameter M can be set per dataset empirically according to the scene statistics. Fur- ther, in Table 5-(b), we show that the way our regularization penalizes points near the camera differs from simply adjust- ing the near bound. The latter changes the absolute location of the ray starting point, while the occlusion effect remains in the starting area regardless of changes to the near bound. Limitations. Our FreeNeRF has two limitations. First, a longer frequency curriculum can make the scene smoother but may decrease LPIPS scores despite achieving compet- Settings (bs=1024) DTU-3 LLFF-3 Ours v.s. occlusion regularization rangeM w/t.Ldistort[3] 15.14 19.08 w/o.Locc 17.40 19.16 w/o. B&W prior 19.03 – B&W prior only 19.19 – M= 5 19.78 19.24 M= 10 19.81 19.43 M= 15 18.57 19.58 M= 20 13.76 19.70 M= 25 11.02 19.54 Oursdefault (bs=1024) 19.81 19.70 Oursdefault (bs=4096)20.20 19.73 (a) Ablation Study on Locc. Near w/o.Locc w/t.Locc Ours v.s. tuning near bounds 0.0 17.40 19.09 0.2 17.34 19.39 0.4 17.43 19.61 0.5 17.40 19.81 0.6 17.35 19.11 0.7 16.73 19.11 0.8 15.08 16.77 (b) Locc v.s. near bounds Table 5. Effect of occlusion regularization range. (a) We re- port PSNR scores on the DTU-3 object and LLFF-3 datasets. En- tries except the last row use a batch size of 1024. “B&W” means using the predicted black & white color as additional prior (see “Hyper-parameters” in the “Setup” section). All entries use a 90%-schedule frequency regularization. (b) In the 3-view DTU ablation setting, we disable/enable Locc and vary the near bound to study the impact of our occlusion regularization. Our results show consistent improvement while adjusting the near bound has little impact. Our default settings are marked in gray. itive PSNR scores. Second, occlusion regularization can cause over-regularization and incomplete representations of near-camera objects in the DTU dataset. Per-scene tuning regularization range can alleviate this issue but we opt not to use it in this paper. Further discussion on these limitations can be found in the Appendix. Addressing these limitations can signiﬁcantly improve FreeNeRF and we leave them as future work. Still, we consider FreeNeRF to be a simple yet intriguing baseline approach for few-shot neural render- ing that differs from the current trend of constructing more intricate pipelines. 5. Conclusion We have presented FreeNeRF, a streamlined approach to few-shot neural rendering. Our study unfolds the deep rela- tion between the input frequency and the failure of few-shot neural rendering. A simple frequency regularizer can dras- tically address this challenge. FreeNeRF outperforms the existing state-of-the-art methods on multiple datasets with minimal overhead. Our results suggest several venues for future investigation. For example, it is intriguing to apply FreeNeRF to other problems suffering from high-frequency noise, such as NeRF in the wild [18], in the dark [20], and even more challenging images in the wild, such as those from autonomous driving scenes. In addition, in the Appendix, we show that the frequency-regularized NeRF produces smoother normal estimation, which can facilitate applications that deal with glossy surfaces, as in RefN- eRF [32]. We hope our work will inspire further research in few-shot neural rendering and the use of frequency regu- larization in neural rendering more generally. 8References [1] Anonymous. Neural radiance ﬁelds with geometric consis- tency for few-shot novel view synthesis. In Submitted to The Eleventh International Conference on Learning Representa- tions, 2023. under review. [2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neu- ral radiance ﬁelds. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 5855–5864, 2021. [3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance ﬁelds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5470–5479, 2022. [4] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general- izable radiance ﬁeld reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14124–14133, 2021. [5] Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard Pons-Moll. Stereo radiance ﬁelds (srf): Learning view syn- thesis for sparse views of novel scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7911–7920, 2021. [6] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra- manan. Depth-supervised nerf: Fewer views and faster train- ing for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12882– 12891, 2022. [7] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenen- baum, and Jiajun Wu. Neural radiance ﬂow for 4D view synthesis and video processing. arXiv preprint arXiv:2012.09790, 2020. [8] Thibaud Ehret, Roger Mar ´ı, and Gabriele Facciolo. Nerf, meet differential geometry! arXiv preprint arXiv:2206.14938, 2022. [9] Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, and Daniel Cohen-Or. Sape: Spatially-adaptive progressive encoding for neural optimization. Advances in Neural Infor- mation Processing Systems, 34:8820–8832, 2021. [10] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object genera- tion with dream ﬁelds. InProceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 867–876, 2022. [11] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5885–5894, 2021. [12] Rasmus Jensen, Anders Dahl, George V ogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis eval- uation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 406–413, 2014. [13] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz- mann. Decomposing nerf for editing via feature ﬁeld dis- tillation. arXiv preprint arXiv:2205.15585, 2022. [14] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, and Zhaoyang Lv. Neu- ral 3d video synthesis, 2021. [15] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene ﬂow ﬁelds for space-time view synthesis of dy- namic scenes. https://arxiv.org/abs/2011.13084, 2020. [16] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si- mon Lucey. Barf: Bundle-adjusting neural radiance ﬁelds. In IEEE International Conference on Computer Vision (ICCV), 2021. [17] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol- umes: Learning dynamic renderable volumes from images. arXiv preprint arXiv:1906.07751, 2019. [18] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck- worth. Nerf in the wild: Neural radiance ﬁelds for uncon- strained photo collections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7210–7219, 2021. [19] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan, and Jonathan T Barron. Nerf in the dark: High dynamic range view synthesis from noisy raw images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16190–16199, 2022. [20] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light ﬁeld fusion: Practical view syn- thesis with prescriptive sampling guidelines. ACM Transac- tions on Graphics (TOG), 38(4):1–14, 2019. [21] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance ﬁelds for view syn- thesis. In ECCV, 2020. [22] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg- nerf: Regularizing neural radiance ﬁelds for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 5480–5490, 2022. [23] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Soﬁen Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerﬁes: Deformable neural radiance ﬁelds. ICCV, 2021. [24] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. [25] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden- hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 9[26] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural radiance ﬁelds for dynamic scenes. https://arxiv.org/abs/2011.13961, 2020. [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In International Conference on Machine Learning , pages 8748–8763. PMLR, 2021. [28] Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasacchi. DeRF: De- composed radiance ﬁelds. https://arxiv.org/abs/2011.12490, 2020. [29] Barbara Roessle, Jonathan T Barron, Ben Mildenhall, Pratul P Srinivasan, and Matthias Nießner. Dense depth pri- ors for neural radiance ﬁelds from sparse input views. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 12892–12901, 2022. [30] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representa- tions with periodic activation functions. Advances in Neural Information Processing Systems, 33:7462–7473, 2020. [31] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra- mamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimen- sional domains. Advances in Neural Information Processing Systems, 33:7537–7547, 2020. [32] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Struc- tured view-dependent appearance for neural radiance ﬁelds. In 2022 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 5481–5490. IEEE, 2022. [33] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. Hf-neus: Improved surface reconstruction using high-frequency de- tails. arXiv preprint arXiv:2206.07850, 2022. [34] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerﬁngmvs: Guided optimization of neural radiance ﬁelds for indoor multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 5610–5619, 2021. [35] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance ﬁelds for free-viewpoint video. https://arxiv.org/abs/2011.12950, 2020. [36] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tomp- kin, Vincent Sitzmann, and Srinath Sridhar. Neural ﬁelds in visual computing and beyond. Computer Graphics Forum, 2022. [37] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance ﬁelds from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578–4587, 2021. [38] Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. Ners: Neural reﬂectance surfaces for sparse-view 3d reconstruction in the wild. Advances in Neural Informa- tion Processing Systems, 34:29835–29847, 2021. 10Supplement to FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization Project page: FreeNeRF 10% visible 20% visible 30% visible 50% visible 70% visible 100% visible Figure A.1. High-frequency inputs cause catastrophic failure in few-shot neural rendering. We train mipNeRF [2] with masked (integrated) positional encoding by setting pos enc[int(L*x%]):]=0, where L denotes the length of frequency bands (Eq. (1)) and x is the masking ratio. Using low-frequency components as inputs enables mipNeRF to learn meaningful scene representations despite their over-smoothness. Please refer to Figure 2 (in the main text) for numerical comparisons. We also provide animated visualizations on our project page. In this supplement, we include additional quantitative and qualitative results to discuss more motivation and limi- tations of FreeNeRF in Appendix A. We also add details of experimental settings and implementations in Appendix B. A. Additional Results High-frequency inputs cause catastrophic failure. Fig- ure A.1 shows more examples to demonstrate the failure mode revealed in Figure 2 that the high-frequency inputs lead to the catastrophic failure of few-shot neural render- ing. When taking in 10% of the total embedding bits, mipNeRF can successfully reconstruct scenes despite their over-smoothness. However, with higher-frequency inputs, the scene reconstructions become more unrecognizable and collapse. This experimental ﬁnding lies at the heart of FreeNeRF: by restricting the inputs to the low-frequency components at the start of training, NeRF can start from sig- niﬁcantly stabilized scene representations at the early stage of training. Upon these stable scene representations, NeRF continues reﬁning the details when high-frequency signals become visible. A.1. Limitations In this subsection, we elaborate on the limitations and showcase the failure cases of FreeNeRF. Trade-off between PSNR and LPIPS.Figure 7 studies the 11Ground Truth  20%-schedule 90%-schedule Figure A.2. High-frequency details comparison. We show the view synthesis results under the 9 input-view setting on the DTU dataset. With enough view information, a shorter frequency reg- ularization enables NeRF models to render more high-frequency details. effect of the duration of frequency regularization on PSNR and LPIPS. From the ﬁgure, we observe a trade-off between PSNR and LPIPS that a long-frequency curriculum usually results in a high PSNR score but a low LPIPS score. For ex- ample, under the 9 input-view setting, we obtain an object PSNR of 25.59 and an object LPIPS of 0.117 with a 90%- schedule and those of 25.38 and 0.096 with a 50%-schedule. Visually, when the number of input views is relatively suf- ﬁcient (but still under few-shot settings), results under a shorter schedule usually present more high-frequency de- tails (see the zoom-in patch in Fig. A.2). We thus use 70%- schedule and 50%-schedule for experiments under 6 and 9 input-view settings, respectively. We also found out that training FreeNeRF longer can obtain better LPIPS perfor- mance, e.g., 0.182 to 0.167 and 0.308 to 0.290 for DTU-3 and LLFF-3 settings, respectively. Limitations of Locc. Over-regularization: our occlusion regularization can lead to an incomplete white desk on the DTU dataset due to over-regularization in some scenes, as shown in Figure A.3-(a). Reducing the regularization range of Locc can ease this issue. A set of per-scene tuned hyper- parameters can potentially provide better results. Remote ﬂoaters: Figure A.3-(b) shows some small cloudy ﬂoaters far from the camera. Our occlusion regularization that pe- nalizes near-camera dense ﬁelds does not solve this prob- lem. However, we do not observe these remote ﬂoaters in NeRF trained with only low-frequency inputs (10% visible). That said, though signiﬁcantly regularized and stabilized, FreeNeRF still overﬁts to spurious occupancy to a certain degree. Better performance is excepted if FreeNeRF further exploits the low-frequency components to avoid such over- ﬁtting, leaving room for future work and improvements. A.2. Depth Evaluation Here we include results to compare the capability of dif- ferent methods in depth estimation. As the datasets do not have actual ground truth depth, we utilized depth maps gen- erated by mipNeRFs that were trained on all views as a substitute. FreeNeRF signiﬁcantly improves its baseline, mipNeRF. RegNeRF, with its patch-based geometry regu- Ground Truth Ours default ( )M = 10 (a) Examples of over-regularized white desk (b) Remote ﬂoaters that are unrecognizable from depth maps M = 5 Figure A.3. Limitations of occlusion regularization. (a) Aggres- sive occlusion regularization results in incomplete white desks that are visually annoying. Reducing the regularization range (from M = 10 to M = 5) can alleviate the issue to some extent. (b) Occlusion regularization does not solve remote ﬂoaters that are far from cameras. larization, achieves better performance on the object-centric DTU dataset, while FreeNeRF performs better on the scene- scale LLFF dataset without explicit geometry regulariza- tion. This experiment demonstrates the different features of FreeNeRF and RegNeRF, as well as the differences be- tween DTU and LLFF datasets. Error=∥Dpseu −Dpred∥ DTU obj depth error↓ LLFF depth error↓ # views 3 6 9 3 6 9 mipNeRF (baseline) 131.97 59.21 18.73 149.18 36.92 19.16 RegNeRF (explicit geo. reg.) 14.58 10.40 6.23 44.52 25.09 18.26 FreeNeRF 14.89 12.98 9.48 39.92 23.61 16.91 A.3. Additional Qualitative Results Table A.1 provides more numeric results in addition to Table 2 on the DTU dataset. FreeNeRF achieves the best results under the “Average” metrics in most settings. How- ever, we observe less improvement in terms of LPIPS. As we analyze in Appendix A.1, the slight blurriness intro- duced by FreeNeRF will result in a low LPIPS score. This is a limitation that could be addressed in the future. A.4. Additional Visualizations Blender. In Figure A.4, we show more qualitative com- parisons between DietNeRF [11] and our FreeNeRF on the Blender dataset. From the zoom-in patches of DietNeRF’s results, we see the generated patches are blurry and do not reﬂect the same distribution of style as that of ground truth. This is due to implicit semantics distillation behavior 12Setting Object LPIPS ↓ Object Average ↓ Full-image LPIPS ↓ Full-image Average ↓ 3-view 6-view 9-view 3-view 6-view 9-view 3-view 6-view 9-view 3-view 6-view 9-view SRF [5] Trained on DTU 0.304 0.250 0.232 0.171 0.132 0.120 0.482 0.401 0.359 0.207 0.162 0.145 PixelNeRF [37] 0.270 0.232 0.220 0.147 0.115 0.100 0.401 0.340 0.323 0.154 0.119 0.105 MVSNeRF [4] 0.197 0.156 0.135 0.113 0.088 0.068 0.385 0.321 0.280 0.184 0.146 0.114 SRF ft [5] Trained on DTU and Optimized per Scene 0.281 0.225 0.205 0.162 0.114 0.093 0.431 0.353 0.325 0.196 0.143 0.125 PixelNeRF ft [37] 0.269 0.223 0.203 0.125 0.104 0.090 0.456 0.351 0.338 0.185 0.121 0.117 MVSNeRF ft [4] 0.197 0.155 0.135 0.113 0.089 0.069 0.384 0.319 0.278 0.185 0.146 0.113 mip-NeRF [2] Optimized per Scene 0.353 0.198 0.092 0.323 0.148 0.056 0.655 0.394 0.209 0.485 0.231 0.098 DietNeRF [11] 0.314 0.201 0.173 0.243 0.101 0.068 0.574 0.336 0.277 0.383 0.149 0.098 RegNeRF [22] 0.190 0.117 0.089 0.112 0.071 0.047 0.341 0.233 0.184 0.189 0.118 0.079 mip-NeRF concat. (repro.) Optimized per Scene 0.348 0.197 0.100 0.311 0.144 0.057 0.643 0.403 0.218 0.472 0.240 0.099 †RegNeRF concat. (repro.) 0.196 0.118 0.088 0.117 0.070 0.046 0.350 0.236 0.183 0.197 0.118 0.078 Our FreeNeRF 0.182 0.137 0.096 0.098 0.068 0.046 0.318 0.240 0.187 0.146 0.094 0.068 Table A.1. Quantitative comparison on DTU. We provide additional quantitative results to Table 2. Results in the bottom row are reproduced by us, and others come from [22]. “concat.”: inputs concatenation (Eq. (2)). †ReNeRF: w/o. appearance regularization. The best, second-best, and third-best entries are marked in red, orange, and yellow, respectively. done by DietNeRF. In contrast, our FreeNeRF reconstructs scenes closer to the ground truth. DTU and LLFF. We provide more rendering results by FreeNeRF in Figures A.5 and A.6 under the 3 input-view setting on the DTU dataset and the LLFF dataset, respec- tively. A.5. FreeNeRF for Normal Estimation We brieﬂy demonstrate a potential FreeNeRF’s applica- tion beyond few-shot neural rendering. Speciﬁcally, we fol- low the similar settings in RefNeRF[32] to train a mipNeRF and a FreeNeRF on the “coffee” scene in the Shiny Blender dataset [32]. This dataset aims to benchmark NeRF’s per- formance on glossy surfaces, where the key challenge is to estimate accurate normal vectors. Figure A.7 shows the comparison between mipNeRF and FreeNeRF. Com- pared to mipNeRF, FreeNeRF produces more accurate nor- mal estimation and achieves much lower mean angular er- ror (MAE) at no sacriﬁce of PSNR score. We conjecture that overﬁtting to high-frequency signals at the start of train- ing is a very common issue in NeRF’s training. However, such partial failure is veiled by good appearance results. We believe these partially degenerated results can be improved with frequency regularization, which makes NeRF’s initial training more stable. B. Experiment Details We strictly follow the experimental settings in DietNeRF [11] and RegNeRF [22] to conduct our experiments. We provide some details in the following for completeness. B.1. Dataset and metrics. Blender Dataset. The Blender dataset [21] has 8 synthetic scenes in total. We follow the data split used in DietNeRF [11] to simulate a few-shot neural rendering scenario. For each scene, the training images with IDs (counting from “0”) 26, 86, 2, 55, 75, 93, 16, 73, and 8 are used as the input views, and 25 images are sampled evenly from the testing images for evaluation. We follow [11] to use a 2× downsampled resolution, resulting in 400 ×400 pixels for each image. DTU Dataset. The DTU dataset [12] is a large-scale multi- view dataset that consists of 124 different scenes. Pixel- NeRF [37] uses a split of 88 training scenes and 15 test scenes to study the “pre-training & per-scene ﬁne-tuning” setting in a few-shot neural rendering scenario. Different from theirs, our method does not require pre-training. We follow [22] to optimize NeRF models directly on the 15 test scenes. The test scan IDs are: 8, 21, 30, 31, 34, 38, 40, 41, 45, 55, 63, 82, 103, 110, and 114. In each scan, the images with the following IDs (counting from “0”) are used as the input views: 25, 22, 28, 40, 44, 48, 0, 8, 13. The ﬁrst 3 and 6 image IDs correspond to the input views in 3- and 6-view settings, respectively. The images with IDs in [1, 2, 9, 10, 11, 12, 14, 15, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 41, 42, 43, 45, 46, 47] serve as the novel views for evaluation. The remaining images are excluded due to wrong exposure. We follow [22, 37] to use a 4 ×downsampled resolution, resulting in 300 ×400 pixels for each image. LLFF Dataset. The LLFF dataset [20] is a forward-facing dataset that contains 8 scenes in total. Adhere to [22, 21], we use every 8-th image as the novel views for evaluation, and evenly sample the input views across the remaining views. Images are downsampled 8×, resulting in 378×504 pixels for each image. Metrics. To compute PSNR scores, we use the formula −10 ·log10(MSE) (assuming the maximum pixel value is 1). Additionally, we utilize the scikit-image’s API3 to com- 3https://scikit-image.org/docs/stable/auto_examples/ 13Ground Truth DietNeRF Ours Lego Chair Materials Mic Figure A.4. Qualitative comparison on the Blender dataset. DietNeRF generates patches that can be reasonable and plausible to some extent but do not closely match the ground truth. This is a limitation of using a pre-trained model for semantic regularization. In contrast, our FreeNeRF reconstructs scenes that are more in line with the ground truth. Scan8 Scan21 Scan30 Scan31 Scan34 Scan38 Scan40 Scan41 Scan45 Scan55 Scan63 Scan103 Scan82 Scan110 Scan114 (a) (b) (c) (d) (e) (a) (b) (c) (d) (e) Figure A.5. Example FreeNeRF’s novel view synthesis results with 3 input views on the DTU dataset. transform/plot_ssim.html 14Fern Leaves Fortress Room Orchids Flower Horns Trex (a) (b) (c) (a) (b) (c) Figure A.6. Example FreeNeRF’s novel view synthesis results with 3 input views on the LLFF dataset. Ground Truth mip-NeRF PSNR /uni2191 Ground Truth mip-NeRF Ours PSNR=28.34 PSNR=28.55 Normals MAE=36.91 Normals MAE=30.54 Normals MAE=37.06 Normals MAE=30.59PSNR=33.61 PSNR=34.13 Normals MAE=34.24 Normals MAE=27.83PSNR=28.65 PSNR=29.60 Ours Normals Mean Angular Error (MAE) /uni2193 Figure A.7. Comparison on normal vectors estimation. Beyond the few-shot neural rendering problem, we train a mipNeRF and a FreeNeRF on the “coffee” scene in the Shiny Blender dataset [32] to demonstrate FreeNeRF’s potential in estimating more accurate normal vectors. The PSNR scores for this scene are 30.839 and 31.364 for mipNeRF and FreeNeRF, respectively. The mean angular errors (the lower, the better) are 36.549 and 31.492 for mipNeRF and FreeNeRF, respectively. Note that we use a much smaller batch size (4096) than that in the original setting (16394), so the numerical results here are not comparable to those in RefNeRF [32]. 15pute the structural similarity index measure (SSIM) score and the interface provided by an open source repository 4 (using a learned VGG model) to compute the learned per- ceptual image patch similarity (LPIPS) score. B.2. Implementations. DietNeRF’s codebase. In this codebase 5, a plain NeRF [21] that consists of two MLPs (one coarse MLP and one ﬁne MLP) is used as the baseline. All NeRF models are trained with the Adam optimizer for 200k iterations. The learning rate starts at 5 ×10−4 and decays exponentially with a rate of 0.1. We refer readers to the codebase for more details. In this codebase, the maximum input frequency L (Eq. (1)) used in the position encoding for coordinates is 9. The original coordinates are concatenated with positional encodings by default. RegNeRF’s codebase.In this codebase6, a plain mipNeRF [2] is used as the baseline. The maximum input frequency of coordinates is 16, which is larger than that of the original NeRF [21]. We further concatenate the original coordinates into the positional encodings. All NeRF models are trained with the Adam optimizer with an exponential learning rate decaying from 2 ·10−3 to 2 ·10−5 and 512 warm-up steps with a multiplier of 0.01 [2]. Following [22], we clip gra- dients by value at 0.1 and then by norm at 0.1 for all ex- periments. All NeRF models in the main experiments are optimized for 500 epochs with a batch size of 4096. This setting results in around 44k, 88k and 132k training itera- tions on the DTU dataset for 3/6/9 input views, respectively, and 70k, 140k and 210k training iterations for those on the LLFF dataset, respectively. Note that in the ablation study we use a batch size of 1024 instead of 4096 for faster train- ing. Occlusion regularization. We use a weight of 0.01 for Locc in all experiments. For simplicity, we compute this loss on the secondary stage’s outputs, i.e. those from the ﬁne MLP in NeRF [21] and the second query in mipNeRF [2]. 4https://github.com/richzhang/PerceptualSimilarity 5https://github.com/ajayjain/DietNeRF 6https://github.com/google- research/google- research/ tree/master/regnerf 16",
      "references": [
        "Neural radiance ﬁelds with geometric consistency for few-shot novel view synthesis.",
        "Mip-nerf: A multiscale representation for anti-aliasing neural radiance ﬁelds.",
        "Mip-nerf 360: Unbounded anti-aliased neural radiance ﬁelds.",
        "Mvsnerf: Fast generalizable radiance ﬁeld reconstruction from multi-view stereo.",
        "Stereo radiance ﬁelds (srf): Learning view synthesis for sparse views of novel scenes.",
        "Depth-supervised nerf: Fewer views and faster training for free.",
        "Neural radiance ﬂow for 4D view synthesis and video processing.",
        "Nerf, meet differential geometry!",
        "Sape: Spatially-adaptive progressive encoding for neural optimization.",
        "Zero-shot text-guided object generation with dream ﬁelds.",
        "Putting nerf on a diet: Semantically consistent few-shot view synthesis.",
        "Large scale multi-view stereopsis evaluation.",
        "Decomposing nerf for editing via feature ﬁeld distillation.",
        "Neural 3d video synthesis.",
        "Neural scene ﬂow ﬁelds for space-time view synthesis of dynamic scenes.",
        "Barf: Bundle-adjusting neural radiance ﬁelds.",
        "Neural volumes: Learning dynamic renderable volumes from images.",
        "Nerf in the wild: Neural radiance ﬁelds for unconstrained photo collections.",
        "Nerf in the dark: High dynamic range view synthesis from noisy raw images.",
        "Local light ﬁeld fusion: Practical view synthesis with prescriptive sampling guidelines.",
        "Nerf: Representing scenes as neural radiance ﬁelds for view synthesis.",
        "Regnerf: Regularizing neural radiance ﬁelds for view synthesis from sparse inputs.",
        "Nerﬁes: Deformable neural radiance ﬁelds.",
        "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
        "Dreamfusion: Text-to-3d using 2d diffusion.",
        "D-NeRF: Neural radiance ﬁelds for dynamic scenes.",
        "Learning transferable visual models from natural language supervision.",
        "DeRF: Decomposed radiance ﬁelds.",
        "Dense depth priors for neural radiance ﬁelds from sparse input views.",
        "Implicit neural representations with periodic activation functions.",
        "Fourier features let networks learn high frequency functions in low dimensional domains.",
        "Ref-nerf: Structured view-dependent appearance for neural radiance ﬁelds.",
        "Hf-neus: Improved surface reconstruction using high-frequency details.",
        "Nerﬁngmvs: Guided optimization of neural radiance ﬁelds for indoor multi-view stereo.",
        "Space-time neural irradiance ﬁelds for free-viewpoint video.",
        "Neural ﬁelds in visual computing and beyond.",
        "Pixelnerf: Neural radiance ﬁelds from one or few images.",
        "Ners: Neural reﬂectance surfaces for sparse-view 3d reconstruction in the wild."
      ],
      "meta_data": {
        "arxiv_id": "2303.07418v1",
        "authors": [
          "Jiawei Yang",
          "Marco Pavone",
          "Yue Wang"
        ],
        "published_date": "2023-03-13T18:59:03Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces FreeNeRF, a minimal-change baseline for few-shot neural rendering that outperforms prior state-of-the-art. The paper identifies high-frequency positional encodings as a key cause of overfitting when NeRF is trained with sparse views, and proposes two lightweight regularizers—frequency regularization and occlusion regularization—that can be implemented with ~1 line of code and incur virtually no extra computation.",
        "methodology": "1) Frequency Regularization: apply a curriculum on positional encoding frequencies—start with raw coordinates (low frequency) and linearly unmask higher-frequency bands up to iteration T, stabilizing early training.\n2) Occlusion Regularization: penalize density values for the first M stratified samples along each camera ray, discouraging near-camera ‘floater’ artifacts.\nNo external supervision, pre-training, or additional patch rendering is required. Implemented on top of vanilla NeRF and mip-NeRF.",
        "experimental_setup": "Datasets: Blender Synthetic (8 scenes, 8 input views); DTU (15 test scans, 3/6/9 input views); LLFF (8 scenes, 3/6/9 input views). Metrics: PSNR, SSIM, LPIPS, geometric mean of MSE/SSIM/LPIPS; additional depth error analysis. Baselines include NeRF, mip-NeRF, DietNeRF, RegNeRF, PixelNeRF, SRF, MVSNeRF. Training follows public codebases (DietNeRF for NeRF, RegNeRF for mip-NeRF) with identical hyper-parameters, adding only the two regularizers. Ablations vary curriculum length and occlusion range. Computational overhead measured as training-time multiplier.",
        "limitations": "1) Frequency curriculum trade-off: longer schedules improve PSNR but reduce perceptual sharpness (LPIPS). 2) Occlusion regularization may over-penalize legitimate geometry near the camera, causing incomplete reconstructions; selecting range M is scene-dependent. 3) Remote ‘floater’ artifacts far from the camera remain unsolved. 4) Approach has been validated only on static scenes with standard datasets; effectiveness on highly complex, unbounded, or dynamic scenes is untested.",
        "future_research_directions": "• Adaptive or learned frequency schedules to balance sharpness and stability.\n• Enhanced occlusion handling that distinguishes true near-surface geometry from artifacts.\n• Extension to unbounded, outdoor, low-light, or dynamic scenes (e.g., NeRF-in-the-Wild, autonomous-driving datasets).\n• Integration with depth or semantic priors to further reduce ambiguity without heavy computation.\n• Application to tasks needing accurate normals or glossy surfaces, leveraging smoother normal estimates from frequency regulation.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning",
      "full_text": "Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning Berken Utku Demirel Department of Computer Science ETH Zürich, Switzerland berken.demirel@inf.ethz.ch Christian Holz Department of Computer Science ETH Zürich, Switzerland christian.holz@inf.ethz.ch Abstract The success of contrastive learning is well known to be dependent on data aug- mentation. Although the degree of data augmentations has been well controlled by utilizing pre-defined techniques in some domains like vision, time-series data augmentation is less explored and remains a challenging problem due to the com- plexity of the data generation mechanism, such as the intricate mechanism in- volved in the cardiovascular system. Moreover, there is no widely recognized and general time-series augmentation method that can be applied across different tasks. In this paper, we propose a novel data augmentation method for quasi- periodic time-series tasks that aims to connect intra-class samples together, and thereby find order in the latent space. Our method builds upon the well-known mixup technique by incorporating a novel approach that accounts for the peri- odic nature of non-stationary time-series. Also, by controlling the degree of chaos created by data augmentation, our method leads to improved feature rep- resentations and performance on downstream tasks. We evaluate our proposed method on three time-series tasks, including heart rate estimation, human ac- tivity recognition, and cardiovascular disease detection. Extensive experiments against state-of-the-art methods show that the proposed approach outperforms prior works on optimal data generation and known data augmentation techniques in the three tasks, reflecting the effectiveness of the presented method. Source code: https://github.com/eth-siplab/Finding_Order_in_Chaos. 1 Introduction Self-supervised learning methods have gained significant attention as they enable the discovery of meaningful representations from raw data without explicit annotations. These self-supervised methods learn representations without labels by designing pretext tasks that transform the unsupervised representation learning problem into a supervised one such as predicting the rotation of images [1], or contexts [2, 3]. Among these methods, contrastive learning (CL), which learns to distinguish semantically similar examples over dissimilar ones, stands out as a powerful approach in self- supervised learning across various domains including computer vision [4–6], speech recognition [7– 10], and natural language processing [11–14]. The success of contrastive learning relies on the creation of similar and dissimilar examples, which is typically achieved through the use of data augmentations [ 15, 16]. Recently, it was shown that data augmentations have a role to create a “ chaos” between different intra-class samples such that they become more alike. For example, two different cars become very similar when they are both cropped to the wheels. [ 17]. However, in time-series data, creating similar samples with augmentation techniques is more challenging due to the complexity of the dynamical data generation mechanisms [18]. Moreover, research on contrastive learning for time series has demonstrated the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2309.13439v2  [cs.LG]  21 Dec 2023absence of a unique data augmentation technique that consistently performs better than others in different tasks [19, 20]. Instead, the choice of augmentation depends on the contextual characteristics of the signal, such as perturbing the high-frequency content of a signal that carries characteristic information in low frequencies does not generate useful data samples that are helpful for contrastive learning to learn class invariant features [21]. Considering these limitations, in this work, we first propose a novel data augmentation method for time series data by performing a tailored mixup while considering the phase and amplitude information as two separate features. Then, we perform specific operations for both features to generate positive samples by controlling the mixup coefficients for each feature to prevent aggressive augmentation. Specifically, our method employs a technique that controls the mixup ratio for each randomly chosen pair based on their distance in the latent space which is acquired through the use of a variational autoencoder (V AE), whose objective is to learn disentangled representations of data without labels. To this end, subjecting to the distance constraint in the latent space, the mixup tries to connect semantically closer samples together more aggressively while preventing the excessive interpolation of dissimilar samples that are likely to belong to different classes. Therefore, the purpose of our proposed method for quasi-periodic time-series data augmentation is to find an order in “chaos” between different samples such that they become more alike by interpolating them in a novel manner to prevent the loss of information. We summarize our contributions as follows: • We propose a novel mixup method for non-stationary quasi-periodic time-series data by considering phase and magnitude as two separate features to generate samples that enhance intra-class similarity and help contrastive learning to learn class-separated representations. • We present a novel approach for sampling mixup coefficients for each pair based on their similarity in the latent space, which is constructed without supervision while learning disentangled representations, to prevent aggressive augmentation between samples. • We show that the tailored mixup with coefficient sampling consistently improves the perfor- mance of contrastive learning in three time-series tasks compared to prior mixup techniques and proposed augmentation methods that generate optimal/hard positives or negatives. 2 Preliminaries 2.1 Notation We use lowercase symbols (x) to denote scalar quantities, bold lowercase symbols (x) for vector values, and capital letters (X) for random variables. Functions with a parametric family of mappings are represented as fθ(.) where θ is the parameters. The discrete Fourier transformation of a real- valued time series sample is denoted as F(.), yielding a complex variable as Xk where X ∈ C and k ∈ [0, fs/2] is the frequency with the maximum value of Nyquist rate. The amplitude and phase of the F(x) are represented as A(x) and P(x). The real and imaginary parts of a complex variable are shown as Re(.) and Im(.). The detailed calculations for operations are given in the Appendix A.1. 2.2 Setup We follow the common CL setup as follows. Given a datasetD = {(xi)}K i=1 where each xi consists of real-valued sequences with length L andC channels. The objective is to train a learnerfθ which seeks to learn an invariant representation such that when it is fine-tuned on a datasetDl = {(xi, yi)}M i=1 with M ≪ K and yi ∈ {1, . . . , N}, it can separate samples from different classes. 2.3 Motivation As stated by prior works, mixup-based methods have poor performance in domains where data has a non-fixed topology, such as trees, graphs, and languages [22, 23]. Here, we demonstrate how we derived our proposed method by revealing the limitations of mixup for time series theoretically while considering the temporal dependencies and non-stationarities. Assumption 2.1 (SNR Matters). There exist one or multiple bandlimited frequency ranges of interest f∗, where the information that average raw time-series data conveys about the labels (i.e.,I(y; x)) is directly proportional to normalized signal power in that frequency range as in Equation 1. 2I(y; x) ∝ Z f∗ Sx(f) / Z ∞ −∞ Sx(f) where Sx(f) = lim N→∞ 1 2N \f\f\f\f\f NX n=−N xne−j2πfn \f\f\f\f\f 2 (1) Assumption 2.1 states that the information from a time series depends on its signal-to-noise ratio (SNR). Prior works showed that specific frequency bands hold inherent information about the characteristics of time series, which helps classification [21, 24]. Assumption 2.2. The true underlying generative process f(.), for a given data distribution D = {xk}K k=1, is quasiperiodic, i.e., f(x + τ) = g(x, f(x)), where τ can be either fixed or varied. Assumption 2.2 posits that the observed data samples from the distribution D are generated by a quasiperiodic function. This is a minimal assumption since the quasiperiodicity is the relaxed version of the periodic functions. In simpler terms, quasiperiodicity can be described as the observed signals exhibiting periodicity on a small scale, while being unpredictable on a larger scale. And, several prior works showed that the data generation mechanism of time-series data for several applications in the real world are quasiperiodic [25–30]. Therefore, Assumption 2.2 is realistic. Proposition 2.3 (Destructive Mixup). If Assumptions 2.1 and 2.2 hold, there exist λ ∼ Beta(α, α) or λ ∼ U(β, 1.0) with high values of β such that when linear mixup techniques are utilized, the lower bound of the mutual information for the augmented sample distribution decreases to zero. 0 ≤ I(y; x+) < I(y; x∗), where x∗is the optimal sample, x+ = λx + (1 − λ)˜ xand Z f∗ Sx∗ (f) = Z ∞ −∞ Sx∗ (f) (2) Proofs can be found in Appendix A. This proposition indicates that although the augmented samples are primarily derived from anchor samples (x) with high ratios, the resulting instances may not contain any task-relevant information. In other words, the augmentation process can potentially discard the whole task-specific information. This destructive behavior of mixup for quasi-periodic data can be attributed to the interference phenomenon in which two waves interact to form the resultant wave of the lower or higher amplitude according to the phase difference as shown in Proposition 2.3. 3 Method We introduce a novel approach to overcome the limitations of mixup by treating the magnitude and phase of sinusoidal signals as two distinct features with separate behaviors. Subsequently, we apply tailored mixup operations to each feature, considering their specific characteristics and effects. We perform the linear mixup for the magnitude of each sinusoidal. However, for the phase, we take a different approach and bring the phase components of the two coherent signals together by adding a small value to the anchor’s phase in the direction of the other sample. The mixup operation performs the linear interpolation of features [31], however, interpolation of two complex variables can result in a complex variable whose phase and magnitude are completely different/far away from those two, i.e., mixup can be destructive extrapolation rather than the interpolation of features. Therefore, we mix the phase of two sinusoidal as follows. We start by calculating the shortest phase difference between the two samples, denoted as ∆Θ, as described in Equation 31. θ ≡ [P(x) − P(˜ x)] (mod 2π) ∆Θ = \u001aθ − 2π, if θ > π θ, otherwise (3) The sign of the calculated phase difference provides information about the relative phase location of the other sample, in either a clockwise or counterclockwise direction in the phasor diagram. And, the absolute value of it represents the shortest angular difference between two samples in radians. Based 1We use phase in radians throughout the paper in the range (−π, π] 3on the calculated phase difference between two samples, we perform mixup operation to generate diverse positive samples as in Equation 4 such that phase and magnitude of augmented instances are interpolated properly according to the anchor sample x, without causing any destructive interference. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) = \u001aP(x) − |∆Θ| ∗(1 − λP ), if ∆Θ > 0 P(x) + |∆Θ| ∗(1 − λP ), otherwise (4) The proposed method which mixes the magnitude and phase of each frequency component with tailored operations, not only prevents destructive interference between time series, resulting in an increase in the lower bound of mutual information (as shown in Theorem 3.1), but also generates diverse augmented instances with the same two samples by using two different mixing coefficients. a) b) Figure 1: The phasor representation of linear mixupa), and proposed mixup b). The anchor, randomly chosen sample, and generated instances are represented as x, ˜ x, and x+, respectively. Theorem 3.1 (Guarantees for Mixing). Under assumptions 2.1 and 2.2, given any λ ∈ (0, 1], the mutual information for the augmented instance lower bounded by the sampled λ and anchor x. λI(y; x) ≤ I(y; x+) < I(y; x∗) where x+ = F−1(A(x+)∠P(x+)) (5) We provide an intuitive demonstration in Figure 1, along with a detailed mathematical proof presented in Appendix A. Our approach also offers increased flexibility in selecting the mixing coefficients of phase and magnitude, based on their sensitivities to the mixing process as well as the augmentation degree for each randomly chosen pair. Since the degree of augmentations has crucial importance for contrastive learning, there can be cases where augmentations are either too weak (intra-class features cannot be clustered together) or too strong (inter-class features can also collapse to the same point) and lead to sub-optimal results [17]. To mitigate this issue and find an order for augmentation degree, we search pairs of samples that are semantically closer, meaning they are more likely to belong to the same class. We then perform the proposed mixup more aggressively on these pairs, creating more closer and diverse samples while decreasing the augmentation strength for less similar pairs. To find similar samples without labels, we train a completely unsupervised β-V AE [32] that maps data points to a latent space such that two random samples are semantically similar if they are close in the latent as shown in Proposition 3.2. Proposition 3.2 (Consistency in Latent Space [33]). Given a well-trained unconditional VAE with the encoder E(.) that produces distribution pE(z|x), the decoder D(.) that produces distribution qD(x|z) while the prior for z is p(z), let z1 and z2 be two latent vectors of two different real samples x1 and x2, i.e., E(x1) = z1 and E(x2) = z2. if the distance d(z1, z2) ≤ δ, then D(z1) and D(z2) will have a similar semantic label as in Equation 6. |I(D(z1); y) − I(D(z2); y)| ≤ϵ, (6) 4where ϵ stands for tolerable semantic difference, δ is the maximum distance to maintain semantic consistency, and d(.) is a distance measure such as cosine similarity between two vectors. The above proposition with Theorem 3.1 motivates us to perform augmentation aggressively if two randomly chosen samples are semantically closer. Therefore, we sample the mixup coefficient for both phase and magnitude from a uniform distribution λA, λP ∼ U(β, 1.0) with low values of β if the distance between the latent vectors is below a threshold, otherwise, they are drawn from a truncated normal distribution λA, λP ∼ Nt(µ, σ,1.0) with a high mean and low standard deviation. 4 Experiments We conduct experiments on the proposed approach and compare it with other mixup methods or optimal/hard positive sample generation in the contrastive learning setup. During our experiments, we use SimCLR [15] framework without specialized architectures or a memory bank for all baselines to have a fair comparison. Results with other CL frameworks can be found in Appendix E. Complete training details and hyper-parameter settings for datasets and baselines are provided in Appendix D. 4.1 Datasets We performed extensive experiments on eight datasets from three tasks that include activity recogni- tion from inertial measurements (IMUs), heart rate prediction from photoplethysmography (PPG), and cardiovascular disease classification from electrocardiogram (ECG). We provided short descriptions of each dataset below, and further detailed information with metrics can be found in Appendix B. Activity recognition We used UCIHAR [34], HHAR [35], and USC [36] for activity recognition. During the evaluation, we assess the cross-person generalization capability of the contrastive models, i.e., the model is evaluated on a previously unseen target domain. We follow the settings in GILE [37] to treat each person as a single domain while the fine-tuning dataset is much smaller than the unsupervised one. Heart rate prediction We used the IEEE Signal Processing Cup in 2015 (IEEE SPC) [ 38], and DaLia [39] for PPG-based heart rate prediction. The SPC provides two datasets, one smaller with lesser artifacts (referred to as SPC12) [38] and a bigger dataset with more participants including heavy motions (referred to as SPC22). In line with previous studies, we adopted the leave-one-session-out (LOSO) cross-validation, which involves evaluating methods on subjects or sessions that were not used for pre-training and fine-tuning. Cardiovascular disease (CVD) classification We conducted our experiments on China Physiologi- cal Signal Challenge 2018 (CPSC2018) [40] and Chapman University, Shaoxing People’s Hospital (Chapman) ECG dataset [41]. We selected the same four specific leads as in [42] while treating each dataset as a single domain with a small portion of the remainder dataset used for fine-tuning the pre-trained model. We split the dataset for fine-tuning and testing based on patients (each patient’s recordings appear in only one set). 4.2 Baselines Comparison with prior mixup techniques We evaluate the effectiveness of our proposed mixup by comparing it with other commonly used mixup methods, including Linear-Mixup [31], Binary- Mixup [43], Geometric-Mixup [22], Cut-Mix [44], Amplitude-Mix [45] and Spec-Mix [46]. When we compare the performance of mixup techniques, we follow the same framework with [47] where the samples of mixture operation only happen in current batch samples. And, the mixup samples are paired with anchors, i.e., without applying mixup second times, for contrastive pre-training. Comparison with methods for optimal sample generation We evaluate the performance of our proposed method by comparing it with other data generation methods and baselines in contrastive learning setup while considering previously known augmentation techniques. Traditional data augmentations for time-series [19], such as resampling, flipping, adding noise, etc. InfoMin which leverages an adversarial training strategy to decrease the mutual information between samples 5while maximizing the NCE loss [ 48]. NNCLR [ 49], which uses nearest neighbors in the learned representation space as the positive samples. Positive feature extrapolation [ 50], which creates hard positives through feature extrapolation. GenRep which uses the latent space of a generative model to generate “views” of the same semantic content by sampling nearby latent vectors [51]. Aug. Bank [21], which proposes an augmentation bank that manipulates frequency components of a sample with a limited budget. STAug [52], which combines spectral and time augmentation for generating samples using the empirical mode decomposition and linear mixup.. DACL [ 22], which creates positive samples by mixing hidden representations. IDAA [53], which is an adversarial method by modifying the data to be hard positives without distorting the key information about their original identities using a V AE. More implementation details for each baseline are given in Appendix C. 4.3 Implementation We use a combination of convolutional with LSTM-based network, which shows superior performance in many time-series tasks [19, 54, 55], as backbones for the encoder fθ(.) where the projector is two fully-connected layers. We use InfoNCE as the loss, which is optimized using Adam with a learning rate of 0.003. We train with a batch size of 256 for 120 epochs and decay the learning rate using the cosine decay schedule. After pre-training, we train a single linear layer classifier on features extracted from the frozen pre-trained network, i.e., linear evaluation, with the same hyperparameters. Reported results are mean and standard deviation values across 3 independent runs with different seeds on the same split. More details about the implementation, architectures, and hyperparameters with the trained V AEs are given in Appendix D. 5 Results and Discussion Tables 1, 2, and 3 present the results of our proposed approach compared to state-of-the-art methods for optimal/hard positive sample generation in contrastive learning setup across the three tasks in eight datasets. Additionally, Figure 2 compares our approach with prior mixup methods (e.g., linear mixup, cutmix) without applying any other additional augmentation techniques. Overall, our proposed method has demonstrated superior performance compared to other methods in seven datasets, with the second-best performance in the remaining dataset, and a minor performance gap. Table 1: Performance Comparison of ours with prior works in Activity Recognition datasets Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ Supervised DCL [37] 77.63 – 51.27 – 60.35 – CoDATS [56] 68.22 – 45.69 – – – GILE [37] 88.17 – 55.61 – – – Self-Supervised Traditional Augs. 87.05±1.07 86.13 ±0.96 85.48 ±1.16 84.31 ±1.31 53.47 ±1.10 52.09 ±0.95 NNCLR [49] 85.31 ±0.91 83.56 ±1.25 83.16 ±1.32 82.15 ±1.25 55.41 ±1.43 52.64 ±1.37 InfoMin [48] 38.07 ±8.15 30.66 ±9.15 31.58 ±10.2 29.72 ±11.1 35.89 ±14.3 37.77 ±9.12 IDAA [53] 82.23 ±0.69 79.84 ±0.89 88.98±0.62 89.01±0.55 59.23 ±1.10 56.11 ±1.54 PosET [50] 88.13 ±0.91 87.35 ±0.96 85.77 ±1.11 85.90 ±1.20 41.37 ±5.63 39.43 ±5.72 STAug [52] 89.83 ±0.71 88.91 ±0.62 87.69 ±1.03 87.73 ±0.93 55.61 ±1.08 56.74 ±1.21 Aug. Bank [21] 65.27 ±1.12 71.16 ±1.24 67.95 ±1.45 75.13 ±1.32 43.28 ±4.37 47.31 ±4.68 GenRep [51] 87.22 ±1.05 86.48 ±0.95 87.05 ±0.95 86.45 ±0.90 50.13 ±2.85 49.50 ±2.73 DACL [22] 73.12 ±1.23 66.28 ±1.11 80.89 ±0.91 81.31 ±0.78 53.61 ±2.60 51.76 ±2.21 Ours 91.60±0.65 90.46±0.53 88.05 ±1.05 87.95 ±1.10 60.13±0.75 59.13±0.69 From these tables, we can see that our proposed method significantly outperforms DACL, which sug- gests creating a positive sample by mixing fixed hidden representations in an intermediate layer [22], by a large margin (up to 20.8% with a 10.1% on average in activity recognition). This suggests that when the representations are not yet linearly separable at the beginning of the contrastive training process, the interpolated representations using mixup may be dissimilar to the actual interpolated samples and may not capture their underlying features. One interesting result from our experiments is that IDAA [53] exhibits comparable performance to our method in some datasets, and even slightly outperforms our approach in the HHAR dataset for activity recognition. Despite using distinct methods to generate positive instances, i.e., adversarial and mixup, our approach and IDAA algorithm 6Table 2: Performance comparison of ours with prior works in Heart Rate Prediction datasets Method IEEE SPC12 IEEE SPC 22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ SupervisedDCL 22.02 28.44 28.10 32.45 6.58 11.30CNN Ensemble∗ [39] 3.89 – 8.74 – 8.58 – Self-SupervisedTraditional Augs. 20.67 ±1.13 26.35 ±0.98 16.84 ±1.10 22.23 ±0.72 12.01 ±0.65 21.09 ±0.86NNCLR [49] 20.28 ±2.21 28.23 ±1.63 23.49 ±1.54 28.75 ±3.66 11.56 ±0.63 19.95 ±0.89InfoMin [48] 36.84 ±5.11 29.78 ±7.31 31.58 ±4.72 29.72 ±4.83 45.89 ±8.71 50.77 ±9.72IDAA [53] 19.02 ±0.96 27.42 ±1.11 15.37 ±1.21 22.41 ±1.42 11.12 ±0.64 20.45 ±0.69PosET [50] 25.60 ±1.93 33.80 ±2.71 23.42 ±1.50 31.51 ±3.71 35.99 ±3.95 39.92 ±3.12STAug [52] 27.44 ±1.93 35.63 ±3.10 19.86 ±2.11 30.70 ±3.54 18.70 ±4.06 30.81 ±3.61Aug. Bank [21] 27.31 ±2.17 37.93 ±2.96 27.84 ±2.03 36.41 ±3.98 35.87 ±4.18 40.61 ±3.74GenRep [51] 21.02 ±1.41 28.42 ±1.65 15.67 ±1.23 22.33 ±1.43 25.41 ±1.62 36.83 ±1.87DACL [22] 21.85 ±1.63 28.17 ±1.75 14.67 ±1.10 20.06 ±1.21 18.44 ±1.32 25.61 ±1.45Ours 16.26±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73 * The entire dataset, excluding the test, is utilized with labels, while in DCL, the labeled data size matches that of the CL share similarities in approaches for positive instance generation in CL setup. The IDAA algorithm aims to create hard positive samples that lie near class boundaries without changing the identity of the sample, while our method interpolates two samples to produce a positive instance that is similar to the original sample while adding noise to the phase and amplitude in the direction of a randomly chosen sample. In other words, both approaches try to keep the sample identity intact by taking special precautions while generating new positive instances, which may explain their similar performance in our experiments. In contrast, approaches that do not prioritize preserving sample identity while generating samples or hidden representations often demonstrate suboptimal performance on average while exhibiting increased variability across tasks. Table 3: Performance comparison between ours and prior work in CVD. Method CPSC 2018 Chapman AUC↑ AUC↑ Supervised CNN [57] — 95.80 Casual CNN [58] — 97.70 Self-Supervised Traditional Augs. 67.86±3.41 74.69 ±2.04 NNCLR [49] 70.06 ±2.05 77.19 ±2.41 InfoMin [48] 64.48 ±6.15 56.34 ±9.12 IDAA [53] 80.90 ±0.73 93.63 ±0.91 PosET [50] 72.58 ±2.12 78.27 ±2.34 STAug [52] 74.15 ±1.15 93.88 ±0.87 Aug. Bank [21] 81.78 ±1.24 94.75 ±0.90 GenRep [51] 52.49 ±3.43 86.72 ±1.13 DACL [22] 82.38 ±0.84 92.28 ±0.97 Ours 85.30±0.45 95.90±0.82 Examples of such methods include PosET [ 50], which generates hard positive samples to im- prove contrastive learning by extrapolating features, STAug [52], which uses empirical mode decompo- sition with linear mixup technique together, and InfoMin [48], which tries to minimize mutual in- formation between two instances in an adversar- ial manner. The performance comparison of prior mixup techniques and our proposed one is shown in Figure 2. On average, our proposed method outper- forms all other mixup techniques while reducing the variance across tasks. What is interesting about this figure is that while the linear [ 31] and ampli- tude mixup [45] reach our method in some datasets for activity recognition, the performance of the lin- ear mixup decreases heavily for the other two tasks whereas the amplitude mixup gives reasonable per- formance. This empirical outcome supports our initial theorem about the destructive effect of mixup, which suggests linear mixup or other derivatives can discard the whole task-specific information in the generated positive sample for quasi-periodic signals even though the mixing coefficient is sampled from a distribution such that the generated samples are much closer to the anchor. 5.1 Ablation Studies Here, we present a comprehensive examination of our proposed method and the effect of its com- ponents on the performance. Mainly, we investigate the effect of the proposed mixup by applying the instance selection algorithm to the linear mixup (w/o Prop. Mixup). Then, we perform our proposed mixup with the constant λA and λP coefficients without investigating latent space distances between pairs (w/o Aug. Degree). Tables 4, 5 and 6 summarize the results. The second row in the tables shows the performance when the proposed mixup method is not applied while choosing mixup coefficients according to the distances in the latent space for the linear mixup. The last row illustrates 710 15 20 25 30 35 40 IEEE SPC12 IEEE SPC22 DaLia Ours Linear Binary Cut Spec Amp 15 35 55 75 95 UCIHAR HHAR USC Ours Linear Binary Cut Spec Amp Geo 65 70 75 80 85 90 95 100 CPSCP 2018 Chapman Ours Linear Binary Cut Spec Amp a) b) c) Accuracy in  % Higher is better. Mean Absolute Error Lower is better. AUC in % Higher is better. Figure 2: The comparison of mixup methods where the error bars represent the deviation across random seeds (explicit numbers are given in Appendix E). a) shows the performance in activity recognition, b) is for heart rate prediction, and finally c) shows the CVD classification. For the last two tasks, we excluded Geomix as its performance is extremely poor and distorts the y-axis scale. the performance change resulting from randomly sampling mixup coefficients without considering any relationship between the selected pair while applying tailored mixup for phase and magnitude. Table 4: Ablation on proposed mixup with coefficient selection in Activity Recognition datasets Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ Ours 91.60±0.65 90.46±0.53 88.05±1.05 87.95±1.10 60.13±0.75 59.13±0.69 w/o Prop. Mixup83.09 (-8.51) 81.65 (-8.81) 85.89 (-2.16) 86.01 (-1.94) 45.10 (-15.03) 43.64 (-15.49) w/o Aug. Degree80.86 (-10.74) 80.18 (-10.26) 87.53 (-0.95) 87.75 (-0.20) 57.00 (-3.13) 54.75 (-4.38) The results obtained from the ablation study support the previous claims and outcomes. For example, when the linear mixup is applied instead of the proposed mixup technique for heart rate prediction (Table 5, w/o Prop. Mixup), the performance decrease is significant compared to the case when coefficients are sampled without considering the distance in the latent space (Table 5, w/o Aug. Degree). This observation indicates that as the periodicity in data increases, linear mixup can lead to significant destructive interferences, whereas our method effectively prevents such issues. Table 5: Ablation on proposed mixup with coefficient selection in Heart Rate Prediction datasets. Method IEEE SPC12 IEEE SPC 22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ Ours 16.26±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73w/o Prop. Mixup20.45 (+4.19) 28.51 (+6.03) 15.29 (+3.04) 24.08 (+5.88) 24.11 (+13.54) 35.45 (+15.18)w/o Aug. Degree19.30 (+3.04) 24.84 (+2.36) 16.01 (+3.76) 21.21 (+3.19) 11.10 (+0.53) 20.13 (-0.24) Table 6: Ablation on proposed mixup with coeffi- cient selection in CVD. Method CPSC 2018 Chapman AUC↑ AUC↑ Ours 85.30 ± 0.45 95.90 ± 0.82 w/o Prop. Mixup 81.20 (-4.10) 86.30 (-9.60) w/o Aug. Degree 80.67 (-4.63) 95.98 (+0.08) While our mixup technique consistently en- hances performance across datasets, we ob- serve a decline when the mixing coefficients are sampled based on the distance in the latent space for two datasets. Also, the performance increase gained by sampling coefficients based on distance is relatively low compared to the proposed mixup. Several factors can explain this observation. First, the V AE might not be well trained due to the limited size of data in each class, i.e., the assumption in Proposition 3.2 does not hold. This can lead to inconsistencies in the semantic similarity of the latent space such that two close samples in the latent space might have different labels. Second, if the number of classes increases for a downstream task, the probability of sampling intra-class samples in a batch will decrease, leading to a lack of performance improvement. Therefore, in future investigations, it might be beneficial to use a different distance metric for quasi-periodic time-series data such that it can scale with the number of classes while considering the lack of big datasets. More ablation studies about the sensitivity of mixing coefficients and performance in different self-supervised learning frameworks, like BYOL [59] can be found in Appendix E.1 and E.2. And, investigations regarding whether we still need known data augmentations are given in Appendix E.3. Examples that visually demonstrate the negative effects of linear mixup and our proposed mixup 8technique to prevent this problem can be found in Appendix F. Comparative results regarding the performance of the tailored mixup in the supervised learning paradigm are given in Appendix G. 6 Related Work The goal of contrastive learning is to contrast positive with negative pairs [60]. In other words, the embedding space is governed by two forces, the attraction of positive pairs and the repellence of negatives, actualized through the contrastive loss [61]. Since label information is unavailable during the training, positive pairs are generated using augmentation techniques on a single sample, while negative pairs are randomly sampled from the entire dataset. Therefore, the choice or generation of positive and negative samples plays a pivotal role in the success of contrastive learning [62–65] and both approaches, generation/selection of positive/negative pairs, have been investigated thoroughly in the literature [ 66–71], we limit our discussion about prior works related to data augmentation techniques that create optimal or hard samples without labels. Adversarial based approaches A growing body of literature has investigated generating samples by using adversarial training for both positives and negatives [48, 72, 73]. A seminal work about the importance of augmentations in CL, InfoMin, presented an adversarial training strategy where players try to minimize and maximize the mutual information using the NCE loss [48]. CLAE, one of the first works that leveraged the adversarial approach, shows that adversarial training generates challenging positive and hard negative pairs [ 72]. Another recent study proposed an adversarial approach that generates hard samples while retaining the original sample identity by leveraging the identity-disentangled feature of V AEs [53]. However, adversarial augmentations may change the original sample identity due to excessive perturbations and it is infeasible to tune the attack strength for every sample to preserve the identity. In other words, these approaches do not consider the sample-specific features and use a constant perturbation coefficient for all samples whereas our proposed method considers the similarity between pairs and tunes the mixing coefficients accordingly. Mixup based approaches Mixup-based methods have been recently explored in contrastive learn- ing [22, 71, 47, 74, 75]. According to a recent theoretical work [22], mixup has implicit data-adaptive regularization effects that promote generalization better than adding Gaussian noise, which is a commonly used augmentation strategy in both time-series and vision data [76–78]. Although, mixup- based approaches have shown success in different problems [ 79, 80], such as domain adaptation, creating samples using mixup in the input space is infeasible in domains where data has a non-fixed topology, such as sequences, trees, and graphs [22]. Therefore, recent works suggest mixing hidden representations of samples, similar to Manifold Mixup [81]. However, this method claims that mixing fixed-length hidden representation via an intermediate layer \"z = αz + (1 − α)˜ z\" can be interpreted as adding noise to a given sample in the direction of another. However, it is an overly optimistic claim because early during training, where in most cases there is usually no linear separability among the representations, this synthesis may result in hidden representations that are completely different and far away from the samples [71, 82]. Therefore, in this work, we take a different approach and modify the mixup method considering its limitations for quasi-periodic non-stationary time-series data. Also, unlike most existing methods that aim to generate hard samples—samples that are close to class boundaries—using adversarial approaches [53, 48, 72] or feature extrapolation [50, 71], our method seeks to connect semantically closer samples together using interpolation in a tailored way. 7 Conclusion In this paper, we first demonstrate the destructive effect of linear mixup for quasi-periodic time- series data, then introduce a novel tailored mixup method to generate positive samples for the contrastive learning formulation while preventing this destructive effect and interpolating the samples appropriately. Theoretically, we show that our proposed method guarantees the interpolation of pairs without causing any loss of information while generating a diverse set of samples. Empirically, our method outperforms the prior approaches in three real-world tasks. By conducting experiments on contrastive and supervised learning settings, we show that our approach is agnostic to the choice of learning paradigm. Thus, it holds the potential for utilization in generating augmented data for different learning paradigms as well. We believe that the method proposed in this paper has the potential to significantly improve learning solutions for a diverse range of time series tasks. 9References [1] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [2] Carl Doersch, Abhinav Kumar Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. 2015 IEEE International Conference on Computer Vision (ICCV), pages 1422–1430, 2015. [3] Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context encoders: Feature learning by inpainting. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pages 2536–2544. IEEE Computer Society, 2016. [4] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), 2:1735–1742, 2006. [5] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimi- native unsupervised feature learning with convolutional neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors,Advances in Neural Infor- mation Processing Systems, volume 27. Curran Associates, Inc., 2014. [6] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [7] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [8] shuang ma, Zhaoyang Zeng, Daniel McDuff, and Yale Song. Contrastive learning of global and local video representations. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, volume 34, pages 7025–7040. Curran Associates, Inc., 2021. [9] Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, and Xuedong Huang. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10937–10947. PMLR, 18–24 Jul 2021. [10] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. w2v-bert: Combining contrastive learning and masked language modeling for self- supervised speech pre-training. 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 244–250, 2021. [11] Hao Fu, Shaojun Zhou, Qihong Yang, Junjie Tang, Guiquan Liu, Kaikui Liu, and Xiaolong Li. Lrc-bert: Latent-representation contrastive knowledge distillation for natural language understanding. Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12830– 12838, May 2021. [12] Qianglong Chen, Feng Ji, Xiangji Zeng, Feng-Lin Li, Ji Zhang, Haiqing Chen, and Yin Zhang. Kace: Generating knowledge aware contrastive explanations for natural language inference. In Annual Meeting of the Association for Computational Linguistics, 2021. [13] X. Y . Shen, Ying Sun, Yao zhong Zhang, and Mani Najmabadi. Semi-supervised intent discovery with contrastive learning. Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI, 2021. 10[14] Yang Liu and Maosong Sun. Contrastive unsupervised word alignment with non-local features. Proceedings of the AAAI Conference on Artificial Intelligence, 29(1), Feb. 2015. [15] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML’20. JMLR.org, 2020. [16] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, C. Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In International Joint Conference on Artificial Intelligence, 2021. [17] Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. [18] Mike West, Raquel Prado, and Andrew D. Krystal. Evaluation and comparison of eeg traces: Latent structure in nonstationary time series. Journal of the American Statistical Association, 94(446):375–387, 1999. [19] Hangwei Qian, Tian Tian, and Chunyan Miao. What makes good contrastive learning on small-scale wearable-based tasks? In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , KDD ’22, page 3761–3771, New York, NY , USA, 2022. Association for Computing Machinery. [20] Qingsong Wen, Liang Sun, Fan Yang, Xiaomin Song, Jingkun Gao, Xue Wang, and Huan Xu. Time series data augmentation for deep learning: A survey. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4653–4660. International Joint Conferences on Artificial Intelligence Organization, 8 2021. Survey Track. [21] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. InProceedings of Neural Information Processing Systems, NeurIPS, 2022. [22] Vikas Verma, Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc Le. Towards domain- agnostic contrastive learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10530–10541. PMLR, 18–24 Jul 2021. [23] Soyoung Yoon, Gyuwan Kim, and Kyumin Park. Ssmix: Saliency-based span mixup for text classification. ArXiv, abs/2106.08062, 2021. [24] Martin Ullrich, Arne Küderle, Julius Hannink, Silvia Del Din, Heiko Gaßner, Franz Marxreiter, Jochen Klucken, Bjoern M. Eskofier, and Felix Kluge. Detection of gait from continuous inertial sensor data using harmonic frequencies. IEEE Journal of Biomedical and Health Informatics, 24(7):1869–1878, 2020. [25] Hong Luo, Deye Yang, Andrew Barszczyk, Naresh Vempala, Jing Wei, Si Jia Wu, Paul Pu Zheng, Genyue Fu, Kang Lee, and Zhong-Ping Feng. Smartphone-based blood pressure measurement using transdermal optical imaging technology. Circulation: Cardiovascular Imaging, 12(8):e008857, 2019. [26] Bryan P. Yan, William H. S. Lai, Christy K. Y . Chan, Stephen Chun-Hin Chan, Lok-Hei Chan, Ka-Ming Lam, Ho-Wang Lau, Chak-Ming Ng, Lok-Yin Tai, Kin-Wai Yip, Olivia T. L. To, Ben Freedman, Yukkee C. Poh, and Ming-Zher Poh. Contact-free screening of atrial fibrillation by a smartphone using facial pulsatile photoplethysmographic signals. Journal of the American Heart Association, 7(8):e008585, 2018. [27] Saman Noorzadeh, Mohammad Niknazar, Bertrand Rivet, Julie Fontecave-Jallon, Pierre-Yves Guméry, and Christian Jutten. Modeling quasi-periodic signals by a non-parametric model: Application on fetal ecg extraction. In 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, pages 1889–1892, 2014. 11[28] A Koulali and P J Clarke. Modelling quasi-periodic signals in geodetic time-series using Gaussian processes. Geophysical Journal International, 226(3):1705–1714, 04 2021. [29] M. Lemay, L. Dang, and J.M. Vesin. Quasi-periodic atrial activity components in the ecg used to discriminate between paroxysmal and chronic atrial fibrillation. In 2008 Computers in Cardiology, pages 821–824, 2008. [30] A Garfinkel, P S Chen, D O Walter, H S Karagueuzian, B Kogan, S J Evans, M Karpoukhin, C Hwang, T Uchida, M Gotoh, O Nwasokwa, P Sager, and J N Weiss. Quasiperiodicity and chaos in cardiac fibrillation. The Journal of Clinical Investigation, 99(2):305–314, 1 1997. [31] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, 2018. [32] Irina Higgins, Loïc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual con- cepts with a constrained variational framework. In International Conference on Learning Representations, 2016. [33] Yinqi Li, Hong Chang, Bingpeng MA, Shiguang Shan, and Xilin Chen. Optimal positive gener- ation via latent transformation for contrastive learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 18327–18342. Curran Associates, Inc., 2022. [34] D. Anguita, Alessandro Ghio, L. Oneto, Xavier Parra, and Jorge Luis Reyes-Ortiz. Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine. In International Workshop on Ambient Assisted Living and Home Care, 2012. [35] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjær- gaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. SenSys ’15, page 127–140, New York, NY , USA, 2015. Association for Computing Machinery. [36] Mi Zhang and Alexander A. Sawchuk. Usc-had: A daily activity dataset for ubiquitous activity recognition using wearable sensors. In Proceedings of the 2012 ACM Conference on Ubiquitous Computing, UbiComp ’12, page 1036–1043, New York, NY , USA, 2012. Association for Computing Machinery. [37] Hangwei Qian, Sinno Jialin Pan, and Chunyan Miao. Latent independent excitation for general- izable sensor-based cross-person activity recognition. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13):11921–11929, May 2021. [38] Zhilin Zhang, Zhouyue Pi, and Benyuan Liu. Troika: A general framework for heart rate monitoring using wrist-type photoplethysmographic signals during intensive physical exercise. IEEE Transactions on Biomedical Engineering, 62(2):522–531, 2015. [39] Attila Reiss, Ina Indlekofer, Philip Schmidt, and Kristof Van Laerhoven. Deep ppg: Large-scale heart rate estimation with convolutional neural networks. Sensors, 19(14), 2019. [40] Eddie Y . K. Ng, Feifei Liu, Chengyu Liu, Lina Zhao, X. Zhang, Xiaoling Wu, Xiaoyan Xu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, and Jianqing Li. An open access database for evaluating the algorithms of electrocardiogram rhythm and morphology abnormality detection. Journal of Medical Imaging and Health Informatics, 2018. [41] Jianwei Zheng, Jianming Zhang, Sidy Danioko, Hai Yao, Hangyuan Guo, and Cyril Rakovski. A 12-lead electrocardiogram database for arrhythmia research covering more than 10,000 patients. Scientific Data, 7(1):48, February 2020. [42] Erick A Perez Alday, Annie Gu, Amit J Shah, Chad Robichaux, An-Kwok Ian Wong, Chengyu Liu, Feifei Liu, Ali Bahrami Rad, Andoni Elola, Salman Seyedi, Qiao Li, Ashish Sharma, Gari D Clifford, and Matthew A Reyna. Classification of 12-lead ecgs: the physionet/computing in cardiology challenge 2020. Physiological Measurement, 41(12):124003, dec 2020. 12[43] Christopher Beckham, Sina Honari, Vikas Verma, Alex M Lamb, Farnoosh Ghadiri, R Devon Hjelm, Yoshua Bengio, and Chris Pal. On adversarial mixup resynthesis. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [44] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Young Joon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6022–6031, 2019. [45] Q. Xu, R. Zhang, Y . Zhang, Y . Wang, and Q. Tian. A fourier-based framework for domain generalization. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14378–14387, Los Alamitos, CA, USA, jun 2021. IEEE Computer Society. [46] Gwantae Kim, David K. Han, and Hanseok Ko. Specmix : A mixed sample data augmentation method for training withtime-frequency domain features. In Interspeech, 2021. [47] Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, and Eric Xing. Un-mix: Rethinking image mixtures for unsupervised visual representation learning. 2022. [48] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [49] D. Dwibedi, Y . Aytar, J. Tompson, P. Sermanet, and A. Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9568–9577, Los Alamitos, CA, USA, oct 2021. IEEE Computer Society. [50] Rui Zhu, Bingchen Zhao, Jingen Liu, Zhenglong Sun, and Chang Wen Chen. Improving contrastive learning by visualizing feature transformation. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 10286–10295, 2021. [51] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola. Generative models as a data source for multiview representation learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [52] Xiyuan Zhang, Ranak Roy Chowdhury, Jingbo Shang, Rajesh Gupta, and Dezhi Hong. Towards diverse and coherent augmentation for time-series forecasting. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5, 2023. [53] Kaiwen Yang, Tianyi Zhou, Xinmei Tian, and Dacheng Tao. Identity-disentangled adversarial augmentation for self-supervised learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 25364–25381. PMLR, 17–23 Jul 2022. [54] Dwaipayan Biswas, Luke Everson, Muqing Liu, Madhuri Panwar, Bram-Ernst Verhoef, Shrishail Patki, Chris H. Kim, Amit Acharyya, Chris Van Hoof, Mario Konijnenburg, and Nick Van Helleputte. Cornet: Deep learning framework for ppg-based heart rate estimation and biometric identification in ambulant environment. IEEE Transactions on Biomedical Circuits and Systems, 13(2):282–291, 2019. [55] Akara Supratak, Hao Dong, Chao Wu, and Yike Guo. Deepsleepnet: a model for automatic sleep stage scoring based on raw single-channel eeg. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 25(11):1998–2008, Nov 2017. [56] Garrett Wilson, Janardhan Rao Doppa, and Diane J. Cook. Multi-source deep domain adaptation with weak supervision for time-series sensor data. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &; Data Mining, KDD ’20, page 1768–1778, New York, NY , USA, 2020. Association for Computing Machinery. 13[57] Dani Kiyasseh, Tingting Zhu, and David A. Clifton. Clocs: Contrastive learning of cardiac signals across space, time, and patients. In International Conference on Machine Learning, 2020. [58] Crystal T. Wei, Ming-En Hsieh, Chien-Liang Liu, and Vincent S. Tseng. Contrastive heartbeats: Contrastive learning for self-supervised ecg representation and phenotyping. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1126–1130, 2022. [59] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent a new approach to self-supervised learning. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [60] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [61] Tri Huynh, Simon Kornblith, Matthew R. Walter, Michael Maire, and Maryam Khademi. Boosting contrastive self-supervised learning with false negative cancellation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) , pages 2785–2795, January 2022. [62] Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun- shi. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning, 2019. [63] Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Working hard to know your neighbor's margins: Local descriptor learning loss. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [64] Hong Xuan, Abby Stylianou, Xiaotong Liu, and Robert Pless. Hard negative examples are hard, but useful. In Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV, page 126–142, Berlin, Heidelberg, 2020. Springer-Verlag. [65] Songwei Ge, Shlok Mishra, Chun-Liang Li, Haohan Wang, and David Jacobs. Robust contrastive learning using negative samples with diminished semantics. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, volume 34, pages 27356–27368. Curran Associates, Inc., 2021. [66] Yue Cao, Zhenda Xie, Bin Liu, Yutong Lin, Zheng Zhang, and Han Hu. Parametric instance classification for unsupervised visual feature learning. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. [67] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 8765– 8775. Curran Associates, Inc., 2020. [68] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ond ˇrej Chum. Mining on manifolds: Metric learning without labels. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7642–7651, 2018. [69] Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah Goodman. On mutual information in contrastive learning for visual representations, 2020. [70] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 815–823, 2015. 14[71] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 21798–21809. Curran Associates, Inc., 2020. [72] Chih-Hui Ho and Nuno Nvasconcelos. Contrastive learning with adversarial examples. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 17081–17093. Curran Associates, Inc., 2020. [73] Xiao Wang, Yuhang Huang, Dan Zeng, and Guo-Jun Qi. Caco: Both positive and negative sam- ples are directly learnable via cooperative-adversarial contrastive learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1–12, 2023. [74] Sungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young Yun. Mixco: Mix-up contrastive learning for visual representation. arXiv preprint arXiv:2010.06300, 2020. [75] Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. i-mix: A domain-agnostic strategy for contrastive representation learning. In ICLR, 2021. [76] Yuzhe Yang, Xin Liu, Jiang Wu, Silviu Borac, Dina Katabi, Ming-Zher Poh, and Daniel McDuff. Simper: Simple self-supervised learning of periodic targets. In The Eleventh International Conference on Learning Representations, 2023. [77] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 2352–2359, 2021. [78] Pengxiang Shi, Wenwen Ye, and Zheng Qin. Self-supervised pre-training for time series classification. In 2021 International Joint Conference on Neural Networks (IJCNN), 2021. [79] Ronghang Zhu, Ronghang Zhu, Xiang Yu, and Sheng Li. Progressive mix-up for few-shot supervised multi-source domain transfer. In ICLR, 2023. [80] JangHyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with supermodular diversity. In International Conference on Learning Representa- tions, 2021. [81] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez- Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 ofProceedings of Machine Learning Research, pages 6438–6447, Long Beach, California, USA, 09–15 Jun 2019. PMLR. [82] Tsz-Him Cheung and Dit-Yan Yeung. {MODALS}: Modality-agnostic automated data aug- mentation in the latent space. In International Conference on Learning Representations, 2021. [83] Leandro Giacomini Rocha, Dwaipayan Biswas, Bram-Ernst Verhoef, Sergio Bampi, Chris Van Hoof, Mario Konijnenburg, Marian Verhelst, and Nick Van Helleputte. Binary cornet: Accelerator for hr estimation from wrist-ppg. IEEE Transactions on Biomedical Circuits and Systems, 14(4):715–726, 2020. [84] Sayeed Shafayet Chowdhury, Rakib Hyder, Md. Samzid Bin Hafiz, and Mohammad Ariful Haque. Real-time robust heart rate estimation from wrist-type ppg signals using multiple reference adaptive noise cancellation. IEEE Journal of Biomedical and Health Informatics , 22(2):450–459, 2018. [85] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [86] Ricky T. Q. Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in variational autoencoders. In S. Bengio, H. Wallach, H. Larochelle, K. Grau- man, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. 15Appendix A Proof In this section, we present complete proofs of our theoretical study, starting with notations. A.1 Notations Fourier transform of a real-valued sample with a finite duration is obtained as in Equation 7. Xk = F(x) = ∞X n=−∞ xne−j2πkn (7) The amplitude and phase for each frequency are calculated from the Fourier transform as follows. A(x) = p Re(Xk)2 + Im(Xk)2 P(x) = arctan2(Im(Xk), Re(Xk)), (8) where arctan is a 2-argument arctangent which is the angle measure in radians. The phasor, as in Figure 1, of a sample is represented as in Equation 9. Xk = F(x) = A(x)ejP (x) (9) A.2 Proof for Proposition 2.3 Proposition A.1 (Destructive Mixup). If Assumptions 2.1 and 2.2 hold, there exist λ ∼ Beta(α, α) or λ ∼ U(β, 1.0) with high values of β such that when linear mixup techniques are utilized, the lower bound of the mutual information for the augmented sample distribution decreases to zero. 0 ≤ I(y; x+) < I(y; x∗) where x+ = λx + (1 − λ)˜ xand Z f∗ Sx∗ (f) = Z ∞ −∞ Sx∗ (f) (10) Proof. x+ = λx + (1 − λ)˜ x (11) From the linearity of Fourier transformation and ignoring k in Xk for the sake of easiness. X+ = λX + (1 − λ) ˜X (12) X+ = ˜X + λ(X − ˜X) (13) Let ˜X = e−jωϕk Xωk, where ϕk and ωk are random phase and frequency modulators for each frequency, sampled from distributions ϕk ∼ Φ, ωk ∼ Ω. X+ = e−jωϕk Xωk + λ(X − e−jωϕk Xωk) (14) X+ = X \u0002 λ + e−jωϕk ωk − λe−jωϕk ωk \u0003 (15) X+ = X \u0002 λ + (1 − λ)e−jωϕk ωk \u0003 (16) X+ = X [λ + (1 − λ)ωk(cos (ωϕk) − j sin (ωϕk))] (17) 16From the quasi-periodicity, assume that the frequency ranges of interest (f∗, i.e., k∗) are overlapped for both samples while the sampled random modulators have the following relationship. ωk∗ ≈ λ 1 − λ and θ ≡ [ωϕk∗ ] (mod 2π), (18) where θ is an odd multiple of π. Equation 17 can be simplified as follows. X+ k∗ = Xk∗ [λ + λ cos (ωϕk∗ )] (19) X+ k∗ ≈ 0 (20) X+ k∗ = ∞X n=−∞ xne−j2πk∗n −→ ∞X n=−∞ xne−j2πk∗n ≈ 0 (21) Sx+(f∗) = lim N→∞ 1 2N \f\f\f\f\f NX n=−N xne−j2πf∗n \f\f\f\f\f 2 (22) From Assumption 2.1, I(y; x+) ∝ Z f∗ Sx+(f) / Z ∞ −∞ Sx+(f) (23) 0 ≤ I(y; x+) < I(y; x∗) (24) We use Euler’s formula to expand Equation 16 to 17. While we use the frequency bins ( k) and frequency values in Hz (f) interchangeably for Equations 21 and 22. Although the above proof is to show the resulting instances may not contain any task-relevant information, it can also be demonstrated that the augmentation process can potentially discard the partial task-specific information (not whole) if ϕk and ωk are close to indicated relationships. 17A.3 Proof for Theorem 3.1 Theorem A.2 (Guarantees for Mixing). Under assumptions 2.1 and 2.2, given any λ ∈ (0, 1], the mutual information for the augmented instance lower bounded by the sampled λ and anchor x. λI(y; x) ≤ I(y; x+) < I(y; x∗) where x+ = F−1(A(x+)∠P(x+)) (25) Proof. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) = \u001aP(x) − |∆Θ| ∗(1 − λP ), if ∆Θ > 0 P(x) + |∆Θ| ∗(1 − λP ), otherwise (26) X+ = A(x+)ejP (x+) (27) \f\fX+\f\f = \f\f\fA(x+)ejP (x+) \f\f\f (28) \f\fX+\f\f = A(x+) where \f\fX+ k \f\f = \f\f\f\f\f ∞X n=−∞ x+ n e−j2πkn \f\f\f\f\f (29) \f\fX+\f\f = λA(x) + (1− λ)A(˜ x) (30) \f\fX+\f\f = λ \f\f\f\f\f ∞X n=−∞ xne−j2πkn \f\f\f\f\f + (1 − λ) \f\f\f\f\f ∞X n=−∞ ˜ xne−j2πkn \f\f\f\f\f (31) λ \f\f\f\f\f ∞X n=−∞ xne−j2πkn \f\f\f\f\f + (1 − λ) \f\f\f\f\f ∞X n=−∞ ˜ xne−j2πkn \f\f\f\f\f ≥ λ \f\f\f\f\f ∞X n=−∞ xne−j2πkn \f\f\f\f\f (32) Z f∗ Sx+(f) ≥ λ Z f∗ Sx(f) (33) Using the R∞ −∞ Sx+(f) = R∞ ∞ S˜x(f) (i.e., both samples are normalized to have the same power) and Assumption 2.1, I(y; x+) ∝ Z f∗ Sx+(f) / Z ∞ −∞ Sx+(f) (34) λI(y; x) ≤ I(y; x+) < I(y; x∗) (35) Proof is completed with Equation 35 by combining equations 33 and 34. Although this proof ignores the effect of phase mixing on the mutual information with the assump- tion 2.1, it is known that phase components carry semantically important features [45]. Therefore, it is necessary to note that the objective of this proof is to demonstrate that by applying mixup separately to the phase and amplitude components, we can avoid destructive interference. 18B Datasets In this section, we give details about the datasets that are used during our experiments. B.1 Human Activity Recognition UCIHAR Human activity recognition using smartphones dataset (UCIHAR) [34] is collected by 30 subjects within an age range of 16 to 48 performing six daily living activities with a waist-mounted smartphone. Six activities include walking, sitting, lying, standing, walking upstairs, and walking downstairs. Data is captured by 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50 Hz. We used the pre-processing technique the same as in [ 37, 19] such that the input contains nine channels with 128 features (it is sampled in sliding window of 2.56 seconds and 50% overlap, resulting in 128 features for each window). Windows are normalized to zero mean and unit standard deviation before feeding to models. Also, we follow the same experimental setup with prior works as follows. The experiments are conducted with a leave-one-domain-out strategy, where one of the domains is chosen to be the unseen target [19]. The contrastive pre-training is conducted with all subjects without any label information except the target one. Training of the linear layer, which is added to the frozen trained encoder, is only performed with the first five subjects of UCIHAR after excluding the target subject. In other words, if the target subject is 0, the subjects from 1 to 29 are used to train the encoder without any label information. Then, subjects from 1 to 4 are used to train the linear layer. And, evaluation is performed for subject 0. This is performed for the first five subjects with three random seeds and the mean value is reported. HHAR Heterogeneity Dataset for Human Activity Recognition (HHAR) is collected by nine subjects within an age range of 25 to 30 performing six daily living activities with eight differ- ent smartphones—Although HHAR includes data from smartwatches as well, we use data from smartphones—that were kept in a tight pouch and carried by the users around their waists [ 35]. Subjects then perform 6 activities: ‘bike’, ‘sit’, ‘stairs down’, ‘stairs up’, ‘stand’, and ‘walk’. Due to variant sampling frequencies of smart devices used in HHAR dataset, we downsample the readings to 50 Hz and apply 100 (two seconds) and 50 as sliding window length with step size, the windows are normalized to zero mean with unit standard deviation. We used the first four subjects (i.e., a, b, c, d) as source domains. USC USC human activity dataset (USC-HAD) is composed of 14 subjects (7 male, 7 female, aged 21 to 49 with a mean of 30.1) executing 12 activities with a sensor on the front right hip. The data dimension is six (3-axis accelerometer, 3-axis gyroscope) and the sample rate is 100 Hz. 12 activities include walking forward, walking left, walking right, walking upstairs, walking downstairs, running forward, jumping up, sitting, standing, sleeping, elevator up, and elevator down. We used the pre-processing technique with a smaller window size such that the input contains six channels with 100 features (it is sampled in a sliding window of 1 second and 50% overlap, resulting in 100 features for each window). The same normalization is also applied to windows before feeding to models. We used the same setup with UCIHAR while source subjects are chosen as the last four this time. B.2 Heart Rate Prediction IEEE SPC This competition provided a training dataset of 12 subjects (SPC12) and a test dataset of 10 subjects [39]. The IEEE SPC dataset overall has 22 recordings of 22 subjects, ages ranging from 18 to 58 performing three different activities [83]. Each recording has sampled data from three accelerometer signals and two PPG signals along with the sampled ECG data and the sampling frequency is 125 Hz. All these recordings were recorded from the wearable device placed on the wrist of each individual. All recordings were captured with a 2-channel pulse oximeter with green LEDs, a tri-axial accelerometer, and a chest ECG for the ground-truth HR estimation. During our experiments, we used PPG channels. We choose the first five subjects of SPC12 as source domains similar to activity recognition setup while the last six subjects of SPC22 are used for source domains to prevent overlapping subjects with SPC12. Dalia PPG dataset for motion compensation and heart rate estimation in Daily Life Activities (DaLia) was recorded from 15 subjects (8 females, 7 males, mean age of 30.6), where each recording was approximately two hours long. PPG signals were recorded while subjects went through different 19daily life activities, for instance sitting, walking, driving, cycling, working, and so on. PPG signals were recorded at a sampling rate of 64 Hz. The first five subjects are used as source domains. All PPG datasets are standardized as follows. Initially, a fourth-order Butterworth bandpass filter with a frequency range of 0.5–4 Hz is applied to PPG signals. Subsequently, a sliding window of 8 seconds with 2-second shifts is employed for segmentation, followed by z-score normalization of each segment. Lastly, the signal is resampled to a frequency of 25 Hz for each segment. B.3 Cardiovascular disease (CVD) classification CPSC China Physiological Signal Challenge 2018 (CPSC2018), held during the 7th International Conference on Biomedical Engineering and Biotechnology in Nanjing, China. This dataset consists of 6,877 (male: 3,699; female: 3,178) and 12 lead ECG recordings lasting from 6 seconds to 60 seconds with 500 Hz. We use the original labelling [40] with one normal and eight abnormal types as follows: atrial fibrillation, first-degree atrioventricular block, left bundle branch block, right bundle branch block, premature atrial contraction, premature ventricular contraction, ST-segment depression, ST-segment elevated. We resampled recordings to 100 Hz and excluded recordings of less than 10 seconds. Chapman Chapman University, Shaoxing People’s Hospital (Chapman) ECG dataset which pro- vides 12-lead ECG with 10 seconds of a sampling rate of 500 Hz. The recordings are downsampled to 100 Hz, resulting in each ECG frame consisting of 1000 samples. The labeling setup follows the same approach as in [41] with four classes: atrial fibrillation, GSVT, sudden bradycardia, and sinus rhythm. The ECG frames are normalized to have a mean of 0 and scaled to have a standard deviation of 1. We split the dataset to 80–20% for training and testing as suggested in [41]. We choose leads I, II, III, and V2 during our experiments for both ECG datasets. We followed a similar setup with prior works [57] and considered each dataset as a single domain different from previous tasks. The fine-tuning of the linear layer, which is added to the frozen pre-trained encoder, is performed with 80% of the same domain. B.4 Metrics We used the common evaluation metrics in the literature for each task. Specifically, we used accuracy (Acc) and F1 score for activity recognition [19], mean absolute error (MAE), and root mean square error (RMSE) for heart rate prediction [39, 84], and the area under the ROC curve (AUC) for cardiovascular disease classification [57]. In this section, we explain how to calculate each metric for different time-series tasks. For activity recognition, the accuracy metric is computed by dividing the sum of true positives and true negatives by the total number of samples where a window has a single label. The MF1 score is calculated as a harmonic mean of the precision and recall where metrics are obtained globally by counting the total true positives, false negatives, and false positives similar to [19]. For heart rate prediction, the Mean Absolute Error (MAE) and Root-Mean-Square Error (RMSE) are calculated using the following equation: MAE = 1 K KX k=1 |HRmodel(k) − HRref(k)| (36) RMSE = sPK k=1(HRmodel(k) − HRref(k))2 K , (37) where K represents the total number of segments. The variables HRmodel(k) and HRref(k) denote the output of the model and reference heart rate values in beats-per-minute for the kth segment, respectively. This performance metric is commonly used in PPG-based heart rate estimation studies [39]. The estimated heart rate values (HRmodel(k)) are obtained using our model, while the reference heart rate values (HRref(k)) are directly taken from datasets. 20The AUC score for CVD classification is calculated using the one-vs-one scheme where the average AUC is computed for all possible pairwise combinations of classes for both datasets. C Baselines C.1 Prior Mixup Techniques In this section, we give a detailed explanation of each mixup technique we compare our proposed method. LinearMix We apply linear mixup as in Equation 38 to generate positive samples, ifx has more than one channel, mixup is applied independently for each of them. x+ = λx + (1 − λ)˜ x (38) BinaryMix We implement the binary mixup [43] by swapping the elements of x with the elements of another randomly chosen sample ˜ xas shown below. x+ = m ⊙ x + (1 − m) ⊙ ˜ x, (39) where m is a binary mask sampled from a Bernouilli(ρ) with high values, and ⊙ stands for Hadamard product. GeometricMix In Geometric Mixup, we create a positive sample corresponding to a sample x by taking its weighted-geometric mean with another randomly chosen sample ˜ xsame as [22] as shown below. x+ = xλ + ˜ x(1−λ) (40) CutMix Cutmix is implemented similarly to Binarymix. However, instead of changing each sample point with a probability, we cut a continuous portion using a rectangle mask M from a signal x and replace it with the same portion of another randomly chosen one ˜ x. The starting point of the mask is uniformly sampled while its length is sampled from lower values such that the augmented sample is more similar to the anchor. If the signal has multiple channels, this process is applied to all channels in the same section. x+ = M ⊙ x + (1 − M) ⊙ ˜ x, and M = rect \u0012b a \u0013 , (41) where b and a are the starting point and length of the rectangle wave, respectively. AmplitudeMix AmplitudeMix is introduced for domain adaptation problems by mixing the ampli- tude information of images without mixing the phase of two samples [45]. In our setup, we perform amplitude mixing on the time series data across all channels while keeping the phase component unchanged. In other words, we perform the following operations. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) = P(x) (42) SpecMix We implement the SpecMix by applying CutMix to the spectrogram of time-series where the spectrogram is calculated using the short-time Fourier transform as follows. X+ k = ∞X n=−∞ xng[n − mR]e−j2πkn, (43) 21where g[n − mR] is an analysis window of length M with hop length of R over the signal and calculating the discrete Fourier transform (DFT) of each segment of windowed data. The length of the Fourier transform is set to the sample size of the input time series while the hop and window parameters are set to the quarter of the length. C.2 Prior Methods for Sample Generation In this section, we give a detailed explanation of prior methods for data generation methods. Traditional Augmentations We apply two separate data augmentation to the anchor for creating two instances, and the encoders are trained to maximize agreement using the contrastive loss in [15]. We search mainly for augmentations that are known in state-of-the-art works [ 19]. The detailed augmentations are given in Table 22. InfoMin We train a model gθ(.), which is restricted to sample-wise 1 × 1 convolutions and ReLU activations same as in [48], to decrease the mutual information between two instances. In the original paper, the input sample is split into two instances ( X1 and X2:3) and then adversarial training is performed. As we do not have RGB channels for time-series data, we added Gaussian noise to the signal for creating other instances and then perform adversarial training. NNCLR We follow a similar setup to SimCLR by applying two separate data augmentations, then we use nearest neighbors in the learned representation space as the positive in contrastive losses [49]. PosET We perform the dimension level mixing with extrapolation of positive features as follows: z+ = λ ⊙ z + (1 − λ) ⊙ ˜ z, (44) where ⊙ is Hadamard product, and λ ∼ Beta(α, α). We add 1 to sampled λ for extrapolation as in [50]. GenRep In the original implementation of GenRep, the authors use implicit generative models (IGMs) such as BigBiGAN [85] that are trained with millions of images to create the anchor and positive instance by sampling nearby latent vectors. However, as the number of samples for training is limited in time series and there is a well-trained generator for different time-series tasks, we use our trained V AE for sampling nearby latent vectors as positives. Mainly, we sample an anchor from real data, feed it to the encoder, add a Gaussian noise sampled from a truncated normal distribution, and use the output of the decoder for the positive sample with the anchor. STAug The Spectral and Time Augmentation (STAug) method is specifically proposed for the time-series forecasting task, where the authors apply the empirical mode decomposition to decompose time series into multiple subcomponents, then reassemble these subcomponents with random weights to generate new synthetic series. Finally, in the time domain, the method uses the linear mixup to generate samples from the reassembled components. Although, the mixing coefficient sampled from a beta distribution in the original implementation, we observe significant performance decreases when the same distribution with parameters is used in our experiments, possibly due to the generated samples being far away from the anchor. We, therefore, investigate the case when the mixing coefficient is sampled from uniform distribution with high values, e.g., same as our method. Since there is no Augmentation Bank The augmentation bank that perturbs frequency components of a time-series signal is proposed in [ 21] where the authors use it for unsupervised domain adaptation with a different framework than SimCLR, namely time-frequency consistency (TF-C). As it is a novel data augmentation technique, we have implemented the frequency augmentation bank as a baseline while using the SimCLR framework for a fair comparison with other methods. The authors also employed a collection of time-based augmentations for the time-domain contrastive encoder. Nonetheless, since these augmentations have already been studied in previous CL setups, we chose to exclusively utilize the frequency augmentation bank. In the paper, the authors mentioned using a small budget with low-frequency perturbations results in a performance increase, thus we chose the budget with a single frequency while choosing the α = 0.5 with the same settings in the paper. 22DACL We perform the mixup for hidden representations, i.e., before applying projection-head, as follows. v+ = λv + (1 − λ)˜ v, (45) where v is the fixed-length hidden representations of samples while λ is sampled from uniform distribution with high values. IDAA We follow the original implementation of authors with their proposed V AE architecture while optimizing the adversarial strength for each time-series task. We apply the FGSM adversarial attack the same as in the original implementation [53] by perturbing the encoded representation of a sample while adding noises along the gradient sign’s direction of the loss. One setup difference between this section and the previous mixup methods is that when we compare our work with PosET, GenRep, DACL, and IDAA, we apply the best traditional data augmentation techniques, which are used for SimCLR implementation, to the specific positive data generation mechanisms. The reason for this approach is that the original implementations of certain works indicate that the proposed methods achieve optimal results when used in conjunction with known augmentations, where our observations align with these findings. The detailed hyperparameters for each baseline with the corresponding time series tasks are given in the following section. D Implementation Details D.1 Parameters for mixing In this section, we provide the parameters that are used during our experiments. To determine the optimal parameters of the baselines for each task, we conduct a grid search. This search is performed on a small validation set taken from the largest dataset of the respective tasks, which are USC, Dalia and Chapman. We believe that this approach ensures fairness and produces more realistic results, as dataset-specific optimizations can lead to overfitting of parameters, particularly in smaller and less diverse datasets. Table 7: Parameters for baselines Method Activity Recognition Heart rate Prediction CVD Classification Linear Mixup λ∼U(0.9,1) λ∼U(0.9,1) λ∼U(0.85,1) Binary Mixup m∼U(0.8,1) m∼U(0.9,1) m∼U(0.9,1) Geometric Mixupλ∼U(0.9,1) λ∼U(0.9,1) λ∼U(0.9,1) CutMix b∼U(0,1) b∼U(0,1) b∼U(0,1) a∼U(0.1,0.4) a∼U(0.1,0.3) a∼U(0.1,0.3) AmplitudeMixλA∼U(0.9,1) λA∼U(0.9,1) λA∼U(0.8,1) SpecMix b∼U(0,1) b∼U(0,1) b∼U(0,1) a∼U(0.1,0.4) a∼U(0.1,0.3) a∼U(0.1,0.3) PosET λ∼Beta(2,2) λ∼Beta(2,2) λ∼Beta(2,2) GenRep λ∼ Nt(0,0.2,1.0) λ∼ Nt(0,0.25,1.0) λ∼ Nt(0,0.2,1.0) DACL λ∼U(0.9,1) λ∼U(0.9,1) λ∼U(0.85,1) IDAA δ= 0.1 δ= 0.15 δ= 0.2 Ours λA∼U(0.7,1),λP ∼U(0.9,1) λA∼U(0.7,1),λP ∼U(0.9,1) λA∼U(0.7,1),λP ∼U(0.9,1) ϵ= 0.7,λA, λP ∼ Nt(0.9,0.1,0.9) ϵ= 0.8,λA, λP ∼ Nt(1,0.1,0.9) ϵ= 0.7,λA, λP ∼ Nt(1,0.1,0.9) D.2 Baseline Encoder Architecture For the baseline encoder model, we adopt the DeepConvLSTM as in [19] where the architecture has 4 convolutional layers with 5 × 1 size of 64 kernels while ReLU is followed each convolution. After the convolutions, the tensor is passed through a dropout layer with a dropout rate of 0.5 to prevent overfitting. Then, the output of dropout is fed into the 2-layer LSTM with 128 units. After training 23the baseline encoder, we attach a linear layer and freeze the previous layers for fine-tuning. This architecture is widely used for the datasets we used during our experiments [37, 83, 19], we therefore adopt the same network across tasks. D.3 V AE Models We use the total correlation variational autoencoder (β-TCV AE) [86] to calculate the distance between two encoded samples in the latent space. We train the model for 100 epochs with a learning rate of 1e − 3 while setting the batch size to 2048. The latent dimensions and the β values are set to 10 and 5, respectively. Below, we present the tables providing detailed information about the architectures of the encoder and decoder for datasets. The output of convolutional layers is fed to the batch normalization before the activation layer is applied. For tasks Heart rate Prediction and CVD Classification, we use task-specific encoder and decoder as the number of channels and input size for datasets in each task are the same. However, two different networks, one for UCIHAR and one for others, are designed for the Activity Recognition due to different number of input channels. Table 8: Encoder Network for UCIHAR in Activity Recognition Encoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x128x9 Convolution Nx32x60x7 32 9x3 2x1 ReLU Convolution Nx32x27x5 32 7x3 2x1 ReLU Convolution Nx64x8x3 64 5x3 3x1 ReLU Convolution Nx128x2x1 128 5x3 2x1 ReLU Convolution Nx512x1x1 512 2x1 1x1 ReLU Convolution Nx20x1x1 10 1x1 1x1 Table 9: Decoder Network for UCIHAR in Activity Recognition Decoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x10x1 Transposed Convolution Nx512x2x9 512 2x9 1x1 ReLU Transposed Convolution Nx128x8x9 128 4x1 6x1 ReLU Transposed Convolution Nx64x16x9 64 4x1 2x1 ReLU Transposed Convolution Nx32x32x9 32 4x1 2x1 ReLU Transposed Convolution Nx32x64x9 32 4x1 2x1 ReLU Transposed Convolution Nx1x128x9 1 4x1 2x1 Table 10: Encoder Network for USC and HHAR in Activity Recognition Encoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x100x6 Convolution Nx32x46x5 32 9x2 2x1 ReLU Convolution Nx32x20x4 32 9x2 2x1 ReLU Convolution Nx64x8x3 64 5x2 2x1 ReLU Convolution Nx128x2x2 128 5x2 2x1 ReLU Convolution Nx512x1x1 512 2x2 1x1 ReLU Convolution Nx20x1x1 10 1x1 1x1 24Table 11: Decoder Network for USC and HHAR in Activity Recognition Decoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x10x1 Transposed Convolution Nx512x2x6 512 2x6 1x1 ReLU Transposed Convolution Nx128x6x6 128 6x1 2x1 ReLU Transposed Convolution Nx64x12x6 64 4x1 2x1 ReLU Transposed Convolution Nx32x25x6 32 5x1 2x1 ReLU Transposed Convolution Nx32x50x6 32 4x1 2x1 ReLU Transposed Convolution Nx1x100x6 1 4x1 2x1 Table 12: Encoder Network for Heart rate Prediction Encoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x200x1 Convolution Nx32x94x1 32 13x1 2x1 ReLU Convolution Nx32x43x1 32 9x1 2x1 ReLU Convolution Nx64x18x1 64 9x1 2x1 ReLU Convolution Nx128x6x1 128 7x1 2x1 ReLU Convolution Nx512x1x1 512 5x1 2x1 ReLU Convolution Nx20x1x1 20 2x1 1x1 Table 13: Decoder Network for Heart rate Prediction Decoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x10x1 Transposed Convolution Nx512x6x1 512 6x1 1x1 ReLU Transposed Convolution Nx128x12x1 128 4x1 2x1 ReLU Transposed Convolution Nx64x25x1 64 5x1 2x1 ReLU Transposed Convolution Nx32x50x1 32 4x1 2x1 ReLU Transposed Convolution Nx32x100x1 32 4x1 2x1 ReLU Transposed Convolution Nx1x200x1 1 4x1 2x1 Table 14: Encoder Network for CVD Classification Encoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x1000x4 Convolution Nx32x330x3 32 12x2 3x1 ReLU Convolution Nx32x107x2 32 10x2 3x1 ReLU Convolution Nx64x34x1 64 8x2 3x1 ReLU Convolution Nx128x9x1 128 8x1 3x1 ReLU Convolution Nx512x1x1 512 7x1 3x1 ReLU Convolution Nx20x1x1 20 1x1 1x1 Table 15: Decoder Network for CVD Classification Decoder Layer Name Output size # of kernels Kernel size Stride Activation Input Nx1x10x1 Transposed Convolution Nx512x4x4 512 6x1 1x1 ReLU Transposed Convolution Nx128x12x4 128 4x1 2x1 ReLU Transposed Convolution Nx64x36x4 64 5x1 2x1 ReLU Transposed Convolution Nx32x109x4 32 4x1 2x1 ReLU Transposed Convolution Nx32x331x4 32 4x1 2x1 ReLU Transposed Convolution Nx1x1000x4 1 4x1 2x1 2545 50 55 60 65 70 75 80 85 90 95 -0.2 -0.1 0 Accuracy ΔλP UCIHAR HHAR USC 70 75 80 85 90 95 100 -0.2 -0.1 0 AUC ΔλP CPSC Chapmana) b) c) 10 12 14 16 18 20 22 24 26 -0.2 -0.1 0 Mean Absolute Error ΔλP IEEE SPC12 IEEE SPC22 Dalia Figure 3: The experiment regarding the effect of phase mixup coefficients in eight datasets. a) shows the performance in activity recognition, b) is for heart rate prediction using PPG, and finally c) shows the cardiovascular disease classification 50 60 70 80 90 100 -0.2 -0.1 0 Accuracy ΔλA UCIHAR HHAR USC 75 80 85 90 95 100 -0.2 -0.1 0 AUC ΔλA CPSC Chapmana) b) c) 10 12 14 16 18 20 22 24 26 -0.2 -0.1 0 Mean Absolute Error ΔλA IEEE SPC12 IEEE SPC22 Dalia Figure 4: The experiment regarding the effect of amplitude mixup coefficients in eight datasets. a) shows the performance in activity recognition, b) is for heart rate prediction using PPG, and finally c) shows the cardiovascular disease classification E Additonal Results E.1 The effect and robustness of mixing coefficients In this section, our experiments focus on observing the impact of a diverse range of mixing coefficients for both phase and amplitude components. We decrease the lower threshold of distributions for sampling the mixing coefficient by 0.1 and 0.2. For example, normally the phase mixup coefficient for Activity Recognition is sampled from truncated normal λP ∼ Nt(1, 0.1, 0.9) and uniform λP ∼ U(0.9, 1). We decrease the low threshold value from 0.9 to 0.8 and 0.7 and report the results for both phase and amplitude. The results are reported in Figures 3 and 4 for eight datasets. From Figures 3 and 4, it can be inferred that the phase component is more sensitive to the changes. In other words, a significant decrease in performance is observed when the mixing coefficients for the phase are sampled from lower values whereas this effect is not as much as severe for the amplitude coefficient, indicating that the amplitude of frequencies is more robust to changes compared to phase. E.2 The performance in other frameworks In this section, we investigate the effect of data augmentations in three different unsupervised learning frameworks which are SimCLR [15], BYOL [59] and TS-TCC [16]. For BYOL, the hidden size of the projector is set to 128, the exponential moving average parameter is set to 0.996. For TS-TCC, the λ1 and λ2 coefficients of temporal and contextual contrasting losses are set to 1, the same as in the original implementation. In TS-TCC, the authors proposed to use a weak (jitter and scale) and strong (permutation and jitter) augmentation together, which is shown as TS-TCC + Traditional Augs in the tables. During our experiments, we followed the original implementation of TS-TCC and applied additional augmentations after the strong one without changing the original contrastive learning framework. We set the scaling ratio to 2 and 10 for permutation (splitting the signal into a random number of segments with a maximum of 10 and randomly shuffling them). These parameters for augmentation strengths are set to the same values as in the original implementation. 26Table 16: Performance comparison of our method in different CL frameworks forActivity Recognition Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ SimCLR + Traditional Augs. 87.05±1.07 86.13±0.96 85.48±1.16 84.31±1.31 53.47±1.10 52.09±0.95SimCLR + Aug. Bank 65.27 ±1.12 71.16±1.24 67.95±1.45 75.13±1.32 43.28±4.37 47.31±4.68SimCLR + DACL 73.12 ±1.23 66.28±1.11 80.89±0.91 81.31±0.78 53.61±2.60 51.76±2.21SimCLR + Ours 91.60 ±0.65 90.46±0.53 88.05±1.05 87.95±1.10 60.13±0.75 59.13±0.69BYOL + Traditional Augs. 83.41±0.95 82.13±1.12 86.41±0.97 86.31±1.10 58.34±1.15 55.04±1.15BYOL + Aug. Bank 73.71 ±0.74 69.80±1.10 84.60±0.93 84.65±1.03 52.00±1.21 49.14±1.18BYOL + DACL 73.86 ±1.12 70.46±1.24 82.76±1.04 84.89±0.93 47.14±2.08 45.34±2.98BYOL + Ours 87.01 ±1.10 84.92±1.13 90.31±1.16 90.45±1.31 56.87±0.91 55.01±0.95TS-TCC + Traditional Augs. 90.95±0.87 90.30±0.64 35.57±1.43 40.13±1.67 39.76±1.61 43.12±1.10TS-TCC + Aug. Bank 76.78 ±0.95 76.52±0.97 20.25±1.54 19.25±1.32 21.37±1.78 20.15±1.48TS-TCC + DACL 73.86 ±1.12 70.46±1.24 33.89±1.87 37.41±1.39 36.74±1.36 40.18±1.45TS-TCC + Ours 91.86±0.97 91.92±1.02 38.45±1.12 43.52±1.33 42.61±1.92 45.06±1.11 Table 17: Performance comparison of our method in different CL frameworks for Heart Rate Prediction Method IEEE SPC12 IEEE SPC22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ SimCLR + Traditional Augs. 20.67±1.13 26.35±0.98 16.84±1.10 22.23±0.72 12.01±0.65 21.09±0.86SimCLR + Aug. Bank 27.31 ±2.17 37.93±2.96 27.84±2.03 36.41±3.98 35.87±4.18 40.61±3.74SimCLR + DACL 21.85 ±1.63 28.17±1.75 14.67±1.10 20.06±1.21 18.44±1.32 25.61±1.45SimCLR + Ours 16.26 ±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73BYOL + Traditional Augs. 20.68±0.98 27.11±0.85 21.16±1.10 26.83±1.05 12.03±0.75 20.77±0.83BYOL + Aug. Bank 26.08 ±1.05 32.62±0.93 21.87±1.03 29.13±1.03 18.63±0.91 28.30±0.87BYOL + DACL 26.45 ±1.23 33.50±1.32 21.29±1.13 27.34±1.33 15.11±0.93 23.21±0.83BYOL + Ours 19.85 ±0.88 26.10±0.94 22.08±1.24 28.20±1.13 11.45±0.63 20.38±0.80TS-TCC + Traditional Augs. 11.08±1.03 16.97±0.92 16.10±1.23 26.11±1.11 16.18±1.03 24.27±0.95TS-TCC + Aug. Bank 11.44 ±1.01 17.06±0.94 13.79±1.21 22.41±1.08 17.28±1.12 25.41±0.98TS-TCC + DACL 11.60 ±1.16 18.26±1.20 15.25±1.26 24.40±1.10 16.27±1.16 24.28±0.97TS-TCC + Ours 10.82±0.65 16.93±0.73 13.63±1.02 21.80±1.11 15.90±0.57 23.81±0.89 Tables 16 17 and 18 compares the performance of three data augmentation techniques, traditional time-series augmentations, DACL and our proposed method, in contrastive learning frameworks of BYOL, SimCLR, and TS-TCC. Table 18: Performance comparison of our method in different CL frameworks for CVD classification Method CPSC 2018 Chapman AUC↑ AUC↑ SimCLR + Traditional Augs. 67.86±3.41 74.69 ±2.04 SimCLR + Aug. Bank 81.78 ±1.24 94.75 ±0.90 SimCLR + DACL 82.38 ±0.84 92.28 ±0.97 SimCLR + Ours 85.30 ±0.45 95.90±0.82 BYOL + Traditional Augs 75.41 ±1.34 85.63 ±1.43 BYOL + Aug. Bank 83.51 ±1.12 91.03 ±1.18 BYOL + DACL 77.61 ±1.16 81.62 ±1.24 BYOL + Ours 83.25 ±1.03 91.23 ±1.15 TS-TCC + Traditional Augs 87.07 ±1.10 92.03 ±1.17 TS-TCC + Aug. Bank 86.67 ±1.04 92.15 ±1.02 TS-TCC + DACL 87.63 ±0.83 92.21 ±0.86 TS-TCC + Ours 88.05±0.37 92.11 ±0.75 The results show that the BYOL is more robust to the choice of augmentations than SimCLR, which is also indicated in the original paper [59]. Also, another important outcome of this ablation experiment is that when the TS-TCC framework is used for datasets HHAR and USC, the performance decreases compared to other datasets. A possible explanation for this decrease in the TS-TCC might be the hyper-parameters of the augmentations that are used in the paper. The authors change the strength of the permutation window from dataset to dataset. In our experiments, we used the same hyperparameter for all activity recognition datasets, which can explain the outcome. This ablation experiment also shows that the degree of traditional augmentations is important for contrastive learning to learn class invariant representations. 27E.3 Do we still need data augmentations? In this section, we conduct experiments to observe the performance of methods without additional augmentations. During our experiments, we searched for the best traditional augmentation technique for each method in a given task. We searched over common time series augmentation methods in literature (Table 22), and applied them with baselines. Specifically, we apply Resample for Activity Recognition, Permutation with Noise for Heart rate Prediction and Noise with Scaling for CVD Classification. We have observed that these augmentations yield the best results for all baselines when applied prior to the proposed techniques. However, for GenRep, we found that applying the augmentations after generating instances results in better performance, similar to the original work [51]. We, therefore, apply these specified augmentations for each baseline and report the corresponding results. Different from other baselines, we observed performance increases for a few datasets when GenRep is applied without any augmentations. This phenomenon can be attributed to the generation of low-quality and less realistic positive samples, where additional augmentations lead to alterations in semantic information, due to less number of samples during training V AE models. However, in the end, we observe that applying additional augmentations always increases the performance on average for all baselines in each task. Table 19: Performance comparison of methods without Augs. in Activity Recognition datasets Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ IDAA [53] 82.23 ±0.69 79.84 ±0.89 88.98±0.62 89.01±0.55 59.23 ±1.10 56.11 ±1.54 w/o Aug. 64.42 (-17.81) 65.17 (-14.67) 86.44 (-2.54) 86.31 (-2.70) 35.22 (-24.01) 33.62 (-22.59) GenRep [51] 87.22±1.05 86.48 ±0.95 87.05 ±0.95 86.45 ±0.90 50.13 ±2.85 49.50 ±2.73 w/o Aug. 88.01 (+0.79) 88.12 (+1.64) 86.51 (-0.54) 86.33 (-0.22) 48.31 (-1.82) 47.33 (-2.17) DACL [22] 73.12 ±1.23 66.28 ±1.11 80.89 ±0.91 81.31 ±0.78 53.61 ±2.60 51.76 ±2.21 w/o Aug. 45.17 (-27.95) 44.84 (-21.44) 56.70 (-24.19) 56.55 (-25.76) 27.12 (-26.49) 26.99 (-24.77) Ours 91.60±0.65 90.46±0.53 88.05 ±1.05 87.95 ±1.10 60.13±0.75 59.13±0.69 w/o Aug. 84.04 (-5.56) 83.34 (-7.12) 86.70 (-1.35) 86.72 (-1.23) 45.55 (-14.58) 44.94 (-14.19) Table 20: Performance comparison of methods without Augs. in Heart Rate Prediction datasets Method IEEE SPC12 IEEE SPC22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ IDAA [53] 19.02 ±0.96 27.42 ±1.11 15.37 ±1.21 22.41 ±1.42 11.12 ±0.64 20.45 ±0.69 w/o Aug. 20.19 (+1.17) 28.51 (+1.09) 16.34 (+0.97) 25.75 (+3.34) 16.01 (+4.89) 25.62 (+5.17) GenRep [51] 21.02±1.41 28.42 ±1.65 15.67 ±1.23 22.33 ±1.43 25.41 ±1.62 36.83 ±1.87 w/o Aug. 20.51 (-0.51) 28.35 (-0.07) 23.07 (+7.40) 33.20 (+10.87) 20.03 (-5.38) 31.01 (-5.82) DACL [22] 21.85 ±1.63 28.17 ±1.75 14.67 ±1.10 20.06 ±1.21 18.44 ±1.32 25.61 ±1.45 w/o Aug. 22.75 (+0.90) 29.90 (+1.73) 20.88 (+6.21) 29.51 (+2.70) 28.24 (+9.45) 37.33 (+11.72) Ours 16.26±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73 w/o Aug. 19.41 (+3.15) 26.23 (+3.75) 16.41 (+4.16) 25.71 (+7.51) 16.73 (+6.16) 27.43 (+7.06) 28Table 21: Performance comparison of methods without Augs. in CVD classification datasets Method CPSC 2018 Chapman AUC↑ AUC↑ IDAA [53] 80.90 ± 0.73 93.63 ± 0.91 w/o Aug. 79.00 (-1.90) 92.37 (-1.26) GenRep [51] 52.49 ± 3.43 86.72 ± 1.13 w/o Aug. 45.17 (-7.32) 84.51 (-2.21) DACL [22] 82.38 ± 0.84 92.28 ± 0.97 w/o Aug. 73.00 (-9.38) 75.10 (-17.18) Ours 85.30 ± 0.45 95.90 ± 0.82 w/o Aug. 79.67 (-5.63) 93.48 (-2.42) Table 22: Common time series augmentations [19] Domain Augmentation Details Time Noise Add Gaussian noise sampled from normal distribution,N(0,0.4) Scale Amplify channels by a random distortion sampled from normal distributionN(2,1.1) Shuffle Randomly permute the channels of the sample. (Not available forHeart rate Prediction) Negate Multiply the value of the signal by a factor of -1 Permute Split signals into no more than 5 segments, then permute the segments and combine them into the original shape Resample Interpolate the time-series to 3 times its original sampling rate and randomly down-sample to its initial dimensions Rotation Rotate the 3-axial (x, y, and z) readings of each IMU sensor by a random degree, which follows a uniform around a random axis in the 3D space. (Only applied forActivity Recognition) Time Flip Flip the time series in time for all channels, i.e.,xAug[n] =x[−n] Random Zero Out Randomly chose a section to zero out Permutation + Noise Combination of Permutation and Noise Noise + Scale Combination of Noise and Scaling Frequency Highpass Apply a highpass filter in the frequency domain to reserve high-frequency components Lowpass Apply a lowpass filter in the frequency domain to reserve low-frequency components Phase shift Shift the phase of time-series data with a randomly generalized number Noise in Frequency Add Gaussian noise, sampled from normal distributionN(0,0.5), to the frequency spectrum 29E.4 The effect of interpolating phase components Here, we investigate the effect of phase interpolation of two samples on the CL performance. In our proposed method, we bring the phase components of the two coherent signals together by adding a small value to the anchor’s phase in the direction of the other sample. In this section, we apply the opposite case of our proposed method and increase the gap of phase difference between the anchor and randomly chosen sample. However, we mix their amplitudes according to our proposed method to only observe the phase effect. In other words, we perform the mixup as in Equation 46. Note that the phase mixing in Equation 46 differs from the proposed method only by the sign change. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) = P(x) + ∆Θ∗ (1 − λP ) (46) Also, It is important to note that we sample the mixing coefficients for both amplitude and phase from the same distributions in the proposed method to have a fair comparison. Tables 23 24 25. Table 23: Performance comparison of our method and its ablation regarding the phase interpolation in SimCLR and BYOL frameworks for Activity Recognition Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ SimCLR + Traditional Augs. 87.05±1.07 86.13±0.96 85.48±1.16 84.31±1.31 53.47±1.10 52.09±0.95SimCLR + Phase Gap 79.62 ±1.10 80.57±1.03 86.55±0.83 86.68±0.71 53.61±2.60 51.76±2.21SimCLR + Ours 91.60±0.65 90.46±0.53 88.05±1.05 87.95±1.10 60.13±0.75 59.13±0.69BYOL + Traditional Augs. 83.41±0.95 82.13±1.12 86.41±0.97 86.31±1.10 58.34±1.15 55.04±1.15BYOL + Phase Gap 78.66 ±0.63 75.45±1.02 85.82±0.91 85.16±0.92 56.14±0.67 56.20±0.75BYOL + Ours 87.01 ±1.10 84.92±1.13 90.31±1.16 90.45±1.31 56.87±0.91 55.01±0.95 Table 24: Performance comparison of our method and its ablation regarding the phase interpolation in SimCLR and BYOL frameworks for Heart Rate Prediction Method IEEE SPC12 IEEE SPC22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ SimCLR + Traditional Augs. 20.67±1.13 26.35±0.98 16.84±1.10 22.23±0.72 12.01±0.65 21.09±0.86SimCLR + Phase Gap 18.90 ±1.43 25.29±1.56 14.60±1.03 19.84±1.15 17.57±1.13 27.72±1.35SimCLR + Ours 16.26±0.72 22.48±0.95 12.25±0.47 18.20±0.61 10.57±0.55 20.37±0.73BYOL + Traditional Augs. 20.68±0.98 27.11±0.85 21.16±1.10 26.83±1.05 12.03±0.75 20.77±0.83BYOL + Phase Gap 25.93 ±0.96 32.68±0.90 21.87±1.03 29.13±1.03 17.46±0.83 27.24±0.83BYOL + Ours 19.85 ±0.88 26.10±0.94 22.08±1.24 28.20±1.13 11.45±0.63 20.38±0.80 Table 25: Performance comparison of our method and its ablation regarding the phase interpolation in SimCLR and BYOL frameworks for CVD classification Method CPSC 2018 Chapman AUC↑ AUC↑ SimCLR + Traditional Augs. 67.86±3.41 74.69 ±2.04 SimCLR + Phase Gap 77.45 ±1.10 91.95 ±0.91 SimCLR + Ours 85.30±0.45 95.90±0.82 BYOL + Traditional Augs 75.41 ±1.34 85.63 ±1.43 BYOL + Phase Gap 83.11 ±1.03 91.02 ±1.11 BYOL + Ours 83.25 ±1.03 91.23 ±1.15 30E.5 The comparison of Mixup methods In this section, we give a detailed comparison of prior mixup methods with ours below tables, which are the explicit numbers for Figure 2. Our method demonstrates superior performance compared to previous mixup techniques in 11 out of 14 metrics, indicating its effectiveness. Additionally, the Amplitude Mixup technique, which yields comparable results in two datasets, further supports our claim regarding the destructive effect of simultaneously mixing phase and magnitude for time series. The relatively lower performance of Amplitude Mixup for some datasets can be explained by its limited diversity in generating positive samples since this technique has no solution for mixing the phase of samples in randomly chosen pairs. In other words, as the phase of the augmented instance is the same as the anchor in Amplitude Mix, the diversity of generated positive samples is less compared to other techniques. Table 26: Performance comparison of ours with prior mixups in Activity Recognition datasets Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ Geo 36.31 ±10.15 33.21 ±12.25 33.16 ±8.32 31.15 ±9.25 24.85 ±9.43 21.64 ±8.94 Amp 81.76 ±0.89 80.78 ±0.78 87.85±0.83 85.53±1.10 41.29 ±0.56 39.77 ±1.03 Spec 40.14 ±2.05 38.34 ±1.95 56.73 ±2.01 53.54 ±1.98 23.45 ±2.55 21.30 ±2.41 Cut 50.21 ±1.34 48.23 ±1.23 57.71 ±1.12 53.87 ±1.09 25.63 ±2.95 23.41 ±3.11 Binary 74.13 ±1.12 71.31 ±1.10 77.12 ±0.75 75.23 ±0.95 42.21 ±0.97 41.53 ±1.10 Linear 82.23 ±2.10 80.25 ±1.93 80.11 ±2.05 81.31 ±1.73 40.15 ±1.43 39.71 ±1.14 Ours 84.30±0.73 83.23±0.58 84.51 ±1.10 83.98 ±1.03 45.36±0.97 43.14±0.81 Table 27: Performance comparison of ours with prior mixups in Heart Rate Prediction datasets Method IEEE SPC12 IEEE SPC 22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ Geo 32.65 ±7.25 48.90 ±9.87 37.15 ±6.74 36.32 ±6.21 38.45 ±7.31 41.32 ±6.21 Amp 23.01 ±0.95 30.10 ±1.04 18.07 ±1.13 23.13 ±1.43 19.05 ±1.63 30.41 ±1.65 Spec 24.09 ±4.10 38.41 ±3.98 24.41 ±4.10 29.93 ±4.10 26.71 ±4.34 35.31 ±3.93 Cut 24.98 ±3.93 35.67 ±4.15 21.77 ±4.45 28.43 ±3.97 31.75 ±4.10 43.56 ±3.88 Binary 32.23 ±1.67 40.21 ±1.98 22.55 ±1.87 28.78 ±2.10 19.71 ±2.15 28.83 ±2.45 Linear 24.31 ±1.54 31.29 ±1.75 18.52 ±1.43 22.54 ±1.49 24.16 ±1.89 32.46 ±1.97 Ours 21.13±0.89 28.21±1.15 16.17±0.85 21.13±1.05 16.64±1.20 28.43±1.43 Table 28: Performance comparison of ours with prior mixups in CVD classification datasets Method CPSC 2018 Chapman AUC↑ AUC↑ Geo 45.65 ± 6.43 61.32 ± 5.79 Amp 84.10 ± 1.05 89.83 ± 1.12 Spec 69.26 ± 3.10 70.48 ± 3.05 Cut 72.20 ± 2.98 79.23 ± 2.75 Binary 80.53 ± 1.62 82.56 ± 1.45 Linear 78.02 ± 1.43 90.21 ± 1.15 Ours 83.79 ± 1.10 93.85 ± 1.05 31F Illustrative Examples In this section, we show examples of the destructive behavior of linear mixup and how our proposed mixup technique solves this problem. In Figure 5 a), we show two PPG waveforms that are obtained from IEEE SPC15 with the same label i.e., the same heart rate value. Also, we give the corresponding frequency domain transformations of these two waveforms in Figure 5 b) where the frequency axis is converted to heart rate in beats-per-minute i.e., 1 Hz corresponds to 60 bpm. 40 60 80 100 120 140 160 180 200 0 0.1 0.2 0.3 0.4 0.5 Anchor  Sample Normalized Power (dB) a) b) 1 2 3 4 5 6 7 8 -2 -1 0 1 2 Anchor  Sample Normalized Magnitude (V) Time (s) Heart rate (bpm) Two samples with the same label where the anchor has a severe motion artifact at 2-3 Hz. 0 Figure 5: a) The waveforms of anchor and random sample, b) The frequency domain ( A(x)) representation of two samples. When the linear mixup is applied as in Equation 47 with a λ of 0.9, the resulting waveform is anticipated to contain heart rate information to an extent similar to both the anchor and the sample. x+ = λx + (1 − λ)˜ x (47) However, when there is a phase difference greater than π/2 between these two samples in the frequencies where the task-specific information is carried, the linear mixup destroys the information. 40 60 80 100 120 140 160 180 2000 0.1 0.2 0.3 0.4 0.5 Anchor  Sample  Linear mixup The linear mixup method  generates an augmented sample where the information in the critical frequency band decreased signiﬁcantly compared to both samples. Normalized Power (dB) Heart rate (bpm) a) b) 1 2 3 4 5 6 7 8 -2 -1 0 1 2 Anchor  Linear mixup Normalized Magnitude (V) Time (s) 0 Figure 6: a) The waveform of anchor and augmented sample with linear mixup, b) The frequency domain (A(x)) representations of samples where the augmented waveform has lost all the information in the critical frequency band, i.e., the task-specific information is lost. 32Figures 5 and 6 demonstrate the destructive behavior of linear mixup instead of feature interpolation. The linear mixup technique destroys the task-specific information even though the two samples have the same labels and the mixup ratio is relatively high. As our proposed mixup prevents this problem and interpolates between features of two samples, the information is not lost but rather enhanced as both samples have the same label, shown in Figure 7. 1 2 3 4 5 6 7 8 -2 -1 0 1 2 0 40 60 80 100 120 140 160 180 2000 0.1 0.2 0.3 0.4 0.5Normalized Power (dB) Heart rate (bpm) Normalized Magnitude (V) Time (s) Anchor  Sample  Proposed mixup Anchor  Proposed mixup a) b) The proposed mixup technique interpolates the features without causing any information loss in the critical frequency band. Figure 7: a) The waveform of anchor and augmented sample with proposed mixup technique, b) The frequency domain (A(x)) representations of samples where the augmented waveform carries the information in the critical frequency band as an interpolation of two samples. As can be seen From figures 6 and 7, our proposed mixup technique not only prevents information loss due to linear mixup but also generates an interpolated sample. 33G Performance in Supervised Learning Paradigm We also conduct experiments in the supervised learning paradigm with our proposed mixup method to see its effectiveness in different learning paradigms. We compare the performance of our method with prior mixup techniques. During the experiments, we follow the original implementation where the mixup is applied to the same minibatch after random shuffling. In the seminal work of mixup [31], the authors stated that interpolating only between inputs with equal labels does not lead to performance gains. Therefore, we only perform the tailored mixup without implementing any V AEs to check the similarity of the randomly chosen samples. We implement the tailored mixup for the supervised learning paradigm as follows. x+ = F−1(A(x+)∠P(x+)) where A(x+) = λAA(x) + (1− λA)A(˜ x) and P(x+) =    P(x) − |∆Θ| ∗(1 − λP ), if ∆Θ > 0 and λA ≥ 0.5 P(x) + |∆Θ| ∗(1 − λP ), if ∆Θ ≤ 0 and λA ≥ 0.5 P(˜ x) − |∆Θ| ∗(1 − λP ), if ∆Θ > 0 and λA < 0.5 P(˜ x) + |∆Θ| ∗(1 − λP ), if ∆Θ ≤ 0 and λA < 0.5 y+ = λAyx + (1 − λA)y˜ x, (48) where the coefficient for the λA is chosen from a beta distribution with α ∈ [0.1, 0.4] within the same range of the original implementation [ 31]. The mixing for the phase is constrained to our original implementation with a uniform λP ∼ U(0.9, 1). We searched for the best α value for each time-series task and augmentation method. Unlike linear mixup and our mixup approach, for cutmix, we followed the recommendation from the original paper and searched the α value close to 1. Table 29: Performance comparison in Activity Recognition within supervised learning scheme Method UCIHAR HHAR USC ACC↑ MF1↑ ACC↑ MF1↑ ACC↑ MF1↑ W/o Augs. 65.66 ±0.23 61.21 ±0.15 91.58 ±0.07 91.64 ±0.11 71.93 ±0.54 68.43 ±0.78 Linear Mix 77.06 ±0.18 73.21 ±0.17 93.64 ±0.17 93.67 ±0.08 74.45 ±0.28 71.93 ±0.43 Amp Mix 70.96 ±0.19 67.14 ±0.33 92.50 ±0.15 92.54 ±0.10 74.02 ±0.19 71.90 ±0.26 Binary Mix 69.01 ±0.36 71.63 ±0.11 92.36 ±0.19 92.42 ±0.10 72.81 ±0.15 70.98 ±0.35 CutMix 67.14 ±0.54 63.31 ±0.48 90.37 ±0.43 90.36 ±0.76 57.89 ±0.34 61.45 ±0.57 Ours 81.60±0.15 79.35±0.13 94.02±0.05 94.00±0.06 74.85±0.19 72.45±0.34 Table 30: Performance comparison in Heart Rate Prediction within supervised learning scheme Method IEEE SPC12 IEEE SPC 22 DaLia MAE↓ RMSE↓ MAE↓ RMSE↓ MAE↓ RMSE↓ W/o Augs. 20.01 ±0.03 27.16 ±0.05 20.29 ±0.87 26.60 ±1.13 6.58 ±0.10 11.30 ±0.58 Linear Mix 20.07 ±0.09 26.93 ±0.10 19.98 ±0.12 24.90 ±0.51 6.97 ±0.14 12.07 ±0.51 Amp Mix 20.14 ±0.07 26.98 ±0.07 19.61 ±0.07 24.11±0.21 11.20 ±0.17 16.07 ±0.43 Binary Mix 21.05 ±0.13 27.02 ±0.08 19.62 ±0.10 25.23 ±0.13 7.35 ±0.16 12.17 ±0.53 CutMix 20.12 ±0.06 26.89±0.11 19.64 ±0.13 24.18 ±0.20 10.78 ±1.23 14.40 ±1.43 Ours 19.97±0.05 26.98 ±0.10 19.45±0.12 24.35 ±0.18 6.49±0.08 11.69±0.10 Table 31: Performance comparison in CVD classification within supervised learning scheme Method CPSC 2018 Chapman AUC↑ AUC↑ W/o Augs. 82.01 ± 0.51 92.27 ± 0.35 Linear Mix 80.29 ± 0.93 93.02 ± 0.33 Amp Mix 80.01 ± 0.36 89.11 ± 0.27 Binary Mix 78.10 ± 0.98 80.31 ± 0.36 CutMix 80.75 ± 0.78 89.17 ± 0.58 Ours 83.75 ± 0.32 95.26 ± 0.24 34",
      "references": [
        "Unsupervised representation learning by predicting image rotations",
        "Unsupervised visual representation learning by context prediction",
        "Context encoders: Feature learning by inpainting",
        "Dimensionality reduction by learning an invariant mapping",
        "Discriminative unsupervised feature learning with convolutional neural networks",
        "Big self-supervised models are strong semi-supervised learners",
        "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
        "Contrastive learning of global and local video representations",
        "Unispeech: Unified speech representation learning with labeled and unlabeled data",
        "w2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training",
        "Lrc-bert: Latent-representation contrastive knowledge distillation for natural language understanding",
        "Kace: Generating knowledge aware contrastive explanations for natural language inference",
        "Semi-supervised intent discovery with contrastive learning",
        "Contrastive unsupervised word alignment with non-local features",
        "A simple framework for contrastive learning of visual representations",
        "Time-series representation learning via temporal and contextual contrasting",
        "Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap",
        "Evaluation and comparison of eeg traces: Latent structure in nonstationary time series",
        "What makes good contrastive learning on small-scale wearable-based tasks?",
        "Time series data augmentation for deep learning: A survey",
        "Self-supervised contrastive pre-training for time series via time-frequency consistency",
        "Towards domain-agnostic contrastive learning",
        "Ssmix: Saliency-based span mixup for text classification",
        "Detection of gait from continuous inertial sensor data using harmonic frequencies",
        "Smartphone-based blood pressure measurement using transdermal optical imaging technology",
        "Contact-free screening of atrial fibrillation by a smartphone using facial pulsatile photoplethysmographic signals",
        "Modeling quasi-periodic signals by a non-parametric model: Application on fetal ecg extraction",
        "Modelling quasi-periodic signals in geodetic time-series using Gaussian processes",
        "Quasi-periodic atrial activity components in the ecg used to discriminate between paroxysmal and chronic atrial fibrillation",
        "Quasiperiodicity and chaos in cardiac fibrillation",
        "mixup: Beyond empirical risk minimization",
        "beta-vae: Learning basic visual concepts with a constrained variational framework",
        "Optimal positive generation via latent transformation for contrastive learning",
        "Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine",
        "Smart devices are different: Assessing and mitigating mobile sensing heterogeneities for activity recognition",
        "Usc-had: A daily activity dataset for ubiquitous activity recognition using wearable sensors",
        "Latent independent excitation for generalizable sensor-based cross-person activity recognition",
        "Troika: A general framework for heart rate monitoring using wrist-type photoplethysmographic signals during intensive physical exercise",
        "Deep ppg: Large-scale heart rate estimation with convolutional neural networks",
        "An open access database for evaluating the algorithms of electrocardiogram rhythm and morphology abnormality detection",
        "A 12-lead electrocardiogram database for arrhythmia research covering more than 10,000 patients",
        "Classification of 12-lead ecgs: the physionet/computing in cardiology challenge 2020",
        "On adversarial mixup resynthesis",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features",
        "A fourier-based framework for domain generalization",
        "Specmix : A mixed sample data augmentation method for training withtime-frequency domain features",
        "Un-mix: Rethinking image mixtures for unsupervised visual representation learning",
        "What makes for good views for contrastive learning?",
        "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations",
        "Improving contrastive learning by visualizing feature transformation",
        "Generative models as a data source for multiview representation learning",
        "Towards diverse and coherent augmentation for time-series forecasting",
        "Identity-disentangled adversarial augmentation for self-supervised learning",
        "Cornet: Deep learning framework for ppg-based heart rate estimation and biometric identification in ambulant environment",
        "Deepsleepnet: a model for automatic sleep stage scoring based on raw single-channel eeg",
        "Multi-source deep domain adaptation with weak supervision for time-series sensor data",
        "Clocs: Contrastive learning of cardiac signals across space, time, and patients",
        "Contrastive heartbeats: Contrastive learning for self-supervised ecg representation and phenotyping",
        "Bootstrap your own latent a new approach to self-supervised learning",
        "Momentum contrast for unsupervised visual representation learning",
        "Boosting contrastive self-supervised learning with false negative cancellation",
        "A theoretical analysis of contrastive unsupervised representation learning",
        "Working hard to know your neighbor's margins: Local descriptor learning loss",
        "Hard negative examples are hard, but useful",
        "Robust contrastive learning using negative samples with diminished semantics",
        "Parametric instance classification for unsupervised visual feature learning",
        "Debiased contrastive learning",
        "Mining on manifolds: Metric learning without labels",
        "On mutual information in contrastive learning for visual representations",
        "Facenet: A unified embedding for face recognition and clustering",
        "Hard negative mixing for contrastive learning",
        "Contrastive learning with adversarial examples",
        "Caco: Both positive and negative samples are directly learnable via cooperative-adversarial contrastive learning",
        "Mixco: Mix-up contrastive learning for visual representation",
        "i-mix: A domain-agnostic strategy for contrastive representation learning",
        "Simper: Simple self-supervised learning of periodic targets",
        "Self-supervised pre-training for time series classification",
        "Progressive mix-up for few-shot supervised multi-source domain transfer",
        "Co-mixup: Saliency guided joint mixup with supermodular diversity",
        "Manifold mixup: Better representations by interpolating hidden states",
        "MODALS: Modality-agnostic automated data augmentation in the latent space",
        "Binary cornet: Accelerator for hr estimation from wrist-ppg",
        "Real-time robust heart rate estimation from wrist-type ppg signals using multiple reference adaptive noise cancellation",
        "Large scale adversarial representation learning",
        "Isolating sources of disentanglement in variational autoencoders"
      ],
      "meta_data": {
        "arxiv_id": "2309.13439v2",
        "authors": [
          "Berken Utku Demirel",
          "Christian Holz"
        ],
        "published_date": "2023-09-23T17:42:13Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces a domain-agnostic data-augmentation strategy, called \"tailored mixup\", for self-supervised contrastive learning on quasi-periodic, non-stationary time-series. The method: (1) decomposes signals into frequency-domain magnitude and phase, performing separate, information-preserving interpolation; (2) controls the mixup strength for each pair through coefficients drawn according to their similarity in an unsupervised β-VAE latent space; and (3) provides theoretical guarantees that the augmentation increases a lower bound on mutual information, avoiding the destructive interference suffered by conventional mixup. Extensive experiments on three representative tasks show consistent improvements over state-of-the-art augmentation and optimal-sample generation baselines.",
        "methodology": "1. Signal decomposition: Apply Fourier transform; treat magnitude A(x) and phase P(x) as independent features.\n2. Magnitude mixing: Linear interpolate magnitudes with coefficient λ_A.\n3. Phase mixing: Shift anchor phase toward the other sample by shortest angular distance |Δθ| scaled by (1−λ_P), preventing cancellation.\n4. Coefficient sampling: For a randomly chosen pair, encode both signals with an unsupervised β-VAE. If latent cosine distance ≤ε, draw λ_A,λ_P ~U(β,1) (aggressive). Otherwise, draw from truncated Normal with high mean (conservative).\n5. Inverse FFT produces augmented sample; used as positive pair in SimCLR-like contrastive loss.\n6. Theoretical analysis proves lower-bounded mutual information (λ I(y;x)).",
        "experimental_setup": "Pre-training: SimCLR framework (also BYOL & TS-TCC ablations); DeepConvLSTM encoder, batch 256, 120 epochs, Adam lr 0.003 with cosine decay.\nDatasets (8 total):\n• Activity recognition – UCIHAR, HHAR, USC (IMU, 50/100 Hz, leave-one-person-out; metrics: Accuracy, Macro-F1).\n• Heart-rate estimation – IEEE SPC15 (SPC12 & SPC22) and DaLia (PPG, 25 Hz segments; metrics: MAE, RMSE).\n• Cardiovascular disease classification – CPSC2018, Chapman (12-lead ECG down-sampled to 100 Hz; metric: AUC, patient-wise split).\nEvaluation: linear probe on small labeled subset; compare against classic augmentations, six mixup variants, and nine optimal/ hard-sample generators (InfoMin, NNCLR, PosET, GenRep, AugBank, STAug, DACL, IDAA, Traditional). Report mean±std over 3 seeds.",
        "limitations": "• Relies on quasi-periodicity and informative frequency bands; may underperform on highly aperiodic or event-driven sequences.\n• Requires accurate FFT and β-VAE training; latent similarity may be unreliable on small or many-class datasets, influencing coefficient choice.\n• Several hyper-parameters (β, ε, λ distributions) tuned per task; generalisation without tuning untested.\n• Extra computational cost from Fourier transforms and VAE.\n• Experiments limited to wearable/physiological data; no evidence on domains such as speech, finance, or multivariate irregularly sampled data.",
        "future_research_directions": "1. Develop adaptive or learnable coefficient selection that removes dependence on VAE or manual thresholds.\n2. Extend the augmentation to broader time-series types, including non-quasi-periodic, irregular, or high-dimensional sensor networks.\n3. Combine tailored mixup with negative-sample generation or adversarial augmentation for further representation enrichment.\n4. Investigate robustness to noise, artifacts, and distribution shifts, and integrate with online/streaming learning settings.\n5. Automate hyper-parameter tuning and explore theoretical bounds for multi-class, large-class-count scenarios.\n6. Evaluate scalability and efficiency, including on-device implementation for real-time applications.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "A Novel Method to Solve Neural Knapsack Problems",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "A Novel Sequential Coreset Method for Gradient Descent Algorithms",
      "full_text": "arXiv:2112.02504v3  [cs.LG]  8 Oct 2022 A Novel Sequential Coreset Method for Gradient Descent Algorithms Jiawei Huang1, Ruomin Huang2, Wenjie Liu1, Nikolaos M. Freris1 and Hu Ding∗ 1 1School of Computer Science and Technology 2School of Data Science University of Science and Technology of China {hjw0330, hrm, lwj1217}@mail.ustc.edu.cn, {nfr,huding}@ustc.edu.cn Abstract A wide range of optimization problems arising in machine lea rning can be solved by gradient descent algorithms, and a central ques tion in this area is how to eﬃciently compress a large-scale dataset so as to reduce the computational complexity. Coreset is a popular data compression tech- nique that has been extensively studied before. However, mo st of existing coreset methods are problem-dependent and cannot be used as a general tool for a broader range of applications. A key obstacle is th at they of- ten rely on the pseudo-dimension and total sensitivity boun d that can be very high or hard to obtain. In this paper, based on the “local ity” prop- erty of gradient descent algorithms, we propose a new framew ork, termed “sequential coreset”, which eﬀectively avoids these obsta cles. Moreover, our method is particularly suitable for sparse optimizatio n whence the coreset size can be further reduced to be only poly-logarith mically depen- dent on the dimension. In practice, the experimental result s suggest that our method can save a large amount of running time compared wi th the baseline algorithms. 1 Introduction Coreset [21] is a popular technique for compressing large-scale datasets so as to speed up existing algorithms. Especially for the optimization proble ms aris- ing in machine learning, coresets have been extensively studied in rec ent years. Roughly speaking, given a large dataset P and a speciﬁed optimization objective (e.g., k-means clustering), the coreset approach is to construct a new d ataset ˜P ∗ Corresponding author. 1with the size | ˜P | ≪ | P |, such that any solution obtained over ˜P will approxi- mately preserve the same quality over the original set P ; that is, we can replace P by ˜P when running an available algorithm for solving this optimization prob- lem. Because | ˜P | ≪ | P |, the runtime can be signiﬁcantly reduced. In this paper, we consider Empirical Risk Minimization (ERM) problems which capture a broad range of applications in machine learning [51]. L et X and Y be the data space and response space, respectively. Given an inpu t training set P = {(x1, y1), (x2, y2), · · · , (xn, yn)}, where each xi ∈ X and each yi ∈ Y, the objective is to learn the hypothesis β (from the hypothesis space Rd) so as to minimize the empirical risk F (β) = 1 n n∑ i=1 f(β, xi, yi), (1) where f(·, ·, ·) is the non-negative real-valued loss function . In practice, the data size n can be very large, thus it is instrumental to consider data compres sion methods (like coresets) to reduce the computational complexity. Let ǫ ∈ [0, 1]. A standard ǫ-coreset is represented as a vector W = [w1, w2, · · · , wn] ∈ Rn with the property that the function ˜F (β) = 1∑ n i=1 wi ∑ n i=1 wif(β, xi, yi) must satisfy ˜F (β) ∈ (1 ± ǫ)F (β), ∀β ∈ Rd. (2) The number of non-zero entries ∥W ∥0 is the coreset size, and thus the goal of compression is to have W to be as sparse as possible. Suppose we have an algorithm that can achieve c-approximation for the ERM problem ( c ≥ 1). Then, we can run the same algorithm on the ǫ-coreset, and let ˆβ be the re- turned c-approximation, i.e., ˜F ( ˆβ) ≤ c · minβ∈Rd ˜F (β). It holds that F ( ˆβ) ≤ c · 1+ǫ 1−ǫ minβ∈Rd F (β), thus the approximation ratio of ˆβ for the original objective function F (·) is only slightly worse than c when ǫ is suﬃciently small. A large part of coreset methods are based on the “sensitivity” idea [29]. First, it computes a constant factor approximation with respect t o the objective function (1); then it estimates the sensitivity σi for each data item ( xi, yi) based on the obtained constant factor approximation; ﬁnally, it takes a r andom sample (as the coreset) over the input set P , where each data item ( xi, yi) is selected with probability proportional to its sensitivity σi, and the total sample size depends on the total sensitivity bound ∑ n i=1 σi along with the “pseudo-dimension” of the objective function [22, 31]. This sensitivity-based coreset framew ork has been successfully applied to solve problems such as k-means clustering and projective clustering [22]. However, there are several obstacles when trying to apply this approach to general ERM problems. For instance, it is not easy to o btain a constant factor approximation; moreover, diﬀerent from cluste ring problems, it is usually challenging to achieve a reasonably low total sensitivity boun d and compute the pseudo-dimension for many practical ERM problems. F or example, the coreset size can be as large as ˜Ω( d2√ n/ǫ2) for logistic regression [50] with O(nd2) construction time. 2Another common class of coreset construction methods is based o n “greedy selection” [14, 36]. The greedy selection procedure is quite similar to t he k-center clustering algorithm [23] and the greedy submodular set cover algorithm [53]. Intuitively, the method greedily selects a subset of the input trainin g set, i.e., the coreset, which are expected to be as diverse as possible; cons equently, the whole training set can be covered by small balls centered at the selec ted subset. Nonetheless, this approach also suﬀers from several drawbacks . First, it is diﬃcult to bound the size of the obtained coreset, when specifying t he error bound induced by the coreset ( e.g., one may need too many balls to cover the training set if their radii are required to be no larger than an upp er bound). Second, the time complexity can be too high, e.g., the greedy k-center clustering procedure usually needs to read the input training set for a large nu mber of passes, and the greedy submodular set cover algorithm usually nee ds a large number of function evaluations. 1.1 Our Contributions The aforementioned issues seriously limit the applications of coreset s in practice. In this paper, we propose a novel and easy-to-implement coreset framework, termed sequential coreset , for the general ERM problem (1). Our idea comes from a simple observation. For many ERM problems, either convex or non- convex, gradient descent algorithms are commonly invoked. In par ticular, these gradient descent algorithms usually share the following locality property : Since the learning rate of a gradient descent algorithm is us ually re- stricted by an upper bound, the trajectory of the hypothesis β in (1) is likely to be “smooth” (except for the ﬁrst few rounds). That is, the cha nge of β should be “small” between successive rounds. This allows to focus, in each round, on a local region rather than the whole hypothesis space Rd. We can thus visualize the trajectory to be decomposed into a sequence of “segments”, where each segment is bounded by an individual ball. See Figure 1 for an illustration. When the trajectory enters a n ew ball (i.e., a new local region), we construct a coreset W = [ w1, w2, · · · , wn] with ˜F (β) ∈ (1 ± ǫ)F (β), ∀β ∈ the current local region . (3) The formal deﬁnition of such a “local” coreset is shown in Section 2. W hen the trajectory approaches the boundary of the ball, we update the c oreset for the next ball. Therefore, we call the method “sequential coreset”. Although building the coreset for a local region is easier than that fo r the global hypothesis space, there remain several technical challeng es to resolve. Partly inspired by the layered sampling idea of [11, 18], we can achieve a coreset of (3) where the coreset size depends on the range of the local re gion. In partic- ular, our method enjoys several signiﬁcant advantages compare d with previous coreset methods: • Our method is not problem-dependent and can be applied to any (con vex or non-convex) ERM problem that uses gradient descent, under s ome mild 3β0β0 β2β2 β4β4 β5β5 β3β3 β1β1 Figure 1: Suppose the trajectory starts from β0, and we construct a “local” coreset within the ball centered at β0; when the trajectory approaches the ball’s boundary, i.e., β1, we update the coreset. Similarly, we update the coreset at β2, β3, and β4, until suﬃciently approximating the stationary point β5. We can view β1, β2, β3, and β4 as a sequence of “anchors”. assumptions. In fact, our method can be extended to apply to oth er iterative algorithms beyond gradient descent, such as subgradien t descent and expectation maximization, as long as they satisfy the locality pro perty. • Our method can avoid to compute the total sensitivity bound and ps eudo- dimension, thus it does not incur any complicated computations ( e.g., SVD) and has only linear construction time. • For special cases of practical interest such as sparse optimizatio n, the core- set size can be further reduced to be only poly-logarithmically depen dent on the dimension. 1.2 Related Works Gradient descent. Given a diﬀerentiable objective function, gradient descent is arguably the most common ﬁrst-order iterative optimization algor ithm for ﬁnding the optimal solution [16]. A number of ERM models can be solved via gradient descent methods, such as Ridge regression [49] and Lo gistic regres- sion [15]. Note that though the objective function of the Lasso reg ression [48] is not diﬀerentiable, several natural generalizations of the tradit ional gradient descent method, such as subgradient methods [4] and proximal gr adient meth- ods [38, 3], have been developed and shown to perform well in practic e. Several extensions of gradient descent have been also widely stud ied in recent years. For example, Nesterov introduced the acceleration techn ique for achieving faster gradient method [40]. In view of the rapid development of deep learning and many other ma- chine learning applications, stochastic gradient descent method an d variants have played a central role in the ﬁeld of large-scale machine learning, due to their scalability to very large, possibly distributed datasets [7, 28, 2 0]. 4Coresets. Compared with other data compression approaches, an obvious advantage of coreset is that it is to be selected from the original inp ut; that is, the obtained coreset can well preserve some favorable proper ties, such as sparsity and interpretability, in the input domain. In the past years , coreset techniques have been widely applied to many optimization problems, su ch as: clustering [11, 22, 25], logistic regression [26, 39, 45, 50, 45], Bayes ian meth- ods [9, 8], linear regression [17, 19, 12, 27, 50], robust optimization [ 18], Gaussian mixture model [32], and active learning [14, 46]. Recently, [34] also pro posed the notion of “accurate” coresets, which do not introduce any ap proximation error when compressing the input dataset. Coresets are also app lied to speed up large-scale or distributed machine learning algorithms [44, 36, 37, 6]. Very recently, [43] also considered the “local” heuristic for corese ts. How- ever, their results are quite diﬀerent from ours. Their method still relies on the problem-dependent pseudo-dimension and the sensitivities; mo reover, their method requires the objective function to be strongly convex. Sketch. Another widely used data summarization method is “sketch” [42]. Diﬀerent from coresets, sketch does not require to generate th e summary from the original input. The sketch technique is particularly popular for s olving linear regression problems (with and without regularization) [1, 13]. 2 Preliminaries Given an instance of the ERM problem (1), we assume that the loss fu nction is Lipschitz smooth. This is a quite common assumption for analyzing ma ny gradient descent methods [52]. Assumption 1 (Lipschitz Smoothness) There exists a real constant L > 0, such that for any 1 ≤ i ≤ n and any β1, β2 in the hypothesis space, we have ∥∇f(β1, xi, yi) − ∇f(β2, xi, yi)∥ ≤ L∥β1 − β2∥, (4) where ∥ · ∥ is the Euclidean norm in the space. For simplicity, we just use ball to deﬁne the local region for constru cting our coreset as (3). Suppose we have the “anchor” βanc ∈ Rd and region range R ≥ 0. Let B(βanc, R) denote the ball centered at βanc with radius R. Below, we provide the formal deﬁnition for the “local” coreset in (3). Deﬁnition 1 (Local ǫ-Coreset) Let P = {(x1, y1), (x2, y2),· · · , (xn, yn)} be an input dataset of the ERM problem (1). Suppose ǫ ∈ (0, 1). Given βanc ∈ Rd and R ≥ 0, the local ǫ-coreset, denoted CS ǫ(βanc, R), is a vector W = [w1, w2, · · · , wn] satisfying that ˜F (β) ∈ (1 ± ǫ)F (β), ∀β ∈ B(βanc, R), (5) where ˜F (β) = 1 ∑ n i=1 wi ∑ n i=1 wif(β, xi, yi). The number of non-zero entries of W is the size of CS ǫ(βanc, R). 5Algorithm 1 Local ǫ-Coreset Construction Input: A training dataset P = {(x1, y1), (x2, y2), · · · , (xn, yn)}, the Lipschitz constant L as described in Assumption 1, βanc ∈ Rd, and the parameters R ≥ 0 and ǫ ∈ (0, 1). 1. Let N = ⌈log n⌉ and H = F (βanc); initialize W = [0 , 0, · · · , 0] ∈ Rn. 2. The set P is partitioned into N + 1 layers {P0, . . . , P N } as in (6) and (7). 3. For each Pj ̸= ∅, 0 ≤ j ≤ N: (a) take a random sample Qj from Pj uniformly at random, where the size |Qj | depends on the parameters ǫ, R, and L (the exact value will be discussed in our following analysis in Section 3.1); (b) for each sampled data item ( xi, yi) ∈ Qj , assign the weight wi = |Pj | |Qj | ; Output: the weight vector W = [ w1, w2, · · · , wn] as the coreset. 3 Local ǫ-Coreset Construction We ﬁrst present the construction algorithm for local ǫ-coreset, and expose the detailed analysis on its quality in Section 3.1. Besides the quality guaran tee of (5), in Section 3.2 we show that our coreset can approximately pr eserve the gradient ∇F (β), which is an important property for gradient descent algorithms. In Section 3.3, we discuss some extensions beyond gradient descen t. Relying on the local ǫ-coreset, we propose the sequential coreset framework and co nsider several important applications in Section 4. Coreset construction. Let N = ⌈log n⌉ (the basis of the logarithm is 2 in this paper). Given the central point βanc ∈ Rd and the local region range ( i.e., the radius) R ≥ 0, we set H = F (βanc) and then partition the input dataset P = {(x1, y1), (x2, y2), · · · , (xn, yn)} into N + 1 layers: P0 = { (xi, yi) ∈ P | f(βanc, xi, yi) ≤ H } , (6) Pj = { (xi, yi) ∈ P | 2j−1H < f (βanc, xi, yi) ≤ 2j H } , 1 ≤ j ≤ N. (7) It is easy to see that P = ∪N j=0Pj , since f(βanc, xi, yi) is always no larger than 2N H for any 1 ≤ i ≤ n. For each 0 ≤ j ≤ N, if Pj ̸= ∅, we take a random sample Qj from Pj uniformly at random, where the size |Qj | will be determined in our following analysis (in Section 3.1); for each sampled data item ( xi, yi) ∈ Qj , we assign the weight wi to be |Pj | |Qj | ; for all the data items of Pj \\ Qj, we let their weights to be 0. At the end, we obtain the weight vector W = [ w1, w2, · · · , wn] as 6our coreset, and consequently ˜F (β) = 1 n ∑ n i=1 wif(β, xi, yi) (it is easy to verify∑ n i=1 wi = n from our construction). The construction procedure is shown in Algorithm 1. Remark 1 Our layered sampling procedure in Algorithm 1 is similar to t he coreset construction idea of [11, 18], which was originally designed for the k- median/means clustering problems. Compared with the sensi tivity based coreset construction idea [29], a signiﬁcant advantage of our metho d is that there is no need to compute the total sensitivity bound and pseudo-dime nsion. These values are problem-dependent and, for some objectives, they can be very high or hard to obtain [39, 50]. 3.1 Theoretical Analysis In this section, we prove the quality guarantee and complexity of th e coreset returned from Algorithm 1. We deﬁne two values before presenting our theorem, M := max 1≤i≤n ∥∇f(βanc, xi, yi)∥ and m := min β∈B(βanc,R) F (β). Theorem 1 With problability 1 − 1 n , Algorithm 1 returns a qualiﬁed coreset CS ǫ(βanc, R) with size ˜O ( ( H+MR+LR2 m ) 2 · d ǫ2 ) 1. Furthermore, when the vec- tor β is restricted to have at most k ∈ Z+ non-zero entries in the hypothesis space Rd, the coreset size can be reduced to be ˜O ( ( H+MR+LR2 m ) 2 · k log d ǫ2 ) . The runtime of Algorithm 1 is O(n·tf ), where tf is the time complexity for computing the loss f(β, x, y ). Remark 2 From Theorem 3 we can see that the coreset size depends on the initial vector βanc and the local region range R. Also note that the value m is non-increasing with R. First, the linear time complexity of Algorithm 1 is easy to see: to obtain the partition and the samples, it just needs to compute f(βanc, xi, yi) for 1 ≤ i ≤ n. Below, we focus on proving the quality guarantee and coreset size. For the sake of simplicity, we use fi(β) to denote f(β, xi, yi) in our analysis. By using Taylor expansion and Assumption 1, we directly have fi(β) ∈ fi(βanc) ± ( ∥∇fi(βanc)∥ R + L 2 ) R2. (8) for any β ∈ B(βanc, R) and 1 ≤ i ≤ n. Then we have the following lemma. Lemma 1 We ﬁx a vector β ∈ B(βanc, R) and an index j from {0, 1, . . . , N }. Given any two numbers λ ∈ (0, 1) and δ > 0, if we set the sample size in Step 1 ˜O(g) := O ( g · polylog ( nHMR ǫm )) . 73(a) of Algorithm 1 to be |Qj | = O ( (2j−1H + MR + LR2)2δ−2 log 1 λ ) , (9) we have Prob   ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ 1 |Qj | ∑ (xi,yi)∈Qj fi(β) − 1 |Pj | ∑ (xi,yi)∈Pj fi(β) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ≥ δ  ≤λ. Proof. For a ﬁxed 1 ≤ j ≤ N, we view fi(β) as an independent random variable for each ( xi, yi) ∈ Pj . Through the partition construction (6) and (7), and the bounds (8), we have fi(β) ≥ 2j−1H − MR − 1 2LR2; fi(β) ≤ 2jH + MR + 1 2LR2.      (10) Let the sample size |Qj| = ⌈1 2 (2j−1H + LR2 + 2MR )2δ−2ln 2 λ ⌉. Through the Hoeﬀding’s inequality [24], we know that Prob   ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ 1 |Qj | ∑ (xi,yi)∈Qj fi(β) − 1 |Pj | ∑ (xi,yi)∈Pj fi(β) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ≥ δ   is no larger than 2 e − 2|Qj |δ 2 (2j−1 H+LR2+2MR)2 ≤ λ. Now we consider the case j = 0. For any data item ( xi, yi) ∈ P0, we have 0 ≤ fi(β) ≤ H + MR + 1 2 LR2. If letting the sample size |Q0| = ⌈1 2 (H + 1 2 LR2 + MR )2δ−2ln 2 λ ⌉, it is easy to verify that the same probability bound also holds. □ After proving Lemma 1, we further show ˜F (β) ≈ F (β) for any ﬁxed β ∈ B(βanc, R). Lemma 2 Suppose ǫ1 ≥ 0. In Lemma 1, if we set δ = ǫ12j−1H for j = 0, 1, · · · , N , then for any ﬁxed β ∈ B(βanc, R), ⏐ ⏐ ⏐˜F (β) − F (β) ⏐ ⏐ ⏐≤ 3 2ǫ1F (βanc) (11) holds with probability at least 1 − (N + 1)λ. Proof. From Lemma 1, it holds that the probability that ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ |Pj | |Qj | ∑ (xi,yi)∈Qj fi(β) − ∑ (xi,yi)∈Pj fi(β) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ≥ | Pj | · ǫ12j−1H (12) 8is at most λ. Recall ˜F (β) = 1 n ∑ n i=1 wif(β, xi, yi), where for each ( xi, yi) ∈ Pj , wi = |Pj | |Qj | if ( xi, yi) ∈ Qj , and wi = 0 if ( xi, yi) ∈ Pj \\ Qj. Thus, by taking the union bound of (12) over 0 ≤ j ≤ N, we have n ⏐ ⏐ ⏐˜F (β) − F (β) ⏐ ⏐ ⏐ = ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ N∑ j=0 |Pj | |Qj| ∑ (xi,yi)∈Qj fi(β) − N∑ j=0 ∑ (xi,yi)∈Pj fi(β) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ≤ N∑ j=0 ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ |Pj | |Qj| ∑ (xi,yi)∈Qj fi(β) − ∑ (xi,yi)∈Pj fi(β) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ≤ N∑ j=0 |Pj |ǫ12j−1H (13) with probability at least (1 − λ)N+1 > 1 − (N + 1)λ. To complete the proof, we also need the following claim. Claim 1 ∑ N j=0 |Pj |2j ≤ 3n. Proof. By the deﬁnition of Pj , we have 2j H = H, if j = 0; 2j H ≤ 2fi(βanc), ∀(xi, yi) ∈ Pj , if j ≥ 1. (14) Therefore, 2 j H is always no larger than 2 fi(βanc) + H for any 0 ≤ j ≤ N and any ( xi, yi) ∈ Pj . Overall, N∑ j=0 |Pj |2jH = N∑ j=0 ∑ (xi,yi)∈Pj 2jH ≤ N∑ j=0 ∑ (xi,yi)∈Pj (2fi(βanc) + H) = 2 nF (βanc) + nH = 3 nH. (15) Thus the claim ∑ N j=0 |Pj |2j ≤ 3n is true. □ By using Claim 1, (13) can be rewritten as n ⏐ ⏐˜F (β) − F (β) ⏐ ⏐≤ 3 2 ǫ1nF (βanc). (16) So we complete the proof. □ To prove ˜F (β) is a qualiﬁed coreset, we need to extend Lemma 2 to any β ∈ B(βanc, R). 9For this purpose, we discretize the region B(βanc, R) ﬁrst (the discretization is only used for our analysis, and we do not need to build the grid in realit y). Imagine that we build a uniform grid inside B(βanc, R) with the side length being equal to ǫ2R√ d , where the exact value of ǫ2 is to be determined later. Inside each grid cell of B(βanc, R), we pick an arbitrary point as its representative point and let G be the set consisting of all the representative points. Based on th e formula of the volume of a ball in Rd, we have |G| = ( O ( 2√πe ǫ2 )) d . (17) So we can simply increase the sample size of Lemma 2, and take the unio n bound over all β ∈ G so as to extend the result as follows. Lemma 3 Suppose ǫ1 ≥ 0. In the sample size (9) of Lemma 1, we set δ = ǫ12j−1H for j = 0 , 1, · · · , N , respectively, and replace λ by λ (N+1)|G| . The fol- lowing ⏐ ⏐˜F (β) − F (β) ⏐ ⏐≤ 3 2ǫ1F (βanc) (18) holds for any β ∈ G, with probability at least 1 − λ. Following Lemma 3, we further derive a uniform bound over all β ∈ B(βanc, R) (not just in G). For any β ∈ B(βanc, R), we let β′ ∈ G be the representative point of the cell containing β. Then we have ∥β − β′∥ ≤ ǫ2R. We deﬁne M′ := max 1≤i≤n max β∈B(βanc,R) ∥∇f(β, xi, yi)∥. By Assumption 1 we immediately know M′ ≤ M + LR. By using the similar manner of (8), for any 1 ≤ i ≤ n we have ⏐ ⏐fi(β) − fi(β′) ⏐ ⏐≤ ǫ2M′R + 1 2 Lǫ2 2R2. (19) This implies both |F (β) − F (β′)| and | ˜F (β) − ˜F (β′)| ≤ ǫ2M′R + 1 2 Lǫ2 2R2. (20) Using triangle inequality, we obtain | ˜F (β) − F (β)| ≤ | ˜F (β) − ˜F (β′)| + | ˜F (β′) − F (β′)| +|F (β′) − F (β)| ≤ 3 2ǫ1F (βanc) + 2 × (ǫ2M′R + 1 2 Lǫ2 2R2), (21) where the last inequality follows from Lemma 3 (note β′ ∈ G) and (20). By letting ǫ1 = 2mǫ 7F (βanc) and ǫ2 = 2ǫ1F (βanc) R ( √ M′2 +2Lǫ1F (βanc)+M′ ) , we have | ˜F (β)−F (β)| ≤ 10ǫF (β) via simple calculations. That is, the returned vector W = [ w1, w2, · · · , wn] is a qualiﬁed coreset CS ǫ(βanc, R). Last, it remains to specify the obtained coreset size. To guarante e the success probability to be at least 1 − 1/n, we set λ = 1 /n. Then we can compute the coreset size, i.e., the number of non-zero entries of W , which equals N∑ j=0 |Qj| = ˜O ( ( H + MR + LR2 m ) 2 · d ǫ2 ) (22) (by combining (9), with the selection of δ in Lemma 2, the choice of λ in Lemma 3 along with (17), and the deﬁnition of ǫ1). For the case that β is restricted to have at most k non-zero entries ( i.e., sparse optimization) , we revisit the size |G| in (17). For a d-dimensional vec- tor, there are ( d k ) diﬀerent combinations for the positions of the k non-zero entries. Thus β can be only located in the union of ( d k ) k-dimensional sub- spaces (similar idea was also used for analyzing compressed sensing [ 2]). In other words, we just need to build the grid (only for the sake of ana lysis) in the union of ( d k ) k-dimensional balls instead of the whole B(βanc, R). Consequently, the new size |G| is ( ( d k ) · ( O ( 2√πe ǫ2 )) k) , and the coreset size is reduced to ˜O ( ( H+MR+LR2 m ) 2 · k log d ǫ2 ) . 3.2 Gradient Preservation Besides the quality guarantee (5), our local coreset also enjoys a nother favorable property. In this section, we show that the gradient ∇ ˜F (β) can be approximately preserved as well, i.e., ∇ ˜F (β) ≈ ∇ F (β) for any β ∈ B(βanc, R). Because the trajectory of β is guided by the gradients, this property gives a hint that our eventually obtained β is likely to be close to the optimal hypothesis β∗ (we also validate this property in our experiments). In some scenarios like st atistical inference and parameter estimation, we expect to achieve not only an almost minimal loss F (β), but also a small diﬀerence between β and β∗. Given a vector v ∈ Rd, we use v[l] to denote its l-th coordinate value, for l = 1 , 2, . . . , d . Under Assumption 1, we obtain (similar with (8)), for any 1 ≤ i ≤ n, ∇fi(β)[l] ∈ ∇ fi(βanc)[l] ± LR. (23) We can apply a similar line of analysis as in Section 3.1 to obtain Theorem 2. We need the following modiﬁcations. First, we need to change the sam ple size Qj (and similarly the total coreset size in (22)) of Algorithm 1 because w e now consider a diﬀerent objective. Also, we achieve an additive erro r for the gradient, instead of the (1 ± ǫ)-multiplicative error as (5). The reason is that the gradient can be almost equal to 0, if the solution approaches to a local or global optimum (but the objective value (1) is usually not equal to 0, e.g., we often add a non-zero penalty item to the objective function). 11Theorem 2 Let σ > 0 be any given small number. With probability 1 − 1 n , Algorithm 1 can return a vector W with ˜O ( (LR+M)2 σ2 · d ) non-zero entries, such that for any β ∈ B(βanc, R) and 1 ≤ l ≤ d, ∇ ˜F (β)[l] ∈ ∇ F (β)[l] ± σ. (24) Furthermore, if the vector β is restricted to have at most k ∈ Z+ non-zero entries in the hypothesis space Rd, the number of non-zero entries of W can be reduced to be ˜O ( (LR+M)2 σ2 · k log d ) . Remark 3 If we want to guarantee both Theorem 3 and 2, we can just set the coreset size as the maximum over both cases. 3.3 Beyond Gradient Descent In Section 3.1, our analysis relied on the fact that the function f(β, xi, yi) is diﬀerentiable. However, for some ERM problems, the loss function c an be non- diﬀerentiable. A representative example is the l1-norm regularized regression, such as [48, 30]. We consider the lp regularized regression with 0 < p ≤ 2. Given a regularization parameter λ > 0, the objective function can be written as F (β) = 1 n n∑ i=1 g(β, xi, yi) + λ∥β∥p, (25) where the function g(β, xi, yi) is assumed to be diﬀerentiable and satisfy Assump- tion 1. We can easily cast (25) to have the form of (1) by setting f(β, xi, yi) = g(β, xi, yi) + λ∥β∥p. Noting that a local ǫ-coreset of the original problem obvi- ously is also a local ǫ-coreset of (25), a coreset algorithm querying the values of all gis is capable to construct the coreset of (25). Actually an ǫ-coreset of (25) can be constructed with only access to the values of all fis as well (See details in Appendix B). 4 Sequential Coreset Framework and Applica- tions The local ǫ-coreset constructed in Section 3 can be directly used for compre ssing input data. However, the trajectory of the hypothesis β (although enjoying the locality property) may span a relatively large range globally in the spac e. As discussed in Remark 2, the coreset size depends on the pre-speciﬁ ed local region range. Therefore, the coreset size can be high, if we want to build in one shot a local coreset that covers the whole trajectory. This motivates u s to propose the sequential coreset framework (see Algorithm 2). In each round of Algorithm 2, we build the local coreset CS ǫ(βt, R) and run the “host” algorithm A on it until either (i) the result becomes stable inside 12Algorithm 2 Sequential Coreset Framework Input: An instance P = {(x1, y1), (x2, y2), · · · , (xn, yn)} of the ERM prob- lem (1) with the initial solution β0 and range R > 0, an available gradient descent algorithm A as the “host”, and the parameter ǫ ∈ (0, 1). 1. For t = 0 , 1, . . ., build the local coreset CS ǫ(βt, R) and run the host algorithm A on it until: (a) if the result becomes stable inside B(βt, R), terminate the loop and return the current β; (b) else, the current β reaches the boundary of B(βt, R), and then set βt+1 = β and t = t + 1. B(βt, R) or (ii) the hypothesis β reaches the boundary of B(βt, R)2. For (i), we just terminate the algorithm and output the result; for (ii), we u pdate βt and proceed the next iteration. Following the sequential coreset f ramework, we consider its applications for several ERM problems in machine learning . Ridge regression. In the original linear regression problem, the data space X = Rd and the response space Y = R, and the goal is to ﬁnd a vector β ∈ Rd such that the objective function F (β) = 1 n ∑ n i=1 |⟨xi, β⟩ − yi|2 is minimized. For Ridge regression [49], we add a squared l2-norm penalty and the objective function becomes F (β) = 1 n n∑ i=1 |⟨xi, β⟩ − yi|2 + λ∥β∥2 2, (26) where λ > 0 is a regularization parameter. Consequently, the loss function f(β, xi, yi) of (26) is taken as |⟨xi, β⟩ − yi|2 + λ∥β∥2 2. Lasso regression. Another popular regularized regression model is Lasso [48]. Compared to (26), the only diﬀerence is that we use an l1-norm penalty i.e., F (β) = 1 n n∑ i=1 |⟨xi, β⟩ − yi|2 + λ∥β∥1, (27) where λ > 0 is a regularization parameter. The loss function f(β, xi, yi) of (27) is |⟨xi, β⟩ − yi|2 + λ∥β∥1. A key advantage of Lasso is that the returned β is a sparse vector. The objective function (27) is not diﬀerentiable, b ut it can still be solved by our sequential coreset framework as discussed in Sec tion 3.3. Logistic regression. For Logistic regression, the response is binary, i.e., 2In practice, we can set a small number σ ∈ (0, 1) and deduce that the boundary is reached when ∥βt − β ∥ > (1 − σ )R. 13yi = 0 or 1 [15]. The objective function F (β) = − 1 n n∑ i=1 { yi log g(⟨xi, β⟩) + (1 − yi) log ( 1 − g(⟨xi, β⟩) ) } , (28) where g(t) := 1 1+e−t (the logistic function). We may add an l1 or l2-norm penalty to (28), in the same way as (26) and (27). The loss function f(β, xi, yi) for Logistic regression is −yi log g(⟨xi, β⟩) − (1 − yi) log ( 1 − g(⟨xi, β⟩) ) . Gaussian Mixture Model (GMM). As emphasized before, our local core- set method does not require the objective function to be convex. Here, we consider a typical non-convex example: GMM training [5]. A mixture of k Gaussian kernels is represented with β := [( ω1, µ1, Σ 1), . . . , (ωk, µk, Σ k)], where ω1, . . . , ω k ≥ 0, ∑ k j=1 ωj = 1, and each ( µj , Σ j ) is the mean and covariance matrix of the j-th Gaussian in RD. GMM is an unsupervised learning prob- lem, where the training dataset contains {x1, · · · , xn} ⊂ RD, and the goal is to minimize the objective function F (β) = − 1 n n∑ i=1 log ( k∑ j=1 ωj N(xi, µj , Σ j ) ) , (29) where N(xi, µj , Σ j ) = 1√ (2π)D|Σ j | exp(− 1 2 (xi − µj)T Σ −1 j (xi − µj )); so f(xi, β) = − log ( ∑ k j=1 ωj N(xi, µj , Σ j ) ) for (29). It is worth noting that (29) is diﬀeren- tiable and Lipschitz smooth and thus can be solved via the gradient de scent method. However, the expectation-maximization (EM) method is mo re popu- lar due to its simplicity and eﬃciency for GMM training. Moreover, the E M method also has the locality property in practice. In our experiment, we still use Algorithm 2 to generate the sequent ial coreset, but run the EM algorithm as the “host” algorithm A. 5 Experimental Evaluation We evaluate the performance of our sequential coreset method f or the applica- tions mentioned in Section 4. All results were obtained on a server eq uipped with 2.4GHz Intel CPUs and 256GB main memory; the algorithms were im ple- mented in Python. 5.1 Ridge and Lasso Regression We consider Ridge and Lasso regression ﬁrst. Datasets. Appliances Energy is a dataset for predicting energy con- sumption which contains 19735 points in R29 [10]. F acebook Commentis a 142 4 6 8 10 Coreset Size (*102) 0.0086 0.0088 0.0090 0.0092 0.0094 0.0096Loss Original OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp 2 4 6 8 10 Coreset Size (*102) 0.2 0.4 0.6 0.8 1.0Errorβ OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp 2 4 6 8 10 Coreset Size (*102) 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20Normalized Runtime OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp Figure 2: The experimental results on Appliances Energy for Ridge regres- sion ( λ = 0 .01). 102   103   104 Coreset Size 0.00023 0.00024 0.00025 0.00026 0.00027 0.00028Loss Original OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp 102   103   104 Coreset Size 0.0 0.2 0.4 0.6 0.8 1.0Errorβ OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp 102   103   104 Coreset Size 0.000 0.005 0.010 0.015 0.020 0.025Normalized Runtime OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp Figure 3: The experimental results on F acebook Commentfor Ridge regres- sion ( λ = 0 .01). 102   103   104 Coreset Size 9 10 11 12 13Loss Original OneShot UniSamp SeqCore-0.5 SeqCore-1 SeqCore-5 SeqCore-10 ImpSamp 102   103   104 Coreset Size 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40Errorβ OneShot UniSamp SeqCore-0.5 SeqCore-1 SeqCore-5 SeqCore-10 ImpSamp 102   103   104 Coreset Size 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Normalized Runtime OneShot UniSamp SeqCore-0.5 SeqCore-1 SeqCore-5 SeqCore-10 ImpSamp Figure 4: The experimental results on the synthetic dataset for R idge regression (λ = 0 .01) 152 4 6 8 10 Coreset Size (*102) 0.018 0.019 0.020 0.021 0.022 0.023 0.024 0.025Loss Original OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp 2 4 6 8 10 Coreset Size (*102) 0.2 0.4 0.6 0.8 1.0 1.2Errorβ OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp 2 4 6 8 10 Coreset Size (*102) 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40Normalized Runtime OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp Figure 5: The experimental results on Appliances Energy for Lasso regres- sion ( λ = 0 .01). 102   103   104 Coreset Size 0.0015 0.0020 0.0025 0.0030 0.0035 0.0040Loss Original OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp 102   103   104 Coreset Size 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75Errorβ OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp 102   103   104 Coreset Size 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200Normalized Runtime OneShot UniSamp SeqCore-0.001 SeqCore-0.005 SeqCore-0.01 SeqCore-0.05 ImpSamp Figure 6: The experimental results on F acebook Commentfor Lasso regres- sion ( λ = 0 .01). 16dataset for predicting comment which contains 602813 points in R54 [47]. Fur- thermore, we generate a synthetic dataset of 10 6 points in R50; each point is randomly sampled from the linear equation y = ⟨h, x⟩, where each coeﬃcient of h is sampled from [ −5, 5] uniformly at random; for each data point we also add a Gaussian noise N (0, 4) to y. Compared methods. As the host algorithm A in Algorithm 2, we apply the standard gradient descent algorithm. Fixing a coreset size, we consider sev- eral diﬀerent data compression methods for comparison. (1) Original: directly run A on the original input data; (2) UniSamp: the simple uniform sampling; (3) ImpSamp: the importance sampling method [50]; (4) SeqCore-R: our sequential coreset method with a speciﬁed region range R; (5) OneShot: build the local coreset as Algorithm 1 in one-shot (without using the sequential idea) 3. Results. We consider three metrics to measure the performance: (1) the total loss, (2) the normalized error to the optimal β∗ (let Errorβ = ∥β−β∗∥2 ∥β∗∥2 where β is the obtained solution and β∗ is the optimal solution obtained from Original), and (3) the normalized runtime (over the runtime of Original). The results of Ridge regression are shown in Figures 2, 3 and 4 (aver aged across 10 trials). We can see that in general our proposed sequential cor eset method has better performance on the loss and Errorβ , though sometimes it is slightly slower than ImpSamp if we set R to be too small. UniSamp is always the fastest one (because it is just simple uniform sampling), but at the c ost of inferior performance in total loss and model estimate error. OneShot is faster than SeqCore-R but often has worse loss and error. Similar results of Lasso regression are shown in Figure 5 and 6. Due to the space limit, more de tailed experimental results (including the results on Logistic regression a nd GMM) are shown in the appendix. 5.2 Gaussian Mixture Models We directly generate the datasets by using the software package [41] (the number of the data points n = 10 5). The host EM algorithm implementation is also from [41]. We separately vary the dimension, Gaussian Components n umber and coreset size. The experimental results are shown in Figure 7 an d Figure 8. The purity evaluates the similarity between our obtained clustering result and the ground truth [35]. We can see our proposed sequential corese t method is slightly slower than UniSamp and ImpSamp [33], but can achieve better purity. 3For OneShot, we do not need to specify the range R, if we ﬁx the coreset size. The range is only used for our sequential coreset method because we nee d to re-build the coreset when β reaches the boundary. 174 8 12 16 20 24 Dimension d 0 10 20 30 40 50 R untime (s) GMM ( k = 12, r = 2%) Original UniSamp ImpSamp SeqCore 4 8 12 16 20 24 Gaussian Components k 0 20 40 60 80 R untime (s) GMM ( d = 12, r = 2%) Original UniSamp ImpSamp SeqCore 2 4 6 8 10 Sampling Ratio r  (%) 0 1 2 3 4 5 R untime (s) GMM ( d = 12, k = 12) UniSamp ImpSamp SeqCore Figure 7: The running times on synthetic dataset for Gaussian Mixture Models. 4 8 12 16 20 24 Dimension d 0.4 0.6 0.8 1.0 Purity GMM ( k = 12, r = 2%) Original UniSamp ImpSamp SeqCore 4 8 12 16 20 24 Gaussian  Components k 0.2 0.4 0.6 0.8 1.0 1.2 Purity GMM ( d = 12, r = 2%) Original UniSamp ImpSamp SeqCore 2 4 6 8 10 Sampling Ratio r  (%) 0.2 0.4 0.6 0.8 1.0 1.2 Purity GMM ( d = 12, k = 12) UniSamp ImpSamp SeqCore Figure 8: The purity values on synthetic dataset for Gaussian Mixture Models. 186 Conclusions and Future Work Based on the simple observation of the locality property, we propos e a novel sequential coreset framework for reducing the complexity of gra dient descent algorithms and some relevant variants. Our framework is easy to imp lement and has provable quality guarantees. Following this work, it is interes ting to consider building coresets for other optimization methods, such as the popular stochastic gradient descent method as well as second order meth ods. 7 Acknowledgements The authors would like to thank Mingyue Wang and the anonymous rev iewers for their helpful discussions and suggestions on improving this paper. T his work was supported in part by the Ministry of Science and Technology of China through grant 2019YFB2102200, the Anhui Dept. of Science and Technolo gy through grant 201903a05020049, and Tencent Holdings Ltd through gran t FR202003. A Proof for Theorem 2 Similar with the proof for Theorem 1, we ﬁx a vector β ∈ B( ˜β, R) and l ∈ {1, · · · , d}. We view ∇fi(β)[l] as an independent random variable for each (xi, yi) ∈ Qj . Note that we have the bound ∇fi(β)[l] ∈ [min j ∇fj( ˜β)[l] − LR, max j ∇f( ˜β)l + LR] by Assumption 1, where the length of the interval is at most 2 M + 2 LR. If keeping the partition of P as Algorithm 1, through the Hoeﬀding’s inequality we know that Prob [ | 1 |Qj | ∑ (xi,yi)∈ Qj ∇fi(β )[l] − 1 |Pj | ∑ (xi,yi)∈ Pj ∇fi(β )[l]| ≥ σ 2 ] ≤ λ (30) if the sample size |Qj| = ⌈8(LR+M σ )2 ln 2 λ ⌉. By taking the union bound of (30) over 0 ≤ j ≤ N, we have n|∇ ˜F (β)[l] − ∇F (β)[l]| ≤ N∑ j=0 | |Pj | |Qj | ∑ (xi,yi)∈Qj ∇fi(β)[l] − ∑ (xi,yi)∈Pj ∇fi(β)[l]| ≤ N∑ j=0 |Pj |σ 2 = nσ 2 (31) 19with probability at least 1 − (N + 1)λ. That is, |∇ ˜F (β)[l] − ∇F (β)[l]| ≤ σ 2 . The, we apply the similar discretization idea. We build the grid with side length being equal to ǫR√ d , so as to obtain the representative points set G with |G| = O((2√πe ǫ )d). It is easy to see that (31) holds for any β ∈ G and any l ∈ { 1, · · · , d} with probability at least 1 − (N + 1)d|G|λ. For any β ∈ B( ˜β, R), we let β′ ∈ G be the representative point of the cell containing β. Then we have ∥β − β′∥ ≤ ǫR. By Assumption 1, we have both ∥∇F (β)[l] − ∇F (β′)[l]∥ and ∥∇ ˜F (β)[l] − ∇ ˜F (β′)[l]∥ ≤ ǫLR. (32) Through the triangle inequality, we know |∇ ˜F (β)[l] − ∇F (β)[l]| ≤ |∇ ˜F (β)[l] − ∇ ˜F (β′)[l]| + |∇ ˜F (β′)[l] − ∇F (β′)[l]| +|∇F (β′)[l] − ∇F (β)[l]| ≤ σ 2 + 2 × ǫLR, (33) where the last inequality comes from (31) and (32). If letting ǫ = σ 2LR , we have |∇ ˜F (β)[l] − ∇F (β)[l]| ≤ σ. Recall that |Qj| = ⌈8(LR+M σ )2 ln 2 λ ⌉ and the success probability is 1 − (N + 1)d|G|λ. Let λ = 1 n(N+1)d|G| and then we have the success probability being at least 1 − 1 n . Finally, the coreset size is N∑ j=0 |Qj| = ˜O ( (LR + M σ )2 · d ) . (34) For the case that β is restricted to have at most k non-zeros entries in sparse optimizations, similar with Theorem 3, we know the coreset size can be reduced to be ˜O ( (LR+M σ )2 · k log d ) . B Details in Section 3.3 Recall that the formula (35) of Section 3.3, F (β) = 1 n n∑ i=1 g(β, xi, yi) + λ∥β∥p, (35) where the function g(β, xi, yi) is assumed to be diﬀerentiable and satisfy As- sumption 1. Let f(β, xi, yi) = g(β, xi, yi) + λ∥β∥p. 20First, we note that problem (35) is usually solved by generalizations o f gra- dient descent method, such as subgradient methods [4] and prox imal gradient methods [38]. The key point is that these algorithms also enjoy the loc ality prop- erty described in Section 1.1. In (8), we provide the upper and lower bounds of fi(β) ( i.e., f(β, xi, yi)) for the non-diﬀerentiable case (25). For 0 < p ≤ 2, by using the H¨ older’s inequality we obtain the similar bounds for non-diﬀe rentiable case : fi(β) ∈ fi(βanc) ± (( ∥∇gi(βanc)∥ R + L 2 ) R2 + λd1/p −1/ 2 n R ) . After replacing (8) by these bounds, we can proceed the same analysis in Section 3.1 and attain a similar result with Theorem 3. Here is the detailed analyse. For any β, βanc ∈ Rd and ∥β − βanc∥2 ≤ R, we have |f(β, xi, yi) − f(βanc, xi, yi)| ≤ | (g(β, xi, yi) − g(βanc, xi, yi)| + λ|∥β∥p − ∥βanc∥p| ≤ ∥∇ gi(βanc)∥2R + LR2 2 + λ∥β − βanc∥p ≤ ∥∇ gi(βanc)∥2R + LR2 2 + λd 1 p − 1 2 · ∥β − βanc∥2, (36) where the last inequality comes from H¨ older’s inequality. By (36), we have fi(β) ∈ fi(βanc) ± (( ∥∇gi(βanc)∥ R + L 2 ) R2 + λd1/p−1/2R ) . We deﬁne M := λd1/p−1/2 + max 1≤i≤n ∥∇g(βanc, xi, yi)∥ and m := min β∈B(βanc,R) F (β). Then we have the following theorem by using the same idea for Theore m 3. Theorem 3 With problability 1 − 1 n , Algorithm 1 returns a qualiﬁed coreset CS ǫ(βanc, R) with size ˜O ( ( H+MR+LR2 m ) 2 · d ǫ2 ) . Furthermore, when the vector β is restricted to have at most k ∈ Z+ non-zero entries in the hypothesis space Rd, the coreset size can be reduced to be ˜O ( ( H+MR+LR2 m ) 2 · k log d ǫ2 ) . The run- time of Algorithm 1 is O(n · tf ), where tf is the time complexity for computing the loss f(β, x, y ). C Assumption 1 for GMM We show that the objective function of GMM training satisﬁes Assum ption 1. In GMM, f(xi, β) = − log ( k∑ j=1 ωjN(xi, µj , Σ j ) ) where N(xi, µj , Σ j) = 1√ (2π)D|Σ j | exp(− 1 2 (xi−µj)T Σ −1 j (xi−µj)),β := [( ω1, µ1, Σ −1 1 ), . . . , (ωk, µk, Σ −1 k )], ∑ k j=1 ωj = 1. For simplicity, we use pij to denote N (xi, µj , Σ j ). We deﬁne ˜m := min 1≤l≤k,1≤i≤n pil and ˜M := max 1≤l≤k,1≤i≤n pil. 21Here we assume that the Gaussian models are λ-semi-sphericial (following the assumption in [32]), which means Σ j has eigenvalues bounded in [ λ, 1 λ ] for 1 ≤ j ≤ k. Also we assume that for any j ∈ { 1, · · · , k}, ∥xi − µj ∥ ≤ r for some r > 0. Therefore we know ˜ m ≥ ( λ 2π ) D 2 e− r2 2λ , ˜M ≤ ( 1 2πλ ) D 2 e− λr 2 2 . Also, we have the following equations: ∂fi(β) ∂µj = Σ j (xi − µj ) γij ; (37) ∂fi(β) ∂Σ −1 j = ( Σ j − (xi − µj ) (xi − µj )T ) γij ; (38) ∂fi(β) ∂ωj = ω−1 j γij . (39) Here {γi1, · · · , γik} are the GMM responsibilities for fi. Then we have γij = ωipij ω1pi1+···+ωkpik . Thus we have ∥ ∂fi(β) ∂µj ∥2 = γij ∥Σ j (xi − µj ) ∥2 ≤ r λ; (40) ∥ ∂fi(β) ∂Σ −1 j ∥F = γij ∥Σ j − (xi − µj ) (xi − µj )T ∥F ≤ ∥ Σ j ∥F + ∥ (xi − µj ) (xi − µj )T ∥F ≤ √ D λ + r2; (41) ∂fi(β) ∂ωj = pij ω1pi1 + · · · + ωkpik ≤ ˜M ˜m ; (42) From (42), (41) and (40) we have ∥∇fi(β)∥ ≤    √ k ( ˜M2 ˜m2 + r2 λ2 + ( √ D λ + r2)2 ) . So fi is Lipschitz continous, i.e., |fi(β1) − fi(β2)| ≤    √ k ( ˜M2 ˜m2 + r2 λ2 + ( √ D λ + r2)2 ) ∥β1 − β2∥. (43) We can use (43) to replace the bound implied from Assumption 1 in Sect ion 3, and thus the same analysis and results hold for GMM. 22References [1] Haim Avron, Kenneth L. Clarkson, and David P. Woodruﬀ. Sharpe r bounds for regularized data ﬁtting. In Approximation, Randomiza- tion, and Combinatorial Optimization. Algorithms and Tech niques, AP- PROX/RANDOM 2017 , volume 81, pages 27:1–27:22, 2017. [2] Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. The johnson-lindenstrauss lemma meets compressed sensing. preprint, 100(1):1–9, 2006. [3] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresho lding al- gorithm for linear inverse problems. SIAM Journal on Imaging Sciences , 2(1):183–202, 2009. [4] Dimitri P. Bertsekas. Convex Optimization Algorithms . Athena Scientiﬁc Belmont, MA, 2015. [5] Christopher M Bishop. Pattern recognition and machine learning . springer, 2006. [6] Zal´ an Borsos, Mojmir Mutny, and Andreas Krause. Coresets v ia bilevel optimization for continual learning and streaming. In Advances in Neural Information Processing Systems, NeurIPS , 2020. [7] L´ eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Rev. , 60(2):223–311, 2018. [8] Trevor Campbell and Boyan Beronov. Sparse variational infere nce: Bayesian coresets from scratch. In Hanna M. Wallach, Hugo Laroc helle, Alina Beygelzimer, Florence d’Alch´ e-Buc, Emily B. Fox, and Roman Gar - nett, editors, Advances in Neural Information Processing Systems 32: An- nual Conference on Neural Information Processing Systems 2 019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pages 11457–11468, 2019. [9] Trevor Campbell and Tamara Broderick. Bayesian coreset cons truction via greedy iterative geodesic ascent. In Jennifer G. Dy and Andreas K rause, ed- itors, Proceedings of the 35th International Conference on Machin e Learn- ing, ICML 2018, Stockholmsm¨ assan, Stockholm, Sweden, Jul y 10-15, 2018 , volume 80 of Proceedings of Machine Learning Research , pages 697–705. PMLR, 2018. [10] Luis M Candanedo, V´ eronique Feldheim, and Dominique Deramaix. Data driven prediction models of energy use of appliances in a low-energy h ouse. Energy and buildings , 140:81–97, 2017. [11] Ke Chen. On coresets for k-median and k-means clustering in me tric and euclidean spaces and their applications. SIAM Journal on Computing , 39(3):923–947, 2009. 23[12] Rachit Chhaya, Anirban Dasgupta, and Supratim Shit. On cores ets for regularized regression. In Proceedings of the 37th International Conference on Machine Learning, ICML , volume 119, pages 1866–1876, 2020. [13] Agniva Chowdhury, Jiasen Yang, and Petros Drineas. An iterat ive, sketching-based framework for ridge regression. In Proceedings of the 35th International Conference on Machine Learning, ICML , volume 80, pages 988–997, 2018. [14] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirza- soleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaha ria. Selection via proxy: Eﬃcient data selection for deep learning. In 8th Inter- national Conference on Learning Representations, ICLR . OpenReview.net, 2020. [15] J. S. Cramer. The early origins of the logit model. Studies in History and Philosophy of Science Part C: Studies in History and Phil osophy of Biological and Biomedical Sciences , 35(4):613 – 626, 2004. [16] Haskell B. Curry. The method of steepest descent for non-lin ear minimiza- tion problems. Quart. Appl. Math. , 2:258–261, 1944. [17] Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael W Mahoney. Sampling algorithms and coresets for ℓp regression. SIAM Journal on Computing , 38(5):2060–2078, 2009. [18] Hu Ding and Zixiu Wang. Layered sampling for robust optimization p rob- lems. In Proceedings of the 37th International Conference on Machin e Learning, ICML, volume 119, pages 2556–2566, 2020. [19] Petros Drineas, Michael W Mahoney, and Shan Muthukrishnan. Sampling algorithms for l2 regression and applications. In Proceedings of the 17th annual ACM-SIAM symposium on Discrete algorithms , pages 1127–1136, 2006. [20] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgrad ient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, 2011. [21] Dan Feldman. Core-sets: An updated survey. Wiley Interdiscip. Rev. Data Min. Knowl. Discov. , 10(1), 2020. [22] Dan Feldman and Michael Langberg. A uniﬁed framework for app roximat- ing and clustering data. In Proceedings of the 43rd ACM Symposium on Theory of Computing, STOC , pages 569–578, 2011. [23] Teoﬁlo F Gonzalez. Clustering to minimize the maximum intercluster dis- tance. Theoretical Computer Science , 38:293–306, 1985. 24[24] Wassily Hoeﬀding. Probability inequalities for sums of bounded ran dom variables. In The Collected Works of Wassily Hoeﬀding , pages 409–426. Springer, 1994. [25] Lingxiao Huang, Shaofeng Jiang, Jian Li, and Xuan Wu. Epsilon-co resets for clustering (with outliers) in doubling metrics. In IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS) , pages 814–825, 2018. [26] Jonathan Huggins, Trevor Campbell, and Tamara Broderick. Co resets for scalable bayesian logistic regression. In Advances in Neural Information Processing Systems, pages 4080–4088, 2016. [27] Praneeth Kacham and David P. Woodruﬀ. Optimal deterministic c oresets for ridge regression. In The 23rd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS , volume 108, pages 4141–4150, 2020. [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic o p- timization. In 3rd International Conference on Learning Representations , ICLR, 2015. [29] Michael Langberg and Leonard J Schulman. Universal ε-approximators for integrals. In Proceedings of the twenty-ﬁrst annual ACM-SIAM symposium on Discrete Algorithms , pages 598–607. SIAM, 2010. [30] Su-In Lee, Honglak Lee, Pieter Abbeel, and Andrew Y. Ng. Eﬃcie nt L1 reg- ularized logistic regression. In Proceedings, The 21st National Conference on Artiﬁcial Intelligence and the 18th Innovative Applicat ions of Artiﬁcial Intelligence Conference, pages 401–408. AAAI Press, 2006. [31] Yi Li, Philip M Long, and Aravind Srinivasan. Improved bounds on t he sample complexity of learning. Journal of Computer and System Sciences , 62(3):516–527, 2001. [32] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldm an. Train- ing Gaussian mixture models at scale via coresets. The Journal of Machine Learning Research, 18(1):5885–5909, 2017. [33] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldm an. Train- ing gaussian mixture models at scale via coresets. Journal of Machine Learning Research, 18(160):1–25, 2018. [34] Alaa Maalouf, Ibrahim Jubran, and Dan Feldman. Fast and accur ate least- mean-squares solvers. In Annual Conference on Neural Information Pro- cessing Systems, NeurIPS , pages 8305–8316, 2019. [35] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch ¨ utze.In- troduction to information retrieval . Cambridge University Press, 2008. 25[36] Baharan Mirzasoleiman, Jeﬀ A. Bilmes, and Jure Leskovec. Core sets for data-eﬃcient training of machine learning models. In Proceedings of the 37th International Conference on Machine Learning, ICML , volume 119, pages 6950–6960, 2020. [37] Baharan Mirzasoleiman, Kaidi Cao, and Jure Leskovec. Corese ts for robust training of deep neural networks against noisy labels. In Annual Conference on Neural Information Processing Systems 2020, NeurIPS , 2020. [38] Soﬁa Mosci, Lorenzo Rosasco, Matteo Santoro, Alessandro V erri, and Silvia Villa. Solving structured sparsity regularization with proximal metho ds. In European Conference on Machine Learning and Knowledge Disc overy in Databases ECML PKDD , volume 6322, pages 418–433, 2010. [39] Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, a nd David Woodruﬀ. On coresets for logistic regression. In Advances in Neural Infor- mation Processing Systems , pages 6561–6570, 2018. [40] Yurii Nesterov. A method of solving a convex programming prob lem with convergence rate O(1/ k2). Soviet Mathematics Doklady , 27(2):372–376, 1983. [41] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirio n, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas , A. Pas- sos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit- learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011. [42] Jeﬀ M. Phillips. Coresets and sketches. Computing Research Repository , 2016. [43] Anant Raj, Cameron Musco, and Lester Mackey. Importance sampling via local sensitivity. In Silvia Chiappa and Roberto Calandra, editors, The 23rd International Conference on Artiﬁcial Intelligence and St atistics, AISTATS 2020, volume 108 of Proceedings of Machine Learning Research, pages 3099– 3109. PMLR, 2020. [44] Sashank J. Reddi, Barnab´ as P´ oczos, and Alexander J. Smola . Communica- tion eﬃcient coresets for empirical loss minimization. In Marina Meila an d Tom Heskes, editors, Proceedings of the Thirty-First Conference on Un- certainty in Artiﬁcial Intelligence, UAI 2015 , pages 752–761. AUAI Press, 2015. [45] Alireza Samadian, Kirk Pruhs, Benjamin Moseley, Sungjin Im, and Ryan R. Curtin. Unconditional coresets for regularized loss minimization. In The 23rd International Conference on Artiﬁcial Intelligence a nd Statistics, AIS- TATS, volume 108, pages 482–492, 2020. 26[46] Ozan Sener and Silvio Savarese. Active learning for convolutiona l neural networks: A core-set approach. In 6th International Conference on Learn- ing Representations, ICLR . OpenReview.net, 2018. [47] Kamaljot Singh, Ranjeet Kaur Sandhu, and Dinesh Kumar. Comm ent vol- ume prediction using neural networks and decision trees. In IEEE UKSim- AMSS 17th International Conference on Computer Modelling a nd Simula- tion, UKSim2015 (UKSim2015) , Cambridge, United Kingdom, mar 2015. [48] Robert Tibshirani. Regression shrinkage and selection via the las so. Journal of the Royal Statistical Society. Series B (Methodological ), 58(1):267–288, 1996. [49] Andrey Tikhonov. Nonlinear ill-posed problems. Applied Mathematical Sciences, 1998. [50] Murad Tukan, Alaa Maalouf, and Dan Feldman. Coresets for nea r-convex functions. In Annual Conference on Neural Information Processing Sys- tems, NeurIPS , 2020. [51] Vladimir Vapnik. Principles of risk minimization for learning theory. I n Advances in Neural Information Processing Systems 4, [NIPS , pages 831– 838, 1991. [52] Philip Wolfe. Convergence conditions for ascent methods. SIAM Rev. , 11(2):226–235, 1969. [53] Laurence A. Wolsey. An analysis of the greedy algorithm for the submod- ular set covering problem. Comb., 2(4):385–393, 1982. 27",
      "references": [
        "Sharper bounds for regularized data fitting",
        "The johnson-lindenstrauss lemma meets compressed sensing",
        "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
        "Convex Optimization Algorithms",
        "Pattern recognition and machine learning",
        "Coresets via bilevel optimization for continual learning and streaming",
        "Optimization methods for large-scale machine learning",
        "Sparse variational inference: Bayesian coresets from scratch",
        "Bayesian coreset construction via greedy iterative geodesic ascent",
        "Data driven prediction models of energy use of appliances in a low-energy house",
        "On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications",
        "On coresets for regularized regression",
        "An iterative, sketching-based framework for ridge regression",
        "Selection via proxy: Efficient data selection for deep learning",
        "The early origins of the logit model",
        "The method of steepest descent for non-linear minimization problems",
        "Sampling algorithms and coresets for ℓp regression",
        "Layered sampling for robust optimization problems",
        "Sampling algorithms for l2 regression and applications",
        "Adaptive subgradient methods for online learning and stochastic optimization",
        "Core-sets: An updated survey",
        "A unified framework for approximating and clustering data",
        "Clustering to minimize the maximum intercluster distance",
        "Probability inequalities for sums of bounded random variables",
        "Epsilon-coresets for clustering (with outliers) in doubling metrics",
        "Coresets for scalable bayesian logistic regression",
        "Optimal deterministic coresets for ridge regression",
        "Adam: A method for stochastic optimization",
        "Universal ε-approximators for integrals",
        "Efficient L1 regularized logistic regression",
        "Improved bounds on the sample complexity of learning",
        "Training Gaussian mixture models at scale via coresets",
        "Fast and accurate least-mean-squares solvers",
        "Introduction to information retrieval",
        "Core sets for data-efficient training of machine learning models",
        "Coresets for robust training of deep neural networks against noisy labels",
        "Solving structured sparsity regularization with proximal methods",
        "On coresets for logistic regression",
        "A method of solving a convex programming problem with convergence rate O(1/ k2)",
        "Scikit-learn: Machine learning in Python",
        "Coresets and sketches",
        "Importance sampling via local sensitivity",
        "Communication efficient coresets for empirical loss minimization",
        "Unconditional coresets for regularized loss minimization",
        "Active learning for convolutional neural networks: A core-set approach",
        "Comment volume prediction using neural networks and decision trees",
        "Regression shrinkage and selection via the lasso",
        "Nonlinear ill-posed problems",
        "Coresets for near-convex functions",
        "Principles of risk minimization for learning theory",
        "Convergence conditions for ascent methods",
        "An analysis of the greedy algorithm for the submodular set covering problem"
      ],
      "meta_data": {
        "arxiv_id": "2112.02504v3",
        "authors": [
          "Jiawei Huang",
          "Ruomin Huang",
          "Wenjie Liu",
          "Nikolaos M. Freris",
          "Hu Ding"
        ],
        "published_date": "2021-12-05T08:12:16Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes a general-purpose “sequential coreset” framework that compresses data online along the trajectory of gradient-based optimization. By exploiting a locality property of gradient descent, the method builds small local coresets with layered importance sampling and updates them whenever the iterate leaves a predefined ball. It avoids computing problem-dependent sensitivity and pseudo-dimension and yields provable (1±ε) risk preservation and gradient preservation guarantees. For sparse problems the coreset size depends only poly-logarithmically on dimension. Theoretical bounds and extensions to sub-gradient, proximal and EM algorithms are provided, and experiments show significant speed-ups on ridge, lasso, logistic regression and Gaussian Mixture Models.",
        "methodology": "1) Local ball model: at iterate β_anc define radius R; partition points into log n layers by loss value. 2) Layered uniform sampling inside each layer with weights |P_j|/|Q_j|; sample size chosen via Hoeffding to guarantee ε approximation over the ball. 3) Sequential procedure: run host optimizer on current coreset until leaving ball, then rebuild coreset anchored at new point—forming a chain of local coresets. 4) Theoretical analysis derives coreset size Õ((H+MR+LR²)/m)^2·d/ε² and Õ(...·k log d/ε²) for k-sparse, plus gradient preservation with additive σ. 5) Extensions described for non-differentiable objectives (e.g., ℓ₁) and GMM with EM.",
        "experimental_setup": "Implemented in Python and run on 2.4 GHz CPU, 256 GB RAM. Datasets: (i) Appliances Energy (19 735 × 29), (ii) Facebook Comment (602 813 × 54), (iii) synthetic linear regression (1 000 000 × 50), (iv) synthetic GMM (100 000 points, varying dimension 4–24, k=4–24). Tasks: ridge & lasso regression (λ=0.01), logistic regression, GMM training. Host optimizers: full-batch gradient descent for regression/logistic; EM for GMM. Baselines: Original (full data), uniform sampling, importance sampling, One-Shot coreset, SeqCore with different R. Metrics: loss, parameter error ‖β−β*‖/‖β*‖, normalized runtime; for GMM, clustering purity. Results averaged over 10 trials; plots provided for varying coreset sizes, dimensions, components and sampling ratios.",
        "limitations": "1) Relies on a fixed radius R and Lipschitz-smooth loss; inappropriate radius choice or non-smooth objectives with sharp changes may hurt performance. 2) Locality assumption may fail for large learning rates, adaptive steps or highly non-convex landscapes. 3) Coreset rebuilding introduces overhead; sequential method can be slower than importance sampling when R is too small. 4) Theoretical size bounds include large constants and depend on unknown quantities (H, M, m). 5) Experiments limited to medium-scale tabular datasets; no evaluation on deep networks or distributed settings.",
        "future_research_directions": "• Adaptive radius selection and automatic detection of locality violations.\n• Integration with stochastic or mini-batch gradient methods and variance reduction.\n• Incremental updates to coresets to lower rebuilding cost.\n• Extension to second-order and momentum methods, or to non-smooth and adversarial objectives.\n• Distributed and federated implementations where coresets reduce communication.\n• Tighter theoretical bounds independent of unknown problem constants.\n• Application to large-scale deep learning and complex real-world datasets.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks",
      "full_text": "Published as a conference paper at ICLR 2020 MIXUP INFERENCE : B ETTER EXPLOITING MIXUP TO DEFEND ADVERSARIAL ATTACKS Tianyu Pang∗, Kun Xu∗, Jun Zhu† Dept. of Comp. Sci. & Tech., BNRist Center, Institute for AI, Tsinghua University; RealAI {pty17,xu-k16}@mails.tsinghua.edu.cn, dcszj@tsinghua.edu.cn ABSTRACT It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally unreasonable behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efﬁcient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants. 1 I NTRODUCTION Deep neural networks (DNNs) have achieved state-of-the-art performance on various tasks (Good- fellow et al., 2016). However, counter-intuitive adversarial examples generally exist in different domains, including computer vision (Szegedy et al., 2014), natural language processing (Jin et al., 2019), reinforcement learning (Huang et al., 2017), speech (Carlini & Wagner, 2018) and graph data (Dai et al., 2018). As DNNs are being widely deployed, it is imperative to improve model robustness and defend adversarial attacks, especially in safety-critical cases. Previous work shows that adversarial examples mainly root from the locally unstable behavior of classiﬁers on the data manifolds (Goodfellow et al., 2015; Fawzi et al., 2016; 2018; Pang et al., 2018b), where a small adversarial perturbation in the input space can lead to an unreasonable shift in the feature space. On the one hand, many previous methods try to solve this problem in the inference phase, by introducing transformations on the input images. These attempts include performing local linear transformation like adding Gaussian noise (Tabacof & Valle, 2016), where the processed inputs are kept nearby the original ones, such that the classiﬁers can maintain high performance on the clean inputs. However, as shown in Fig. 1(a), the equivalent perturbation, i.e., the crafted adversarial perturbation, is still δand this strategy is easy to be adaptively evaded since the randomness ofx0 w.r.t x0 is local (Athalye et al., 2018). Another category of these attempts is to apply various non-linear transformations, e.g., different operations of image processing (Guo et al., 2018; Xie et al., 2018; Raff et al., 2019). They are usually off-the-shelf for different classiﬁers, and generally aim to disturb the adversarial perturbations, as shown in Fig. 1(b). Yet these methods are not quite reliable since there is no illustration or guarantee on to what extent they can work. On the other hand, many efforts have been devoted to improving adversarial robustness in the training phase. For examples, the adversarial training (AT) methods (Madry et al., 2018; Zhang et al., 2019; Shafahi et al., 2019) induce locally stable behavior via data augmentation on adversarial examples. However, AT methods are usually computationally expensive, and will often degenerate model ∗Equal contribution. †Corresponding author. 1 arXiv:1909.11515v2  [cs.LG]  20 Feb 2020Published as a conference paper at ICLR 2020 !\"!\" #−\" %&% %' %(&%(Mixup Inference(Global Linear Transformation) !% %()%!)%(Local Linear Transformation !% %( %∗ !∗ %(∗ Non-Linear TransformationObserved inputsVirtual inputsThe processedinputs fed into classifiers (a) (b) (c) Figure 1: Intuitive mechanisms in the input space of different input-processing based defenses. xis the crafted adversarial example, x0 is the original clean example, which is virtual and unknown for the classiﬁers. δis the adversarial perturbation. performance on the clean inputs or under general-purpose transformations like rotation (Engstrom et al., 2019). In contrast, the mixup training method (Zhang et al., 2018) introduces globally linear behavior in-between the data manifolds, which can also improve adversarial robustness (Zhang et al., 2018; Verma et al., 2019a). Although this improvement is usually less signiﬁcant than it resulted by AT methods, mixup-trained models can keep state-of-the-art performance on the clean inputs; meanwhile, the mixup training is computationally more efﬁcient than AT. The interpolated AT method (Lamb et al., 2019) also shows that the mixup mechanism can further beneﬁt the AT methods. However, most of the previous work only focuses on embedding the mixup mechanism in the training phase, while the induced global linearity of the model predictions is not well exploited in the inference phase. Compared to passive defense by directly classifying the inputs (Zhang et al., 2018; Lamb et al., 2019), it would be more effective to actively defend adversarial attacks by breaking their locality via the globally linear behavior of the mixup-trained models. In this paper, we develop an inference principle for mixup-trained models, named mixup inference (MI). In each execution, MI performs a global linear transformation on the inputs, which mixups the input xwith a sampled clean example xs, i.e., ˜x= λx+ (1 −λ)xs (detailed in Alg. 1), and feed ˜xinto the classiﬁer as the processed input. There are two basic mechanisms for robustness improving under the MI operation (detailed in Sec. 3.2.1), which can be illustrated by simple geometric intuition in Fig. 1(c). One is perturbation shrinkage: if the input is adversarial, i.e., x= x0 + δ, the perturbation δwill shrink by a factor λafter performing MI, which is exactly the mixup ratio of MI according to the similarity between triangles. Another one is input transfer: after the MI operation, the reduced perturbation λδacts on random ˜x0. Comparing to the spatially or semantically local randomness introduced by Gaussian noise or image processing, ˜x0 introduces spatially global and semantically diverse randomness w.r.tx0. This makes it less effective to perform adaptive attacks against MI (Athalye et al., 2018). Furthermore, the global linearity of the mixup-trained models ensures that the information of x0 remained in ˜x0 is proportional to λ, such that the identity of x0 can be recovered from the statistics of ˜x0. In experiments, we evaluate MI on CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) under the oblivious attacks (Carlini & Wagner, 2017) and the adaptive attacks (Athalye et al., 2018). The results demonstrate that our MI method is efﬁcient in defending adversarial attacks in inference, and is also compatible with other variants of mixup, e.g., the interpolated AT method (Lamb et al., 2019). Note that Shimada et al. (2019) also propose to mixup the input points in the test phase, but they do not consider their method from the aspect of adversarial robustness. 2 P RELIMINARIES In this section, we ﬁrst introduce the notations applied in this paper, then we provide the formula of mixup in training. We introduce the adversarial attacks and threat models in Appendix A.1. 2.1 N OTATIONS Given an input-label pair (x,y), a classiﬁer F returns the softmax prediction vector F(x) and the predicted label ˆy = arg maxj∈[L] Fj(x), where Lis the number of classes and [L] = {1,··· ,L}. The classiﬁer F makes a correct prediction on xif y= ˆy. In the adversarial setting, we augment the 2Published as a conference paper at ICLR 2020 data pair (x,y) to a triplet (x,y,z ) with an extra binary variable z, i.e., z= {1, if xis adversarial, 0, if xis clean. (1) The variable z is usually considered as hidden in the inference phase, so an input x(either clean or adversarially corrupted) can be generally denoted as x = x0 + δ·1z=1. Here x0 is a clean sample from the data manifold p(x) with label y0, 1z=1 is the indicator function, and δis a potential perturbation crafted by adversaries. It is worthy to note that the perturbation δshould not change the true label of the input, i.e., y= y0. For ℓp-norm adversarial attacks (Kurakin et al., 2017; Madry et al., 2018), we have ∥δ∥p ≤ϵ, where ϵis a preset threshold. Based on the assumption that adversarial examples are off the data manifolds, we formally have x0 + δ /∈supp(p(x)) (Pang et al., 2018a). 2.2 M IXUP IN TRAINING In supervised learning, the most commonly used training mechanism is the empirical risk minimiza- tion (ERM) principle (Vapnik, 2013), which minimizes 1 n ∑n i=1 L(F(xi),yi) on the training dataset D= {(xi,yi)}n i=1 with the loss function L. While computationally efﬁcient, ERM could lead to memorization of data (Zhang et al., 2017) and weak adversarial robustness (Szegedy et al., 2014). As an alternative, Zhang et al. (2018) introduce the mixup training mechanism, which minimizes 1 m ∑m j=1 L(F(˜xj),˜yj). Here ˜xj = λxj0 + (1−λ)xj1; ˜yj = λyj0 + (1−λ)yj1, the input-label pairs (xj0,yj0) and (xj1,yj1) are randomly sampled from the training dataset, λ∼Beta(α,α) and αis a hyperparameter. Training by mixup will induce globally linear behavior of models in-between data manifolds, which can empirically improve generalization performance and adversarial robustness (Zhang et al., 2018; Tokozume et al., 2018a;b; Verma et al., 2019a;b). Compared to the adversarial training (AT) methods (Goodfellow et al., 2015; Madry et al., 2018), trained by mixup requires much less computation and can keep state-of-the-art performance on the clean inputs. 3 M ETHODOLOGY Although the mixup mechanism has been widely shown to be effective in different domains (Berthelot et al., 2019; Beckham et al., 2019; Verma et al., 2019a;b), most of the previous work only focuses on embedding the mixup mechanism in the training phase, while in the inference phase the global linearity of the trained model is not well exploited. Compared to passively defending adversarial examples by directly classifying them, it would be more effective to actively utilize the globality of mixup-trained models in the inference phase to break the locality of adversarial perturbations. 3.1 M IXUP INFERENCE The above insight inspires us to propose the mixup inference (MI)method, which is a specialized inference principle for the mixup-trained models. In the following, we apply colored y, ˆyand ys to visually distinguish different notations. Consider an input triplet (x,y,z ), where zis unknown in advance. When directly feeding xinto the classiﬁer F, we can obtain the predicted label ˆy. In the adversarial setting, we are only interested in the cases where xis correctly classiﬁed by F if it is clean, or wrongly classiﬁed if it is adversarial (Kurakin et al., 2018). This can be formally denoted as 1y̸=ˆy = 1z=1. (2) The general mechanism of MI works as follows. Every time we execute MI, we ﬁrst sample a label ys ∼ps(y), then we sample xs from ps(x|ys) and mixup it with xas ˜x= λx+ (1 −λ)xs. ps(x,y) denotes the sample distribution, which is constrained to be on the data manifold, i.e., supp(ps(x)) ⊂ supp(p(x)). In practice, we execute MI for N times and average the output predictions to obtain FMI(x), as described in Alg. 1. Here we ﬁx the mixup ratio λin MI as a hyperparameter, while similar properties hold if λcomes from certain distribution. 3.2 T HEORETICAL ANALYSES Theoretically, with unlimited capability and sufﬁcient clean samples, a well mixup-trained model F can be denoted as a linear function H on the convex combinations of clean examples (Hornik et al., 1989; Guo et al., 2019), i.e., ∀xi,xj ∼p(x) and λ∈[0,1], there is H(λxi + (1 −λ)xj) = λH(xi) + (1−λ)H(xj). (3) 3Published as a conference paper at ICLR 2020 Algorithm 1Mixup Inference (MI) Input: The mixup-trained classiﬁer F; the input x. Hyperparameters: The sample distribution ps; the mixup ratio λ; the number of execution N. Initialize FMI(x) = 0; for k= 1 to N do Sample ys,k ∼ps(ys), xs,k ∼ps(xs|ys,k); Mixup xwith xs,k as ˜xk = λx+ (1 −λ)xs,k; Update FMI(x) = FMI(x) + 1 NF(˜xk); end for Return: The prediction FMI(x) of input x. Specially, we consider the case where the training objective Lis the cross-entropy loss, then H(xi) should predict the one-hot vector of label yi, i.e., Hy(xi) = 1y=yi. If the input x = x0 + δ is adversarial, then there should be an extra non-linear part G(δ; x0) of F, since x is off the data manifolds. Thus for any input x, the prediction vector can be compactly denoted as F(x) = F(x0 + δ·1z=1) = H(x0) + G(δ; x0) ·1z=1. (4) According to Eq. (3) and Eq. (4), the output of ˜xin MI is given by: F(˜x) = H(˜x0) + G(λδ; ˜x0) ·1z=1 = λH(x0) + (1−λ)H(xs) + G(λδ; ˜x0) ·1z=1, (5) where ˜x0 = λx0 + (1 −λ)xs is a virtual unperturbed counterpart of ˜xas shown in Fig. 1(c). Note that FMI(x) in Alg. 1 is a Monte Carlo approximation of Eps[F(˜x)] as FMI(x) = 1 N N∑ i=1 F(˜xi) ∞ −→Eps[F(˜x)], (6) where ∞ −→represents the limitation when the execution timesN →∞. Now we separately investigate the y-th and ˆy-th (could be the same one) components of F(˜x) according to Eq. (5), and see how these two components differ from those of F(x). These two components are critical because they decide whether we can correctly classify or detect adversarial examples (Goodfellow et al., 2016). Note that there is Hy(x0) = 1 and Hys(xs) = 1, thus we have the y-th components as Fy(x) = 1 + Gy(δ; x0) ·1z=1; Fy(˜x) = λ+ (1 −λ) ·1y=ys + Gy(λδ; ˜x0) ·1z=1. (7) Furthermore, according to Eq. (2), there is 1y=ˆy = 1z=0. We can represent the ˆy-th components as Fˆy(x) = 1z=0 + Gˆy(δ; x0) ·1z=1; Fˆy(˜x) = λ·1z=0 + (1 −λ) ·1ˆy=ys + Gˆy(λδ; ˜x0) ·1z=1. (8) From the above formulas we can ﬁnd that, except for the hidden variable z, the sampling label ys is another variable which controls the MI output F(˜x) for each execution. Different distributions of sampling ys result in different versions of MI. Here we consider two easy-to-implement cases: MI with predicted label (MI-PL): In this case, the sampling label ys is the same as the predicted label ˆy, i.e., ps(y) = 1y=ˆy is a Dirac distribution on ˆy. MI with other labels (MI-OL): In this case, the label ys is uniformly sampled from the labels other than ˆy, i.e., ps(y) = Uˆy(y) is a discrete uniform distribution on the set {y∈[L]|y̸= ˆy}. We list the simpliﬁed formulas of Eq. (7) and Eq. (8) under different cases in Table 1 for clear representation. With the above formulas, we can evaluate how the model performance changes with and without MI by focusing on the formula of ∆F(x; ps) = FMI(x) −F(x) ∞ −→Eps[F(˜x)] −F(x). (9) Speciﬁcally, in the general-purpose setting where we aim to correctly classify adversarial exam- ples (Madry et al., 2018), we claim that the MI method improves the robustness if the prediction 4Published as a conference paper at ICLR 2020 Table 1: The the simpliﬁed formulas of Eq. (7) and Eq. (8) in different versions of MI. Here MI-PL indicates mixup inference with predicted label; MI-OL indicates mixup inference with other labels. MI-PL MI-OL z= 0 z= 1 z= 0 z= 1 Fy(x) 1 1 +Gy(δ; x0) 1 1 +Gy(δ; x0) Fy(˜x) 1 λ+ Gy(λδ; ˜x0) λ λ+ (1−λ) ·1y=ys + Gy(λδ; ˜x0) Fˆy(x) 1 Gˆy(δ; x0) 1 Gˆy(δ; x0) Fˆy(˜x) 1 (1 −λ) +Gˆy(λδ; ˜x0) λ Gˆy(λδ; ˜x0) value on the true label yincreases while it on the adversarial label ˆydecreases after performing MI when the input is adversarial (z= 1). This can be formally denoted as ∆Fy(x; ps)|z=1 >0; ∆Fˆy(x; ps)|z=1 <0. (10) We refer to this condition in Eq. (10) as robustness improving condition (RIC). Further, in the detection-purpose setting where we want to detect the hidden variable zand ﬁlter out adversarial inputs, we can take the gap of the ˆy-th component of predictions before and after the MI operation, i.e., ∆Fˆy(x; ps) as the detection metric (Pang et al., 2018a). To formally measure the detection ability on z, we use the detection gap (DG), denoted as DG = ∆Fˆy(x; ps)|z=1 −∆Fˆy(x; ps)|z=0. (11) A higher value of DG indicates that ∆Fˆy(x; ps) is better as a detection metric. In the following sections, we speciﬁcally analyze the properties of different versions of MI according to Table 1, and we will see that the MI methods can be used and beneﬁt in different defense strategies. 3.2.1 M IXUP INFERENCE WITH PREDICTED LABEL In the MI-PL case, when the input is clean (i.e., z = 0 ), there is F(x) = F(˜x), which means ideally the MI-PL operation does not inﬂuence the predictions on the clean inputs. When the input is adversarial (i.e., z= 1), MI-PL can be applied as a general-purpose defense or a detection-purpose defense, as we separately introduce below: General-purpose defense:If MI-PL can improve the general-purpose robustness, it should satisfy RIC in Eq. (10). By simple derivation and the results of Table 1, this means that Exs∼ps(x|ˆy) [Gk(δ; x0) −Gk(λδ; ˜x0)] {>1 −λ, if k= ˆy, <λ −1, if k= y. (12) Since an adversarial perturbation usually suppress the predicted conﬁdence on the true label and pro- mote it on the target label (Goodfellow et al., 2015), there should beGˆy(δ; ˜x0) >0 and Gy(δ; ˜x0) <0. Note that the left part of Eq. (12) can be decomposed into Exs∼ps(x|ˆy) [Gk(δ; x0) −Gk(δ; ˜x0)]   input transfer + Exs∼ps(x|ˆy) [Gk(δ; ˜x0) −Gk(λδ; ˜x0)]   perturbation shrinkage . (13) Here Eq. (13) indicates the two basic mechanisms of the MI operations defending adversarial attacks, as shown in Fig. 1(c). The ﬁrst mechanism is input transfer, i.e., the clean input that the adversarial perturbation acts on transfers from the deterministic x0 to stochastic ˜x0. Compared to the Gaussian noise or different image processing methods which introduce spatially or semantically local randomness, the stochastic ˜x0 induces spatially global and semantically diverse randomness. This will make it harder to perform an adaptive attack in the white-box setting (Athalye et al., 2018). The second mechanism isperturbation shrinkage, where the original perturbationδshrinks by a factor λ. This equivalently shrinks the perturbation threshold since ∥λδ∥p = λ∥δ∥p ≤λϵ, which means that MI generally imposes a tighter upper bound on the potential attack ability for a crafted perturbation. Besides, empirical results in previous work also show that a smaller perturbation threshold largely weakens the effect of attacks (Kurakin et al., 2018). Therefore, if an adversarial attack defended by these two mechanisms leads to a prediction degradation as in Eq. (12), then applying MI-PL would improve the robustness against this adversarial attack. Similar properties also hold for MI-OL as described in Sec. 3.2.2. In Fig. 2, we empirically demonstrate that most of the existing adversarial attacks, e.g., the PGD attack (Madry et al., 2018) satisﬁes these properties. 5Published as a conference paper at ICLR 2020 MI-OL 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 !\"#$ !\"%#$ !\"%# !\"# −'\"(;#$*−1,−1−'\"*(;%#$ * !\" 1,−1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 * !-\" !-\"#$ !-\"# 1,−1 '-\"(;#$ !-\"%#$ !-\"%#*−1,−1+'-\"*(;%#$ MI-PL 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 * !\" !\"#$ !\"# −'\"(;#$ !\"%#$ !\"%# 1−*−'\"*(;%#$ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 * !-\" !-\"#$ !-\"# '-\"(;#$ !-\"%#$ !-\"%# 1−*+'-\"*(;%#$ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 * ∆' (1−*)(,−2),−1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 * ∆' 1−* Clean inputsAdversarial inputs∆'\"='\"*(;%#$−'\"(;#$ ∆'-\"='-\"(;#$−'-\"*(;%#$ Figure 2: The results are averaged on 100 randomly test clean samples of CIFAR-10. The adversarial attack is untargeted PGD-10. Note that the ∆Gy calculated here is the minus value of it in Eq. (12) and Eq. (15). Detection-purpose defense:According to Eq. (11), the formula of DG for MI-PL is DGMI-PL = Exs∼ps(x|ˆy)[Gˆy(δ; x0) −Gˆy(λδ; ˜x0)] −(1 −λ). (14) By comparing Eq. (12) and Eq. (14), we can ﬁnd that they are consistent with each other, which means that for a given adversarial attack, if MI-PL can better defend it in general-purpose, then ideally MI-PL can also better detect the crafted adversarial examples. 3.2.2 M IXUP INFERENCE WITH OTHER LABELS As to MI-OL, when the input is clean (z= 0), there would be a degeneration on the optimal clean prediction as Fy(˜x) = Fˆy(˜x) = λ, since the sampled xs does not come from the true label y. As compensation, MI-OL can better improve robustness compared to MI-PL when the input is adversarial (z= 1), since the sampled xs also does not come from the adversarial label ˆyin this case. General-purpose defense:Note that in the MI-OL formulas of Table 1, there is a term of 1y=ys. Since we uniformly select ys from the set [L] \\{ˆy}, there is E(1y=ys) = 1 L−1 . According to the RIC, MI-OL can improve robustness against the adversarial attacks if there satisﬁes Eys∼Uˆy(y)Exs∼ps(x|ys) [Gk(δ; x0) −Gk(λδ; ˜x0)] { >0, if k= ˆy, < (λ−1)(L−2) L−1 , if k= y. (15) Note that the conditions in Eq. (15) is strictly looser than Eq. (12), which means MI-OL can defend broader range of attacks than MI-PL, as veriﬁed in Fig. 2. Detection-purpose defense:According to Eq. (11) and Table 1, the DG for MI-OL is DGMI-OL = Eys∼Uˆy(y)Exs∼ps(x|ys)[Gˆy(δ; x0) −Gˆy(λδ; ˜x0)] −(1 −λ). (16) It is interesting to note thatDGMI-PL = DGMI-OL, thus the two variants of MI have the same theoretical performance in the detection-purpose defenses. However, in practice we ﬁnd that MI-PL performs better than MI-OL in detection, since empirically mixup-trained models cannot induce ideal global linearity (cf. Fig. 2 in Zhang et al. (2018)). Besides, according to Eq. (6), to statistically make sure that the clean inputs will be correctly classiﬁed after MI-OL, there should be ∀k∈[L] \\{y}, Eys∼Uˆy(y)Exs∼ps(x|ys)[Fy −Fk] >0 =⇒λ>L −1. (17) 4 E XPERIMENTS In this section, we provide the experimental results on CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) to demonstrate the effectiveness of our MI methods on defending adversarial attacks. Our codes are available at https://github.com/P2333/Mixup-Inference. 6Published as a conference paper at ICLR 2020 Table 2: Classiﬁcation accuracy (%) on the oblivious adversarial examples crafted on 1,000 randomly sampled test points of CIFAR-10. Perturbation ϵ= 8/255 with step size 2/255. The subscripts indicate the number of iteration steps when performing attacks. The notation ≤1 represents accuracy less than 1%. The parameter settings for each method can be found in Table 4. Untargeted Mode Targeted Mode Methods Cle. PGD10 PGD50 PGD200 PGD10 PGD50 PGD200 Mixup 93.8 3.6 3.2 3.1 ≤1 ≤1 ≤1 Mixup + Gaussian noise 84.4 13.5 9.6 8.8 37.7 28.6 27.9 Mixup + Random rotation 82.0 21.8 18.7 18.2 38.9 32.5 26.5 Mixup + Xie et al. (2018) 82.1 23.0 19.6 19.1 38.4 31.1 25.2 Mixup + Guo et al. (2018) 83.3 31.2 28.8 28.3 57.8 49.1 48.9 ERM + MI-OL (ablation study) 81.6 7.4 6.4 6.1 33.0 26.7 23.2 Mixup + MI-OL 83.9 26.1 18.8 18.3 55.6 51.2 50.8 Mixup + MI-Combined 82.9 33.7 31.0 30.7 56.1 49.7 49.4 Interpolated AT 89.7 46.7 43.5 42.5 65.6 62.5 61.9 Interpolated AT + Gaussian noise 84.7 55.6 53.7 53.5 70.1 69.1 69.0 Interpolated AT + Random rotation 83.4 57.8 56.7 55.9 69.8 68.2 67.4 Interpolated AT + Xie et al. (2018) 82.1 59.7 58.4 57.9 71.1 69.7 69.3 Interpolated AT + Guo et al. (2018) 83.9 60.9 60.7 60.3 73.2 72.1 71.6 AT + MI-OL (ablation study) 81.2 56.2 55.8 55.1 67.7 67.2 66.4 Interpolated AT + MI-OL 84.2 64.5 63.8 63.3 75.3 74.7 74.5 0.23 0.78 0.08 0.72 0.05 0.88 0.01 0.84 00.10.20.30.40.50.60.70.80.91 MixupMixup + MI-PLPGD-10 (untargeted)PGD-50 (untargeted)PGD-10 (targeted)PGD-50 (targeted) Random guess (a) AUC scores 0 10 20 30 40 50 60 70 80 90 100 Accuracy on clean examples (%) 0 5 10 15 20 25 30 35 40 45 50Accuracy on adversarial examples (%) mixup + Gaussian noise mixup + Rotation mixup + Xie et al. (2018) mixup + Guo et al. (2018) ERM + MI-OL mixup + MI-OL mixup + MI-Combined (b) Adversarial accuracy w.r.t clean accuracy Figure 3: Results on CIFAR-10. (a) AUC scores on 1,000 randomly selected test clean samples and 1,000 adversarial counterparts crafted on these clean samples. (b) The adversarial accuracy w.r.t clean accuracy on 1,000 randomly selected test samples. The adversarial attack is untargeted PGD-10, with ϵ= 8/255 and step size 2/255. Each point for a certain method corresponds to a set of hyperparameters. 4.1 S ETUP In training, we use ResNet-50 (He et al., 2016) and apply the momentum SGD optimizer (Qian, 1999) on both CIFAR-10 and CIFAR-100. We run the training for200 epochs with the batch size of 64. The initial learning rate is 0.01 for ERM, mixup and AT; 0.1 for interpolated AT (Lamb et al., 2019). The learning rate decays with a factor of 0.1 at 100 and 150 epochs. The attack method for AT and interpolated AT is untargeted PGD-10 with ϵ = 8/255 and step size 2/255 (Madry et al., 2018), and the ratio of the clean examples and the adversarial ones in each mini-batch is 1 : 1 (Lamb et al., 2019). The hyperparameter αfor mixup and interpolated AT is1.0 (Zhang et al., 2018). All defenses with randomness are executed 30 times to obtain the averaged predictions (Xie et al., 2018). 4.2 E MPIRICAL VERIFICATION OF THEORETICAL ANALYSES To verify and illustrate our theoretical analyses in Sec. 3, we provide the empirical relationship between the output predictions of MI and the hyperparameter λin Fig. 2. The notations and formulas annotated in Fig. 2 correspond to those introduced in Sec. 3. We can see that the results follow our theoretical conclusions under the assumption of ideal global linearity. Besides, both MI-PL and MI-OL empirically satisfy RIC in this case, which indicates that they can improve robustness under the untargeted PGD-10 attack on CIFAR-10, as quantitatively demonstrated in the following sections. 7Published as a conference paper at ICLR 2020 Table 3: Classiﬁcation accuracy (%) on the oblivious adversarial examples crafted on 1,000 randomly sampled test points of CIFAR-100. Perturbation ϵ= 8/255 with step size 2/255. The subscripts indicate the number of iteration steps when performing attacks. The notation ≤1 represents accuracy less than 1%. The parameter settings for each method can be found in Table 5. Untargeted Mode Targeted Mode Methods Cle. PGD10 PGD50 PGD200 PGD10 PGD50 PGD200 Mixup 74.2 5.5 5.3 5.2 ≤1 ≤1 ≤1 Mixup + Gaussian noise 65.0 5.5 5.3 5.3 10.0 4.3 4.1 Mixup + Random rotation 66.2 7.8 6.7 6.3 21.4 15.5 15.2 Mixup + Xie et al. (2018) 66.3 9.6 7.6 7.4 30.2 22.5 22.3 Mixup + Guo et al. (2018) 66.1 13.1 10.8 10.5 33.3 26.3 26.1 Mixup + MI-OL 68.8 12.6 9.4 9.1 37.0 29.0 28.7 Mixup + MI-Combined 67.0 14.8 11.7 11.3 31.4 26.9 26.7 Interpolated AT 64.7 26.6 24.1 24.0 52.0 50.1 49.8 Interpolated AT + Gaussian noise 60.4 32.6 31.6 31.4 50.1 50.0 49.6 Interpolated AT + Random rotation 62.6 34.5 32.4 32.1 51.0 49.9 49.7 Interpolated AT + Xie et al. (2018) 62.1 42.2 41.5 41.3 57.1 56.3 55.8 Interpolated AT + Guo et al. (2018) 61.5 36.2 33.7 33.3 53.8 52.4 52.2 Interpolated AT + MI-OL 62.0 43.8 42.8 42.5 58.1 56.7 56.5 4.3 P ERFORMANCE UNDER OBLIVIOUS ATTACKS In this subsection, we evaluate the performance of our method under the oblivious-box attacks (Carlini & Wagner, 2017). The oblivious threat model assumes that the adversary is not aware of the existence of the defense mechanism, e.g., MI, and generate adversarial examples based on the unsecured classiﬁcation model. We separately apply the model trained by mixup and interpolated AT as the classiﬁcation model. The AUC scores for the detection-purpose defense are given in Fig. 3(a). The results show that applying MI-PL in inference can better detect adversarial attacks, while directly detecting by the returned conﬁdence without MI-PL performs even worse than a random guess. We also compare MI with previous general-purpose defenses applied in the inference phase, e.g., adding Gaussian noise or random rotation (Tabacof & Valle, 2016); performing random padding or resizing after random cropping (Guo et al., 2018; Xie et al., 2018). The performance of our method and baselines on CIFAR-10 and CIFAR-100 are reported in Table 2 and Table 3, respectively. Since for each defense method, there is a trade-off between the accuracy on clean samples and adversarial samples depending on the hyperparameters, e.g., the standard deviation for Gaussian noise, we carefully select the hyperparameters to ensure both our method and baselineskeep a similar performance on clean data for fair comparisons. The hyperparameters used in our method and baselines are reported in Table 4 and Table 5. In Fig. 3(b), we further explore this trade-off by grid searching the hyperparameter space for each defense to demonstrate the superiority of our method. As shown in these results, our MI method can signiﬁcantly improve the robustness for the trained mod- els with induced global linearity, and is compatible with training-phase defenses like the interpolated AT method. As a practical strategy, we also evaluate a variant of MI, calledMI-Combined, which applies MI-OL if the input is detected as adversarial by MI-PL with a default detection threshold; otherwise returns the prediction on the original input. We also perform ablation studies of ERM / AT + MI-OL in Table 2, where no global linearity is induced. The results verify that our MI methods indeed exploit the global linearity of the mixup-trained models, rather than simply introduce randomness. 4.4 P ERFORMANCE UNDER WHITE -BOX ADAPTIVE ATTACKS Following Athalye et al. (2018), we test our method under the white-box adaptive attacks (detailed in Appendix B.2). Since we mainly adopt the PGD attack framework, which synthesizes adversarial examples iteratively, the adversarial noise will be clipped to make the input image stay within the valid range. It results in the fact that with mixup on different training examples, the adversarial perturbation will be clipped differently. To address this issue, we average the generated perturbations over the adaptive samples as the ﬁnal perturbation. The results of the adversarial accuracy w.r.t the number of adaptive samples are shown in Fig. 4. We can see that even under a strong adaptive attack, equipped with MI can still improve the robustness for the classiﬁcation models. 8Published as a conference paper at ICLR 2020 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Mixup + MI-OL Interpolated AT + MI-OL Adversarial accuracyNumber of adaptive samplesNumber of adaptive samplesNumber of adaptive samplesNumber of adaptive samples Untargetedmode UntargetedmodeTargetedmode 0 5 10 15 20 25 300 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Targetedmode Adaptive PGD-10Adaptive PGD-50 Adaptive PGD-200 Figure 4: Classiﬁcation accuracy under the adaptive PGD attacks on CIFAR-10. The number of adaptive samples refers to the execution times of sampling xs in each iteration step of adaptive PGD. The dash lines are the accuracy of trained models without MI-OL under PGD attacks. 5 C ONCLUSION In this paper, we propose the MI method, which is specialized for the trained models with globally linear behaviors induced by, e.g., mixup or interpolated AT. As analyzed in Sec. 3, MI can exploit this induced global linearity in the inference phase to shrink and transfer the adversarial perturbation, which breaks the locality of adversarial attacks and alleviate their aggressivity. In experiments, we empirically verify that applying MI can return more reliable predictions under different threat models. ACKNOWLEDGEMENTS This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, U19B2034, U1811461), Beijing NSF Project (No. L172037), Beijing Academy of Artiﬁcial Intelligence (BAAI), Tsinghua-Huawei Joint Research Program, a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute for Intelligent Computing, the JP Morgan Faculty Research Program and the NVIDIA NV AIL Program with GPU/DGX Acceleration. REFERENCES Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning (ICML), 2018. Christopher Beckham, Sina Honari, Alex Lamb, Vikas Verma, Farnoosh Ghadiri, R Devon Hjelm, and Christopher Pal. Adversarial mixup resynthesizers. In Advances in Neural Information Processing Systems (NeurIPS), 2019. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In ACM Workshop on Artiﬁcial Intelligence and Security (AISec), 2017. Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW), pp. 1–7. IEEE, 2018. Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019. Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. In International Conference on Machine Learning (ICML), 2018. Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 9Published as a conference paper at ICLR 2020 Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a translation sufﬁce: Fooling cnns with simple transformations. In International Conference on Machine Learning (ICML), 2019. Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classi- ﬁers: from adversarial to random noise. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1632–1640, 2016. Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classiﬁer. In Advances in Neural Information Processing Systems (NeurIPS), 2018. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org. Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR), 2015. Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial images using input transformations. In International Conference on Learning Representations (ICLR), 2018. Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regular- ization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), volume 33, pp. 3714–3722, 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), pp. 630–645. Springer, 2016. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989. Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017. Hiroshi Inoue. Data augmentation by pairing samples for images classiﬁcation. arXiv preprint arXiv:1801.02929, 2018. Di Jin, Zhijing Jin, Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural language attack on text classiﬁcation and entailment. arXiv preprint arXiv:1907.11932, 2019. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In The International Conference on Learning Representations (ICLR) Workshops, 2017. Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, et al. Adversarial attacks and defences competition. arXiv preprint arXiv:1804.00097, 2018. Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training: Achieving robust neural networks without sacriﬁcing accuracy. arXiv preprint arXiv:1906.06784, 2019. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018. Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 427–436, 2015. Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial examples. In Advances in Neural Information Processing Systems (NeurIPS) , pp. 4579–4589, 2018a. 10Published as a conference paper at ICLR 2020 Tianyu Pang, Chao Du, and Jun Zhu. Max-mahalanobis linear discriminant analysis networks. In International Conference on Machine Learning (ICML), 2018b. Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1): 145–151, 1999. Edward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Barrage of random transforms for adversarially robust defense. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6528–6537, 2019. Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in Neural Information Processing Systems (NeurIPS), 2019. Takuya Shimada, Shoichiro Yamaguchi, Kohei Hayashi, and Sosuke Kobayashi. Data interpolating prediction: Alternative interpretation of mixup. arXiv preprint arXiv:1906.08412, 2019. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014. Pedro Tabacof and Eduardo Valle. Exploring the space of adversarial images. In 2016 International Joint Conference on Neural Networks (IJCNN), pp. 426–433. IEEE, 2016. Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Learning from between-class examples for deep sound recognition. International Conference on Learning Representations (ICLR), 2018a. Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classi- ﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5486–5494, 2018b. Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013. Vikas Verma, Alex Lamb, Christopher Beckham, Aaron Courville, Ioannis Mitliagkis, and Yoshua Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer. In International Conference on Machine Learning (ICML), 2019a. Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for semi-supervised learning. arXiv preprint arXiv:1903.03825, 2019b. Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. In International Conference on Learning Representations (ICLR), 2018. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand- ing deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR), 2017. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning (ICML), 2019. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations (ICLR), 2018. 11Published as a conference paper at ICLR 2020 A M ORE BACKGROUNDS In this section, we provide more backgrounds which are related to our work in the main text. A.1 A DVERSARIAL ATTACKS AND THREAT MODELS Adversarial attacks.Although deep learning methods have achieved substantial success in different domains (Goodfellow et al., 2016), human imperceptible adversarial perturbations can be easily crafted to fool high-performance models, e.g., deep neural networks (DNNs) (Nguyen et al., 2015). One of the most commonly studied adversarial attack is the projected gradient descent (PGD) method (Madry et al., 2018). Let rbe the number of iteration steps, x0 be the original clean example, then PGD iteratively crafts the adversarial example as x∗ i = clipx,ϵ(x∗ i−1 + ϵi ·sign(∇x∗ i−1L(x∗ i−1,y))), (18) where clipx,ϵ(·) is the clipping function. Here x∗ 0 is a randomly perturbed image in the neighborhood of x0, i.e., ˚U(x0,ϵ), and the ﬁnally returned adversarial example is x= x∗ r = x0 + δ, following our notations in the main text. Threat models.Here we introduce different threat models in the adversarial setting. As suggested in Carlini et al. (2019), a threat model includes a set of assumptions about the adversarys goals, capabilities, and knowledge. Adversary’s goals could be simply fooling the classiﬁers to misclassify, which is referred to as untargeted mode. Alternatively, the goals can be more speciﬁc to make the model misclassify certain examples from a source class into a target class, which is referred to as targeted mode. In our experiments, we evaluate under both modes, as shown in Table 2 and Table 3. Adversary’s capabilities describe the constraints imposed on the attackers. Adversarial examples require the perturbation δto be bounded by a small threshold ϵunder ℓp-norm, i.e., ∥δ∥p ≤ϵ. For example, in the PGD attack, we consider under the ℓ∞-norm. Adversary’s knowledge describes what knowledge the adversary is assumed to have. Typically, there are three settings when evaluating a defense method: •Oblivious adversaries are not aware of the existence of the defense Dand generate adver- sarial examples based on the unsecured classiﬁcation model F (Carlini & Wagner, 2017). •White-box adversaries know the scheme and parameters of D, and can design adaptive methods to attack both the model F and the defense Dsimultaneously (Athalye et al., 2018). •Black-box adversaries have no access to the parameters of the defense Dor the model F with varying degrees of black-box access (Dong et al., 2018). In our experiments, we mainly test under the oblivious setting (Sec. 4.3) and white-box setting (Sec. 4.4), since previous work has already demonstrated that randomness itself is efﬁcient on defending black-box attacks (Guo et al., 2018; Xie et al., 2018). A.2 I NTERPOLATED ADVERSARIAL TRAINING To date, the most widely applied framework for adversarial training (AT) methods is the saddle point framework introduced in Madry et al. (2018): min θ ρ(θ), where ρ(θ) = E(x,y)∼p[max δ∈S L(x+ δ,y; θ)]. (19) Here θrepresents the trainable parameters in the classiﬁer F, and Sis a set of allowed perturbations. In implementation, the inner maximization problem for each input-label pair (x,y) is approximately solved by, e.g., the PGD method with different random initialization (Madry et al., 2018). As a variant of the AT method, Lamb et al. (2019) propose the interpolated AT method, which combines AT with mixup. Interpolated AT trains on interpolations of adversarial examples along with interpolations of unperturbed examples (cf. Alg. 1 in Lamb et al. (2019)). Previous empirical results demonstrate that interpolated AT can obtain higher accuracy on the clean inputs compared to the AT method without mixup, while keeping the similar performance of robustness. 12Published as a conference paper at ICLR 2020 Table 4: The parameter settings for the methods in Table 2. The number of execution for each random method is 30. Methods Parameter Settings Mixup - Mixup + Gaussian noise Noise standard deviation σ= 0.04 Mixup + Random rotation Rotation degree range [−40◦,40◦] Mixup + Xie et al. (2018) The random crop size is randomly selected from [16,24] Mixup + Guo et al. (2018) The random crop size is randomly selected from [22,30] ERM + MI-OL (ablation study) The λOL = 0.6 Mixup + MI-OL The λOL = 0.5 Mixup + MI-Combined The λOL = 0.5, λOL = 0.4, threshold is 0.2 Interpolated AT - Interpolated AT + Gaussian noise Noise standard deviation σ= 0.075 Interpolated AT + Random rotation Rotation degree range [−30◦,30◦] Interpolated AT + Xie et al. (2018) The random crop size is randomly selected from [20,28] Interpolated AT + Guo et al. (2018) The random crop size is randomly selected from [20,28] AT + MI-OL (ablation study) The λOL = 0.8 Interpolated AT + MI-OL The λOL = 0.6 B T ECHNICAL DETAILS We provide more technical details about our method and the implementation of the experiments. B.1 M ORE DISCUSSION ON THE MI METHOD Generality. According to Sec. 3, except for the mixup-trained models, the MI method is generally compatible with any trained model with induced global linearity. These models could be trained by other methods, e.g., manifold mixup (Verma et al., 2019a; Inoue, 2018; Lamb et al., 2019). Besides, to better defend white-box adaptive attacks, the mixup ratio λin MI could also be sampled from certain distribution to put in additional randomness. Empirical gap.As demonstrated in Fig. 2, there is a gap between the empirical results and the theo- retical formulas in Table 1. This is because that the mixup mechanism mainly acts as a regularization in training, which means the induced global linearity may not satisfy the expected behaviors. To improve the performance of MI, a stronger regularization can be imposed, e.g., training with mixup for more epochs, or applying matched λboth in training and inference. B.2 A DAPTIVE ATTACKS FOR MIXUP INFERENCE Following Athalye et al. (2018), we design the adaptive attacks for our MI method. Speciﬁcally, according to Eq. (6), the expected model prediction returned by MI is: FMI(x) = Eps[F(λx+ (1 −λ)xs)]. (20) Note that generally the λ in MI comes from certain distribution. For simplicity, we ﬁx λ as a hyperparameter in our implementation. Therefore, the gradients of the prediction w.r.t. the input xis: ∂FMI(x) ∂x = Eps [∂F(λx+ (1 −λ)xs) ∂x ] (21) = Eps [∂F(u) ∂u ⏐⏐⏐ u=λx+(1−λ)xs ·∂λx+ (1 −λ)xs ∂x ] (22) = λEps [∂F(u) ∂u |u=λx+(1−λ)xs ] . (23) 13Published as a conference paper at ICLR 2020 Table 5: The parameter settings for the methods in Table 3. The number of execution for each random method is 30. Methods Parameter Settings Mixup - Mixup + Gaussian noise Noise standard deviation σ= 0.025 Mixup + Random rotation Rotation degree range [−20◦,20◦] Mixup + Xie et al. (2018) The random crop size is randomly selected from [18,26] Mixup + Guo et al. (2018) The random crop size is randomly selected from [24,32] Mixup + MI-OL The λOL = 0.5 Mixup + MI-Combined The λOL = 0.5, λOL = 0.4, threshold is 0.2 Interpolated AT - Interpolated AT + Gaussian noise Noise standard deviation σ= 0.06 Interpolated AT + Random rotation Rotation degree range [−20◦,20◦] Interpolated AT + Xie et al. (2018) The random crop size is randomly selected from [22,30] Interpolated AT + Guo et al. (2018) The random crop size is randomly selected from [24,32] Interpolated AT + MI-OL The λOL = 0.6 Clean Adversarial Figure 5: Adversarial examples crafted by adaptive attacks with ϵ= 16/255 on CIFAR-10, against the defense of Interpolated AT + MI-OL. In the implementation of adaptive PGD attacks, we ﬁrst sample a series of examples {xs,k}NA k=1, where NA is the number of adaptive samples in Fig. 3. Then according to Eq. (18), the sign of gradients used in adaptive PGD can be approximated by sign (∂FMI(x) ∂x ) ≈sign (NA∑ k=1 ∂F(u) ∂u ⏐⏐⏐ u=λx+(1−λ)xs,k ) . (24) B.3 H YPERPARAMETER SETTINGS The hyperparameter settings of the experiments shown in Table 2 and Table 3 are provided in Table 4 and Table 5, respectively. Since the original methods in Xie et al. (2018) and Guo et al. (2018) are both designed for the models on ImageNet, we adapt them for CIFAR-10 and CIFAR-100. Most of our experiments are conducted on the NVIDIA DGX-1 server with eight Tesla P100 GPUs. 14",
      "references": [
        "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
        "Adversarial mixup resynthesizers",
        "Mixmatch: A holistic approach to semi-supervised learning",
        "Adversarial examples are not easily detected: Bypassing ten detection methods",
        "Audio adversarial examples: Targeted attacks on speech-to-text",
        "On evaluating adversarial robustness",
        "Adversarial attack on graph structured data",
        "Boosting adversarial attacks with momentum",
        "A rotation and a translation suffice: Fooling cnns with simple transformations",
        "Robustness of classifiers: from adversarial to random noise",
        "Adversarial vulnerability for any classifier",
        "Deep Learning",
        "Explaining and harnessing adversarial examples",
        "Countering adversarial images using input transformations",
        "Mixup as locally linear out-of-manifold regularization",
        "Identity mappings in deep residual networks",
        "Multilayer feedforward networks are universal approximators",
        "Adversarial attacks on neural network policies",
        "Data augmentation by pairing samples for images classification",
        "Is bert really robust? natural language attack on text classification and entailment",
        "Learning multiple layers of features from tiny images",
        "Adversarial examples in the physical world",
        "Adversarial attacks and defences competition",
        "Interpolated adversarial training: Achieving robust neural networks without sacrificing accuracy",
        "Towards deep learning models resistant to adversarial attacks",
        "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
        "Towards robust detection of adversarial examples",
        "Max-mahalanobis linear discriminant analysis networks",
        "On the momentum term in gradient descent learning algorithms",
        "Barrage of random transforms for adversarially robust defense",
        "Adversarial training for free!",
        "Data interpolating prediction: Alternative interpretation of mixup",
        "Intriguing properties of neural networks",
        "Exploring the space of adversarial images",
        "Learning from between-class examples for deep sound recognition",
        "Between-class learning for image classification",
        "The nature of statistical learning theory",
        "Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer",
        "Interpolation consistency training for semi-supervised learning",
        "Mitigating adversarial effects through randomization",
        "Understanding deep learning requires rethinking generalization",
        "mixup: Beyond empirical risk minimization",
        "Theoretically principled trade-off between robustness and accuracy"
      ],
      "meta_data": {
        "arxiv_id": "1909.11515v2",
        "authors": [
          "Tianyu Pang",
          "Kun Xu",
          "Jun Zhu"
        ],
        "published_date": "2019-09-25T14:21:55Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Mixup Inference (MI), an inference-time defense tailored for models trained with mixup or its variants, which actively exploits the induced global linearity to reduce (shrink) and relocate (transfer) adversarial perturbations. Provides two practical variants (MI-PL and MI-OL) plus a combined detection/defense scheme, theoretical analysis of why MI improves robustness, and empirical evidence on CIFAR-10/100 showing significant gains over existing test-time defenses and compatibility with interpolated adversarial training.",
        "methodology": "At inference, an input image x is linearly combined with a randomly sampled clean image xs: x̃ = λx + (1-λ)xs. The classifier, trained with mixup, is run N times with different xs and the softmax outputs are averaged. Two strategies choose xs: (1) MI-PL draws xs from the predicted class, (2) MI-OL draws from other classes. The linearity of mixup-trained networks ensures that the true signal scales with λ while the adversarial noise scales with λ, effectively shrinking perturbations; random xs relocates the noise, hindering adaptive attacks. Formal analysis derives robustness-improving conditions and detection metrics. The same mechanism is compatible with interpolated adversarial training models.",
        "experimental_setup": "Models: ResNet-50 trained on CIFAR-10 and CIFAR-100 using (a) standard mixup (α=1) and (b) interpolated adversarial training (PGD-10, ϵ=8/255). Training: 200 epochs, batch 64, SGD with momentum, learning-rate decay. Evaluation: (i) Oblivious PGD attacks (untargeted & targeted, ϵ=8/255, step size 2/255, 10/50/200 steps). (ii) White-box adaptive PGD that differentiates through MI using multiple adaptive samples. Metrics: classification accuracy on clean and adversarial examples, and AUC for adversarial detection. Baseline test-time defenses include Gaussian noise, random rotation, random resize-padding (Xie et al.), and random cropping (Guo et al.). Randomized defenses executed 30 times and averaged.",
        "limitations": "1. Effectiveness depends on the classifier exhibiting near-linear behavior; models trained without mixup (plain ERM) gain little. 2. Requires multiple stochastic forward passes, increasing inference cost and latency. 3. Choice of λ trades off clean accuracy versus robustness and needs tuning. 4. Experiments are restricted to small-scale vision datasets (CIFAR-10/100) and a single architecture; generalizability to large-scale data (ImageNet) or other domains is untested. 5. While MI improves robustness, adaptive white-box attacks still degrade accuracy, indicating residual vulnerability.",
        "future_research_directions": "• Extend MI to larger datasets (ImageNet) and other modalities such as NLP or speech.\n• Develop adaptive or learned schemes for selecting λ and the sampling distribution to balance robustness and accuracy with fewer passes.\n• Integrate MI with certified defenses or stronger adversarial training to obtain additive robustness.\n• Analyze theoretical robustness bounds and characterize conditions under which global linearity holds in practical networks.\n• Optimize computational efficiency, e.g., single-shot or ensemble-free approximations, for real-time applications.\n• Investigate MI in semi-supervised, self-supervised, or manifold mixup settings and assess benefits for other tasks like detection or regression.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity",
      "full_text": "Published as a conference paper at ICLR 2021 Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity Jang-Hyun Kim, Wonho Choo, Hosan Jeong, Hyun Oh Song Department of Computer Science and Engineering, Seoul National University Neural Processing Research Center {janghyun,wonho.choo,grazinglion,hyunoh}@mllab.snu.ac.kr Abstract While deep neural networks show great performance on ﬁtting to the training distribution, improving the networks’ generalization performance to the test distribution and robustness to the sensitivity to input perturbations still remain as a challenge. Although a number of mixup based augmentation strategies have been proposed to partially address them, it remains unclear as to how to best utilize the supervisory signal within each input data for mixup from the optimization perspective. We propose a new perspective on batch mixup and formulate the optimal construction of a batch of mixup data maximizing the data saliency measure of each individual mixup data and encouraging the supermodular diversity among the constructed mixup data. This leads to a novel discrete optimization problem minimizing the diﬀerence between submodular functions. We also propose an eﬃcient modular approximation based iterative submodular minimization algorithm for eﬃcient mixup computation per each minibatch suitable for minibatch based neural network training. Our experiments show the proposed method achieves the state of the art generalization, calibration, and weakly super- vised localization results compared to other mixup methods. The source code is available athttps://github.com/snu-mllab/Co-Mixup . 1 Introduction Deep neural networks have been applied to a wide range of artiﬁcial intelligence tasks such as computer vision, natural language processing, and signal processing with remarkable performance (Ren et al., 2015; Devlin et al., 2018; Oord et al., 2016). However, it has been shown that neural networks have excessive representation capability and can even ﬁt random data (Zhang et al., 2016). Due to these characteristics, the neural networks can easily overﬁt to training data and show a large generalization gap when tested on previously unseen data. To improve the generalization performance of the neural networks, a body of research has been proposed to develop regularizers based on priors or to augment the training data with task-dependent transforms (Bishop, 2006; Cubuk et al., 2019). Recently, a new task- independent data augmentation technique, calledmixup, has been proposed (Zhang et al., 2018). The original mixup, calledInput Mixup, linearly interpolates a given pair of input data and can be easily applied to various data and tasks, improving the generalization performance and robustness of neural networks. Other mixup methods, such asmanifold mixup(Verma et al., 2019) orCutMix (Yun et al., 2019), have also been proposed addressing diﬀerent ways to mix a given pair of input data.Puzzle Mix (Kim et al., 2020) utilizes saliency information and local statistics to ensure mixup data to have rich supervisory signals. However, these approaches only consider mixing a given random pair of input data and do not fully utilize the rich informative supervisory signal in training data including collection of object saliency, relative arrangement, etc. In this work, we simultaneously consider mix- matching diﬀerent salient regions among all input data so that each generated mixup example accumulates as many salient regions from multiple input data as possible while ensuring Correspondence to: Hyun Oh Song. 1 arXiv:2102.03065v1  [cs.LG]  5 Feb 2021Published as a conference paper at ICLR 2021 Input batch Input Mixup CutMix Puzzle Mix Co-Mixup Weasel (0.8) Deer (0.2) Bird (0.4)  Fish (0.6) Bird (0.4)  Dog (0.6) Shark (1.0)Weasel (0.6) Deer (0.4) Weasel (0.5) Deer (0.5) Random sampled pair Bird (0.2) Dog (0.3) Fish (0.5) Figure 1: Example comparison of existing mixup methods and the proposed Co-Mixup. We provide more samples in Appendix H. diversity among the generated mixup examples. To this end, we propose a novel optimization problem that maximizes the saliency measure of each individual mixup example while encouraging diversity among them collectively. This formulation results in a novel discrete submodular-supermodular objective. We also propose a practical modular approximation method for the supermodular term and present an eﬃcient iterative submodular minimization algorithm suitable for minibatch-based mixup for neural network training. As illustrated in the Figure 1, while the proposed method,Co-Mixup, mix-matches the collection of salient regions utilizing inter-arrangements among input data, the existing methods do not consider the saliency information (Input Mixup & CutMix) or disassemble salient parts (Puzzle Mix). We verify the performance of the proposed method by training classiﬁers on CIFAR-100, Tiny-ImageNet, ImageNet, and the Google commands dataset (Krizhevsky et al., 2009; Chrabaszcz et al., 2017; Deng et al., 2009; Warden, 2017). Our experiments show the models trained with Co-Mixup achieve the state of the performance compared to other mixup baselines. In addition to the generalization experiment, we conduct weakly-supervised object localization and robustness tasks and conﬁrm Co-Mixup outperforms other mixup baselines. 2 Related works Mixup Data augmentation has been widely used to prevent deep neural networks from over-ﬁtting to the training data (Bishop, 1995). The majority of conventional augmentation methods generate new data by applying transformations depending on the data type or the target task (Cubuk et al., 2019). Zhang et al. (2018) proposedmixup, which can be independently applied to various data types and tasks, and improves generalization and robustness of deep neural networks.Input mixup(Zhang et al., 2018) linearly interpolates between two input data and utilizes the mixed data with the corresponding soft label for training. Following this work,manifold mixup (Verma et al., 2019) applies the mixup in the hidden feature space, andCutMix (Yun et al., 2019) suggests a spatial copy and paste based mixup strategy on images. Guo et al. (2019) trains an additional neural network to optimize a mixing ratio.Puzzle Mix (Kim et al., 2020) proposes a mixup method based on saliency and local statistics of the given data. In this paper, we propose a discrete optimization-based mixup method simultaneously ﬁnding the best combination of collections of salient regions among all input data while encouraging diversity among the generated mixup examples. Saliency The seminal work from Simonyan et al. (2013) generates a saliency map using a pre-trained neural network classiﬁer without any additional training of the network. Following the work, measuring the saliency of data using neural networks has been studied to obtain a more precise saliency map (Zhao et al., 2015; Wang et al., 2015) or to reduce the saliency computation cost (Zhou et al., 2016; Selvaraju et al., 2017). The saliency information is widely applied to the tasks in various domains, such as object segmentation or speech recognition (Jung and Kim, 2011; Kalinli and Narayanan, 2007). 2Published as a conference paper at ICLR 2021 Submodular-Supermodular optimization A submodular (supermodular) function is a set function with diminishing (increasing) returns property (Narasimhan and Bilmes, 2005). It is known that any set function can be expressed as the sum of a submodular and supermodular function (Lovász, 1983), called BP function. Various problems in machine learning can be naturally formulated as BP functions (Fujishige, 2005), but it is known to be NP-hard (Lovász, 1983). Therefore, approximate algorithms based on modular approximations of submodular or supermodular terms have been developed (Iyer and Bilmes, 2012). Our formulation falls into a category of BP function consisting of smoothness function within a mixed output (submodular) and a diversity function among the mixup outputs (supermodular). 3 Preliminary Existing mixup methods return {h(x1,xi(1)),...,h (xm,xi(m))} for given input data {x1,...,x m}, where h : X×X →X is a mixup function and(i(1),...,i (m)) is a ran- dom permutation of the data indices. In the case of input mixup,h(x,x′) is λx+ (1 −λ)x′, where λ∈[0,1] is a random mixing ratio. Manifold mixup applies input mixup in the hidden feature space, and CutMix usesh(x,x′) = 1B ⊙x+ (1 −1B) ⊙x′, where1B is a binary rectangular-shape mask for an imagex and ⊙represents the element-wise product. Puzzle Mix deﬁnesh(x,x′) as z⊙Π⊺x+ (1 −z) ⊙Π′⊺x′, whereΠ is a transport plan andz is a discrete mask. In detail, forx∈Rn, Π ∈{0,1}n and z∈Ln for L= {l L |l= 0,1,...,L }. In this work, we extend the existing mixup functions ash : Xm →X m′ which performs mixup on a collection of input data and returns another collection. LetxB ∈Rm×n denote the batch of input data in matrix form. Then, our proposed mixup function is h(xB) = ( g(z1 ⊙xB),...,g (zm′⊙xB) ) , where zj ∈Lm×n for j = 1,...,m ′ with L= {l L |l = 0,1,...,L }and g : Rm×n →Rn returns a column-wise sum of a given matrix. Note that, thekth column ofzj, denoted as zj,k ∈Lm, can be interpreted as the mixing ratio amongm inputs at thekth location. Also, we enforce∥zj,k∥1 = 1 to maintain the overall statistics of the given input batch. Given the one-hot target labelsyB ∈{0,1}m×C of the input data withC classes, we generate soft target labels for mixup data asy⊺ B˜oj for j = 1,...,m ′, where ˜oj = 1 n ∑n k=1 zj,k ∈[0,1]m represents the input source ratio of thejth mixup data. We train models to estimate the soft target labels by minimizing the cross-entropy loss. 4 Method 4.1 Objective Saliency Our main objective is to maximize the saliency measure of mixup data while maintaining the local smoothness of data,i.e., spatially nearby patches in a natural image look similar, temporally adjacent signals have similar spectrum in speech, etc. (Kim et al., 2020). As we can see from CutMix in Figure 1, disregarding saliency can give a misleading supervisory signal by generating mixup data that does not match with the target soft label. While the existing mixup methods only consider the mixup between two inputs, we generalize the number of inputsm to any positive integer. Note, eachkth location of outputs hasm candidate sources from the inputs. We model the unary labeling cost as the negative value of the saliency, and denote the cost vector at thekth location asck ∈Rm. For the saliency measure, we calculate the gradient values of training loss with respect to the input and measure ℓ2 norm of the gradient values across input channels (Simonyan et al., 2013; Kim et al., 2020). Note that this method does not require any additional architecture dependent modules for saliency calculation. In addition to the unary cost, we encourage adjacent locations to have similar labels for the smoothness of each mixup data. In summary, the objective can be formulated as follows: m′ ∑ j=1 n∑ k=1 c⊺ kzj,k + β m′ ∑ j=1 ∑ (k,k′)∈N (1 −z⊺ j,kzj,k′) −η m′ ∑ j=1 n∑ k=1 log p(zj,k), 3Published as a conference paper at ICLR 2021 0.5 1.0 1.5 2.0 sum supermodular unary Diverse z∗ Salient but not salient but not diverseObjective value (a) 1 2 3 4 5 6 0 20 40 60 80 Number of mixed inputs Counts (b) small τ large τ 1.0 1.1 1.2 1.3 1.4 1.5 1 1 1 1.22 1.36 no-mix CutMix Co-Mixup Input PuzzleMix Batch saliency (c) 0 0.2 0.4 0.6 0.8 1.0 0.4 0.6 0.8 1.0 baselines τ Diversity (d) Figure 2: (a) Analysis of our BP optimization problem. The x-axis is a one-dimensional arrangement of solutions: The mixed output is more salient but not diverse towards the right and less salient but diverse on the left. The unary term (red) decreases towards the right side of the axis, while the supermodular term (green) increases. By optimizing the sum of the two terms (brown), we obtain the balanced outputz∗. (b) A histogram of the number of inputs mixed for each output given a batch of 100 examples from the ImageNet dataset. As τ increases, more inputs are used to create each output on average. (c) Mean batch saliency measurement of a batch of mixup data using the ImageNet dataset. We normalize the saliency measure of each input to sum up to 1. (d) Diversity measurement of a batch of mixup data. We calculate the diversity as1 −∑ j ∑ j′̸=j ˜o⊺ j˜oj′/m, where˜oj = oj/∥oj∥1. We can control the diversity among Co-Mixup data (red) and ﬁnd the optimum by controllingτ. where the prior p is given by zj,k ∼ 1 LMulti(L,λ) with λ = ( λ1,...,λ m) ∼ Dirichlet(α,...,α ), which is a generalization of the mixing ratio distribution of Zhang et al. (2018), andNdenotes a set of adjacent locations (i.e., neighboring image patches in vision, subsequent spectrums in speech, etc.). Diversity Note that the naive generalization above leads to the identical outputs because the objective is separable and identical for each output. In order to obtain diverse mixup outputs, we model a similarity penalty between outputs. First, we represent the input source information of thejth output by aggregating assigned labels as∑n k=1 zj,k. For simplicity, let us denote∑n k=1 zj,k as oj. Then, we measure the similarity betweenoj’s by using the inner-product onRm. In addition to the input source similarity between outputs, we model the compatibility between input sources, represented as a symmetric matrixAc ∈Rm×m + . Speciﬁcally, Ac[i1,i2] quantiﬁes the degree to which inputi1 and i2 are suitable to be mixed together. In summary, we use inner-product onA= (1 −ω)I+ ωAc for ω ∈[0,1], resulting in a supermodular penalty term. Note that, by minimizing⟨oj,oj′⟩A = o⊺ jAoj′, ∀j ̸= j′, we penalize output mixup examples with similar input sources and encourage each individual mixup examples to have high compatibility within. In this work, we measure the distance between locations of salient objects in each input and use the distance matrix Ac[i,j] = ∥argmaxksi[k] −argmaxksj[k]∥1, wheresi is the saliency map of theith input and k is a location index (e.g., k is a 2-D index for image data). From now on, we denote this inner-product term as thecompatibility term. Over-penalization The conventional mixup methods perform mixup as many as the number of examples in a given mini-batch. In our setting, this is the case whenm= m′. However, the compatibility penalty between outputs is inﬂuenced by the pigeonhole principle. For example, suppose the ﬁrst output consists of two inputs. Then, the inputs must be used again for the remainingm′−1 outputs, or onlym−2 inputs can be used. In the latter case, the number of available inputs (m−2) is less than the outputs (m′−1), and thus, the same input must be used more than twice. Empirically, we found that the remaining compatibility term above over-penalizes the optimization so that a substantial portion of outputs are returned as singletons without any mixup. To mitigate the over-penalization issue, we apply clipping to the compatibility penalty term. Speciﬁcally, we model the objective so that no extra penalty would occur when the compatibility among outputs is below a certain level. 4Published as a conference paper at ICLR 2021 Now we present our main objective as following: z∗= argmin zj,k∈Lm, ∥zj,k∥1=1 f(z), where f(z) := m′ ∑ j=1 n∑ k=1 c⊺ kzj,k + β m′ ∑ j=1 ∑ (k,k′)∈N (1 −z⊺ j,kzj,k′) (1) + γmax   τ, m′ ∑ j=1 m′ ∑ j′̸=j ( n∑ k=1 zj,k )⊺ A ( n∑ k=1 zj′,k )      =fc(z) −η m′ ∑ j=1 n∑ k=1 log p(zj,k). In Figure 2, we describe the properties of the BP optimization problem of Equation (1) and statistics of the resulting mixup data. Next, we verify the supermodularity of the compatibility term. We ﬁrst extend the deﬁnition of the submodularity of a multi-label function as follows (Windheuser et al., 2012). Deﬁnition 1. For a given label setL, a functions: Lm ×Lm →R is pairwise submodular, if ∀x,x′∈Lm, s(x,x) +s(x′,x′) ≤s(x,x′) +s(x′,x). A functionsis pairwise supermodular, if −s is pairwise submodular. Proposition 1. The compatibility termfc in Equation (1) is pairwise supermodular for every pair of(zj1,k,zj2,k) if A is positive semi-deﬁnite. Proof. See Appendix B.1. Finally note that, A = (1 −ω)I + ωAc, where Ac is a symmetric matrix. By using spectral decomposition, Ac can be represented asUDU⊺, where D is a diagonal matrix and U⊺U = UU⊺ = I. Then, A= U((1 −ω)I+ ωD)U⊺, and thus for smallω >0, we can guarantee A to be positive semi-deﬁnite. 4.2 Algorithm Our main objective consists of modular (unary, prior), submodular (smoothness), and super- modular (compatibility) terms. To optimize the main objective, we employ the submodular- supermodular procedure by iteratively approximating the supermodular term as a modular function (Narasimhan and Bilmes, 2005). Note thatzj represents the labeling of thejth out- put andoj represents the aggregated input source information of thejth output, ∑n k=1 zj,k. Before introducing our algorithm, we ﬁrst inspect the simpler case without clipping. Proposition 2. The compatibility termfc without clipping is modular with respect tozj. Proof. Note, A is a positive symmetric matrix by the deﬁnition. Then, for an index j0, we can represent fc without clipping in terms of oj0 as ∑m′ j=1 ∑m′ j′=1,j′̸=jo⊺ jAoj′ = 2 ∑m′ j=1,j̸=j0 o⊺ jAoj0 +∑m′ j=1,j̸=j0 ∑m′ j′=1,j′/∈{j0,j}o⊺ jAoj′ = (2 ∑m′ j=1,j̸=j0 Aoj)⊺oj0 +c= v⊺ -j0 oj0 + c, where v-j0 ∈Rm and c ∈R are values independent withoj0 . Finally, v⊺ -j0 oj0 + c =∑n k=1 v⊺ -j0 zj0,k + c is a modular function ofzj0 . By Proposition 2, we can apply a submodular minimization algorithm to optimize the objective with respect tozj when there is no clipping. Thus, we can optimize the main objective without clipping in coordinate descent fashion (Wright, 2015). For the case with clipping, we modularize the supermodular compatibility term under the following criteria: 1. The modularized function value should increase as the compatibility across outputs increases. 2. The modularized function should not apply an extra penalty for the compatibility below a certain level. 5Published as a conference paper at ICLR 2021 Input batch Mixed output batch 3. Solve modularized Equation (1) for zj 1. Compute modularapproximateddiversityforzj 2. Perform modularization of Equation (1) with respect tozj Figure 3: Visualization of the proposed mixup procedure. For a given batch of input data (left), a batch of mixup data (right) is generated, which mix-matches diﬀerent salient regions among the input data while preserving the diversity among the mixup examples. The histograms on the right represent the input source information of each mixup data (oj). Borrowing the notation from the proof in Proposition 2, for an indexj, fc(z) = max{τ,v⊺ -joj+ c}= max{τ −c,v⊺ -joj}+ c. Note, oj = ∑n k=1 zj,k represents the input source information of the jth output andv-j = 2 ∑m′ j′=1,j′̸=jAoj′ encodes the status of the other outputs. Thus, we can interpret the supermodular term as a penalization of each label ofoj in proportion to the correspondingv-j value (criterion 1), but not for the compatibility belowτ−c(criterion 2). As a modular function which satisﬁes the criteria above, we use the following function: fc(z) ≈max{τ′,v-j}⊺oj for ∃τ′∈R. (2) Note that, by satisfying the criteria above, the modular function reﬂects the diversity and over-penalization desiderata described in Section 4.1. We illustrate the proposed mixup procedure with the modularized diversity penalty in Figure 3. Proposition 3. The modularization given by Equation(2) satisﬁes the criteria above. Proof. See Appendix B.2. Algorithm 1Iterative submodular minimiza- tion Initialize z as z(0). Let z(t) denote a solution of thetth step. Φ: modularization operator based on Equa- tion (2). for t= 1,...,T do for j = 1,...,m ′do f(t) j (zj) := f(zj; z(t) 1:j−1,z(t−1) j+1:m′). ˜f(t) j = Φ(f(t) j ). Solve z(t) j = argmin ˜f(t) j (zj). end for end for return z(T) By applying the modular approximation de- scribed in Equation (2) tofc in Equation (1), we can iteratively apply a submodular min- imization algorithm to obtain the ﬁnal so- lution as described in Algorithm 1. In de- tail, each step can be performed as follows: 1) Conditioning the main objective f on the current values except zj, denoted as fj(zj) = f(zj; z1:j−1,zj+1:m′). 2) Modu- larization of the compatibility term offj as Equation (2), resulting in a submodular function ˜fj. We denote the modularization operator asΦ, i.e., ˜fj = Φ(fj). 3) Applying a submodular minimization algorithm to˜fj. Please refer to Appendix C for implementa- tion details. Analysis Narasimhan and Bilmes (2005) proposed a modularization strategy for general supermodular set functions, and apply a submodular minimization algorithm that can monotonically decrease the original BP objective. However, the proposed Algorithm 1 based on Equation (2) is much more suitable for minibatch based mixup for neural network training than the set modularization proposed by Narasimhan and Bilmes (2005) in terms of complexity and modularization variance due to randomness. For simplicity, let us assume 6Published as a conference paper at ICLR 2021 each zj,k is anm-dimensional one-hot vector. Then, our problem is to optimizem′n one-hot m-dimensional vectors. To apply the set modularization method, we need to assign each possible value ofzj,k as an element of{1,2,...,m }. Then the supermodular term in Equation (1) can be interpreted as a set function withm′nmelements, and to apply the set modularization,O(m′nm) sequential evaluations of the supermodular term are required. In contrast, Algorithm 1 calculatesv-j in Equation (2) in onlyO(m′) time per each iteration. In addition, each modularization step of the set modularization method requires a random permutation of them′nm elements. In this case, the optimization can be strongly aﬀected by the randomness from the permutation step. As a result, the optimal labeling of eachzj,k from the compatibility term is strongly inﬂuenced by the random ordering undermining the interpretability of the algorithm. Please refer to Appendix D for empirical comparison between Algorithm 1 and the method by Narasimhan and Bilmes (2005). 5 Experiments We evaluate our proposed mixup method on generalization, weakly supervised object local- ization, calibration, and robustness tasks. First, we compare the generalization performance of the proposed method against baselines by training classiﬁers on CIFAR-100 (Krizhevsky et al., 2009), Tiny-ImageNet (Chrabaszcz et al., 2017), ImageNet (Deng et al., 2009), and the Google commands speech dataset (Warden, 2017). Next, we test the localization performance of classiﬁers following the evaluation protocol of Qin and Kim (2019). We also measure calibration error (Guo et al., 2017) of classiﬁers to verify Co-Mixup successfully alleviates the over-conﬁdence issue by Zhang et al. (2018). In Section 5.4, we evaluate the robustness of the classiﬁers on the test dataset with background corruption in response to the recent problem raised by Lee et al. (2020) that deep neural network agents often fail to generalize to unseen environments. Finally, we perform a sensitivity analysis of Co-Mixup and provide the results in Appendix F.3. 5.1 Classification We ﬁrst train PreActResNet18 (He et al., 2016), WRN16-8 (Zagoruyko and Komodakis, 2016), and ResNeXt29-4-24 (Xie et al., 2017) on CIFAR-100 for 300 epochs. We use stochastic gradient descent with an initial learning rate of 0.2 decayed by factor 0.1 at epochs 100 and 200. We set the momentum as 0.9 and add a weight decay of 0.0001. With this setup, we train a vanilla classiﬁer and reproduce the mixup baselines (Zhang et al., 2018; Verma et al., 2019; Yun et al., 2019; Kim et al., 2020), which we denote asVanilla, Input, Manifold, CutMix, Puzzle Mixin the experiment tables. Note that we use identical hyperparameters regarding Co-Mixup over all of the experiments with diﬀerent models and datasets, which are provided in Appendix E. Table 1 shows Co-Mixup signiﬁcantly outperforms all other baselines in Top-1 error rate. Co-Mixup achieves 19.87% in Top-1 error rate with PreActResNet18, outperforming the best baseline by 0.75%. We further test Co-Mixup on diﬀerent models (WRN16-8 & ResNeXt29- 4-24) and verify Co-Mixup improves Top-1 error rate over the best performing baseline. Dataset (Model) Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup CIFAR-100 (PreActResNet18) 23.59 22.43 21.64 21.29 20.62 19.87 CIFAR-100 (WRN16-8) 21.70 20.08 20.55 20.14 19.24 19.15 CIFAR-100 (ResNeXt29-4-24) 21.79 21.70 22.28 21.86 21.12 19.78 Tiny-ImageNet (PreActResNet18) 43.40 43.48 40.76 43.11 36.52 35.85 ImageNet (ResNet-50, 100 epochs) 24.03 22.97 23.30 22.92 22.49 22.39 Google commands (VGG-11) 4.84 3.91 3.67 3.76 3.70 3.54 Table 1: Top-1 error rate on various datasets and models. For CIFAR-100, we train each model with three diﬀerent random seeds and report the mean error. We further test Co-Mixup on other datasets; Tiny-ImageNet, ImageNet, and the Google commands dataset (Table 1). For Tiny-ImageNet, we train PreActResNet18 for 1200 epochs 7Published as a conference paper at ICLR 2021 0 1 1 Conﬁdence Accuracy Vanilla 0 1 1 Input 0 1 1 Manifold 0 1 1 CutMix 0 1 1 Puzzle 0 1 1 Co-Mixup Figure 4: Conﬁdence-Accuracy plots for classiﬁers on CIFAR-100. From the ﬁgure, the Vanilla network shows over-conﬁdent predictions, whereas other mixup baselines tend to have under-conﬁdent predictions. We can ﬁnd that Co-Mixup has best-calibrated predictions. following the training protocol of Kim et al. (2020). As a result, Co-Mixup consistently improves Top-1 error rate over baselines by 0.67%. In the ImageNet experiment, we follow the experimental protocol provided in Puzzle Mix (Kim et al., 2020), which trains ResNet-50 (He et al., 2015) for 100 epochs. As a result, Co-Mixup outperforms all of the baselines in Top-1 error rate. We further test Co-Mixup on the speech domain with the Google commands dataset and VGG-11 (Simonyan and Zisserman, 2014). We provide a detailed experimental setting and dataset description in Appendix F.1. From Table 1, we conﬁrm that Co-Mixup is the most eﬀective in the speech domain as well. 5.2 Localization We compare weakly supervised object localization (WSOL) performance of classiﬁers trained on ImageNet (in Table 1) to demonstrate that our mixup method better guides a classiﬁer to focus on salient regions. We test the localization performance using CAM (Zhou et al., 2016), a WSOL method using a pre-trained classiﬁer. We evaluate localization performance following the evaluation protocol in Qin and Kim (2019), with binarization threshold 0.25 in CAM. Table 2 summarizes the WSOL performance of various mixup methods, which shows that our proposed mixup method outperforms other baselines. 5.3 Calibration We evaluate the expected calibration error (ECE) (Guo et al., 2017) of classiﬁers trained on CIFAR-100. Note, ECE is calculated by the weighted average of the absolute diﬀerence between the conﬁdence and accuracy of a classiﬁer. As shown in Table 2, the Co-Mixup classiﬁer has the lowest calibration error among baselines. From Figure 4, we ﬁnd that other mixup baselines tend to haveunder-conﬁdent predictions resulting in higher ECE values even thanVanilla network (also pointed out by Wen et al. (2020)), whereas Co-Mixup has best-calibrated predictions resulting in relatively 48% less ECE value. We provide more ﬁgures and results with other datasets in Appendix F.2. Task Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup Localization (Acc. %) (↑) 54.36 55.07 54.86 54.91 55.22 55.32 Calibration (ECE %) (↓) 3.9 17.7 13.1 5.6 7.5 1.9 Table 2: WSOL results on ImageNet and ECE (%) measurements of CIFAR-100 classiﬁers. 5.4 Robustness In response to the recent problem raised by Lee et al. (2020) that deep neural network agents often fail to generalize to unseen environments, we consider the situation where the statistics of the foreground object, such as color or shape, is unchanged, but with the corrupted (or replaced) background. In detail, we consider the following operations: 1) replacement with another image and 2) adding Gaussian noise. We use ground-truth bounding boxes to separate the foreground from the background, and then apply the previous operations independently to obtain test datasets. We provide a detailed description of datasets in Appendix G. 8Published as a conference paper at ICLR 2021 With the test datasets described above, we evaluate the robustness of the pre-trained classiﬁers. As shown in Table 3, Co-Mixup shows signiﬁcant performance gains at various background corruption tests compared to the other mixup baselines. For each corruption case, the classiﬁer trained with Co-Mixup outperforms the others in Top-1 error rate with the performance margins of 2.86% and 3.33% over the Vanilla model. Corruption type Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup Random replacement 41.63 39.41 39.72 46.20 39.23 38.77 (+17.62) ( +16.47) ( +16.47) ( +23.16) ( +16.69) ( +16.38) Gaussian noise 29.22 26.29 26.79 27.13 26.11 25.89 (+5.21) ( +3.35) ( +3.54) ( +4.09) ( +3.57) ( +3.49) Table 3: Top-1 error rates of various mixup methods for background corrupted ImageNet validation set. The values in the parentheses indicate the error rate increment by corrupted inputs compared to clean inputs. 5.5 Baselines with multiple inputs To further investigate the eﬀect of the number of inputs for the mixup in isolation, we conduct an ablation study on baselines using multiple mixing inputs. For fair comparison, we use Dirichlet(α,...,α ) prior for the mixing ratio distribution and select the best performing α in {0.2,1.0,2.0}. Note that we overlay multiple boxes in the case of CutMix. Table 4 reports the classiﬁcation test errors on CIFAR-100 with PreActResNet18. From the table, we ﬁnd that mixing multiple inputs decreases the performance gains of each mixup baseline. These results demonstrate that mixing multiple inputs could lead to possible degradation of the performance and support the necessity of considering saliency information and diversity as in Co-Mixup. # inputs for mixup Input Manifold CutMix Co-Mixup # inputs= 2 22.43 21.64 21.29 # inputs= 3 23.03 22.13 22.01 19.87 # inputs= 4 23.12 22.07 22.20 Table 4: Top-1 error rates of mixup baselines with multiple mixing inputs on CIFAR-100 and PreActResNet18. We report the mean values of three diﬀerent random seeds. Note that Co-Mixup optimally determines the number of inputs for each output by solving the optimization problem. 6 Conclusion We presented Co-Mixup for optimal construction of a batch of mixup examples by ﬁnding the best combination of salient regions among a collection of input data while encouraging diversity among the generated mixup examples. This leads to a discrete optimization problem minimizing a novel submodular-supermodular objective. In this respect, we present a practical modular approximation and iterative submodular optimization algorithm suitable for minibatch based neural network training. Our experiments on generalization, weakly supervised object localization, and robustness against background corruption show Co-Mixup achieves the state of the art performance compared to other mixup baseline methods. The proposed generalized mixup framework tackles the important question of ‘what to mix?’ while the existing methods only consider ‘how to mix?’. We believe this work can be applied to new applications where the existing mixup methods have not been applied, such as multi-label classiﬁcation, multi-object detection, or source separation. 9Published as a conference paper at ICLR 2021 Acknowledgements This research was supported in part by Samsung Advanced Institute of Technology, Samsung Electronics Co., Ltd, Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2020-0-00882, (SW STAR LAB) Development of deployable learning intelligence via self-sustainable and trustworthy machine learning), and Research Resettlement Fund for the new faculty of Seoul National University. Hyun Oh Song is the corresponding author. References C.M.Bishop. Trainingwithnoiseisequivalenttotikhonovregularization. Neural computation, 7(1):108–116, 1995. C. M. Bishop.Pattern recognition and machine learning. springer, 2006. P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of imagenet as an alternative to the cifar datasets.arXiv preprint arXiv:1707.08819, 2017. E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation strategies from data.In CVPR, 2019. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F. F. Li. Imagenet: a large-scale hierarchical image database. In CVPR, 2009. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805, 2018. S. Fujishige.Submodular functions and optimization. Elsevier, 2005. C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In ICML, 2017. H. Guo, Y. Mao, and R. Zhang. Mixup as locally linear out-of-manifold regularization.In AAAI, 2019. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.In CVPR, 2015. K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks.In ECCV, 2016. T. Horel and Y. Singer. Maximization of approximately submodular functions.In NeurIPS, 2016. R. Iyer and J. Bilmes. Algorithms for approximate minimization of the diﬀerence between submodular functions, with applications.arXiv preprint arXiv:1207.0560, 2012. C. Jung and C. Kim. A uniﬁed spectral-domain approach for saliency detection and its application to automatic object segmentation.IEEE Transactions on Image Processing, 21(3):1272–1283, 2011. O. Kalinli and S. S. Narayanan. A saliency-based auditory attention model with applications to unsupervised prominent syllable detection in speech. InEighth Annual Conference of the International Speech Communication Association, 2007. J.-H. Kim, W. Choo, and H. O. Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup.In ICML, 2020. A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Citeseer, 2009. Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 10Published as a conference paper at ICLR 2021 K. Lee, K. Lee, J. Shin, and H. Lee. Network randomization: A simple technique for generalization in deep reinforcement learning.In ICLR, 2020. L. Lovász. Submodular functions and convexity. InMathematical Programming The State of the Art, pages 235–257. Springer, 1983. T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning.IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–1993, 2018. M. Narasimhan and J. A. Bilmes. A submodular-supermodular procedure with applications to discriminative structure learning.UAI, 2005. A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio.arXiv preprint arXiv:1609.03499, 2016. Z. Qin and D. Kim. Rethinking softmax with cross-entropy: Neural network classiﬁer as mutual information estimator.arXiv preprint arXiv:1911.10688, 2019. S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks.In NeurIPS, 2015. R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization.In ICCV, 2017. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classiﬁcation models and saliency maps.arXiv preprint arXiv:1312.6034, 2013. V. Verma, A. Lamb, C. Beckham, A. Najaﬁ, I. Mitliagkas, A. Courville, D. Lopez-Paz, and Y. Bengio. Manifold mixup: Better representations by interpolating hidden states.In ICML, 2019. L. Wang, H. Lu, X. Ruan, and M.-H. Yang. Deep networks for saliency detection via local estimation and global search.In CVPR, 2015. P. Warden. URL https://research.googleblog.com/2017/08/launching-speech-commands- dataset.html., 2017. Y. Wen, G. Jerfel, R. Muller, M. W. Dusenberry, J. Snoek, B. Lakshminarayanan, and D. Tran. Improving calibration of batchensemble with data augmentation.In ICML Workshop on Uncertainty and Robustness in Deep Learning, 2020. T. Windheuser, H. Ishikawa, and D. Cremers. Generalized roof duality for multi-label optimization: Optimal lower bounds and persistency.In ECCV, 2012. S. J. Wright. Coordinate descent algorithms.Mathematical Programming, 151(1):3–34, 2015. S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. pages 1492–1500, 2017. S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features.In ICCV, 2019. S. Zagoruyko and N. Komodakis. Wide residual networks.arXiv preprint arXiv:1605.07146, 2016. C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization.arXiv preprint arXiv:1611.03530, 2016. H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018. 11Published as a conference paper at ICLR 2021 R. Zhao, W. Ouyang, H. Li, and X. Wang. Saliency detection by multi-context deep learning. In CVPR, 2015. B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. In CVPR, 2016. A Supplementary notes for objective A.1 Notations In Table 5, we provide a summary of notations in the main text. Notation Meaning m, m′, n # inputs, # outputs, dimension of data ck ∈Rm (1 ≤k≤n) labeling cost for m input sources at thekth location zj,k ∈Lm (1 ≤j ≤m′, 1 ≤k≤n) input source ratio at the kth location of thejth output zj ∈Lm×n labeling of thejth output oj ∈Rm aggregation of the labeling of thejth output A∈Rm×m compatibility between inputs Table 5: A summary of notations. A.2 Interpretation of compatibility In our main objective Equation (1), we introduce a compatibility matrixA= (1 −ω)I+ ωAc between inputs. By minimizing⟨oj,oj′⟩A for j ̸= j′, we encourage each individual mixup examples to have high compatibility within. Figure 5 explains how the compatibility term works by comparing simple cases. Note that our framework can reﬂect any compatibility measures for the optimal mixup. 1 3 2 1 2 3 $%& $'& $&' $%' Output\t1Output\t2Output\t1Output\t2 Figure 5: Let us consider Co-Mixup with three inputs and two outputs. The ﬁgure represents two Co-Mixup results. Each input is denoted as a number and color-coded. Let us assume that input 1 and input 2 are more compatible,i.e., A12 ≫A23 and A12 ≫A13. Then, the left Co-Mixup result has a larger inner-product value⟨o1,o2⟩A than the right. Thus the mixup result on the right has higher compatibility than the result on the left within each output example. B Proofs B.1 Proof of proposition 1 Lemma 1.For a positive semi-deﬁnite matrixA∈Rm×m + and x,x′∈Rm, s(x,x′) = x⊺Ax′ is pairwise supermodular. Proof. s(x,x) +s(x′,x′) −s(x,x′) −s(x′,x) = x⊺Ax+ x⊺Ax−2x⊺Ax′= (x−x′)⊺A(x−x′), and becauseA is positive semi-deﬁnite,(x−x′)⊺A(x−x′) ≥0. 12Published as a conference paper at ICLR 2021 Proposition 1. The compatibility termfc in Equation (1) is pairwise supermodular for every pair of(zj1,k,zj2,k) if A is positive semi-deﬁnite. Proof. For j1 and j2, s.t., j1 ̸= j2, max { τ,∑m′ j=1 ∑m′ j′=1,j′̸=j(∑n k=1 zj,k)⊺A(∑n k=1 zj′,k) } = max{τ,c + 2z⊺ j1,kAzj2,k} = −min{−τ,−c −2z⊺ j1,kAzj2,k}, for ∃c ∈ R. By Lemma 1, −z⊺ j1,kAzj2,k is pairwise submodular, and because a budget additive function preserves submodularity (Horel and Singer, 2016),min{−τ,−c−2z⊺ j1,kAzj2,k}is pairwise submodular with respect to(zj1,k,zj2,k). B.2 Proof of proposition 3 Proposition 3. The modularization given by Equation (2) satisﬁes the criteria. Proof. Note, by the deﬁnition in Equation (1), the compatibility between thejth and j′th outputs is o⊺ j′Aoj, and thus, v⊺ -joj represents the compatibility between thejth output and the others. In addition,∥oj∥1 = ∥∑n k=1 zj,k∥1 = ∑n k=1 ∥zj,k∥1 = n. In a local view, for the given oj, let us deﬁne a vectoro′ j as o′ j[i1] = oj[i1] + α and o′ j[i2] = oj[i2] −α for α > 0. Without loss of generality, let us assume v-j is sorted in ascending order. Then, v⊺ -joj ≤v⊺ -jo′ j implies i1 > i2, and because the max function preserves the ordering, max{τ′,v-j}⊺oj ≤max{τ′,v-j}⊺o′ j. Thus, the criterion 1 is locally satisﬁed. Next, for τ′ > 0, ∥max{τ′,v-j}⊺oj∥1 ≥τ′∥oj∥1 = τ′n. Let ∃i0 s.t. for i < i0,v-j[i] < τ′, and for i≥i0,v-j[i] ≥τ′. Then, foroj containing positive elements only in indices smaller thani0, max{τ′,v-j}⊺oj = τ′n which means there is no extra penalty from the compatibility. In this respect, the proposed modularization satisﬁes the criterion 2 as well. C Implementation details We perform the optimization after down-sampling the given inputs and saliency maps to the speciﬁed size (4 ×4). After the optimization, we up-sample the optimal labeling to match the size of the inputs and then mix inputs according to the up-sampled labeling. For the saliency measure, we calculate the gradient values of training loss with respect to the input data and measure ℓ2 norm of the gradient values across input channels (Simonyan et al., 2013). In classiﬁcation experiments, we retain the gradient information of network weights obtained from the saliency calculation for regularization. For the distance in the compatibility term, we measureℓ1-distance between the most salient regions. For the initialization in Algorithm 1, we usei.i.d. samples from a categorical distribution with equal probabilities. We use alpha-beta swap algorithm from pyGCO1 to solve the minimization step in Algorithm 1, which can ﬁnd local-minima of a multi-label submodular function. However, the worst-case complexity ofalpha-beta swap algorithm with|L|= 2 is O(m2n), and in the case of mini-batch with 100 examples, iteratively applying the algorithm canbecomeabottleneckduringthenetworktraining. Tomitigatethecomputationaloverhead, we partition the mini-batch (each of size 20) and then apply Algorithm 1 independently per each partition. The worst-case complexity theoretic of the naive implementation of Algorithm 1 increases exponentially as|L|increases. Speciﬁcally, the worst-case theoretic complexity of thealpha- beta swap algorithm is proportional to the square of the number of possible states ofzj,k, which is proportional tom|L|−1. To reduce the number of possible states in a multi-label case, we solve the problem for binary labels (|L|= 2) at the ﬁrst inner-cycle and then extend to multi labels (|L|= 3) only for the currently assigned indices of each output in the subsequent cycles. This reduces the number of possible states toO(m+ ¯m|L|−1) where ¯m≪m. Here, ¯m means the number of currently assigned indices for each output. 1https://github.com/Borda/pyGCO 13Published as a conference paper at ICLR 2021 Based on the above implementation, we train models with Co-Mixup in a feasible time. For example, in the case of ImageNet training with 16 Intel I9-9980XE CPU cores and 4 NVIDIA RTX 2080Ti GPUs, Co-Mixup training requires 0.964s per batch, whereas the vanilla training without mixup requires 0.374s per batch. Note that Co-Mixup requires saliency computation, and when we compare the algorithm with Puzzle Mix, which performs the same saliency computation, Co-Mixup is only slower about 1.04 times. Besides, as we down-sample the data to the ﬁxed size regardless of the data dimension, the additional computation cost of Co-Mixup relatively decreases as the data dimension increases. Finally, we present the empirical time complexity of Algorithm 1 in Figure 6. As shown in the ﬁgure, Algorithm 1 has linear time complexity over|L|empirically. Note that we use|L|= 3 in all of our main experiments, including a classiﬁcation task. 2 3 4 1.1 1.2 1.3 |L| Average Execution Time (ms) 2 1020 50 100 0 2 4 6 8 10 m Average Execution Time (ms) Figure 6: Mean execution time (ms) of Algorithm 1 per each batch of data over 100 trials. The left ﬁgure shows the time complexity of the algorithm over|L|and the right ﬁgure shows the time complexity over the number of inputsm. Note that the other parameters are ﬁxed equal to the classiﬁcation experiments setting,m= m′= 20, n= 16, and|L|= 3. D Algorithm Analysis In this section, we perform comparison experiments to analyze the proposed Algorithm 1. First, we compare our algorithm with the exact brute force search algorithm to inspect the optimality of the algorithm. Next, we compare our algorithm with the BP algorithm proposed by Narasimhan and Bilmes (2005). D.1 Comparison with Brute Force To inspect the optimality of the proposed algorithm, we compare the function values of the solutions of Algorithm 1, brute force search algorithm, and random guess. Due to the exponential time complexity of the brute force search, we compare the algorithms on small scale experiment settings. Speciﬁcally, we test algorithms on settings of(m= m′= 2, n= 4), (m= m′= 2, n= 9), and(m= m′= 3, n= 4) varying the number of inputs and outputs (m, m′) and the dimension of datan. We generate unary cost matrix in the objectivef by sampling data from uniform distribution. We perform experiments with 100 diﬀerent random seeds and summarize the results on Table 6. From the table, we ﬁnd that the proposed algorithm achieves near optimal solutions over various settings. We also measure relative errors between ours and random guess, (f(zours) −f(zbrute))/(f(zrandom) −f(zbrute)). As a result, our algorithm achieves relative error less than0.01. D.2 Comparison with another BP algorithm We compare the proposed algorithm and the BP algorithm proposed by Narasimhan and Bilmes (2005). We evaluate function values of solutions by each method using a random 14Published as a conference paper at ICLR 2021 Conﬁguration Ours Brute force (optimal) Random guess Rel. error (m= m′= 2, n= 4) 1.91 1.90 3.54 0.004 (m= m′= 2, n= 9) 1.93 1.91 3.66 0.01 (m= m′= 3, n= 4) 2.89 2.85 22.02 0.002 Table 6: Mean function values of the solutions over 100 diﬀerent random seeds. Rel. error means the relative error between ours and random guess. unary cost matrix from a uniform distribution. We compare methods over various scales by controlling the number of mixing inputsm. Table 7 shows the averaged function values with standard deviations in the parenthesis. As we can see from the table, the proposed algorithm achieves much lower function values and deviations than the method by Narasimhan and Bilmes (2005) over various settings. Note that the method by Narasimhan and Bilmes (2005) has high variance due to randomization in the algorithm. We further compare the algorithm convergence time in Table 8. The experiments verify that the proposed algorithm is much faster and eﬀective than the method by Narasimhan and Bilmes (2005). Algorithm m= 5 m= 10 m= 20 m= 50 m= 100 Ours 3.1 (1.7) 15 (6.6) 54 (15) 205 (26) 469 (31) Narasimhan 269 (58) 1071 (174) 4344 (701) 24955 (4439) 85782 (14337) Random 809 (22) 7269 (92) 60964 (413) 980973 (2462) 7925650 (10381) Table 7: Mean function values of the solutions over 100 diﬀerent random seeds. We report the standard deviations in the parenthesis. Random represents the random guess algorithm. Algorithm m= 5 m= 10 m= 20 m= 50 m= 100 Ours 0.02 0.04 0.11 0.54 2.71 Narasimhan 0.06 0.09 0.27 1.27 7.08 Table 8: Convergence time (s) of the algorithms. E Hyperparameter settings We perform Co-Mixup after down-sampling the given inputs and saliency maps to the pre-deﬁned resolutions regardless of the size of the input data. In addition, we normalize the saliency of each input to sum up to 1 and deﬁne unary cost using the normalized saliency. As a result, we use an identical hyperparameter setting for various datasets; CIFAR-100, Tiny-ImageNet, and ImageNet. In details, we use(β,γ,η,τ ) = (0.32,1.0,0.05,0.83) for all of experiments. Note thatτ is normalized according to the size of inputs (n) and the ratio of the number of inputs and outputs (m/m′), and we use an isotropic Dirichlet distribution with α= 2 for priorp. For a compatibility matrix, we useω= 0.001. For baselines, we tune the mixing ratio hyperparameter,i.e., the beta distribution parameter (Zhang et al., 2018), among{0.2,1.0,2.0}for all of the experiments if the speciﬁc setting is not provided in the original papers. F Additional Experimental Results F.1 Another Domain: Speech In addition to the image domain, we conduct experiments on the speech domain, verifying Co-Mixup works on various domains. Following (Zhang et al., 2018), we train LeNet (LeCun 15Published as a conference paper at ICLR 2021 0 1 1 Conﬁdence Accuracy Vanilla 0 1 1 Input 0 1 1 Manifold 0 1 1 CutMix 0 1 1 Puzzle 0 1 1 Co-Mixup Figure 7: Conﬁdence-Accuracy plots for classiﬁers on CIFAR-100. Note, ECE is calculated by the mean absolute diﬀerence between the two values. et al., 1998) and VGG-11 (Simonyan and Zisserman, 2014) on the Google commands dataset (Warden, 2017). The dataset consists of 65,000 utterances, and each utterance is about one-second-long belonging to one out of 30 classes. We train each classiﬁer for 30 epochs with the same training setting and data pre-processing of Zhang et al. (2018). In more detail, we use160 ×100 normalized spectrograms of utterances for training. As shown in Table 9, we verify that Co-Mixup is still eﬀective in the speech domain. Model Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup LeNet 11.24 10.83 12.33 12.80 10.89 10.67 VGG-11 4.84 3.91 3.67 3.76 3.70 3.57 Table 9: Top-1 classiﬁcation test error on the Google commands dataset. We stop training if validation accuracy does not increase for 5 consecutive epochs. F.2 Calibration In this section, we summarize the expected calibration error (ECE) (Guo et al., 2017) of classiﬁers trained with various mixup methods. For evaluation, we use the oﬃcial code provided by the TensorFlow-Probability library2 and set the number of bins as 10. As shown in Table 10, Co-Mixup classiﬁers have the lowest calibration error on CIFAR-100 and Tiny-ImageNet. As pointed by Guo et al. (2017), the Vanilla networks have over- conﬁdent predictions, but however, we ﬁnd that mixup classiﬁers tend to have under-conﬁdent predictions (Figure 7; Figure 8). As shown in the ﬁgures, Co-Mixup successfully alleviates the over-conﬁdence issue and does not suﬀer from under-conﬁdence predictions. Dataset Vanilla Input Manifold CutMix Puzzle Mix Co-Mixup CIFAR-100 3.9 17.7 13.1 5.6 7.5 1.9 Tiny-ImageNet 4.5 6.2 6.8 12.0 5.6 2.5 ImageNet 5.9 1.2 1.7 4.3 2.1 2.1 Table 10: Expected calibration error (%) of classiﬁers trained with various mixup methods on CIFAR-100, Tiny-ImageNet and ImageNet. Note that, at all of three datasets, Co-Mixup outperforms all of the baselines in Top-1 accuracy. F.3 Sensitivity analysis We measure the Top-1 error rate of the model by sweeping the hyperparameter to show the sensitivity using PreActResNet18 on CIFAR-100 dataset. We sweep the label smoothness coeﬃcient β ∈{0,0.16,0.32,0.48,0.64}, compatibility coeﬃcientγ ∈{0.6,0.8,1.0,1.2,1.4}, clipping levelτ ∈{0.79,0.81,0.83,0.85,0.87}, compatibility matrix parameterω ∈{0,5 · 10−4,10−3,5 ·10−3,10−2}, and the size of partitionm∈{2,4,10,20,50}. Table 11 shows that Co-Mixup outperforms the best baseline (PuzzleMix, 20.62%) with a large pool of 2https://www.tensorﬂow.org/probability/api_docs/python/tfp/stats/expected_calibration_error 16Published as a conference paper at ICLR 2021 0 1 1 Conﬁdence Accuracy Vanilla 0 1 1 Input 0 1 1 Manifold 0 1 1 CutMix 0 1 1 Puzzle 0 1 1 Co-Mixup Figure 8: Conﬁdence-Accuracy plots for classiﬁers on Tiny-ImageNet. 0 1 1 Conﬁdence Accuracy Vanilla 0 1 1 Input 0 1 1 Manifold 0 1 1 CutMix 0 1 1 Puzzle 0 1 1 Co-Mixup Figure 9: Conﬁdence-Accuracy plots for classiﬁers on ImageNet. hyperparameters. We also ﬁnd that Top-1 error rate increases as the partition batch sizem increases untilm= 20. Smoothness coeﬃcient, β = 0 β = 0.16 β = 0.32 β = 0.48 β = 0.64 β 20.29 20.18 19.87 20.35 21.24 Compatibility coeﬃcient, γ = 0.6 γ = 0.8 γ = 1.0 γ = 1.2 γ = 1.4 γ 20.3 19.99 19.87 20.09 20.13 Clipping parameter, τ = 0.79 τ = 0.81 τ = 0.83 τ = 0.85 τ = 0.87 τ 20.45 20.14 19.87 20.15 20.23 Compatibility matrix ω= 0 ω= 5·10−4 ω= 10−3 ω= 5·10−3 ω= 10−2 parameter, ω 20.51 20.42 19.87 20.18 20.14 Partition size, m= 2 m= 4 m= 10 m= 20 m= 50 m 20.3 20.22 20.15 19.87 19.96 Table 11: Hyperparameter sensitivity results (Top-1 error rates) on CIFAR-100 with PreAc- tResNet18. We report the mean values of three diﬀerent random seeds. F.4 Comparison with non-mixup baselines We compare the generalization performance of Co-Mixup with non-mixup baselines, verifying the proposed method achieves the state of the art generalization performance not only for the mixup-based methods but for other general regularization based methods. One of the regularization methods called VAT (Miyato et al., 2018) uses virtual adversarial loss, which is deﬁned as the KL-divergence of predictions between input data against local perturbation. We perform the experiment with VAT regularization on CIFAR-100 with PreActResNet18 for 300 epochs in the supervised setting. We tuneα (coeﬃcient of VAT regularization term) in {0.001, 0.01, 0.1},ϵ(radius ofℓ-inf ball) in {1, 2}, and the number of noise update steps in {0, 1}. Table 12 shows that Co-Mixup, which achieves Top-1 error rate of 19.87%, outperforms the VAT regularization method. G Detailed description for background corruption We build the background corrupted test datasets based on ImageNet validation dataset to compare the robustness of the pre-trained classiﬁers against the background corruption. 17Published as a conference paper at ICLR 2021 # update=0 # update=1 VAT loss coeﬃcient ϵ= 1 ϵ= 2 ϵ= 1 ϵ= 2 α= 0.001 23.38 23.62 24.76 26.22 α= 0.01 23.14 23.67 28.33 31.95 α= 0.1 23.65 23.88 34.75 39.82 Table 12: Top-1 error rates of VAT on CIFAR-100 dataset with PreActResNet18. ImageNet consists of images{x1,...,x M}, labels{y1,...,y M}, and the corresponding ground- truth bounding boxes{b1,...,b M}. We use the ground-truth bounding boxes to separate the foreground from the background. Letzj be a binary mask of imagexj, which has value1 inside of the ground-truth bounding boxbj. Then, we generate two types of background corrupted sample˜xj by considering the following operations: 1. Replacement with another image as ˜xj = xj ⊙zj + xi(j) ⊙(1 −zj) for a random permutation {i(1),...,i (M)}. 2. Adding Gaussian noise as˜xj = xj ⊙zj + ϵ⊙(1 −zj), whereϵ∼N(0,0.12). We clip pixel values of˜xj to [0, 1]. Figure 10 visualizes subsets of the background corruption test datasets. (a)  (b) Figure 10: Each subﬁgure shows background corrupted samples used in the robustness experiment. (a) Replacement with another image in ImageNet. (b) Adding Gaussian noise. The red boxes on the images represent ground-truth bounding boxes. H Co-Mixup generated samples In Figure 12, we present Co-Mixup generated image samples by using images from ImageNet. We use an input batch consisting of 24 images, which is visualized in Figure 11. As can be seen from Figure 12, Co-Mixup eﬃciently mix-matches salient regions of the given inputs maximizing saliency and creates diverse outputs. In Figure 12, inputs with the target objects on the left side are mixed with the objects on the right side, and objects on the top side are mixed with the objects on the bottom side. In Figure 13, we present Co-Mixup generated image samples with largerτ using the same input batch. By increasingτ, we can encourage Co-Mixup to use more inputs to mix per each output. 18Published as a conference paper at ICLR 2021 Figure 11: Input batch. 19Published as a conference paper at ICLR 2021 Figure 12: Mixed output batch. 20Published as a conference paper at ICLR 2021 Figure 13: Another mixed output batch with largerτ. 21",
      "references": [
        "Trainingwithnoiseisequivalenttotikhonovregularization",
        "Pattern recognition and machine learning",
        "A downsampled variant of imagenet as an alternative to the cifar datasets",
        "Autoaugment: Learning augmentation strategies from data",
        "Imagenet: a large-scale hierarchical image database",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "Submodular functions and optimization",
        "On calibration of modern neural networks",
        "Mixup as locally linear out-of-manifold regularization",
        "Deep residual learning for image recognition",
        "Identity mappings in deep residual networks",
        "Maximization of approximately submodular functions",
        "Algorithms for approximate minimization of the difference between submodular functions, with applications",
        "A unified spectral-domain approach for saliency detection and its application to automatic object segmentation",
        "A saliency-based auditory attention model with applications to unsupervised prominent syllable detection in speech",
        "Puzzle mix: Exploiting saliency and local statistics for optimal mixup",
        "Learning multiple layers of features from tiny images",
        "Gradient-based learning applied to document recognition",
        "Network randomization: A simple technique for generalization in deep reinforcement learning",
        "Submodular functions and convexity",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
        "A submodular-supermodular procedure with applications to discriminative structure learning",
        "Wavenet: A generative model for raw audio",
        "Rethinking softmax with cross-entropy: Neural network classifier as mutual information estimator",
        "Faster r-cnn: Towards real-time object detection with region proposal networks",
        "Grad-cam: Visual explanations from deep networks via gradient-based localization",
        "Very deep convolutional networks for large-scale image recognition",
        "Deep inside convolutional networks: Visualising image classification models and saliency maps",
        "Manifold mixup: Better representations by interpolating hidden states",
        "Deep networks for saliency detection via local estimation and global search",
        "Improving calibration of batchensemble with data augmentation",
        "Generalized roof duality for multi-label optimization: Optimal lower bounds and persistency",
        "Coordinate descent algorithms",
        "Wide residual networks",
        "Aggregated residual transformations for deep neural networks",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features",
        "Understanding deep learning requires rethinking generalization",
        "mixup: Beyond empirical risk minimization",
        "Saliency detection by multi-context deep learning",
        "Learning deep features for discriminative localization"
      ],
      "meta_data": {
        "arxiv_id": "2102.03065v1",
        "authors": [
          "Jang-Hyun Kim",
          "Wonho Choo",
          "Hosan Jeong",
          "Hyun Oh Song"
        ],
        "published_date": "2021-02-05T09:12:02Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Co-Mixup, a saliency-guided mixup framework that jointly constructs a whole batch of synthetic examples by (1) maximizing the amount of per-example salient information and (2) enforcing diversity across the batch.  The problem is formulated as minimization of a novel submodular–supermodular (BP) objective, and an efficient modular approximation with iterative submodular minimization is proposed, enabling practical minibatch-level optimization.  Co-Mixup achieves state-of-the-art gains in generalization, calibration, weakly-supervised localization, and robustness compared with existing mixup variants.",
        "methodology": "• Generalizes mixup to blend arbitrary numbers of inputs within a minibatch via label masks z.  \n• Unary term: negative saliency score (ℓ2 norm of input gradient) encourages each mixed pixel to come from salient regions; β smoothness term enforces spatial coherence.\n• Diversity term: supermodular penalty based on inner products of aggregated source vectors o_j with compatibility matrix A (built from saliency-peak distances) discourages similar source compositions among outputs.\n• Overall objective = submodular (smoothness) + modular (unary, prior) + supermodular (diversity) + clipping to avoid over-penalization.\n• Solved with an iterative procedure: at each step the supermodular part is modularized (Equation 2) and the resulting submodular problem is minimized via α–β swap graph-cut; iterate over outputs.\n• Implementation tricks: down-sample saliency to 4×4, partition minibatch (size 20) to cut cost; complexity ~ linear in |L|.\n• Produces soft labels from Dirichlet prior; same hyper-parameters across datasets.",
        "experimental_setup": "Datasets: CIFAR-100, Tiny-ImageNet, ImageNet (ILSVRC-12, 100-epoch schedule), Google Speech Commands.  \nModels: PreActResNet-18, WRN-16-8, ResNeXt-29-4×24, ResNet-50, VGG-11, LeNet for speech.\nBaselines: Vanilla (no mixup), Input Mixup, Manifold Mixup, CutMix, PuzzleMix, VAT (extra study), multi-input variants.\nMetrics: Top-1 error/accuracy, Expected Calibration Error (ECE), CAM-based WSOL accuracy (threshold 0.25), robustness test on ImageNet with background replacement or Gaussian noise.\nTraining details: SGD, lr 0.2 with step decay (CIFAR), 300 epochs (CIFAR), 1200 epochs (Tiny-ImageNet), 100 epochs (ImageNet); identical Co-Mixup hyper-params (β=0.32, γ=1, η=0.05, τ=0.83, ω=0.001).  Saliency gradients reused for weight updates.",
        "limitations": "1. Additional computation: requires per-batch saliency back-prop and discrete optimization; ~2.5× slower than vanilla, though comparable to PuzzleMix.\n2. Uses down-sampling and modular approximation; solution is approximate and quality depends on hyper-parameters β, γ, τ, ω and partition size.\n3. Relies on meaningful gradient-based saliency from a partially trained model; early-stage noise could affect mixup quality.\n4. Currently demonstrated only on classification; not evaluated on multilabel, detection, segmentation where multiple objects overlap.\n5. Compatibility matrix assumes Euclidean distance between single peak saliency locations, which may be crude for complex scenes.",
        "future_research_directions": "• Extend Co-Mixup to multi-label classification, object detection, instance/semantic segmentation, and source separation tasks.\n• Explore alternative or learned saliency measures (e.g., class-agnostic attention, integrated gradients) and dynamic compatibility metrics.\n• Develop end-to-end differentiable or approximate continuous relaxations to remove discrete optimization overhead.\n• Investigate theoretical bounds and convergence guarantees for the modular approximation strategy.\n• Apply the framework to other modalities (text, time-series, graphs) and to semi-supervised or self-supervised learning settings.\n• Optimize computational efficiency with GPU-friendly graph-cut solvers or approximate sampling methods.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Adversarial AutoAugment",
      "full_text": "arXiv:1912.11188v1  [cs.CV]  24 Dec 2019 Published as a conference paper at ICLR 2020 ADV E R S A R IA L AU TO AU G M E N T Xinyu Zhang Qiang W ang Huawei Huawei zhangxinyu10@huawei.com wangqiang168@huawei.com Jian Zhang Zhao Zhong Huawei Huawei zhangjian157@huawei.com zorro.zhongzhao@huawei.com ABSTRACT Data augmentation (DA) has been widely utilized to improve g eneralization in training deep neural networks. Recently, human-designed d ata augmentation has been gradually replaced by automatically learned augmenta tion policy. Through ﬁnding the best policy in well-designed search space of data augmentation, Au- toAugment (Cubuk et al., 2018) can signiﬁcantly improve val idation accuracy on image classiﬁcation tasks. However, this approach is not co mputationally practi- cal for large-scale problems. In this paper, we develop an ad versarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object an d augmentation pol- icy search loss. The augmentation policy network attempts t o increase the train- ing loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we re use the computation in target network training for policy evaluation, and dispe nse with the retraining of the target network. Compared to AutoAugment, this leads t o about 12× reduc- tion in computing cost and 11× shortening in time overhead on ImageNet. W e show experimental results of our approach on CIF AR-10/CIF A R-100, ImageNet, and demonstrate signiﬁcant performance improvements over state-of-the-art. On CIF AR-10, we achieve a top-1 test error of 1.36%, which is the currently best per- forming single model. On ImageNet, we achieve a leading perf ormance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data. 1 I NTRODUC TI ON Massive amount of data have promoted the great success of dee p learning in academia and industry. The performance of deep neural networks (DNNs) would be impr oved substantially when more su- pervised data is available or better data augmentation meth od is adapted. Data augmentation such as rotation, ﬂipping, cropping, etc., is a powerful technique to increase the amount and diversit y of data. Experiments show that the generalization of a neural networ k can be efﬁciently improved through manually designing data augmentation policies. However, t his needs lots of knowledge of human expert, and sometimes shows the weak transferability acros s different tasks and datasets in practi- cal applications. Inspired by neural architecture search ( NAS)(Zoph & Le, 2016; Zoph et al., 2017; Zhong et al., 2018a;b; Guo et al., 2018), a reinforcement lea rning (RL) (Williams, 1992) method called AutoAugment is proposed by Cubuk et al. (2018), which can automatically learn the aug- mentation policy from data and provide an exciting performa nce improvement on image classiﬁca- tion tasks. However, the computing cost is huge for training and evaluating thousands of sampled policies in the search process. Although proxy tasks, i.e., smaller models and reduced datasets, are taken to accelerate the searching process, tens of thousand s of GPU-hours of consumption are still required. In addition, these data augmentation policies op timized on proxy tasks are not guaranteed to be optimal on the target task, and the ﬁxed augmentation po licy is also sub-optimal for the whole training process. 1Published as a conference paper at ICLR 2020 Policy Network  Target Network  Network  Training Dataset  Pre-process Large Batch  Policy  Search  Minimize  Training Loss  Maximize  Training Loss  Training  Losses  Moving Average  & Normalize  {߬ଵ,߬ଶ,߬ଷ … ,߬ெ} + Sampled Policies  ߬ଵ ߬ଶ ߬ெ ߬ଷ Mini-Batch  … ࣦଵ ࣦଶ ࣦெ … Figure 1: The overview of our proposed method. W e formulate i t as a Min-Max game. The data of each batch is augmented by multiple pre-processing c omponents with sampled policies {τ1, τ 2, · · · , τ M }, respectively. Then, a target network is trained to minimiz e the loss of a large batch, which is formed by multiple augmented instances of th e input batch. W e extract the training losses of a target network corresponding to different augme ntation policies as the reward signal. Fi- nally, the augmentation policy network is trained with the g uideline of the processed reward signal, and aims to maximize the training loss of the target network t hrough generating adversarial policies. In this paper, we propose an efﬁcient data augmentation meth od to address the problems mentioned above, which can directly search the best augmentation poli cy on the full dataset during training a target network, as shown in Figure 1. W e ﬁrst organize the ne twork training and augmentation policy search in an adversarial and online manner. The augme ntation policy is dynamically changed along with the training state of the target network, rather t han ﬁxed throughout the whole training process like normal AutoAugment (Cubuk et al., 2018). Due to reusing the computation in policy evaluation and dispensing with the retraining of the target network, the computing cost and time overhead are extremely reduced. Then, the augmentation pol icy network is taken as an adversary to explore the weakness of the target network. W e augment the da ta of each min-batch with various adversarial policies in parallel, rather than the same data augmentation taken in batch augmentation (BA) (Hoffer et al., 2019). Then, several augmented instanc es of each mini-batch are formed into a large batch for target network learning. As an indicator of t he hardness of augmentation policies, the training losses of the target network are used to guide the po licy network to generate more aggres- sive and efﬁcient policies based on REINFORCE algorithm (Wi lliams, 1992). Through adversarial learning, we can train the target network more efﬁciently an d robustly. The contributions can be summarized as follows: • Our method can directly learn augmentation policies on targ et tasks, i.e., target networks and full datasets, with a quite low computing cost and time ov erhead. The direct policy search avoids the performance degradation caused by the pol icy transfer from proxy tasks to target tasks. • W e propose an adversarial framework to jointly optimize tar get network training and aug- mentation policy search. The harder samples augmented by ad versarial policies are con- stantly fed into the target network to promote robust featur e learning. Hence, the general- ization of the target network can be signiﬁcantly improved. • The experiment results show that our proposed method outper forms previous augmentation methods. For instance, we achieve a top-1 test error of 1.36% with PyramidNet+ShakeDrop (Y amada et al., 2018) on CIF AR-10, which is the state-of-the -art performance. On Ima- geNet, we improve the top-1 accuracy of ResNet-50 (He et al., 2016) from 76.3% to 79.4% without extra data, which is even 1.77% better than AutoAugment (Cubuk et al., 2018). 2Published as a conference paper at ICLR 2020 2 R ELATED WORK Common data augmentation, which can generate extra samples by some label-preserved transforma- tions, is usually used to increase the size of datasets and im prove the generalization of networks, such as on MINST , CIF AR-10 and ImageNet (Krizhevsky et al., 2012; W an et al., 2013; Szegedy et al., 2015). However, human-designed augmentation policies are speciﬁed for different datasets. For example, ﬂipping, the widely used transformation on CIF AR- 10/CIF AR-100 and ImageNet, is not suitable for MINST , which will destroy the property of origi nal samples. Hence, several works (Lemley et al., 2017; Cubuk et al., 2018 ; Lin et al., 2019; Ho et al., 2019) have attempted to automatically learn data augmentation polici es. Lemley et al. (2017) propose a method called Smart Augmentation, which merges two or more samples of a class to improve the generaliza- tion of a target network. The result also indicates that an au gmentation network can be learned when a target network is being training. Through well designing t he search space of data augmentation policies, AutoAugment (Cubuk et al., 2018) takes a recurren t neural network (RNN) as a sample controller to ﬁnd the best data augmentation policy for a sel ected dataset. T o reduce the computing cost, the augmentation policy search is performed on proxy t asks. Population based augmentation (PBA) (Ho et al., 2019) replaces the ﬁxed augmentation polic y with a dynamic schedule of aug- mentation policy along with the training process, which is m ostly related to our work. Inspired by population based training (PBT) (Jaderberg et al., 2017), t he augmentation policy search problem in PBA is modeled as a process of hyperparameter schedule lea rning. However, the augmentation schedule learning is still performed on proxy tasks. The lea rned policy schedule should be manually adjusted when the training process of a target network is non -matched with proxy tasks. Another related topic is Generative Adversarial Networks ( GANs) (Goodfellow et al., 2014), which has recently attracted lots of research attention due to its fascinating performance, and also been used to enlarge datasets through directly synthesizing new images (Tran et al., 2017; Perez & W ang, 2017; Antoniou et al., 2017; Gurumurthy et al., 2017; Frid-A dar et al., 2018). Although we formu- late our proposed method as a Min-Max game, there exists an ob vious difference with traditional GANs. W e want to ﬁnd the best augmentation policy to perform i mage transformation along with the training process, rather than synthesize new images. Pe ng et al. (2018) also take such an idea to optimize the training process of a target network in human po se estimation. 3 M ETHOD In this section, we present the implementation of Adversarial AutoAugment. First, the motivation for the adversarial relation between network learning and a ugmentation policy is discussed. Then, we introduce the search space with the dynamic augmentation policy. Finally, the joint framework for network training and augmentation policy search is pres ented in detail. 3.1 M OT IV AT IO N S Although some human-designed data augmentations have been used in the training of DNNs, such as randomly cropping and horizontally ﬂipping on CIF AR-10/ CIF AR-100 and ImageNet, limited randomness will make it very difﬁcult to generate effective samples at the tail end of the training. T o struggle with the problem, more randomness about image tr ansformation is introduced into the search space of AutoAugment (Cubuk et al., 2018) (described in Section 3.2). However, the learned policy is ﬁxed for the entire training process. All of possib le instances of each example will be send to the target network repeatedly, which still results i n an inevitable overﬁtting in a long-epoch training. This phenomenon indicates that the learned polic y is not adaptive to the training process of a target network, especially found on proxy tasks. Hence, th e dynamic and adversarial augmentation policy with the training process is considered as the crucia l feature in our search space. Another consideration is how to improve the efﬁciency of the policy search. In AutoAugment (Cubuk et al., 2018), to evaluate the performance of augment ation policies, a lot of child models should be trained from scratch nearly to convergence. The co mputation in training and evaluat- ing the performance of different sampled policies can not be reused, which leads to huge waste of computation resources. In this paper, we propose a computin g-efﬁcient policy search framework through reusing prior computation in policy evaluation. On ly one target network is used to evaluate the performance of different policies with the help of the tr aining losses of corresponding augmented 3Published as a conference paper at ICLR 2020 Epoch 30 Epoch 90 Epoch 120  …...  Epoch 60  …...  …...  …...  TranslateX, 6  Posterize, 5  Solarize, 6  Posterize, 5  Color, 8  Constrast, 8  Cutout, 3  ShearX, 9  Posterize, 5  Cutout, 7  TranslateX, 9  Cutout, 3  Rotate, 7  Color, 5  Equalize, 9  Invert, 8  TranslateX, 6 Posterize, 5  TranslateY, 8  Cutout, 5  Rotate, 9  Sharpness, 7  Equalize, 4  TranslateX, 6  TranslateX, 1 Color, 8  Rotate, 5  Invert, 7  ShearY, 8  TranslateX, 8  AutoContrast, 3  Posterize, 7  Original  ߬ଵ ߬ଶ ߬ଷ ߬ெ …...  Policy  …...  Figure 2: An example of dynamic augmentation policies learn ed with ResNet-50 on ImageNet. With the training process of the target network, harder augm entation policies are sampled to combat overﬁtting. Intuitively, more geometric transformations , such as TranslateX, ShearY and Rotate, are picked in our sampled policies, which is obviously diffe rent from AutoAugment (Cubuk et al., 2018) concentrating on color-based transformations. instances. The augmentation policy network is learned from the intermediate state of the target net- work, which makes generated augmentation policies more agg ressive and adaptive. On the contrary, to combat harder examples augmented by adversarial policie s, the target network has to learn more robust features, which makes the training more efﬁciently. 3.2 S E A RCH SPACE In this paper, the basic structure of the search space of Auto Augment (Cubuk et al., 2018) is re- served. An augmentation policy is deﬁned as that it is compos ed by 5 sub-policies, each sub-policy contains two image operations to be applied orderly, each op eration has two corresponding parame- ters, i.e., the probability and magnitude of the operation. Finally, the 5 best policies are concatenated to form a single policy with 25 sub-policies. For each image i n a mini-batch, only one sub-policy will be randomly selected to be applied. T o compare with Auto Augment (Cubuk et al., 2018) con- veniently, we just slightly modify the search space with rem oving the probability of each operation. This is because that we think the stochasticity of an operati on with a probability requires a certain epochs to take effect, which will detain the feedback of the i ntermediate state of the target network. There are totally 16 image operations in our search space, in cluding ShearX/Y , TranslateX/Y , Rotate, AutoContrast, Invert, Equalize, Solarize, Posterize, Con trast, Color, Brightness, Sharpness, Cutout (Devries & T aylor, 2017) and Sample Pairing (Inoue, 2018). T he range of the magnitude is also discretized uniformly into 10 values. T o guarantee the convergence during adversarial learning,the magnitude of all the operations are set in a moderate range. 1 Besides, the randomness during the training process is introduced into our search space. Hence , the search space of the policy in each epoch has |S| = (16 ×10)10 ≈ 1. 1×1022 possibilities. Considering the dynamic policy, the number of possible policies with the whole training process can be e xpressed as |S|#epochs. An example of dynamically learning the augmentation policy along with the training process is shown in Figure 2. W e observe that the magnitude (an indication of difﬁculty ) gradually increases with the training process. 1 The more details about the parameter setting please refer to AutoAugment (Cubuk et al., 2018). 4Published as a conference paper at ICLR 2020 3.3 A DV E RS A RIA L LE A RN IN G In this section, the adversarial framework of jointly optim izing network training and augmentation policy search is presented in detail. W e use the augmentatio n policy network A(·, θ) as an adver- sary, which attempts to increase the training loss of the tar get network F(·, w) through adversarial learning. The target network is trained by a large batch form ed by multiple augmented instances of each batch to promote invariant learning (Salazar et al., 20 18), and the losses of different augmen- tation policies applied on the same data are used to train the augmentation policy network by RL algorithm. Considering the target network F(·, w) with a loss function L[F(x, w), y], where each example is transformed by some random data augmentation o(·), the learning process of the target network can be deﬁned as the following minimization problem w∗ = arg min w E x∼ Ω L[F(o(x), w), y], (1) where Ω is the training set, x and y are the input image and the corresponding label, respective ly. The problem is usually solved by vanilla SGD with a learning r ate η and batch size N, and the training procedure for each batch can be expressed as wt+1 = wt − η 1 N N∑ n=1 ∇wL[F(o(xn), w, y n]. (2) T o improve the convergence performance of DNNs, more random and efﬁcient data augmentation is performed under the help of the augmentation policy netwo rk. Hence, the minimization problem should be slightly modiﬁed as w∗ = arg min w E x∼ Ω E τ ∼A (·, θ ) L[F(τ(x), w), y], (3) where τ(·) represents the augmentation policy generated by the networ k A(·, θ). Accordingly, the training rule can be rewritten as wt+1 = wt − η 1 M · N M∑ m=1 N∑ n=1 ∇wL[F(τm(xn), w), y n], (4) where we introduce M different instances of each input example augmented by adve rsarial policies {τ1, τ 2, · · · , τ M }. For convenience, we denote the training loss of a mini-batc h corresponding to the augmentation policy τm as Lm = 1 N N∑ n=1 L[F(τm(xn), w), y n]. (5) Hence, we have an equivalent form of Equation 4 wt+1 = wt − η 1 M M∑ m=1 ∇wLm. (6) Note that the training procedure can be regarded as a larger N · M batch training or an average over M instances of gradient computation without changing the lea rning rate, which will lead to a reduction of gradient variance and a faster convergence of t he target network Hoffer et al. (2019). However, overﬁtting will also come. T o overcome the problem , the augmentation policy network is designed to increase the training loss of the target netwo rk with harder augmentation policies. Therefore, we can mathematically express the object as the f ollowing maximization problem θ∗ = arg max θ J(θ), where J(θ) = E x∼ Ω E τ ∼A (·, θ ) L[F(τ(x), w), y]. (7) Similar to AutoAugment (Cubuk et al., 2018), the augmentati on policy network is also implemented as a RNN shown in Figure 3. At each time step of the RNN controll er, the softmax layer will predict 5Published as a conference paper at ICLR 2020 Select the  type of Op0  Select the  magnitude  of Op0  Select the  type of Op1  Select the  magnitude  of Op1  Hidden  Layer  Softmax  Layer  Embedding Embedding Embedding Embedding  Embedding  Layer  Figure 3: The basic architecture of the controller for gener ating a sub-policy, which consists of two operations with corresponding parameters, the type and mag nitude of each operation. When a policy contains Q sub-policies, the basic architecture will be repeated Q times. Following the setting of AutoAugment (Cubuk et al., 2018), the number of sub-policie s Q is set to 5 in this paper. an action corresponding to a discrete parameter of a sub-pol icy, and then an embedding of the predicted action will be fed into the next time step. In our ex periments, the RNN controller will predict 20 discrete parameters to form a whole policy. However, there has a severe problem in jointly optimizing ta rget network training and augmentation policy search. This is because that non-differentiable aug mentation operations break gradient ﬂow from the target network F to the augmentation policy network A (W ang et al., 2017; Peng et al., 2018). As an alternative approach, REINFORCE algorithm (Wi lliams, 1992) is applied to optimize the augmentation policy network as ∇θ J(θ) = ∇θ E x∼ Ω E τ ∼A (·, θ ) L[F(τ(x), w), y] ≈ ∑ m Lm∇θ pm = ∑ m Lmpm∇θ log pm = E τ ∼A (·, θ ) Lm∇θ log pm ≈ 1 M M∑ m=1 Lm∇θ log pm, (8) where pm represents the probability of the policy τm. T o reduce the variance of gradient ∇θ J(θ), we replace the training loss of a mini-batch Lm with ˆLm a moving average over a certain mini- batches2, and then normalize it among M instances as ˜Lm. Hence, the training procedure of the augmentation policy network can be expressed as ∇θ J(θ) ≈ 1 M M∑ m=1 ˜Lm∇θ log pm, θe+1 = θe + β 1 M M∑ m=1 ˜Lm∇θ log pm, (9) The adversarial learning of target network training and aug mentation policy search is summarized as Algorithm 1. 4 E XPERIME NT S AND ANALYSIS In this section, we ﬁrst reveal the details of experiment set tings. Then, we evaluate our proposed method on CIF AR-10/CIF AR-100, ImageNet, and compare it wit h previous methods. Results in Figure 4 show our method achieves the state-of-the-art perf ormance with higher computing and time efﬁciency 3. 2 The length of the moving average is ﬁxed to an epoch in our expe riments. 3 T o clearly present the advantage of our proposed method, we n ormalize the performance of our method in the Figure 4, and the performance of AutoAugment is plotted a ccordingly . 6Published as a conference paper at ICLR 2020 Algorithm 1 Joint Training of T arget Network and Augmentation Policy Ne twork Initialization: target network F(·, w), augmentation policy network A(·, θ) Input: input examples x, corresponding labels y 1: for 1 ≤ e ≤ epochs do 2: Initialize ˆLm = 0 , ∀m ∈ { 1, 2, · · · , M }; 3: Generate M policies with the probabilities {p1, p 2, · · · , p M }; 4: for 1 ≤ t ≤ T do 5: Augment each batch data with M generated policies, respectively; 6: Update we,t +1 according to Equation 4; 7: Update ˆLm through moving average, ∀m ∈ { 1, 2, · · · , M }; 8: Collect { ˆL1, ˆL2, · · · , ˆLM }; 9: Normalize ˆLm among M instances as ˜Lm, ∀m ∈ { 1, 2, · · · , M }; 10: Update θe+1 via Equation 9; 11: Output w∗ , θ∗ 4.1 E X P E RIM E N T SE T T IN G S The RNN controller is implemented as a one-layer LSTM (Hochr eiter & Schmidhuber, 1997). W e set the hidden size to 100, and the embedding size to 32. W e use Adam optimizer (Kingma & Ba, 2015) with a initial learning rate 0. 00035 to train the controller. T o avoid unexpected rapid conver- gence, an entropy penalty of a weight of 0. 00001 is applied. All the reported results are the mean of ﬁve runs with different initializations. 4.2 E X P E RIM E N T S O N CIFAR-10 A N D CIFAR-100 CIF AR-10 dataset (Krizhevsky & Hinton, 2009) has totally 60 000 images. The training and test sets have 50000 and 10000 images, respectively. Each image i n size of 32 × 32 belongs to one of 10 classes. W e evaluate our proposed method with the fo llowing models: Wide-ResNet- 28-10 (Zagoruyko & Komodakis, 2016), Shake-Shake (26 2x32d ) (Gastaldi, 2017), Shake-Shake (26 2x96d) (Gastaldi, 2017), Shake-Shake (26 2x112d) (Gast aldi, 2017), PyramidNet+ShakeDrop (Han et al., 2017; Y amada et al., 2018). All the models are tra ined on the full training set. T raining details: The Baseline is trained with the standard data augmentation , namely, randomly cropping a part of 32 × 32 from the padded image and horizontally ﬂipping it with a prob ability of 0. 5. The Cutout (Devries & T aylor, 2017) randomly select a 16 × 16 patch of each image, and then set the pixels of the selected patch to zeros. For our met hod, the searched policy is applied in addition to standard data augmentation and Cutout. For each image in the training process, standard data augmentation, the searched policy and Cutout are appli ed in sequence. For Wide-ResNet-28- 10, the step learning rate (LR) schedule is adopted. The cosi ne LR schedule is adopted for the other models. More details about model hyperparameters are suppl ied in A.1. Choice of M: T o choose the optimal M, we select Wide-ResNet-28-10 as a target network, and evaluate the performance of our proposed method verse diffe rent M, where M ∈ { 2, 4, 8, 16, 32}. From Figure 5, we can observe that the test accuracy of the mod el improves rapidly with the increase of M up to 8. The further increase of M does not bring a signiﬁcant improvement. Therefore, to balance the performance and the computing cost, M is set to 8 in all the following experiments. CIF AR-10 results: In T able 1, we report the test error of these models on CIF AR-1 0. For all of these models, our proposed method can achieve better perfor mance compared to previous methods. W e achieve 0. 78% and 0. 68% improvement on Wide-ResNet-28-10 compared to AutoAugment and PBA, respectively. W e achieve a top-1 test error of 1. 36% with PyramidNet+ShakeDrop, which is 0. 1% better than the current state-of-the-art reported in Ho et a l. (2019). As shown in Figure 6(a) and 6(b),we further visualize the probability distributio n of the parameters of the augmentation poli- cies learned with PyramidNet+ShakeDrop on CIF AR-10 over ti me. From Figure 6(a), we can ﬁnd that the percentages of some operations, such as TranslateY , Rotate, Posterize, and SampleParing, gradually increase along with the training process. Meanwh ile, more geometric transformations, such as TranslateX, TranslateY , and Rotate, are picked in th e sampled augmentation policies, which 7Published as a conference paper at ICLR 2020 $FFXUDF\\  RQ\u0003&,)$5\u0010\u0014\u0013\u0013 \u0003 2XU\u00030HWKRG  $XWR$XJPHQW  $FFXUDF\\  RQ\u0003,PDJH1HW  $FFXUDF\\  RQ\u0003&,)$5\u0010\u0014\u0013 \u0003 &RPSXWLQJ  (IILFLHQF\\  7LPH  (IILFLHQF\\  \u0014\u0013\u0013\b\u0003 \u0014\u0013\u0013\b\u0003\u0014\u0013\u0013\b\u0003 \u0014\u0013 \u0013\b \u0003 \u0014\u0013 \u0013\b \u0003 Figure 4: The Comparison of normalized performance between AutoAugment and our method. Please refer to the following tables for more details. /uni00000015/uni00000017/uni0000001b/uni00000014/uni00000019/uni00000016/uni00000015 /uni00000030 /uni0000001c/uni0000001a/uni00000011/uni00000017 /uni0000001c/uni0000001a/uni00000011/uni00000018 /uni0000001c/uni0000001a/uni00000011/uni00000019 /uni0000001c/uni0000001a/uni00000011/uni0000001a /uni0000001c/uni0000001a/uni00000011/uni0000001b /uni0000001c/uni0000001a/uni00000011/uni0000001c /uni0000001c/uni0000001b/uni00000011/uni00000013 /uni0000001c/uni0000001b/uni00000011/uni00000014 /uni0000001c/uni0000001b/uni00000011/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c Figure 5: The T op-1 test accuracy of Wide- ResNet-28-10 on CIF AR-10 verse different M, where M ∈ { 2, 4, 8, 16, 32}. 0 100 200 300 400 500 600 epochs 0 20 40 60 80 100Percentage(%) ShearX ShearY TranslateX TranslateY Rotate AutoContrast Invert Equalize Solarize Posterize SampleParing Cutout Color Constrast Brightness Sharpness (a) Operations 0 100 200 300 400 500 600 epochs 0 20 40 60 80 100Percentage(%) 9 8 7 6 5 4 3 2 1 0 (b) Magnitudes Figure 6: Probability distribution of the parameters in the learned augmentation policies on CIF AR- 10 over time. The number in (b) represents the magnitude of on e operation. Larger number stands for more dramatic image transformations. The probability d istribution of each parameter is the mean of each ﬁve epochs. is different from color-focused AutoAugment (Cubuk et al., 2018) on CIF AR-10. Figure 6(b) shows that large magnitudes gain higher percentages during train ing. However, at the tail of training, low magnitudes remain considerable percentages. This indicat es that our method does not simply learn the transformations with the extremes of the allowed magnit udes to spoil the target network. CIF AR-100 results: W e also evaluate our proposed method on CIF AR-100, as shown i n T able 2. As we can observe from the table, we also achieve the state-of -the-art performance on this dataset. 4.3 E X P E RIM E N T S O N IM AG E NE T As a great challenge in image recognition, ImageNet dataset (Deng et al., 2009) has about 1.2 mil- lion training images and 50000 validation images with 1000 c lasses. In this section, we directly search the augmentation policy on the full training set and t rain ResNet-50 (He et al., 2016), ResNet- 50-D (He et al., 2018) and ResNet-200 (He et al., 2016) from sc ratch. T raining details: For the baseline augmentation, we randomly resize and crop e ach input image to a size of 224 × 224, and then horizontally ﬂip it with a probability of 0. 5. For AutoAugment (Cubuk et al., 2018) and our method, the baseline augmentati on and the augmentation policy are both used for each image. The cosine LR schedule is adopted in the training process. The model hyperparameters on ImageNet is also detailed in A.1. ImageNet results: The performance of our proposed method on ImageNet is presen ted in T able 3. It can be observed that we achieve a top-1 accuracy 79. 40% on ResNet-50 without extra data. T o 8Published as a conference paper at ICLR 2020 T able 1: T op-1 test error (%) on CIF AR-10. W e replicate the re sults of Baseline, Cutout and Au- toAugment methods from Cubuk et al. (2018), and the results o f PBA from Ho et al. (2019) in all of our experiments. Model Baseline Cutout AutoAugment PBA Our Method Wide-ResNet-28-10 3.87 3.08 2.68 2.58 1.90±0.15 Shake-Shake (26 2x32d) 3.55 3.02 2.47 2.54 2.36±0.10 Shake-Shake (26 2x96d) 2.86 2.56 1.99 2.03 1.85±0.12 Shake-Shake (26 2x112d) 2.82 2.57 1.89 2.03 1.78±0.05 PyramidNet+ShakeDrop 2.67 2.31 1.48 1.46 1.36±0.06 T able 2: T op-1 test error (%) on CIF AR-100. Model Baseline Cutout AutoAugment PBA Our Method Wide-ResNet-28-10 18.80 18.41 17.09 16.73 15.49±0.18 Shake-Shake (26 2x96d) 17.05 16.00 14.28 15.31 14.10±0.15 PyramidNet+ShakeDrop 13.99 12.19 10.67 10.94 10.42±0.20 the best of our knowledge, this is the highest top-1 accuracy for ResNet-50 learned on ImageNet. Besides, we only replace the ResNet-50 architecture with Re sNet-50-D, and achieve a consistent improvement with a top-1 accuracy of 80. 00%. T able 3: T op-1 / T op-5 test error (%) on ImageNet. Note that th e result of ResNet-50-D is achieved only through substituting the architecture. Model Baseline AutoAugment PBA Our Method ResNet-50 23.69 / 6.92 22.37 / 6.18 - 20.60±0.15 / 5.53±0.05 ResNet-50-D 22.84 / 6.48 - - 20.00±0.12 / 5.25±0.03 ResNet-200 21.52 / 5.85 20.00 / 4.90 - 18.68±0.18 / 4.70±0.05 4.4 A BL AT IO N ST U DY T o check the effect of each component in our proposed method, we report the test error of ResNet-50 on ImageNet the following augmentation methods in T able 4. • Baseline: Training regularly with the standard data augmentation an d step LR schedule. • Fixed: Augmenting all the instances of each batch with the standar d data augmentation ﬁxed throughout the entire training process. • Random: Augmenting all the instances of each batch with randomly an d dynamically generated policies. • Ours: Augmenting all the instances of each batch with adversaria l policies sampled by the policy network along with the training process. From the table, we can ﬁnd that Fixed can achieve 0. 99% error reduction compared to Baseline. This shows that a large-batch training with multiple augmen ted instances of each mini-batch can indeed improve the generalization of the model, which is con sistent with the conclusion presented in Hoffer et al. (2019). In addition, the test error of Random is 1. 02% better than Fixed. This indicates that augmenting batch with randomly generated policies can reduce overﬁtting in a certain extent. Furthermore, our method achieves the best test error of 20. 60% through augmenting samples with adversarial policies. From the result, we can conclude that these policies generated by the policy network are more adaptive to the training process, and make t he target network have to learn more robust features. 4.5 C O M P U T IN G CO S T A N D TIM E OV E RH E A D Computing Cost: The computation in target network training is reused for pol icy evaluation. This makes the computing cost in policy search become negligible . Although there exists an increase of 9Published as a conference paper at ICLR 2020 T able 4: T op-1 test error (%) of ResNet-50 with different aug mentation methods on ImageNet. Method Aug. Policy Enlarge Batch LR Schedule T est Error Baseline standard M = 1 step 23.69 Fixed standard M = 8 cosine 22.70 Random random M = 8 cosine 21.68 Ours adversarial M = 8 cosine 20.60 computing cost in target network training, the total comput ing cost in training one target network with augmentation policies is quite small compared to prior work. Time Overhead: Since we just train one target network with a large batch dist ributedly and simul- taneously, the time overhead of the large-batch training is equal to the regular training. Meanwhile, the joint optimization of target network training and augme ntation policy search dispenses with the process of ofﬂine policy search and the retraining of a targe t network, which leads to a extreme time overhead reduction. In T able 5, we take the training of ResNet-50 on ImageNet as an example to compare the computing cost and time overhead of our method and AutoAugment. From th e table, we can ﬁnd that our method is 12× less computing cost and 11× shorter time overhead than AutoAugment. T able 5: The comparison of computing cost (GPU hours) and tim e overhead (days) in training ResNet-50 on ImageNet between AutoAugment and our method. T he computing cost and time overhead are estimated on 64 NVIDIA T esla V100s. Method Computing Cost Time Overhead Searching Training T otal Searching Training T otal AutoAugment 15000 160 15160 10 1 11 Our Method ∼0 1280 1280 ∼0 1 1 4.6 T RA N S F E RA BIL IT Y ACRO S S DATA S E T S A N D ARCH IT E CT U RE S T o further show the higher efﬁciency of our method, the trans ferability of the learned augmentation policies is evaluated in this section. W e ﬁrst take a snapsho t of the adversarial training process of ResNet-50 on ImageNet, and then directly use the learned dyn amic augmentation policies to regu- larly train the following models: Wide-ResNet-28-10 on CIF AR-10/100, ResNet-50-D on ImageNet and ResNet200 on ImageNet. T able 6 presents the experimenta l results of the transferability. From the table, we can ﬁnd that a competitive performance can be st ill achieved through direct policy transfer. This indicates that the learned augmentation policies tra nsfer well across datasets and architectures. However, compared to the proposed method, t he policy transfer results in an obvious performance degradation, especially the transfer across d atasets. T able 6: T op-1 test error (%) of the transfer of the augmentat ion policies learned with ResNet-50 on ImageNet. Method Dataset AutoAugment Our Method Policy Transfer Wide-ResNet-28-10 CIF AR-10 2.68 1.90 2.45±0.13 Wide-ResNet-28-10 CIF AR-100 17.09 15.49 16.48±0.15 ResNet-50-D ImageNet - 20.00 20.20±0.05 ResNet-200 ImageNet 20.00 18.68 19.05±0.10 5 C ONCLUSIO N In this paper, we introduce the idea of adversarial learning into automatic data augmentation. The policy network tries to combat the overﬁtting of the target n etwork through generating adversarial policies with the training process. T o oppose this, robust f eatures are learned in the target network, which leads to a signiﬁcant performance improvement. Meanw hile, the augmentation policy search is performed along with the training of a target network, and the computation in network training is reused for policy evaluation, which can extremely reduce the search cost and make our method more computing-efﬁcient. 10Published as a conference paper at ICLR 2020 REFERENC ES Antreas Antoniou, Amos J. Storkey, and Harrison Edwards. Da ta augmentation generative adver- sarial networks. ICLR, 2017. Ekin D. Cubuk, Barret Zoph, Dandelion Man ´ e, V ijay V asudeva n, and Quoc V . Le. Autoaugment: Learning augmentation policies from data. CVPR, 2018. Jia Deng, W ei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li F ei-Fei. Imagenet: A large-scale hierarchical image database. CVPR, 2009. T errance Devries and Graham W . T aylor. Improved regulariza tion of convolutional neural networks with cutout. CoRR, abs/1708.04552, 2017. Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldber ger, and Hayit Greenspan. Synthetic data augmentation using GAN for improved liver lesion class iﬁcation. IEEE International Sym- posium on Biomedical Imaging (ISBI), 2018. Xavier Gastaldi. Shake-shake regularization. CoRR, abs/1705.07485, 2017. Ian J. Goodfellow , Jean Pouget-Abadie, Mehdi Mirza, Bing Xu , David W arde-Farley, Sherjil Ozair, Aaron Courville, and Y oshua Bengio. Generative adversaria l networks. NIPS, 2014. Minghao Guo, Zhao Zhong, W ei Wu, Dahua Lin, and Junjie Y an. IR LAS: inverse reinforcement learning for architecture search. CoRR, abs/1812.05285, 2018. Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and V enkatesh Babu Radhakrishnan. Deli- gan : Generative adversarial networks for diverse and limit ed data. CVPR, 2017. Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal resi dual networks. CVPR, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep r esidual learning for image recog- nition. CVPR, 2016. T ong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classiﬁcation with convolutional neural networks. CoRR, abs/1812.01187, 2018. Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen . Population based augmentation: Efﬁcient learning of augmentation policy schedules. ICML, 2019. Sepp Hochreiter and Jrgen Schmidhuber. Long short-term mem ory. Neural Computation, 1997. Elad Hoffer, T al Ben-Nun, Itay Hubara, Niv Giladi, T orsten H oeﬂer, and Daniel Soudry. Augment your batch: better training with larger batches. CoRR, abs/1901.09335, 2019. Hiroshi Inoue. Data augmentation by pairing samples for ima ges classiﬁcation. CoRR, abs/1801.02929, 2018. Max Jaderberg, V alentin Dalibard, Simon Osindero, W ojciec h M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol V inyals, Tim Green, Iain Dunning, Karen Simon yan, Chrisantha Fernando, and Koray Kavukcuoglu. Population based training of neural net works. CoRR, abs/1711.09846, 2017. Diederik P . Kingma and Jimmy Ba. Adam: A method for stochasti c optimization. ICLR, 2015. Alex Krizhevsky and Geoffrey E. Hinton. Learning multiple l ayers of features from tiny images. T echnical report, University of T oronto, 2009. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Im agenet classiﬁcation with deep convo- lutional neural networks. NIPS, 2012. Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation - learning an optimal data augmentation strategy. CoRR, abs/1703.08383, 2017. Chen Lin, Minghao Guo, Chuming Li, W ei Wu, Dahua Lin, W anli Ou yang, and Junjie Y an. Online hyper-parameter learning for auto-augmentation strategy . CoRR, abs/1905.07373, 2019. 11Published as a conference paper at ICLR 2020 Xi Peng, Zhiqiang T ang, Fei Y ang, Rog ´ erio Schmidt Feris, an d Dimitris N. Metaxas. Jointly op- timize data augmentation and network training: Adversaria l data augmentation in human pose estimation. CVPR, 2018. Luis Perez and Jason W ang. The effectiveness of data augment ation in image classiﬁcation using deep learning. CoRR, abs/1712.04621, 2017. Julian Salazar, Davis Liang, Zhiheng Huang, and Zachary C. L ipton. Invariant representation learn- ing for robust deep networks. NeurIPS W orkshop, 2018. Christian Szegedy, W ei Liu, Y angqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, V incent V anhoucke, and Andrew Rabinovich. G oing deeper with convolutions. CVPR, 2015. T oan Tran, Trung Pham, Gustavo Carneiro, Lyle J. Palmer, and Ian D. Reid. A bayesian data augmentation approach for learning deep models. NIPS, 2017. Li W an, Matthew Zeiler, Sixin Zhang, Y ann LeCun, and Rob Ferg us. Regularization of neural networks using dropconnect. ICML, 2013. Xiaolong W ang, Abhinav Shrivastava, and Abhinav Gupta. A-f ast-rcnn: Hard positive generation via adversary for object detection. CVPR, 2017. Ronald J. Williams. Simple statistical gradient-followin g algorithms for connectionist reinforcement learning. Machine Learning, 1992. Y oshihiro Y amada, Masakazu Iwamura, and Koichi Kise. Shake drop regularization. CoRR, abs/1802.02375, 2018. Sergey Zagoruyko and Nikos Komodakis. Wide residual networ ks. British Machine V ision Confer- ence, 2016. Zhao Zhong, Junjie Y an, and Cheng-Lin Liu. Practical networ k blocks design with Q-learning. CVPR, 2018a. Zhao Zhong, Zichen Y ang, Boyang Deng, Junjie Y an, W ei Wu, Jin g Shao, and Cheng-Lin Liu. BlockQNN: Efﬁcient block-wise neural network architectur e generation. CoRR, abs/1808.05584, 2018b. Barret Zoph and Quoc V . Le. Neural architecture search with r einforcement learning. ICLR, 2016. Barret Zoph, V ijay V asudevan, Jonathon Shlens, and Quoc V . L e. Learning transferable architectures for scalable image recognition. CVPR, 2017. A A PPENDIX A.1 H Y P E RPA RA M E T E RS W e detail the model hyperparameters on CIF AR-10/CIF AR-100 and ImageNet in T able 7. 12Published as a conference paper at ICLR 2020 T able 7: Model hyperparameters on CIF AR-10/CIF AR-100 and I mageNet. LR represents learning rate, and WD represents weight decay. W e do not speciﬁcally t une these hyperparameters, and all of these are consistent with previous works, expect for the n umber of epochs. Dataset Model Batch Size (N · M) LR WD Epoch CIF AR-10 Wide-ResNet-28-10 128 · 8 0.1 5e-4 200 CIF AR-10 Shake-Shake (26 2x32d) 128 · 8 0.2 1e-4 600 CIF AR-10 Shake-Shake (26 2x96d) 128 · 8 0.2 1e-4 600 CIF AR-10 Shake-Shake (26 2x112d) 128 · 8 0.2 1e-4 600 CIF AR-10 PyramidNet+ShakeDrop 128 · 8 0.1 1e-4 600 CIF AR-100 Wide-ResNet-28-10 128 · 8 0.1 5e-4 200 CIF AR-100 Shake-Shake (26 2x96d) 128 · 8 0.1 5e-4 1200 CIF AR-100 PyramidNet+ShakeDrop 128 · 8 0.5 1e-4 1200 ImageNet ResNet-50 2048 · 8 0.8 1e-4 120 ImageNet ResNet-50-D 2048 · 8 0.8 1e-4 120 ImageNet ResNet-200 2048 · 8 0.8 1e-4 120 13",
      "references": [
        "Data augmentation generative adversarial networks",
        "Autoaugment: Learning augmentation policies from data",
        "Imagenet: A large-scale hierarchical image database",
        "Improved regularization of convolutional neural networks with cutout",
        "Synthetic data augmentation using GAN for improved liver lesion classification",
        "Shake-shake regularization",
        "Generative adversarial networks",
        "IR LAS: inverse reinforcement learning for architecture search",
        "DeLiGAN : Generative adversarial networks for diverse and limited data",
        "Deep pyramidal residual networks",
        "Deep residual learning for image recognition",
        "Bag of tricks for image classification with convolutional neural networks",
        "Population based augmentation: Efficient learning of augmentation policy schedules",
        "Long short-term memory",
        "Augment your batch: better training with larger batches",
        "Data augmentation by pairing samples for images classification",
        "Population based training of neural networks",
        "Adam: A method for stochastic optimization",
        "Learning multiple layers of features from tiny images",
        "Imagenet classification with deep convolutional neural networks",
        "Smart augmentation - learning an optimal data augmentation strategy",
        "Online hyper-parameter learning for auto-augmentation strategy",
        "Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation",
        "The effectiveness of data augmentation in image classification using deep learning",
        "Invariant representation learning for robust deep networks",
        "Going deeper with convolutions",
        "A bayesian data augmentation approach for learning deep models",
        "Regularization of neural networks using dropconnect",
        "A-fast-rcnn: Hard positive generation via adversary for object detection",
        "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
        "Shakedrop regularization",
        "Wide residual networks",
        "Practical network blocks design with Q-learning",
        "BlockQNN: Efficient block-wise neural network architecture generation",
        "Neural architecture search with reinforcement learning",
        "Learning transferable architectures for scalable image recognition"
      ],
      "meta_data": {
        "arxiv_id": "1912.11188v1",
        "authors": [
          "Xinyu Zhang",
          "Qiang Wang",
          "Jian Zhang",
          "Zhao Zhong"
        ],
        "published_date": "2019-12-24T03:17:17Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes Adversarial AutoAugment, an online, adversarial data-augmentation policy search method that jointly trains a policy network and the target model, eliminating costly proxy-task search used by AutoAugment and achieving up to 12× lower compute and state-of-the-art accuracy on CIFAR-10/100 and ImageNet.",
        "methodology": "Formulates training as a min–max game: an RNN controller (one-layer LSTM) samples augmentation policies to maximize target-network loss, trained with REINFORCE using the target network’s per-policy batch losses as reward; the target network is simultaneously trained on large batches formed by M (typically 8) differently augmented instances of each mini-batch to minimize loss. Policies are dynamic per epoch, drawn from a search space of 16 image operations with 10 discrete magnitudes (probability parameter removed). Computation used for model training is reused for policy evaluation, avoiding retraining.",
        "experimental_setup": "Datasets: CIFAR-10 and CIFAR-100 (50k train/10k test, 32×32 images) and ImageNet ILSVRC-12 (1.28M train/50k val, 1k classes).\nModels: CIFAR—Wide-ResNet-28-10, Shake-Shake (26 2×32d/96d/112d), PyramidNet+ShakeDrop; ImageNet—ResNet-50, ResNet-50-D, ResNet-200.\nBaselines: standard augmentation, Cutout, AutoAugment, Population-Based Augmentation (PBA).\nTraining details: stochastic gradient descent with cosine/step LR, weight decay 1e-4–5e-4, enlarged batch size N·M (e.g., 128·8 on CIFAR, 2048·8 on ImageNet), M selected via ablation (best at 8). Metrics: top-1 and top-5 error/accuracy on test or validation sets. Compute estimates given on 64 NVIDIA V100 GPUs; comparisons show 15,160 vs 1,280 GPU-h.",
        "limitations": "Still relies on substantial GPU resources, large batch training, and distributed setup; although cheaper than AutoAugment, not lightweight for low-resource users. Search space limited to predefined 16 operations with discrete magnitudes, possibly missing task-specific or continuous augmentations. Uses REINFORCE, which has high variance and may require entropy regularization and careful tuning. Empirical selection of hyperparameters like M; performance degrades when transferring policies across very different datasets. Evaluation restricted to image classification; effectiveness on other domains/tasks unverified.",
        "future_research_directions": "1. Extend adversarial augmentation framework to other domains (object detection, segmentation, medical imaging) and modalities (text, audio).\n2. Enlarge or learn the augmentation search space, including continuous magnitudes, learned geometric or neural style transforms.\n3. Replace REINFORCE with lower-variance differentiable or gradient-based policy optimization.\n4. Investigate adaptive strategies for choosing M or batch composition to balance compute and performance.\n5. Explore theoretical analysis of convergence and robustness, and integrate with adversarial robustness training schemes.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Deep AutoAugment",
      "full_text": "Published as a conference paper at ICLR 2022 DEEP AUTO AUGMENT Yu Zheng1, Zhi Zhang2, Shen Yan1, Mi Zhang1 1Michigan State University, 2Amazon Web Services zhengy30@msu.edu, zhiz@amazon.com, {yanshen6, mizhang}@msu.edu ABSTRACT While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this work, instead of ﬁxing a set of hand-picked default augmentations alongside the searched data augmentations, we propose a fully automated approach for data augmentation search named Deep AutoAugment (DeepAA). DeepAA progressively builds a multi-layer data augmentation pipeline from scratch by stacking augmentation layers one at a time until reaching conver- gence. For each augmentation layer, the policy is optimized to maximize the cosine similarity between the gradients of the original and augmented data along the direction with low variance. Our experiments show that even without default aug- mentations, we can learn an augmentation policy that achieves strong performance with that of previous works. Extensive ablation studies show that the regularized gradient matching is an effective search method for data augmentation policies. Our code is available at: https://github.com/MSU-MLSys-Lab/DeepAA. 1 I NTRODUCTION Augmentation Policy Default Transformation  1 (A) Transformation  2 Transformation  2 Transformation  1 Transformation  1 Default Transformation  2 Augmentation Policy Transformation  2 Transformation  2 Transformation  1 Transformation  1 Transformation  K-1 Transformation  K-1 Transformation  K Transformation  K (B) ... Figure 1: (A) Existing automated data augmentation meth- ods with shallow augmentation policy followed by hand- picked transformations. (B) DeepAA with deep augmentation policy with no hand-picked transformations. Data augmentation (DA) is a powerful tech- nique for machine learning since it effec- tively regularizes the model by increas- ing the number and the diversity of data points (Goodfellow et al., 2016; Zhang et al., 2017). A large body of data aug- mentation transformations has been pro- posed (Inoue, 2018; Zhang et al., 2018; DeVries & Taylor, 2017; Yun et al., 2019; Hendrycks et al., 2020; Yan et al., 2020) to improve model performance. While ap- plying a set of well-designed augmentation transformations could help yield consider- able performance enhancement especially in image recognition tasks, manually select- ing high-quality augmentation transforma- tions and determining how they should be combined still require strong domain exper- tise and prior knowledge of the dataset of interest. With the recent trend of automated machine learning (AutoML), data augmen- tation search ﬂourishes in the image domain (Cubuk et al., 2019; 2020; Ho et al., 2019; Lim et al., 2019; Hataya et al., 2020; Li et al., 2020; Liu et al., 2021), which yields signiﬁcant performance improvement over hand-crafted data augmentation methods. Although data augmentation policies in previous works (Cubuk et al., 2019; 2020; Ho et al., 2019; Lim et al., 2019; Hataya et al., 2020; Li et al., 2020) contain multiple transformations applied sequentially, only one or two transformations of each sub-policy are found through searching whereas the rest transformations are hand-picked and applied by default in addition to the found policy (Figure 1(A)). From this perspective, we believe that previous automated methods are not entirely automatedas they are still built upon hand-crafted default augmentations. 1 arXiv:2203.06172v2  [cs.CV]  15 Mar 2022Published as a conference paper at ICLR 2022 In this work, we propose Deep AutoAugment (DeepAA), a multi-layer data augmentation search method which aims to remove the need of hand-crafted default transformations (Figure 1(B)). DeepAA fully automates the data augmentation process by searching a deep data augmentation policy on an expanded set of transformations that includes the widely adopted search space and the default transformations (e.g. ﬂips, Cutout, crop). We formulate the search of data augmentation policy as a regularized gradient matching problem by maximizing the cosine similarity of the gradients between augmented data and original data with regularization. To avoid exponential growth of dimensionality of the search space when more augmentation layers are used, we incrementally stack augmentation layers based on the data distribution transformed by all the previous augmentation layers. We evaluate the performance of DeepAA on three datasets – CIFAR-10, CIFAR-100, and ImageNet – and compare it with existing automated data augmentation search methods including AutoAugment (AA) (Cubuk et al., 2019), PBA (Ho et al., 2019), Fast AutoAugment (FastAA) (Lim et al., 2019), Faster AutoAugment (Faster AA) (Hataya et al., 2020), DADA (Li et al., 2020), RandAugment (RA) (Cubuk et al., 2020), UniformAugment (UA) (LingChen et al., 2020), TrivialAugment (TA) (M¨uller & Hutter, 2021), and Adversarial AutoAugment (AdvAA) (Zhang et al., 2019). Our results show that, without any default augmentations, DeepAA achieves the best performance compared to existing automatic augmentation search methods on CIFAR-10, CIFAR-100 on Wide-ResNet-28-10 and ImageNet on ResNet-50 and ResNet-200 with standard augmentation space and training procedure. We summarize our main contributions below: • We propose Deep AutoAugment (DeepAA), a fully automated data augmentation search method that ﬁnds a multi-layer data augmentation policy from scratch. • We formulate such multi-layer data augmentation search as a regularized gradient matching problem. We show that maximizing cosine similarity along the direction of low variance is effective for data augmentation search when augmentation layers go deep. • We address the issue of exponential growth of the dimensionality of the search space when more augmentation layers are added by incrementally adding augmentation layers based on the data distribution transformed by all the previous augmentation layers. • Our experiment results show that, without using any default augmentations, DeepAA achieves stronger performance compared with prior works. 2 R ELATED WORK Automated Data Augmentation.Automating data augmentation policy design has recently emerged as a promising paradigm for data augmentation. The pioneer work on automated data augmentation was proposed in AutoAugment (Cubuk et al., 2019), where the search is performed under reinforce- ment learning framework. AutoAugment requires to train the neural network repeatedly, which takes thousands of GPU hours to converge. Subsequent works (Lim et al., 2019; Li et al., 2020; Liu et al., 2021) aim at reducing the computation cost. Fast AutoAugment (Lim et al., 2019) treats data augmentation as inference time density matching which can be implemented efﬁciently with Bayesian optimization. Differentiable Automatic Data Augmentation (DADA) (Li et al., 2020) further reduces the computation cost through a reparameterized Gumbel-softmax distribution (Jang et al., 2017). RandAugment (Cubuk et al., 2020) introduces a simpliﬁed search space containing two interpretable hyperparameters, which can be optimized simply by grid search. Adversarial AutoAugment (AdvAA) (Zhang et al., 2019) searches for the augmentation policy in an adversarial and online manner. It also incorporates the concept of Batch Augmentaiton (Berman et al., 2019; Hoffer et al., 2020), where multiple adversarial policies run in parallel. Although many automated data augmentation methods have been proposed, the use of default augmentations still imposes strong domain knowledge. Gradient Matching. Our work is also related to gradient matching. In (Du et al., 2018), the authors showed that the cosine similarity between the gradients of different tasks provides a signal to detect when an auxiliary loss is helpful to the main loss. In (Wang et al., 2020), the authors proposed to use cosine similarity as the training signal to optimize the data usage via weighting data points. A similar approach was proposed in (M¨uller et al., 2021), which uses the gradient inner product as a per-example reward for optimizing data distribution and data augmentation under the reinforcement learning framework. Our approach also utilizes the cosine similarity to guide the data augmentation 2Published as a conference paper at ICLR 2022 search. However, our implementation of cosine similarity is different from the above from two aspects: we propose a Jacobian-vector product form to backpropagate through the cosine similarity, which is computational and memory efﬁcient and does not require computing higher order derivative; we also propose a sampling scheme that effectively allows the cosine similarity to increase with added augmentation stages. 3 D EEP AUTO AUGMENT 3.1 O VERVIEW Data augmentation can be viewed as a process of ﬁlling missing data points in the dataset with the same data distribution (Hataya et al., 2020). By augmenting a single data point multiple times, we expect the resulting data distribution to be close to the full dataset under a certain type of transformation. For example, by augmenting a single image with proper color jittering, we obtain a batch of augmented images which has similar distribution of lighting conditions as the full dataset. As the distribution of augmented data gets closer to the full dataset, the gradient of the augmented data should be steered towards a batch of original data sampled from the dataset. In DeepAA, we formulate the search of the data augmentation policy as a regularized gradient matching problem, which manages to steer the gradient to a batch of original data by augmenting a single image multiple times. Speciﬁcally, we construct the augmented training batch by augmenting a single training data point multiple times following the augmentation policy. We construct a validation batch by sampling a batch of original data from the validation set. We expect that by augmentation, the gradient of augmented training batch can be steered towards the gradient of the validation batch. To do so, we search for data augmentation that maximizes the cosine similarity between the gradients of the validation data and the augmented training data. The intuition is that an effective data augmentation should preserve data distribution (Chen et al., 2020) where the distribution of the augmented images should align with the distribution of the validation set such that the training gradient direction is close to the validation gradient direction. Another challenge for augmentation policy search is that the search space can be prohibitively large with deep augmentation layers (K ≥5). This was not a problem in previous works, where the augmentation policies is shallow ( K ≤2). For example, in AutoAugment Cubuk et al. (2019), each sub-policy contains K = 2transformations to be applied sequentially, and the search space of AutoAugment contains 16 image operations and 10 discrete magnitude levels. The resulting number of combinations of transformations in AutoAugment is roughly (16 ×10)2 = 25,600, which is handled well in previous works. However, when discarding the default augmentation pipeline and searching for data augmentations from scratch, it requires deeper augmentation layers in order to perform well. For a data augmentation with K = 5sequentially applied transformations, the number of sub-policies is (16 ×10)5 ≈1011, which is prohibitively large for the following two reasons. First, it becomes less likely to encounter a good policy by exploration as good policies become more sparse on high dimensional search space. Second, the dimension of parameters in the policy also grows with K, making it more computational challenging to optimize. To tackle this challenge, we propose to build up the full data augmentation by progressively stacking augmentation layers, where each augmentation layer is optimized on top of the data distribution transformed by all previous layers. This avoids sampling sub-policies from such a large search space, and the number of parameters of the policy is reduced from |T|K to T for each augmentation layer. 3.2 S EARCH SPACE Let O denote the set of augmentation operations ( e.g. identity, rotate, brightness), m denote an operation magnitude in the set M, and xdenote an image sampled from the space X. We deﬁne the set of transformations as the set of operations with a ﬁxed magnitude as T := {t|t= o(·; m), o∈ O and m∈M}. Under this deﬁnition, every tis a map t: X→X , and there are |T|= |M|·|O| possible transformations. In previous works (Cubuk et al., 2019; Lim et al., 2019; Li et al., 2020; Hataya et al., 2020), a data augmentation policy Pconsists of several sub-policies. As explained above, the size of candidate sub-policies grows exponentially with depth K. Therefore, we propose a practical method that builds up the full data augmentation by progressively stacking augmentation layers. The ﬁnal data augmentation policy hence consists of Klayers of sequentially applied policy P= {P1,··· ,PK}, where policy Pk is optimized conditioned on the data distribution augmented 3Published as a conference paper at ICLR 2022 by all previous (k−1) layers of policies. Thus we write the policy as a conditional distribution Pk := pθk(n|{P1,··· ,Pk−1}) where ndenotes the indices of transformations in T. For the purpose of clarity, we use a simpliﬁed notation as pθk to replace pθk(n|{P1,··· ,Pk−1}). 3.3 A UGMENTATION POLICY SEARCH VIA REGULARIZED GRADIENT MATCHING Assume that a single data point xis augmented multiple times following the policy pθ. The resulting average gradient of such augmentation is denoted as g(x,θ), which is a function of data xand policy parameters θ. Let vdenote the gradients of a batch of the original data. We optimize the policy by maximizing the cosine similarity between the gradients of the augmented data and a batch of the original data as follows: θ= arg max θ cosineSimilarity(v,g(x,θ)) (1) = arg max θ vT ·g(x,θ) ∥v∥·∥g(x,θ)∥ where ∥·∥denotes the L2-norm. The parameters of the policy can be updated via gradient ascent: θ←θ+ η∇θ cosineSimilarity(v,g(x,θ)), (2) where ηis the learning rate. 3.3.1 P OLICY SEARCH FOR ONE LAYER We start with the case where the data augmentation policy only contains a single augmentation layer, i.e., P= {pθ}. Let L(x; w) denote the classiﬁcation loss of data point xwhere w∈RD represents the ﬂattened weights of the neural network. Consider applying augmentation on a single data point xfollowing the distribution pθ. The resulting averaged gradient can be calculated analytically by averaging all the possible transformations in T with the corresponding probability p(θ): g(x; θ) = |T|∑ n=1 pθ(n)∇wL(tn(x); w) (3) = G(x) ·pθ where G(x) = [ ∇wL(t1(x); w),··· ,∇wL(t|T|(x); w) ] is a D×|T|Jacobian matrix, and pθ = [pθ(1),··· ,pθ(|T|)]T is a |T|dimensional categorical distribution. The gradient w.r.t. the cosine similarity in Eq. (2) can be derived as: ∇θ cosineSimilarity(v,g(x; θ)) =∇θpθ ·r (4) where r= G(x)T ( v ∥g(θ)∥−vTg(θ) ∥g(θ)∥2 · g(θ) ∥g(θ)∥ ) (5) which can be interpreted as a reward for each transformation. Therefore, pθ ·rin Eq.(4) represents the average reward under policy pθ. 3.3.2 P OLICY SEARCH FOR MULTIPLE LAYERS The above derivation is based on the assumption that g(θ) can be computed analytically by Eq.(3). However, when K ≥2, it becomes impractical to compute the average gradient of the augmented data given that the search space dimensionality grows exponentially with K. Consequently, we need to average the gradient of all |T|K possible sub-policies. To reduce the parameters of the policy to T for each augmentation layer, we propose to incrementally stack augmentations based on the data distribution transformed by all the previous augmentation layers. Speciﬁcally, let P= {P1,··· ,PK}denote the K-layer policy. The policy Pk modiﬁes the data distribution on top of the data distribution augmented by the previous (k−1) layers. Therefore, the policy at the kth layer is a distribution Pk = pθk(n) conditioned on the policies {P1,··· ,Pk−1} 4Published as a conference paper at ICLR 2022 where each one is a |T|-dimensional categorical distribution. Given that, the Jacobian matrix at the kth layer can be derived by averaging over the previous(k−1) layers of policies as follows: G(x)k = |T|∑ nk−1=1 ··· |T|∑ n1=1 pθk−1 (nk−1) ···pθ1 (n1)[∇wL((t1 ◦tnk−1 ···◦ tn1 )(x); w),··· , ∇wL((t|T|◦tnk−1 ◦···◦ tn1 )(x); w)] (6) where Gk can be estimated via the Monte Carlo method as: ˜Gk(x) = ∑ ˜nk−1∼pθk ··· ∑ ˜n1∼pθ1 [∇wL((t1 ◦t˜nk−1 ···◦ t˜n1 )(x); w),··· , ∇wL((t|T|◦t˜nk−1 ◦···◦ t˜n1 )(x); w)] (7) where ˜nk−1 ∼pθk−1 (n), ··· ,˜n1 ∼pθ1 (n). The average gradient at the kth layer can be estimated by the Monte Carlo method as: ˜g(x; θk) = ∑ ˜nk∼pθk ··· ∑ ˜n1∼pθ1 ∇wL((t˜nk ◦···◦ t˜n1 )(x); w) . (8) Therefore, the reward at the kth layer is derived as: ˜rk(x) = ( ˜Gk(x) )T ( v ∥˜gk(x; θk)∥−vT˜gk(x; θk) ∥˜gk(x; θk)∥2 · ˜gk(x; θk) ∥˜gk(x; θk)∥ ) . (9) To prevent the augmentation policy from overﬁtting, we regularize the optimization by avoiding optimizing towards the direction with high variance. Thus, we penalize the average reward with its standard deviation as rk = Ex{˜rk(x)}−c· √ Ex{(˜rk(x) −Ex{˜rk(x)})2}, (10) where we use 16 randomly sampled images to calculate the expectation. The hyperparameter c controls the degree of regularization, which is set to 1.0. With such regularization, we prevent the policy from converging to the transformations with high variance. Therefore the parameters of policy Pk (k≥2) can be updated as: θ←θk + η∇θk cosineSimilarity(v,g(θk)) (11) where ∇θ cosineSimilarity(v,gk(x; θ)) =∇θpθk ·rk. (12) 4 E XPERIMENTS AND ANALYSIS Benchmarks and Baselines.We evaluate the performance of DeepAA on three standard benchmarks: CIFAR-10, CIFAR-100, ImageNet, and compare it against a baseline based on standard augmentations (i.e., ﬂip left-righ, pad-and-crop for CIFAR-10/100, and Inception-style preprocesing (Szegedy et al., 2015) for ImageNet) as well as nine existing automatic augmentation methods including (1) AutoAugment (AA) (Cubuk et al., 2019), (2) PBA (Ho et al., 2019), (3) Fast AutoAugment (Fast AA) (Lim et al., 2019), (4) Faster AutoAugment (Hataya et al., 2020), (5) DADA (Li et al., 2020), (6) RandAugment (RA) (Cubuk et al., 2020), (7) UniformAugment (UA) (LingChen et al., 2020), (8) TrivialAugment (TA) (M¨uller & Hutter, 2021), and (9) Adversarial AutoAugment (AdvAA) (Zhang et al., 2019). Search Space.We set up the operation setO to include 16 commonly used operations (identity, shear- x, shear-y, translate-x, translate-y, rotate, solarize, equalize, color, posterize, contrast, brightness, sharpness, autoContrast, invert, Cutout) as well as two operations (i.e., ﬂips and crop) that are used as the default operations in the aforementioned methods. Among the operations in O, 11 operations are associated with magnitudes. We then discretize the range of magnitudes into 12 uniformly spaced levels and treat each operation with a discrete magnitude as an independent transformation. Therefore, the policy in each layer is a 139-dimensional categorical distribution corresponding to |T|= 139 {operation, magnitude}pairs. The list of operations and the range of magnitudes in the standard augmentation space are summarized in Appendix A. 5Published as a conference paper at ICLR 2022 4.1 P ERFORMANCE ON CIFAR-10 AND CIFAR-100 Policy Search.Following (Cubuk et al., 2019), we conduct the augmentation policy search based on Wide-ResNet-40-2 (Zagoruyko & Komodakis, 2016). We ﬁrst train the network on a subset of 4,000 randomly selected samples from CIFAR-10. We then progressively update the policy network parameters θk (k= 1,2,··· ,K) for 512 iterations for each of the Kaugmentation layers. We use the Adam optimizer (Kingma & Ba, 2015) and set the learning rate to 0.025 for policy updating. Policy Evaluation.Using the publicly available repository of Fast AutoAugment (Lim et al., 2019), we evaluate the found augmentation policy on both CIFAR-10 and CIFAR-100 using Wide-ResNet- 28-10 and Shake-Shake-2x96d models. The evaluation conﬁgurations are kept consistent with that of Fast AutoAugment. Results. Table 1 reports the Top-1 test accuracy on CIFAR-10/100 for Wide-ResNet-28-10 and Shake-Shake-2x96d, respectively. The results of DeepAA are the average of four independent runs with different initializations. We also show the 95% conﬁdence interval of the mean accuracy. As shown, DeepAA achieves the best performance compared against previous works using the standard augmentation space. Note that TA(Wide) uses a wider (stronger) augmentation space on this dataset. Baseline AA PBA FastAA FasterAA DADA RA UA TA(RA) TA(Wide)1 DeepAA CIFAR-10WRN-28-10 96.1 97.4 97.4 97.3 97.4 97.3 97.3 97.33 97.46 97.46 97.56±0.14Shake-Shake(26 2x96d) 97.1 98.0 98.0 98.0 98.0 98.0 98.0 98.1 98.05 98.21 98.11±0.12 CIFAR-100WRN-28-10 81.2 82.9 83.3 82.7 82.7 82.5 83.3 82.82 83.54 84.33 84.02±0.18Shake-Shake(26 2x96d) 82.9 85.7 84.7 85.1 85.0 84.7 - - - 86.19 85.19±0.28 Table 1: Top-1 test accuracy on CIFAR-10/100 for Wide-ResNet-28-10 and Shake-Shake-2x96d. The results of DeepAA are averaged over four independent runs with different initializations. The 95% conﬁdence interval is denoted by ±. 4.2 P ERFORMANCE ON IMAGE NET Policy Search.We conduct the augmentation policy search based on ResNet-18 (He et al., 2016). We ﬁrst train the network on a subset of 200,000 randomly selected samples from ImageNet for 30 epochs. We then use the same settings as in CIFAR-10 for updating the policy parameters. Policy Evaluation.We evaluate the performance of the found augmentation policy on ResNet-50 and ResNet-200 based on the public repository of Fast AutoAugment (Lim et al., 2019). The parameters for training are the same as the ones of (Lim et al., 2019). In particular, we use step learning rate scheduler with a reduction factor of 0.1, and we train and evaluate with images of size 224x224. Results. The performance on ImageNet is presented in Table 2. As shown, DeepAA achieves the best performance compared with previous methods without the use of default augmentation pipeline. In particular, DeepAA performs better on larger models (i.e. ResNet-200), as the performance of DeepAA on ResNet-200 is the best within the 95% conﬁdence interval. Note that while we train DeepAA using the image resolution (224×224), we report the best results of RA and TA, which are trained with a larger image resolution (244×224) on this dataset. Baseline AA Fast AA Faster AA DADA RA UA TA(RA)1 TA(Wide)2 DeepAA ResNet-50 76.3 77.6 77.6 76.5 77.5 77.6 77.63 77.85 78.07 78.30±0.14 ResNet-200 78.5 80.0 80.6 - - - 80.4 - - 81.32±0.17 Table 2: Top-1 test accuracy (%) on ImageNet for ResNet-50 and ResNet-200. The results of DeepAA are averaged over four independent runs with different initializations. The 95% conﬁdence interval is denoted by ±. 4.3 P ERFORMANCE WITH BATCH AUGMENTATION Batch Augmentation (BA) is a technique that draws multiple augmented instances of the same sample in one mini-batch. It has been shown to be able to improve the generalization performance of the 1On CIFAR-10/100, TA (Wide) uses a wider (stronger) augmentation space, while the other methods including TA (RA) uses the standard augmentation space. 6Published as a conference paper at ICLR 2022 network (Berman et al., 2019; Hoffer et al., 2020). AdvAA (Zhang et al., 2019) directly searches for the augmentation policy under the BA setting whereas for TA and DeepAA, we apply BA with the same augmentation policy used in Table 1. Note that since the performance of BA is sensitive to the hyperparameters (Fort et al., 2021), we have conducted a grid search on the hyperparameters of both TA and DeepAA (details are included in Appendix D). As shown in Table 3, after tuning the hyperparameters, the performance of TA (Wide) using BA is already better than the reported performance in the original paper. The performance of DeepAA with BA outperforms that of both AdvAA and TA (Wide) with BA. AdvAA TA(Wide) (original paper) TA(Wide) (ours) DeepAA CIFAR-10 98.1±0.15 98.04±0.06 98.06±0.23 98.21±0.14 CIFAR-100 84.51±0.18 84.62±0.14 85.40±0.15 85.61±0.17 Table 3: Top-1 test accuracy (%) on CIFAR-10/100 dataset with WRN-28-10 with Batch Augmentation (BA), where eight augmented instances were drawn for each image. The results of DeepAA are averaged over four independent runs with different initializations. The 95% conﬁdence interval is denoted by ±. 4.4 U NDERSTANDING DEEPAA Figure 2: Top-1 test accuracy (%) on ImageNet of DeepAA-simple, DeepAA, and other automatic augmentation methods on ResNet-50. Effectiveness of Gradient Matching. One uniqueness of DeepAA is the regularized gradient matching objective. To examine its effectiveness, we remove the impact coming from multiple aug- mentation layers, and only conduct search for a sin- gle layer of augmentation policy. When evaluating the searched policy, we apply the default augmenta- tion in addition to the searched policy. We refer to this variant as DeepAA-simple. Figure 2 compares the Top-1 test accuracy on ImageNet using ResNet- 50 between DeepAA-simple, DeepAA, and other automatic augmentation methods. While there is 0.22% performance drop compared to DeepAA, with a single augmentation layer, DeepAA-simple still outperforms other methods and is able to achieve similar performance compared to TA (Wide) but with a standard augmentation space and trains on a smaller image size (224×224 vs 244×224). Policy Search Cost.Table 4 compares the policy search time on CIFAR-10/100 and ImageNet in GPU hours. DeepAA has comparable search time as PBA, Fast AA, and RA, but is slower than Faster AA and DADA. Note that Faster AA and DADA relax the discrete search space to a continuous one similar to DARTS (Liu et al., 2018). While such relaxation leads to shorter searching time, it inevitably introduces a discrepancy between the true and relaxed augmentation spaces. Dataset AA PBA Fast AA Faster AA DADA RA DeepAA CIFAR-10/100 5000 5 3.5 0.23 0.1 25 9 ImageNet 15000 - 450 2.3 1.3 5000 96 Table 4: Policy search time on CIFAR-10/100 and ImageNet in GPU hours. Impact of the Number of Augmentation Layers.Another uniqueness of DeepAA is its multi-layer search space that can go beyondtwo layers which existing automatic augmentation methods were designed upon. We examine the impact of the number of augmentation layers on the performance of DeepAA. Table 5 and Table 6 show the performance on CIFAR-10/100 and ImageNet respectively with increasing number of augmentation layers. As shown, for CIFAR-10/100, the performance gradually improves when more augmentation layers are added until we reach ﬁve layers. The performance does not improve when the sixth layer is added. For ImageNet, we have similar 1TA (RA) achieves 77.55% top-1 accuracy with image resolution 224×224. 2TA (Wide) achieves 77.97% top-1 accuracy with image resolution 224×224. 7Published as a conference paper at ICLR 2022 Figure 3: The distribution of operations at each layer of the policy for CIFAR-10/100 and ImageNet. The probability of each operation is summed up over all 12 discrete intensity levels (see Appendix B and C) of the corresponding transformation. observation where the performance stops improving when more than ﬁve augmentation layers are included. 1 layer 2 layers 3 layers 4 layers 5 layers 6 layers CIFAR-10 96.3±0.21 96.6±0.18 96.9±0.12 97.4±0.14 97.56±0.14 97.6±0.12 CIFAR-100 80.9±0.31 81.7±0.24 82.2±0.21 83.7±0.24 84.02±0.18 84.0±0.19 Table 5: Top-1 test accuracy of DeepAA on CIFAR-10/100 for different numbers of augmentation layers. The results are averaged over 4 independent runs with different initializations with the 95% conﬁdence interval denoted by ±. 1 layer 3 layers 5 layers 7 layers ImageNet 75.27±0.19 78.18±0.22 78.30±0.14 78.30±0.14 Table 6: Top-1 test accuracy of DeepAA on ImageNet with ResNet-50 for different numbers of augmentation layers. The results are averaged over 4 independent runs w/ different initializations with the 95% conﬁdence interval denoted by ±. Figure 3 illustrates the distributions of operations in the policy for CIFAR-10/100 and ImageNet respectively. As shown in Figure 3(a), the augmentation of CIFAR-10/100 converges to identity transformation at the sixth augmentation layer, which is a natural indication of the end of the augmentation pipeline. We have similar observation in Figure 3(b) for ImageNet, where the identity transformation dominates in the sixth augmentation layer. These observations match our results listed in Table 5 and Table 6. We also include the distribution of the magnitude within each operation for CIFAR-10/100 and ImageNet in Appendix B and Appendix C. Validity of Optimizing Gradient Matching with Regularization.To evaluate the validity of opti- mizing gradient matching with regularization, we designed a search-free baseline named “DeepTA”. In DeepTA, we stack multiple layers of TA on the same augmentation space of DeepAA without using default augmentations. As stated in Eq.(10) and Eq.(12), we explicitly optimize the gradient similarities with the average reward minus its standard deviation. The ﬁrst term – the average reward Ex{˜rk(x)}– encourages the direction of high cosine similarity. The second term – the standard deviation of the reward √ Ex{(˜rk(x) −Ex{˜rk(x)})2}– acts as a regularization that penalizes the direction with high variance. These two terms jointly maximize the gradient similarity along the direction with low variance. To illustrate the optimization trajectory, we design two metrics that are closely related to the two terms in Eq.(10): the mean value, and the standard deviation of the improvement of gradient similarity. The improvement of gradient similarity is obtained by subtracting the cosine similarity of the original image batch from that of the augmented batch. In our experiment, the mean and standard deviation of the gradient similarity improvement are calculated over 256 independently sampled original images. 8Published as a conference paper at ICLR 2022 (a) Mean of the gradient similarity improvement (b) Standard deviation of the gradi- ent similarity improvement (c) Mean accuracy over different aug- mentation depth Figure 4: Illustration of the search trajectory of DeepAA in comparison with DeepTA on CIFAR-10. As shown in Figure 4(a), the cosine similarity of DeepTA reaches the peak at the ﬁfth layer, and stacking more layers decreases the cosine similarity. In contrast, for DeepAA, the cosine similarity increases consistently until it converges to identity transformation at the sixth layer. In Figure 4(b), the standard deviation of DeepTA signiﬁcantly increases when stacking more layers. In contrast, in DeepAA, as we optimize the gradient similarity along the direction of low variance, the standard deviation of DeepAA does not grow as fast as DeepTA. In Figure 4(c), both DeepAA and DeepTA reach peak performance at the sixth layer, but DeepAA achieves better accuracy compared against DeepTA. Therefore, we empirically show that DeepAA effectively scales up the augmentation depth by increasing cosine similarity along the direction with low variance, leading to better results. Comparison with Other Policies.In Figure 7 in Appendix E, we compare the policy of DeepAA with the policy found by other data augmentation search methods including AA, FastAA and DADA. We have three interesting observations: • AA, FastAA and DADA assign high probability (over 1.0) on ﬂip, Cutout and crop, as those transformations are hand-picked and applied by default. DeepAA ﬁnds a similar pattern that assigns high probability on ﬂip, Cutout and crop. • Unlike AA, which mainly focused on color transformations, DeepAA has high probability over both spatial and color transformations. • FastAA has evenly distributed magnitudes, while DADA has low magnitudes (common issues in DARTS-like method). Interestingly, DeepAA assigns high probability to the stronger magnitudes. 5 C ONCLUSION In this work, we present Deep AutoAugment (DeepAA), a multi-layer data augmentation search method that ﬁnds deep data augmentation policy without using any hand-picked default transforma- tions. We formulate data augmentation search as a regularized gradient matching problem, which maximizes the gradient similarity between augmented data and original data along the direction with low variance. Our experimental results show that DeepAA achieves strong performance without using default augmentations, indicating that regularized gradient matching is an effective search method for data augmentation policies. Reproducibility Statement: We have described our experiment settings in great details. The evaluation of the found data augmentation policy is based the public repository of Fast AutoAugment. We believe that our results can be readily reproduced. ACKNOWLEDGEMENT We thank Yi Zhu, Hang Zhang, Haichen Shen, Mu Li, and Alexander Smola for their help with this work. This work was partially supported by NSF Award PFI:BIC-1632051 and Amazon AWS Machine Learning Research Award. 9Published as a conference paper at ICLR 2022 REFERENCES Maxim Berman, Herv´e J´egou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a uniﬁed image embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019. Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation. Journal of Machine Learning Research, 21(245):1–71, 2020. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 113–123, 2019. Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, volume 33, pp. 702–703, 2020. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Yunshu Du, Wojciech M Czarnecki, Siddhant M Jayakumar, Mehrdad Farajtabar, Razvan Pascanu, and Balaji Lakshminarayanan. Adapting auxiliary losses using gradient similarity. arXiv preprint arXiv:1812.02224, 2018. Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, and Samuel L Smith. Drawing multiple augmentation samples per image during training efﬁciently decreases test error. arXiv preprint arXiv:2105.13343, 2021. Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.Deep learning. MIT press Cambridge, 2016. Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment: Learning augmentation strategies using backpropagation. In European Conference on Computer Vision, pp. 1–16. Springer, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi- narayanan. Augmix: A simple data processing method to improve robustness and uncertainty. International Conference on Learning Representations, 2020. Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efﬁcient learning of augmentation policy schedules. In International Conference on Machine Learning, pp. 2731–2741. PMLR, 2019. Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129–8138, 2020. Hiroshi Inoue. Data augmentation by pairing samples for images classiﬁcation. arXiv preprint arXiv:1801.02929, 2018. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Differentiable automatic data augmentation. In European Conference on Computer Vision, pp. 580–595. Springer, 2020. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32, 2019. 10Published as a conference paper at ICLR 2022 Tom Ching LingChen, Ava Khonsari, Amirreza Lashkari, Mina Raﬁ Nazari, Jaspreet Singh Sambee, and Mario A Nascimento. Uniformaugment: A search-free probabilistic data augmentation approach. arXiv preprint arXiv:2003.14348, 2020. Aoming Liu, Zehao Huang, Zhiwu Huang, and Naiyan Wang. Direct differentiable augmentation search. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12219–12228, 2021. Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations, 2018. Samuel M¨uller, Andr´e Biedenkapp, and Frank Hutter. In-loop meta-learning with gradient-alignment reward. arXiv preprint arXiv:2102.03275, 2021. Samuel G. M¨uller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmenta- tion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 774–782, October 2021. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015. Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell, and Graham Neubig. Optimizing data usage via differentiable rewards. In International Conference on Machine Learning, pp. 9983–9995. PMLR, 2020. Ross Wightman, Hugo Touvron, and Herv ´e J ´egou. Resnet strikes back: An improved training procedure in timm. volume 34, 2021. Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. In arXiv preprint arXiv: 2001.00677, 2020. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6023–6032, 2019. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference 2016. British Machine Vision Association, 2016. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand- ing deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. International Conference on Learning Representations, 2018. Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial autoaugment. In International Conference on Learning Representations, 2019. 11Published as a conference paper at ICLR 2022 A A LIST OF STANDARD AUGMENTATION SPACE Operation Magnitude Identity - ShearX [-0.3, 0.3] ShearY [-0.3, 0.3] TranslateX [-0.45, 0.45] TranslateY [-0.45, 0.45] Rotate [-30, 30] AutoContrast - Invert - Equalize - Solarize [0, 256] Posterize [4, 8] Contrast [0.1, 1.9] Color [0.1, 1.9] Brightness [0.1, 1.9] Sharpness [0.1, 1.9] Flips - Cutout 16 (60) Crop - Table 7: List of operations in the search space and the corresponding range of magnitudes in the standard augmentation space. Note that some operations do not use magnitude parameters. We add ﬂip and crop to the search space which were found in the default augmentation pipeline in previous works. Flips operates by randomly ﬂipping the images with 50% probability. In line with previous works, crop denotes pad-and-crop and resize-and-crop transforms for CIFAR10/100 and ImageNet respectively. We set Cutout magnitude to 16 for CIFAR10/100 dataset to be the same as the Cutout in the default augmentation pipeline. We set Cutout magnitude to 60 pixels for ImageNet which is the upper limit of the magnitude used in AA (Cubuk et al., 2019). 12Published as a conference paper at ICLR 2022 B T HE DISTRIBUTION OF MAGNITUDES FOR CIFAR-10/100 Figure 5: The distribution of discrete magnitudes of each augmentation transformation in each layer of the policy for CIFAR-10/100. The x-axis represents the discrete magnitudes and the y-axis represents the probability. The magnitude is discretized to 12 levels with each transformation having its own range. A large absolute value of the magnitude corresponds to high transformation intensity. Note that we do not show identity, autoContrast, invert, equalize, ﬂips, Cutout and crop because they do not have intensity parameters. 13Published as a conference paper at ICLR 2022 C T HE DISTRIBUTION OF MAGNITUDES FOR IMAGE NET Figure 6: The distribution of discrete magnitudes of each augmentation transformation in each layer of the policy for ImageNet. The x-axis represents the discrete magnitudes and the y-axis represents the probability. The magnitude is discretized to 12 levels with each transformation having its own range. A large absolute value of the magnitude corresponds to high transformation intensity. Note that we do not show identity, autoContrast, invert, equalize, ﬂips, Cutout and crop because they do not have intensity parameters. 14Published as a conference paper at ICLR 2022 D H YPERPARAMETERS FOR BATCH AUGMENTATION The performance of BA is sensitive to the training settings (Fort et al., 2021; Wightman et al., 2021). Therefore, we conduct a grid search on the learning rate, weight decay and number of epochs for TA and DeepAA with Batch Augmentation. The best found parameters are summarized in Table 8 in Appendix. We did not tune the hyperparameters of AdvAA (Zhang et al., 2019) since AdvAA claims to be adaptive to the training process. Dataset Augmentation Model Batch Size Learning Rate Weight Decay Epoch CIFAR-10 TA (Wide) WRN-28-10 128×8 0.2 0.0005 100 DeepAA WRN-28-10 128×8 0.2 0.001 100 CIFAR-100 TA (Wide) WRN-28-10 128×8 0.4 0.0005 35 DeepAA WRN-28-10 128×8 0.4 0.0005 35 Table 8: Model hyperparameters of Batch Augmentation on CIFAR10/100 for TA (Wide) and DeepAA. Learning rate, weight decay and number of epochs are found via grid search. 15Published as a conference paper at ICLR 2022 E C OMPARISON OF DATA AUGMENTATION POLICY Sampling probability of each transformations cumulated over all augmentation layers  (a) DeepAA (b) AA (c) FastAA (d) DADA Figure 7: Comparison of the policy of DeepAA and some publicly available augmentaiotn policy found by other methods including AA, FastAA and DADA on CIFAR-10. Since the compared methods have varied numbers of augmentation layers, we cumulate the probability of each operation over all the augmentation layers. Thus, the cumulative probability can be larger than 1. For AA, Fast AA and DADA, we add additional 1.0 probability to ﬂip, Cutout and Crop, since they are applied by default. In addition, we normalize the magnitude to the range [-5, 5], and use color to distinguish different magnitudes. 16",
      "references": [
        "Multigrain: a unified image embedding for classes and instances",
        "A group-theoretic framework for data augmentation",
        "Autoaugment: Learning augmentation strategies from data",
        "Randaugment: Practical automated data augmentation with a reduced search space",
        "Improved regularization of convolutional neural networks with cutout",
        "Adapting auxiliary losses using gradient similarity",
        "Drawing multiple augmentation samples per image during training efficiently decreases test error",
        "Deep learning",
        "Faster autoaugment: Learning augmentation strategies using backpropagation",
        "Deep residual learning for image recognition",
        "Augmix: A simple data processing method to improve robustness and uncertainty",
        "Population based augmentation: Efficient learning of augmentation policy schedules",
        "Augment your batch: Improving generalization through instance repetition",
        "Data augmentation by pairing samples for images classification",
        "Categorical reparameterization with gumbel-softmax",
        "Adam: A method for stochastic optimization",
        "Differentiable automatic data augmentation",
        "Fast autoaugment",
        "Uniformaugment: A search-free probabilistic data augmentation approach",
        "Direct differentiable augmentation search",
        "Darts: Differentiable architecture search",
        "In-loop meta-learning with gradient-alignment reward",
        "Trivialaugment: Tuning-free yet state-of-the-art data augmentation",
        "Going deeper with convolutions",
        "Optimizing data usage via differentiable rewards",
        "Resnet strikes back: An improved training procedure in timm",
        "Improve unsupervised domain adaptation with mixup training",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features",
        "Wide residual networks",
        "Understanding deep learning requires rethinking generalization",
        "mixup: Beyond empirical risk minimization",
        "Adversarial autoaugment"
      ],
      "meta_data": {
        "arxiv_id": "2203.06172v2",
        "authors": [
          "Yu Zheng",
          "Zhi Zhang",
          "Shen Yan",
          "Mi Zhang"
        ],
        "published_date": "2022-03-11T18:57:27Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Deep AutoAugment (DeepAA), a fully automatic data-augmentation search framework that eliminates hand-engineered default transformations by (1) learning a deep, multi-layer augmentation pipeline built progressively from scratch, (2) formulating policy search as a regularized gradient-matching problem that maximizes cosine similarity between gradients of original and augmented data along low-variance directions, and (3) avoiding the exponential growth of search space through layer-wise optimization. DeepAA achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, and ImageNet without using default augmentations.",
        "methodology": "• Search space: 139 image transformations (16 standard ops + flips, crop, Cutout; each with 12 discrete magnitude levels where applicable).\n• Policy represented as K sequential layers (K≈5); each layer is a categorical distribution over the 139 transforms.\n• Objective: For a single sample augmented multiple times, maximize cosine similarity between gradients of augmented batch and a validation batch of original images; implemented via Jacobian-vector products.\n• Regularization: Penalize directions with high gradient variance (reward = mean – c·std, c=1).\n• Optimization: Gradient ascent on policy parameters (Adam, lr = 0.025); layers added one at a time, using Monte-Carlo sampling to estimate gradients for deeper layers.\n• Final policy applied during training; optionally combined with Batch Augmentation for further gains.",
        "experimental_setup": "Policy search:\n – CIFAR-10: 4 k training samples, Wide-ResNet-40-2, 512 updates per layer.\n – ImageNet: 200 k samples, ResNet-18, 30 epochs.\nEvaluation:\n – CIFAR-10/100: Wide-ResNet-28-10 and Shake-Shake-26-2×96d.\n – ImageNet: ResNet-50 and ResNet-200.\nTraining uses standard data-processing pipelines; compared against AA, PBA, FastAA, FasterAA, DADA, RandAugment, UniformAugment, TrivialAugment, AdvAA and baseline flips/crop/Cutout.\nMetrics: Top-1 test accuracy; multiple runs (4) with 95 % confidence intervals.\nSearch cost: 9 GPU-h (CIFAR) and 96 GPU-h (ImageNet).",
        "limitations": "1. Still computationally heavier than some differentiable methods (DADA, FasterAA).\n2. Relies on predefined discrete operations and magnitude levels; cannot discover new transform types or continuous magnitudes.\n3. Gradient similarity estimated with limited samples (16) and a single validation batch; may be noisy or dataset-dependent.\n4. Evaluation restricted to image classification; generalizability to other tasks or modalities not demonstrated.\n5. Performance plateaus after ~5 layers; underlying reasons and stability of very deep augmentation pipelines remain unexplored.",
        "future_research_directions": "• Extend gradient-matching augmentation search to other domains (e.g., object detection, segmentation, text, audio).\n• Introduce continuous or learnable transformation parameters or discover new augmentation primitives.\n• Reduce search cost via meta-learning, parameter sharing, or hardware-aware optimization.\n• Study effects on robustness, adversarial resistance, and out-of-distribution generalization.\n• Jointly optimize augmentation policies with neural architecture or training-schedule search for holistic AutoML pipelines.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Cross-Image Context for Single Image Inpainting",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space",
      "full_text": "RandAugment: Practical automated data augmentation with a reduced search space Ekin D. Cubuk ∗, Barret Zoph∗, Jonathon Shlens, Quoc V . Le Google Research, Brain Team {cubuk, barretzoph, shlens, qvl}@google.com Abstract Recent work has shown that data augmentation has the potential to signiﬁcantly improve the generalization of deep learning models. Recently, automated augmentation strate- gies have led to state-of-the-art results in image classiﬁca- tion and object detection. While these strategies were op- timized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and im- proved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a sep- arate search phase which increases the training complex- ity and may substantially increase the computational cost. Additionally, due to the separate search phase, these ap- proaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAug- ment has a signiﬁcantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmenta- tion approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0% accuracy, a 0.6% increase over the previous state-of-the-art and 1.0% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3% improvement over base- line augmentation, and is within 0.3% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online. 1 ∗Authors contributed equally. 1github.com/tensorflow/tpu/tree/master/models/ official/efficientnet search CIFAR-10 SVHN ImageNet ImageNet space PyramidNet WRN ResNet E. Net-B7 Baseline 0 97.3 98.5 76.3 84.0 AA 1032 98.5 98.9 77.6 84.4 Fast AA 1032 98.3 98.8 77.6 - PBA 1061 98.5 98.9 - - RA (ours) 102 98.5 99.0 77.6 85.0 Table 1. RandAugment matches or exceeds predictive perfor- mance of other augmentation methods with a signiﬁcantly re- duced search space. We report the search space size and the test accuracy achieved for AutoAugment (AA) [5], Fast AutoAugment [25], Population Based Augmentation (PBA) [20] and the pro- posed RandAugment (RA) on CIFAR-10 [22], SVHN [34], and ImageNet [6] classiﬁcation tasks. Architectures presented include PyramidNet [15], Wide-ResNet-28-10 [53], ResNet-50 [17], and EfﬁcientNet-B7 [47]. Search space size is reported as the order of magnitude of the number of possible augmentation policies. All accuracies are the percentage on a cross-validated validation or test split. Dash indicates that results are not available. 1. Introduction Data augmentation is a widely used method for gen- erating additional data to improve machine learning sys- tems, for image classiﬁcation [43, 23, 7, 54], object detec- tion [13], instance segmentation [10], and speech recogni- tion [21, 16, 36]. Unfortunately, data augmentation meth- ods require expertise, and manual work to design policies that capture prior knowledge in each domain. This require- ment makes it difﬁcult to extend existing data augmentation methods to other applications and domains. Learning policies for data augmentation has recently emerged as a method to automate the design of augmen- tation strategies and therefore has the potential to address some weaknesses of traditional data augmentation methods [5, 57, 20, 25]. Training a machine learning model with a learned data augmentation policy may signiﬁcantly im- prove accuracy [5], model robustness [32, 52, 41], and per- formance on semi-supervised learning [50] for image clas- siﬁcation; likewise, for object detection tasks on COCO and PASCAL-VOC [57]. Notably, unlike engineering bet- 1 arXiv:1909.13719v2  [cs.CV]  14 Nov 2019ter network architectures [59], all of these improvements in predictive performance incur no additional computational cost at inference time. In spite of the beneﬁts of learned data augmentation poli- cies, the computational requirements as well as the added complexity of two separate optimization procedures can be prohibitive. The original presentation of neural architecture search (NAS) realized an analogous scenario in which the dual optimization procedure resulted in superior predictive performance, but the original implementation proved pro- hibitive in terms of complexity and computational demand. Subsequent work accelerated training efﬁciency and the ef- ﬁcacy of the procedure [30, 38, 28, 29], eventually making the method amenable to a uniﬁed optimization based on a differentiable process [30]. In the case of learned augmen- tations, subsequent work identiﬁed more efﬁcient search methods [20, 25], however such methods still require a sep- arate optimization procedure, which signiﬁcantly increases the computational cost and complexity of training a ma- chine learning model. The original formulation for automated data augmenta- tion postulated a separate search on a small, proxy task whose results may be transferred to a larger target task [59, 58]. This formulation makes a strong assumption that the proxy task provides a predictive indication of the larger task [28, 2]. In the case of learned data augmentation, we provide experimental evidence to challenge this core as- sumption. In particular, we demonstrate that this strategy is sub-optimal as the strength of the augmentation depends strongly on model and dataset size. These results suggest that an improved data augmentation may be possible if one could remove the separate search phase on a proxy task. In this work, we propose a practical method for auto- mated data augmentation – termed RandAugment – that does not require a separate search. In order to remove a sep- arate search, we ﬁnd it necessary to dramatically reduce the search space for data augmentation. The reduction in pa- rameter space is in fact so dramatic that simple grid search is sufﬁcient to ﬁnd a data augmentation policy that outper- forms all learned augmentation methods that employ a sep- arate search phase. Our contributions can be summarized as follows: •We demonstrate that the optimal strength of a data aug- mentation depends on the model size and training set size. This observation indicates that a separate opti- mization of an augmentation policy on a smaller proxy task may be sub-optimal for learning and transferring augmentation policies. •We introduce a vastly simpliﬁed search space for data augmentation containing 2 interpretable hyper- parameters. One may employ simple grid search to tailor the augmentation policy to a model and dataset, removing the need for a separate search process. •Leveraging this formulation, we demonstrate state-of- the-art results on CIFAR [22], SVHN [34], and Im- ageNet [6]. On object detection [27], our method is within 0.3% mAP of state-of-the-art. On ImageNet we achieve a state-of-the-art accuracy of 85.0%, a 0.6% increment over previous methods and 1.0% over base- line augmentation. 2. Related Work Data augmentation has played a central role in the train- ing of deep vision models. On natural images, horizon- tal ﬂips and random cropping or translations of the images are commonly used in classiﬁcation and detection mod- els [53, 23, 13]. On MNIST, elastic distortions across scale, position, and orientation have been applied to achieve im- pressive results [43, 4, 49, 42]. While previous examples augment the data while keeping it in the training set dis- tribution, operations that do the opposite can also be effec- tive in increasing generalization. Some methods randomly erase or add noise to patches of images for increased valida- tion accuracy [8, 55], robustness [46, 52, 11], or both [32]. Mixup [54] is a particularly effective augmentation method on CIFAR-10 and ImageNet, where the neural network is trained on convex combinations of images and their corre- sponding labels. Object-centric cropping is commonly used for object detection tasks [31], whereas [9] adds new objects on training images by cut-and-paste. Moving away from individual operations to augment data, other work has focused on ﬁnding optimal strategies for combining different operations. For example, Smart Augmentation learns a network that merges two or more samples from the same class to generate new data [24]. Tran et al. generate augmented data via a Bayesian approach, based on the distribution learned from the training set [48]. DeVries et al. use transformations (e.g. noise, interpo- lations and extrapolations) in the learned feature space to augment data [7]. Furthermore, generative adversarial net- works (GAN) have been used to choose optimal sequences of data augmentation operations[39]. GANs have also been used to generate training data directly [37, 33, 56, 1, 44], however this approach does not seem to be as beneﬁcial as learning sequences of data augmentation operations that are pre-deﬁned [40]. Another approach to learning data augmentation strate- gies from data is AutoAugment [5], which originally used reinforcement learning to choose a sequence of operations as well as their probability of application and magnitude. Application of AutoAugment policies involves stochasticity at multiple levels: 1) for every image in every minibatch, a sub-policy is chosen with uniform probability. 2) oper- ations in each sub-policy has an associated probability ofFigure 1. Example images augmented by RandAugment. In these examples N=2 and three magnitudes are shown corre- sponding to the optimal distortion magnitudes for ResNet-50, EfﬁcientNet-B5 and EfﬁcientNet-B7, respectively. As the dis- tortion magnitude increases, the strength of the augmentation in- creases. application. 3) Some operations have stochasticity over di- rection. For example, an image can be rotated clockwise or counter-clockwise. The layers of stochasticity increase the amount of diversity that the network is trained on, which in turn was found to signiﬁcantly improve generalization on many datasets. More recently, several papers used the Au- toAugment search space and formalism with improved op- timization algorithms to ﬁnd AutoAugment policies more efﬁciently [20, 25]. Although the time it takes to search for policies has been reduced signiﬁcantly, having to imple- ment these methods in a separate search phase reduces the applicability of AutoAugment. For this reason, this work aims to eliminate the search phase on a separate proxy task completely. Some of the developments in RandAugment were in- spired by the recent improvements to searching over data augmentation policies. For example, Population Based Augmentation (PBA) [20] found that the optimal magnitude of augmentations increased during the course of training, which inspired us to not search over optimal magnitudes for each transformation but have a ﬁxed magnitude schedule, which we discuss in detail in Section 3. Furthermore, au- thors of Fast AutoAugment [25] found that a data augmen- tation policy that is trained for density matching leads to improved generalization accuracy, which inspired our ﬁrst order differentiable term for improving augmentation (see Section 4.7). transforms = [ ’Identity’, ’AutoContrast’, ’Equalize’, ’Rotate’, ’Solarize’, ’Color’, ’Posterize’, ’Contrast’, ’Brightness’, ’Sharpness’, ’ShearX’, ’ShearY’, ’TranslateX’, ’TranslateY’] def randaugment(N, M): \"\"\"Generate a set of distortions. Args: N: Number of augmentation transformations to apply sequentially. M: Magnitude for all the transformations. \"\"\" sampled_ops = np.random.choice(transforms, N) return [(op, M) for op in sampled_ops] Figure 2. Python code for RandAugment based on numpy. 3. Methods The primary goal of RandAugment is to remove the need for a separate search phase on a proxy task. The reason we wish to remove the search phase is because a separate search phase signiﬁcantly complicates training and is com- putationally expensive. More importantly, the proxy task may provide sub-optimal results (see Section 4.1). In or- der to remove a separate search phase, we aspire to fold the parameters for the data augmentation strategy into the hyper-parameters for training a model. Given that previ- ous learned augmentation methods contained 30+ parame- ters [5, 25, 20], we focus on vastly reducing the parameter space for data augmentation. Previous work indicates that the main beneﬁt of learned augmentation policies arise from increasing the diversity of examples [5, 20, 25]. Indeed, previous work enumerated a policy in terms of choosing which transformations to apply out of K=14 available transformations, and probabilities for applying each transformation: • identity • autoContrast • equalize • rotate • solarize • color • posterize • contrast • brightness • sharpness • shear-x • shear-y • translate-x • translate-y In order to reduce the parameter space but still maintain im- age diversity, we replace the learned policies and probabili- ties for applying each transformation with a parameter-free procedure of always selecting a transformation with uni- form probability 1 K. Given N transformations for a training image, RandAugment may thus express KN potential poli- cies. The ﬁnal set of parameters to consider is the magnitude of the each augmentation distortion. Following [5], we em- ploy the same linear scale for indicating the strength of each transformation. Brieﬂy, each transformation resides on aninteger scale from 0 to 10 where a value of 10 indicates the maximum scale for a given transformation. A data aug- mentation policy consists of identifying an integer for each augmentation [5, 25, 20]. In order to reduce the parame- ter space further, we observe that the learned magnitude for each transformation follows a similar schedule during train- ing (e.g. Figure 4 in [20]) and postulate that a single global distortion M may sufﬁce for parameterizing all transforma- tions. We experimented with four methods for the schedule of M during training: constant magnitude, random magni- tude, a linearly increasing magnitude, and a random magni- tude with increasing upper bound. The details of this exper- iment can be found in Appendix A.1.1. The resulting algorithm contains two parameters N and M and may be expressed simply in two lines of Python code (Figure 2). Both parameters are human-interpretable such that larger values of N and M increase regulariza- tion strength. Standard methods may be employed to efﬁ- ciently perform hyperparameter optimization [45, 14], how- ever given the extremely small search space we ﬁnd that naive grid search is quite effective (Section 4.1). We justify all of the choices of this proposed algorithm in this subse- quent sections by comparing the efﬁcacy of the learned aug- mentations to all previous learned data augmentation meth- ods. 4. Results To explore the space of data augmentations, we exper- iment with core image classiﬁcation and object detection tasks. In particular, we focus on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets as well as COCO object de- tection so that we may compare with previous work. For all of these datasets, we replicate the corresponding architec- tures and set of data transformations. Our goal is to demon- strate the relative beneﬁts of employing this method over previous learned augmentation methods. 4.1. Systematic failures of a separate proxy task A central premise of learned data augmentation is to con- struct a small, proxy task that may be reﬂective of a larger task [58, 59, 5]. Although this assumption is sufﬁcient for identifying learned augmentation policies to improve per- formance [5, 57, 36, 25, 20], it is unclear if this assumption is overly stringent and may lead to sub-optimal data aug- mentation policies. In this ﬁrst section, we challenge the hypothesis that for- mulating the problem in terms of a small proxy task is ap- propriate for learned data augmentation. In particular, we explore this question along two separate dimensions that are commonly restricted to achieve a small proxy task: model size and dataset size. To explore this hypothesis, we sys- tematically measure the effects of data augmentation poli- cies on CIFAR-10. First, we train a family of Wide-ResNet baseline PBA Fast AA AA RA CIFAR-10 Wide-ResNet-28-2 94.9 - - 95.9 95.8 Wide-ResNet-28-10 96.1 97.4 97.3 97.4 97.3 Shake-Shake 97.1 98.0 98.0 98.0 98.0 PyramidNet 97.3 98.5 98.3 98.5 98.5 CIFAR-100 Wide-ResNet-28-2 75.4 - - 78.5 78.3 Wide-ResNet-28-10 81.2 83.3 82.7 82.9 83.3 SVHN (core set) Wide-ResNet-28-2 96.7 - - 98.0 98.3 Wide-ResNet-28-10 96.9 - - 98.1 98.3 SVHN Wide-ResNet-28-2 98.2 - - 98.7 98.7 Wide-ResNet-28-10 98.5 98.9 98.8 98.9 99.0 Table 2. Test accuracy (%) on CIFAR-10, CIFAR-100, SVHN and SVHN core set. Comparisons across default data augmenta- tion (baseline), Population Based Augmentation (PBA) [20] and Fast AutoAugment (Fast AA) [25], AutoAugment (AA) [5] and proposed RandAugment (RA). Note that baseline and AA are replicated in this work. SVHN core set consists of 73K examples. The Shake-Shake model [12] employed a 26 2 ×96d conﬁgura- tion, and the PyramidNet model used the ShakeDrop regulariza- tion [51]. Results reported by us are averaged over 10 independent runs. Bold indicates best results. architectures [53], where the model size may be system- atically altered through the widening parameter governing the number of convolutional ﬁlters. For each of these net- works, we train the model on CIFAR-10 and measure the ﬁnal accuracy compared to a baseline model trained with default data augmentations (i.e. ﬂip left-right and random translations). The Wide-ResNet models are trained with the additional K=14 data augmentations (see Methods) over a range of global distortion magnitudes M parameterized on a uniform linear scale ranging from [0, 30] 2. Figure 3a demonstrates the relative gain in accuracy of a model trained across increasing distortion magnitudes for three Wide-ResNet models. The squares indicate the dis- tortion magnitude with which achieves the highest accu- racy. Note that in spite of the measurement noise, Figure 3a demonstrates systematic trends across distortion magni- tudes. In particular, plotting all Wide-ResNet architectures versus the optimal distortion magnitude highlights a clear monotonic trend across increasing network sizes (Figure 3b). Namely, larger networks demand larger data distor- tions for regularization. Figure 1 highlights the visual dif- ference in the optimal distortion magnitude for differently sized models. Conversely, a learned policy based on [5] provides a ﬁxed distortion magnitude (Figure 3b, dashed line) for all architectures that is clearly sub-optimal. A second dimension for constructing a small proxy task 2Note that the range of magnitudes exceeds the speciﬁed range of mag- nitudes in the Methods because we wish to explore a larger range of mag- nitudes for this preliminary experiment. We retain the same scale as [5] for a value of 10 to maintain comparable results.Figure 3. Optimal magnitude of augmentation depends on the size of the model and the training set. All results report CIFAR-10 validation accuracy for Wide-ResNet model architectures [53] averaged over 20 random initializations, where N = 1. (a) Accuracy of Wide-ResNet-28-2, Wide-ResNet-28-7, and Wide-ResNet-28-10 across varying distortion magnitudes. Models are trained for 200 epochs on 45K training set examples. Squares indicate the distortion magnitude that achieves the maximal accuracy. (b) Optimal distortion magnitude across 7 Wide-ResNet-28 architectures with varying widening parameters ( k). (c) Accuracy of Wide-ResNet-28-10 for three training set sizes (1K, 4K, and 10K) across varying distortion magnitudes. Squares indicate the distortion magnitude that achieves the maximal accuracy. (d) Optimal distortion magnitude across 8 training set sizes. Dashed curves show the scaled expectation value of the distortion magnitude in the AutoAugment policy [5]. is to train the proxy on a small subset of the training data. Figure 3c demonstrates the relative gain in accu- racy of Wide-ResNet-28-10 trained across increasing dis- tortion magnitudes for varying amounts of CIFAR-10 train- ing data. The squares indicate the distortion magnitude with that achieves the highest accuracy. Note that in spite of the measurement noise, Figure 3c demonstrates systematic trends across distortion magnitudes. We ﬁrst observe that models trained on smaller training sets may gain more im- provement from data augmentation (e.g. 3.0% versus 1.5% in Figure 3c). Furthermore, we see that the optimal distor- tion magnitude is larger for models that are trained on larger datasets. At ﬁrst glance, this may disagree with the expec- tation that smaller datasets require stronger regularization. Figure 3d demonstrates that the optimal distortion mag- nitude increases monotonically with training set size. One hypothesis for this counter-intuitive behavior is that aggres- sive data augmentation leads to a low signal-to-noise ratio in small datasets. Regardless, this trend highlights the need for increasing the strength of data augmentation on larger datasets and the shortcomings of optimizing learned aug- mentation policies on a proxy task comprised of a subset of the training data. Namely, the learned augmentation may learn an augmentation strength more tailored to the proxy task instead of the larger task of interest. The dependence of augmentation strength on the dataset and model size indicate that a small proxy task may provide a sub-optimal indicator of performance on a larger task. This empirical result suggests that a distinct strategy may be necessary for ﬁnding an optimal data augmentation pol- icy. In particular, we propose in this work to focus on a uniﬁed optimization of the model weights and data augmen- tation policy. Figure 3 suggest that merely searching for a shared distortion magnitude M across all transformations may provide sufﬁcient gains that exceed learned optimiza- tion methods [5]. Additionally, we see that optimizing in- dividual magnitudes further leads to minor improvement in performance (see Section A.1.2 in Appendix). Furthermore, Figure 3a and 3c indicate that merely sam- pling a few distortion magnitudes is sufﬁcient to achieve good results. Coupled with a second free parameter N, we consider these results to prescribe an algorithm for learning an augmentation policy. In the subsequent sec- tions, we identify two free parameters N and M specify- ing RandAugment through a minimal grid search and com- pare these results against computationally-heavy learneddata augmentations based on proxy tasks. 4.2. CIFAR CIFAR-10 has been extensively studied with previous data augmentation methods and we ﬁrst test this proposed method on this data. The default augmentations for all methods include ﬂips, pad-and-crop and Cutout [8]. N and Mwere selected based on the validation performance on 5K held out examples from the training set for 1 and 5 settings for N and M, respectively. Results indicate that RandAug- ment achieves either competitive (i.e. within 0.1%) or state- of-the-art on CIFAR-10 across four network architectures (Table 2). As a more challenging task, we additionally com- pare the efﬁcacy of RandAugment on CIFAR-100 for Wide- ResNet-28-2 and Wide-ResNet-28-10. On the held out 5K dataset, we sampled 2 and 4 settings for N and M, respec- tively (i.e. N={1,2}and M={2,6,10,14}). For Wide- ResNet-28-2 and Wide-ResNet-28-10, we ﬁnd that N=1, M=2 and N=2, M=14 achieves best results, respectively. Again, RandAugment achieves competitive or superior re- sults across both architectures (Table 2). 4.3. SVHN Because SVHN is composed of numbers instead of nat- ural images, the data augmentation strategy for SVHN may differ substantially from CIFAR-10. Indeed, [5] identiﬁed a qualitatively different policy for CIFAR-10 than SVHN. Likewise, in a semi-supervised setting for CIFAR-10, a pol- icy learned from CIFAR-10 performs better than a policy learned from SVHN [50]. SVHN has a core training set of 73K images [34]. In addition, SVHN contains 531K less difﬁcult “extra” im- ages to augment training. We compare the performance of the augmentation methods on SVHN with and without the extra data on Wide-ResNet-28-2 and Wide-ResNet-28-10 (Table 2). In spite of the large differences between SVHN and CIFAR, RandAugment consistently matches or outper- forms previous methods with no alteration to the list of transformations employed. Notably, for Wide-ResNet-28- 2, applying RandAugment to the core training dataset im- proves performance more than augmenting with 531K ad- ditional training images (98.3% vs. 98.2%). For, Wide- ResNet-28-10, RandAugment is competitive with augment- ing the core training set with 531K training images (i.e. within 0.2%). Nonetheless, Wide-ResNet-28-10 with Ran- dAugment matches the previous state-of-the-art accuracy on SVHN which used a more advanced model [5]. 4.4. ImageNet Data augmentation methods that improve CIFAR-10 and SVHN models do not always improve large-scale tasks such as ImageNet. For instance, Cutout substantially improves CIFAR and SVHN performance [8], but fails to improve ImageNet [32]. Likewise, AutoAugment does not increase the performance on ImageNet as much as other tasks [5], especially for large networks (e.g. +0.4% for AmoebaNet- C [5] and +0.1% for EfﬁcientNet-B5 [47]). One plausible reason for the lack of strong gains is that the small proxy task was particularly impoverished by restricting the task to ∼10% of the 1000 ImageNet classes. Table 3 compares the performance of RandAugment to other learned augmentation approaches on ImageNet. Ran- dAugment matches the performance of AutoAugment and Fast AutoAugment on the smallest model (ResNet-50), but on larger models RandAugment signiﬁcantly outperforms other methods achieving increases of up to +1.3% above the baseline. For instance, on EfﬁcientNet-B7, the resulting model achieves 85.0% – a new state-of-the-art accuracy – exhibiting a 1.0% improvement over the baseline augmen- tation. These systematic gains are similar to the improve- ments achieved with engineering new architectures [59, 28], however these gains arise without incurring additional com- putational cost at inference time. 4.5. COCO To further test the generality of this approach, we next explore a related task of large-scale object detection on the COCO dataset [27]. Learned augmentation policies have improved object detection and lead to state-of-the-art results [57]. We followed previous work by training on the same architectures and following the same training schedules (see Appendix A.3). Brieﬂy, we employed RetinaNet [26] with ResNet-101 and ResNet-200 as a backbone [17]. Models were trained for 300 epochs from random initialization. Table 4 compares results between a baseline model, Au- toAugment and RandAugment. AutoAugment leveraged additional, specialized transformations not afforded to Ran- dAugment in order to augment the localized bounding box of an image [57]. In addition, note that AutoAugment expended ∼15K GPU hours for search, where as Ran- dAugment was tuned by on merely 6 values of the hyper- parameters (see Appendix A.3). In spite of the smaller li- brary of specialized transformations and the lack of a sep- arate search phase, RandAugment surpasses the baseline model and provides competitive accuracy with AutoAug- ment. We reserve for future work to expand the transforma- tion library to include bounding box speciﬁc transformation to potentially improve RandAugment results even further. 4.6. Investigating the dependence on the included transformations RandAugment achieves state-of-the-art results across different tasks and datasets using the same list of transfor- mations. This result suggests that RandAugment is largely insensitive to the selection of transformations for differ- ent datasets. To further study the sensitivity, we experi-baseline Fast AA AA RA ResNet-50 76.3 / 93.1 77.6 / 93.7 77.6 / 93.8 77.6 / 93.8 EfﬁcientNet-B5 83.2 / 96.7 - 83.3 / 96.7 83.9 / 96.8 EfﬁcientNet-B7 84.0 / 96.9 - 84.4 / 97.1 85.0 / 97.2 Table 3. ImageNet results. Top-1 and Top-5 accuracies (%) on ImageNet. Baseline and AutoAugment (AA) results on ResNet-50 are from [5]. Fast AutoAugment (Fast AA) results are from [25]. EfﬁcientNet results with and without AutoAugment are from [47]. Highest accuracy for each model is presented in bold. Note that Population Based Augmentation (PBA) [20] has not been implemented on ImageNet. model augmentation mAP search space Baseline 38.8 0 ResNet-101 AutoAugment 40.4 1034 RandAugment 40.1 102 Baseline 39.9 0 ResNet-200 AutoAugment 42.1 1034 RandAugment 41.9 102 Table 4. Results on object detection. Mean average precision (mAP) on COCO detection task. Higher is better. Search space size is reported as the order of magnitude of the number of possible augmentation policies. Models are trained for 300 epochs from random initialization following [57]. Figure 4. Average performance improves when more transfor- mations are included in RandAugment. All panels report me- dian CIFAR-10 validation accuracy for Wide-ResNet-28-2 model architectures [53] trained with RandAugment ( N = 3, M = 4) using randomly sampled subsets of transformations. No other data augmentation is included in training. Error bars indicate 30 th and 70th percentile. (a) Median accuracy for randomly sampled subsets of transformations. (b) Median accuracy for subsets with and with- out the Rotate transformation. (c) Median accuracy for subsets with and without the translate-x transformation. (d) Median accuracy for subsets with and without the posterize transfor- mation. Dashed curves show the accuracy of the model trained without any augmentations. mented with RandAugment on a Wide-ResNet-28-2 trained on CIFAR-10 for randomly sampled subsets of the full list of 14 transformations. We did not use ﬂips, pad-and-crop, or cutout to only focus on the improvements due to Ran- dAugment with random subsets. Figure 4a suggests that the median validation accuracy due to RandAugment improves as the number of transformations is increased. However, even with only two transformations, RandAugment leads to more than 1% improvement in validation accuracy on aver- age. To get a sense for the effect of individual transforma- tions, we calculate the average improvement in validation accuracy for each transformation when they are added to a random subset of transformations. We list the transforma- tions in order of most helpful to least helpful in Table 5. We see that while geometric transformations individually make the most difference, some of the color transformations lead to a degradation of validation accuracy on average. Note that while Table 5 shows the average effect of adding in- dividual transformations to randomly sampled subsets of transformations, Figure 4a shows that including all trans- formations together leads to a good result. The transfor- mation rotate is most helpful on average, which was also observed previously [5, 57]. To see the effect of represen- tative transformations in more detail, we repeat the anal- ysis in Figure 4a for subsets with and without ( rotate, translate-x, and posterize). Surprisingly, rotate can signiﬁcantly improve performance and lower variation even when included in small subsets of RandAugment transfor- mations, while posterize seems to hurt all subsets of all sizes. 4.7. Learning the probabilities for selecting image transformations RandAugment selects all image transformations with equal probability. This opens up the question of whether learning Kprobabilities may improve performance further. Most of the image transformations (except posterize, equal- ize, and autoContrast ) are differentiable, which permits back- propagation to learn the Kprobabilities [30]. Let us denote αij as the learned probability of selecting image transfor- mation ifor operation j. For K=14 image transformations and N=2 operations, αij constitutes 28 parameters. We ini- tialize all weights such that each transformation is equal probability (i.e. RandAugment), and update these param- eters based on how well a model classiﬁes a held out set oftransformation ∆ (%) transformation ∆ (%) rotate 1.3 shear-x 0.9 shear-y 0.9 translate-y 0.4 translate-x 0.4 autoContrast 0.1 sharpness 0.1 identity 0.1 contrast 0.0 color 0.0 brightness 0.0 equalize -0.0 solarize -0.1 posterize -0.3 Table 5. Average improvement due to each transformation. Average difference in validation accuracy (%) when a particular transformation is added to a randomly sampled set of transfor- mations. For this ablation study, Wide-ResNet-28-2 models were trained on CIFAR-10 using RandAugment (N = 3, M = 4) with the randomly sampled set of transformations, with no other data augmentation. baseline AA RA + 1 st Reduced CIFAR-10 Wide-ResNet-28-2 82.0 85.6 85.3 85.5 Wide-ResNet-28-10 83.5 87.7 86.8 87.4 CIFAR-10 Wide-ResNet-28-2 94.9 95.9 95.8 96.1 Wide-ResNet-28-10 96.1 97.4 97.3 97.4 Table 6. Differentiable optimization for augmentation can im- prove RandAugment. Test accuracy (%) from differentiable Ran- dAugment for reduced (4K examples) and full CIFAR-10. The 1st-order approximation (1 st) is based on density matching (Sec- tion 4.7). Models trained on reduced CIFAR-10 were trained for 500 epochs. CIFAR-10 models trained using the same hyperpa- rameters as previous. Each result is averaged over 10 independent runs. validation images distorted by αij. This approach was in- spired by density matching [25], but instead uses a differen- tiable approach in lieu of Bayesian optimization. We label this method as a 1st-order density matching approximation. To test the efﬁcacy of density matching to learn the prob- abilities of each transformation, we trained Wide-ResNet- 28-2 and Wide-ResNet-28-10 on CIFAR-10 and the reduced form of CIFAR-10 containing 4K training samples. Ta- ble 6 indicates that learning the probabilities αij slightly improves performance on reduced and full CIFAR-10 (RA vs 1st). The 1 st-order method improves accuracy by more than 3.0% for both models on reduced CIFAR-10 compared to the baseline of ﬂips and pad-and-crop. On CIFAR-10, the 1st-order method improves accuracy by 0.9% on the smaller model and 1.2% on the larger model compared to the base- line. We further see that the 1 st-order method always per- forms better than RandAugment, with the largest improve- ment on Wide-ResNet-28-10 trained on reduced CIFAR-10 (87.4% vs. 86.8%). On CIFAR-10, the 1 st-order method outperforms AutoAugment on Wide-ResNet-28-2 (96.1% vs. 95.9%) and matches AutoAugment on Wide-ResNet- 28-10 3. Although the density matching approach is promis- 3As a baseline comparison, in preliminary experiments we additionally ing, this method can be expensive as one must apply all K transformations N times to each image independently. Hence, because the computational demand ofKN transfor- mations is prohibitive for large images, we reserve this for future exploration. In summary, we take these results to in- dicate that learning the probabilities through density match- ing may improve the performance on small-scale tasks and reserve explorations to larger-scale tasks for the future. 5. Discussion Data augmentation is a necessary method for achieving state-of-the-art performance [43, 23, 7, 54, 13, 36]. Learned data augmentation strategies have helped automate the de- sign of such strategies and likewise achieved state-of-the- art results [5, 25, 20, 57]. In this work, we demonstrated that previous methods of learned augmentation suffers from systematic drawbacks. Namely, not tailoring the number of distortions and the distortion magnitude to the dataset size nor the model size leads to sub-optimal performance. To remedy this situation, we propose a simple parameterization for targeting augmentation to particular model and dataset sizes. We demonstrate that RandAugment is competitive with or outperforms previous approaches [5, 25, 20, 57] on CIFAR-10/100, SVHN, ImageNet and COCO without a separate search for data augmentation policies. In previous work, scaling learned data augmentation to larger dataset and models have been a notable obstacle. For example, AutoAugment and Fast AutoAugment could only be optimized for small models on reduced subsets of data [5, 25]; population based augmentation was not re- ported for large-scale problems [20]. The proposed method scales quite well to datasets such as ImageNet and COCO while incurring minimal computational cost (e.g. 2 hyper- parameters), but notable predictive performance gains. An open question remains how this method may improve model robustness [32, 52, 41] or semi-supervised learning [50]. Future work will study how this method applies to other ma- chine learning domains, where data augmentation is known to improve predictive performance, such as image segmen- tation [3], 3-D perception [35], speech recognition [19] or audio recognition [18]. In particular, we wish to better un- derstand if or when datasets or tasks may require a separate search phase to achieve optimal performance. Finally, an open question remains how one may tailor the set of trans- formations to a given tasks in order to further improve the predictive performance of a given model. learn αij based on differentiating through a virtual training step [30]. In this approach, the 2 nd-order approximation yielded consistently negative results (see Appendix A.1).6. Acknowledgements We thank Samy Bengio, Daniel Ho, Ildoo Kim, Jaehoon Lee, Zhaoqi Leng, Hanxiao Liu, Raphael Gontijo Lopes, Ruoming Pang, Ben Poole, Mingxing Tan, and the rest of the Brain team for their help.References [1] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017. 2 [2] Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, and Jon Shlens. Searching for efﬁcient multi-scale archi- tectures for dense image prediction. In Advances in Neural Information Processing Systems, pages 8699–8710, 2018. 2 [3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017. 8 [4] Dan Ciregan, Ueli Meier, and J ¨urgen Schmidhuber. Multi- column deep neural networks for image classiﬁcation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 3642–3649. IEEE, 2012. 2 [5] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. 1, 2, 3, 4, 5, 6, 7, 8 [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 1, 2 [7] Terrance DeVries and Graham W Taylor. Dataset augmen- tation in feature space. arXiv preprint arXiv:1702.05538 , 2017. 1, 2, 8 [8] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 2, 6 [9] Debidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut, paste and learn: Surprisingly easy synthesis for instance de- tection. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 1301–1310, 2017. 2 [10] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting instance segmentation via probability map guided copy- pasting. arXiv preprint arXiv:1908.07801, 2019. 1 [11] Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk. Adversarial examples are a natural consequence of test error in noise. arXiv preprint arXiv:1901.10513, 2019. 2 [12] Xavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017. 4, 13 [13] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll´ar, and Kaiming He. Detectron, 2018. 1, 2, 8 [14] Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1487–1495. ACM, 2017. 4 [15] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyrami- dal residual networks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6307–6315. IEEE, 2017. 1 [16] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014. 1 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. 1, 6 [18] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn archi- tectures for large-scale audio classiﬁcation. In 2017 ieee in- ternational conference on acoustics, speech and signal pro- cessing (icassp), pages 131–135. IEEE, 2017. 8 [19] Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, et al. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal processing magazine, 29, 2012. 8 [20] Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen. Population based augmentation: Efﬁcient learn- ing of augmentation policy schedules. arXiv preprint arXiv:1905.05393, 2019. 1, 2, 3, 4, 7, 8 [21] Naoyuki Kanda, Ryu Takeda, and Yasunari Obuchi. Elastic spectral distortion for low resource speech recognition with deep neural networks. In 2013 IEEE Workshop on Auto- matic Speech Recognition and Understanding , pages 309– 314. IEEE, 2013. 1 [22] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Uni- versity of Toronto, 2009. 1, 2 [23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional neural net- works. In Advances in Neural Information Processing Sys- tems, 2012. 1, 2, 8 [24] Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation learning an optimal data augmentation strategy. IEEE Access, 5:5858–5869, 2017. 2 [25] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. arXiv preprint arXiv:1905.00397, 2019. 1, 2, 3, 4, 7, 8 [26] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017. 6 [27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision , pages 740–755. Springer, 2014. 2, 6 [28] Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Mur- phy. Progressive neural architecture search. arXiv preprint arXiv:1712.00559, 2017. 2, 6 [29] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representa-tions for efﬁcient architecture search. In International Con- ference on Learning Representations, 2018. 2 [30] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. 2, 7, 8, 12 [31] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European con- ference on computer vision, pages 21–37. Springer, 2016. 2 [32] Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin D Cubuk. Improving robustness without sacriﬁcing accuracy with patch gaussian augmentation. arXiv preprint arXiv:1906.02611, 2019. 1, 2, 6, 8 [33] Seongkyu Mun, Sangwook Park, David K Han, and Hanseok Ko. Generative adversarial network based acoustic scene training set augmentation and selection using svm hyper- plane. In Detection and Classiﬁcation of Acoustic Scenes and Events Workshop, 2017. 2 [34] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis- sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Work- shop on Deep Learning and Unsupervised Feature Learning, 2011. 1, 2, 6 [35] Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Al- sharif, Patrick Nguyen, et al. Starnet: Targeted compu- tation for object detection in point clouds. arXiv preprint arXiv:1908.11069, 2019. 8 [36] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaug- ment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. 1, 4, 8 [37] Luis Perez and Jason Wang. The effectiveness of data aug- mentation in image classiﬁcation using deep learning. arXiv preprint arXiv:1712.04621, 2017. 2 [38] Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efﬁcient neural architecture search via parameter sharing. In International Conference on Machine Learning, 2018. 2 [39] Alexander J Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher R ´e. Learning to compose domain-speciﬁc transformations for data augmentation. In Advances in Neural Information Processing Systems , pages 3239–3249, 2017. 2 [40] Suman Ravuri and Oriol Vinyals. Classiﬁcation accuracy score for conditional generative models. arXiv preprint arXiv:1905.10887, 2019. 2 [41] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize to im- agenet? arXiv preprint arXiv:1902.10811, 2019. 1, 8 [42] Ikuro Sato, Hiroki Nishimura, and Kensuke Yokoi. Apac: Augmented pattern classiﬁcation with neural networks. arXiv preprint arXiv:1505.03229, 2015. 2 [43] Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of International Confer- ence on Document Analysis and Recognition, 2003. 1, 2, 8 [44] Leon Sixt, Benjamin Wild, and Tim Landgraf. Render- gan: Generating realistic labeled data. arXiv preprint arXiv:1611.01331, 2016. 2 [45] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Prac- tical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959, 2012. 4 [46] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 2 [47] Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019. 1, 6, 7, 13 [48] Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and Ian Reid. A bayesian data augmentation approach for learn- ing deep models. In Advances in Neural Information Pro- cessing Systems, pages 2794–2803, 2017. 2 [49] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using drop- connect. In International Conference on Machine Learning, pages 1058–1066, 2013. 2 [50] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019. 1, 6, 8 [51] Yoshihiro Yamada, Masakazu Iwamura, and Koichi Kise. Shakedrop regularization. arXiv preprint arXiv:1802.02375, 2018. 4, 13 [52] Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D Cubuk, and Justin Gilmer. A fourier perspective on model robustness in computer vision. arXiv preprint arXiv:1906.08988, 2019. 1, 2, 8 [53] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In British Machine Vision Conference, 2016. 1, 2, 4, 5, 7 [54] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 1, 2, 8 [55] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. arXiv preprint arXiv:1708.04896, 2017. 2 [56] Xinyue Zhu, Yifan Liu, Zengchang Qin, and Jiahong Li. Data augmentation in emotion classiﬁcation using genera- tive adversarial networks. arXiv preprint arXiv:1711.00648, 2017. 2 [57] Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le. Learning data aug- mentation strategies for object detection. arXiv preprint arXiv:1906.11172, 2019. 1, 4, 6, 7, 8, 13 [58] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations, 2017. 2, 4 [59] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of IEEE Conference on Com- puter Vision and Pattern Recognition, 2017. 2, 4, 6A. Appendix A.1. Second order term from bilevel optimization For the second order term for the optimization of aug- mentation parameters, we follow the formulation in [30], which we summarize below. We treat the optimization of augmentation parameters and weights of the neural network as a bilevel optimization problem, whereαare the augmen- tation parameters and w are the weights of the neural net- work. Then the goal is to ﬁnd the optimal augmentation parameters αsuch that when weights are optimized on the training set using data augmentation given byαparameters, the validation loss is minimized. In other words: minαLval(w∗(α),α) s.t.w∗(α) = argminw Ltrain(w,α). (1) Then, again following [30], we approximate this bilevel op- timization by a single virtual training step, ∇αLval(w∗(α),α) ≈ ∇αLval(w−ξ∇wLtrain(w,α),α), (2) where ξ is the virtual learning rate. Eq. 2 can be expanded as ∇αLval(w∗(α),α) ≈ ∇αLval(w−ξ∇wLtrain(w,α),α) − ξ∇2 α,wLtrain(w,α)∇w′ Lval(w′,α), (3) where w′ = w−ξ∇wLtrain(w,α). In the case where the virtual learning rate, ξ, is zero, the second term disap- pears and the ﬁrst term becomes ∇Lval(w,α), which was called the ﬁrst-order approximation [30]. This ﬁrst-order approximation was found to be highly signiﬁcant for archi- tecture search, where most of the improvement (0.3% out of 0.5%) could be achieved using this approximation in a more efﬁcient manner (1.5 days as opposed to 4 days). Unfortu- nately, when α represents augmentation parameters, ﬁrst- order approximation is irrelevant since the predictions of a model on the clean validation images do not depend on the augmentation parameters α. Then we are left with just the second order approximation, where ξ >0, which we ap- proximate via ﬁnite difference approximation as ∇2 α,wLtrain(w,α)∇w′ Lval(w′,α) ≈ ∇αLtrain(w+,α) −∇αLtrain(w−,α) 2ϵ , (4) where w±= w±ϵ∇w′ Lval(w′,α) and ϵis a small number. A.1.1 Magnitude methods A random magnitude uniformly randomly samples the dis- tortion magnitude between two values. A constant mag- nitude sets the distortion magnitude to a constant number Magnitude Method Accuracy Random Magnitude 97.3 Constant Magnitude 97.2 Linearly Increasing Magnitude 97.2 Random Magnitude with Increasing Upper Bound 97.3 Table 7. Results for different ways of setting the global magni- tude parameter M. All magnitude methods were run on CIFAR- 10 with Wide-ResNet-28-10 for 200 epochs. The reported accu- racy is the average of 10 runs on the validation set for the best hyperparamter setting for that magnitude method. All magnitude methods searched over had 48 different hyperparameter settings tried. Figure 5. Performance when magnitude is changed for one im- age transformation. This plot uses a shared magnitude for all image transformations and then changes the magnitude of only one operation while keeping the others ﬁxed. Two different archi- tectures were tried (WRN-28-2 and WRN-28-10) and two differ- ent image transformations were changed (Rotate and TranslateX), which results in the 4 lines shown. Twenty different magnitudes were tried for the selected transformation ([0 − 19]). The squares indicate the optimal magnitude found and the diamonds indicate the magnitude used for all other transformations (4 for WRN-28-2 and 5 for WRN-28-10). during the course of training. A linearly increasing mag- nitude interpolates the distortion magnitude during training between two values. A random magnitude with increasing upper bound is similar to a random magnitude, but the upper bound is increased linearly during training. In preliminary experiments, we found that all strategies worked equally well. Thus, we selected a constant magnitude because this strategy includes only a single hyper-parameter, and we em- ploy this for the rest of the work. The results from our ex- periment on trying the different magnitude strategies can be see in Table 7.A.1.2 Optimizing individual transformation magni- tudes Figure 5 demonstrates that changing the magnitude for one transformation, when keeping the rest ﬁxed results in a very minor accuracy change. This suggests that tying all magni- tudes together into a single value M is not greatly hurting the model performance. Across all for settings in Figure 5 the difference in accuracy of the tied magnitude vs the opti- mal one found was 0.19% 0.18% for the rotation operation experiments and 0.07% 0.05% for the TranslateX experi- ments. Changing one transformation does not have a huge impact on performance, which leads us to think that tying all magnitude parameters together is a sensible approach that drastically reduces the size of the search-space. A.2. Experimental Details A.2.1 CIFAR The Wide-ResNet models were trained for 200 epochs with a learning rate of 0.1, batch size of 128, weight decay of 5e- 4, and cosine learning rate decay. Shake-Shake [12] model was trained for 1800 epochs with a learning rate of 0.01, batch size of 128, weight decay of 1e-3, and cosine learning rate decay. ShakeDrop [51] models were trained for 1800 epochs with a learning rate of 0.05, batch size of 64 (as 128 did not ﬁt on a single GPU), weight decay of 5e-5, and cosine learning rate decay. On CIFAR-10, we used 3 for the number of operations applied (N) and tried 4, 5, 7, 9, and 11 for magnitude. For Wide-ResNet-2 and Wide-ResNet-10, we ﬁnd that the op- timal magnitude is 4 and 5, respectively. For Shake-Shake (26 2x96d) and PyramidNet + ShakeDrop models, the opti- mal magnitude was 9 and 7, respectively. A.2.2 SVHN For both SVHN datasets, we applied cutout after RandAug- ment as was done for AutoAugment and related methods. On core SVHN, for both Wide-ResNet-28-2 and Wide- ResNet-28-10, we used a learning rate of 5e-3, weight de- cay of 5e-3, and cosine learning rate decay for 200 epochs. We set N = 3and tried 5, 7, 9, and 11 for magnitude. For both Wide-ResNet-28-2 and Wide-ResNet-28-10, we ﬁnd the optimal magnitude to be 9. On full SVHN, for both Wide-ResNet-28-2 and Wide- ResNet-28-10, we used a learning rate of 5e-3, weight de- cay of 1e-3, and cosine learning rate decay for 160 epochs. We set N = 3and tried 5, 7, 9, and 11 for magnitude. For Wide-ResNet-28-2, we ﬁnd the optimal magnitude to be 5; whereas for Wide-ResNet-28-10, we ﬁnd the optimal mag- nitude to be 7. A.2.3 ImageNet The ResNet models were trained for 180 epochs using the standard ResNet-50 training hyperparameters. The image size was 224 by 244, the weight decay was 0.0001 and the momentum optimizer with a momentum parameter of 0.9 was used. The learning rate was 0.1, which gets scaled by the batch size divided by 256. A global batch size of 4096 was used, split across 32 workers. For ResNet-50 the opti- mal distortion magnitude was 9 and ( N = 2). The distor- tion magnitudes we tried were 5, 7, 9, 11, 13, 15 and the values of N that were tried were 1, 2 and 3. The EfﬁcientNet experiments used the default hyper pa- rameters and training schedule, which can be found in [47]. We trained for 350 epochs, used a batch size of 4096 split across 256 replicas. The learning rate was 0.016, which gets scaled by the batch size divided by 256. We used the RM- SProp optimizer with a momentum rate of 0.9, epsilon of 0.001 and a decay of 0.9. The weight decay used was 1e-5. For EfﬁcientNet B5 the image size was 456 by 456 and for EfﬁcientNet B7 it was 600 by 600. For EfﬁcientNet B5 we tried N = 2and N = 3and found them to perform about the same. We found the optimal distortion magnitude for B5 to be 17. The different magnitudes we tried were 8, 11, 14, 17, 21. For EfﬁcientNet B7 we used N = 2and found the optimal distortion magnitude to be 28. The magnitudes tried were 17, 25, 28, 31. The default augmentation of horizontal ﬂipping and ran- dom crops were used on ImageNet, applied before Ran- dAugment. The standard training and validation splits were employed for training and evaluation. A.3. COCO We applied horizontal ﬂipping and scale jitters in addi- tion to RandAugment. We used the same list of data aug- mentation transformations as we did in all other classiﬁca- tion tasks. Geometric operations transformed the bounding boxes the way it was deﬁned in Ref. [57]. We used a learn- ing rate of 0.08 and a weight decay of 1e 4. The focal loss parameters are set to be α = 0.25 and γ = 1.5. We set N = 1 and tried distortion magnitudes between 4 and 9. We found the optimal distortion magnitude for ResNet-101 and ResNet-200 to be 5 and 6, respectively.",
      "references": [
        "Data augmentation generative adversarial networks",
        "Searching for efﬁcient multi-scale archi- tectures for dense image prediction",
        "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs",
        "Multi- column deep neural networks for image classiﬁcation",
        "Autoaugment: Learning augmentation policies from data",
        "Imagenet: A large-scale hierarchical image database",
        "Dataset augmen- tation in feature space",
        "Improved regular- ization of convolutional neural networks with cutout",
        "Cut, paste and learn: Surprisingly easy synthesis for instance de- tection",
        "Instaboost: Boosting instance segmentation via probability map guided copy- pasting",
        "Adversarial examples are a natural consequence of test error in noise",
        "Shake-shake regularization",
        "Detectron",
        "Google vizier: A service for black-box optimization",
        "Deep pyrami- dal residual networks",
        "Deep speech: Scaling up end-to-end speech recognition",
        "Deep residual learning for image recognition",
        "Cnn archi- tectures for large-scale audio classiﬁcation",
        "Deep neural networks for acoustic modeling in speech recognition",
        "Population based augmentation: Efﬁcient learn- ing of augmentation policy schedules",
        "Elastic spectral distortion for low resource speech recognition with deep neural networks",
        "Learning multiple layers of features from tiny images",
        "Imagenet classiﬁcation with deep convolutional neural net- works",
        "Smart augmentation learning an optimal data augmentation strategy",
        "Fast autoaugment",
        "Focal loss for dense object detection",
        "Microsoft coco: Common objects in context",
        "Progressive neural architecture search",
        "Hierarchical representa- tions for efﬁcient architecture search",
        "Darts: Differentiable architecture search",
        "Ssd: Single shot multibox detector",
        "Improving robustness without sacriﬁcing accuracy with patch gaussian augmentation",
        "Generative adversarial network based acoustic scene training set augmentation and selection using svm hyper- plane",
        "Reading digits in natural images with unsupervised feature learning",
        "Starnet: Targeted compu- tation for object detection in point clouds",
        "Specaug- ment: A simple data augmentation method for automatic speech recognition",
        "The effectiveness of data aug- mentation in image classiﬁcation using deep learning",
        "Efﬁcient neural architecture search via parameter sharing",
        "Learning to compose domain-speciﬁc transformations for data augmentation",
        "Classiﬁcation accuracy score for conditional generative models",
        "Do imagenet classiﬁers generalize to imagenet?",
        "Apac: Augmented pattern classiﬁcation with neural networks",
        "Best practices for convolutional neural networks applied to visual document analysis",
        "Render- gan: Generating realistic labeled data",
        "Practical bayesian optimization of machine learning algorithms",
        "Intriguing properties of neural networks",
        "Efﬁcientnet: Rethinking model scaling for convolutional neural networks",
        "A bayesian data augmentation approach for learning deep models",
        "Regularization of neural networks using drop- connect",
        "Unsupervised data augmentation",
        "Shakedrop regularization",
        "A fourier perspective on model robustness in computer vision",
        "Wide residual networks",
        "mixup: Beyond empirical risk minimiza- tion",
        "Random erasing data augmentation",
        "Data augmentation in emotion classiﬁcation using generative adversarial networks",
        "Learning data augmentation strategies for object detection",
        "Neural architecture search with reinforcement learning",
        "Learning transferable architectures for scalable image recognition"
      ],
      "meta_data": {
        "arxiv_id": "1909.13719v2",
        "authors": [
          "Ekin D. Cubuk",
          "Barret Zoph",
          "Jonathon Shlens",
          "Quoc V. Le"
        ],
        "published_date": "2019-09-30T14:05:14Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces RandAugment, a simplified automated data-augmentation strategy that removes the expensive separate search phase used by prior methods (e.g., AutoAugment). By reducing the policy search space to two interpretable hyper-parameters—the number of random transforms N and a shared magnitude M—RandAugment can be tuned directly on the target task via a small grid search. The method achieves state-of-the-art or competitive results on CIFAR-10/100, SVHN, ImageNet, and COCO while being orders of magnitude cheaper to apply.",
        "methodology": "1. Empirically show that optimal augmentation strength depends on model and dataset size, making proxy-task search sub-optimal.\n2. Define a fixed library of 14 image transformations. For each training image, uniformly sample N transforms and apply each with a common magnitude M (0–10 scale).\n3. Perform a simple grid search over small discrete sets of N and M directly on the training/validation split of the target task.\n4. Optional extension: differentiate through density matching to learn per-operation probabilities (first-order approximation).\n5. Compare against AutoAugment, Fast AutoAugment, and Population-Based Augmentation.",
        "experimental_setup": "Datasets: CIFAR-10/100, SVHN (core 73k and full 604k), ImageNet-1K, COCO 2017 detection.\nModels: Wide-ResNet-28 (various widths), PyramidNet+ShakeDrop, Shake-Shake 26×96d, ResNet-50, EfficientNet-B5/B7, RetinaNet with ResNet-101/200 backbones.\nTraining details: standard flips/crops (baseline), Cutout for SVHN; cosine or step LR schedules; 200–1800 epochs for CIFAR/SVHN, 180 epochs for ResNet-50 ImageNet, 350 epochs for EfficientNets, 300 epochs for COCO.\nHyper-parameter search: grid over N∈{1–3} and M (4–31 depending on dataset); choose via 5k validation split (CIFAR) or few trials on ImageNet/COCO.\nMetrics: Top-1/Top-5 accuracy for classification, mAP for detection. Baselines and competitor methods replicated when possible.",
        "limitations": "1. Uses a fixed, manually chosen set of 14 image transforms; may not capture task-specific augmentations (e.g., bounding-box aware ops for detection).\n2. Still requires grid search over N and M, albeit small; optimal values may vary across architectures and datasets.\n3. Applies a single shared magnitude to all operations, which may be sub-optimal for some transforms.\n4. Evaluation limited to vision tasks; effectiveness on other modalities (speech, text, 3-D) untested.\n5. Optional differentiable extension incurs high compute because each image must be processed by all K transforms.",
        "future_research_directions": "1. Learn per-operation probabilities and magnitudes in a differentiable or evolutionary manner without large compute overhead.\n2. Expand and automate selection of transformation libraries, including domain- or task-specific ops (e.g., bbox-consistent transforms, color space specific, 3-D, audio).\n3. Study effects on robustness, adversarial resistance, and semi-/self-supervised learning.\n4. Integrate RandAugment jointly with neural architecture search or training-time hyper-parameter optimization.\n5. Develop adaptive schemes that adjust N and M during training based on validation feedback, model size, or dataset growth.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Delegated Classification",
      "full_text": "Delegated Classification Eden Saig, Inbal Talgam-Cohen, Nir Rosenfeld Technion – Israel Institute of Technology Haifa, Israel {edens,italgam,nirr}@cs.technion.ac.il Abstract When machine learning is outsourced to a rational agent, conflicts of interest might arise and severely impact predictive performance. In this work, we propose a theoretical framework for incentive-aware delegation of machine learning tasks. We model delegation as a principal-agent game, in which accurate learning can be incentivized by the principal using performance-based contracts. Adapting the eco- nomic theory of contract design to this setting, we define budget-optimal contracts and prove they take a simple threshold form under reasonable assumptions. In the binary-action case, the optimality of such contracts is shown to be equivalent to the classic Neyman-Pearson lemma, establishing a formal connection between contract design and statistical hypothesis testing. Empirically, we demonstrate that budget- optimal contracts can be constructed using small-scale data, leveraging recent advances in the study of learning curves and scaling laws. Performance and eco- nomic outcomes are evaluated using synthetic and real-world classification tasks. 1 Introduction The acclaimed success of machine learning at effectively solving difficult prediction tasks across diverse problem domains has made it highly appealing for firms, institutions, and individual practi- tioners. But machine learning has also become increasingly complex, cumbersome, and difficult to operate—and not all those who seek to learn have access to the necessary expertise, infrastructure, and designated resources required for learning effectively. This gap has created a new market for outsourced machine learning, in which a client interested in obtaining an accurate predictive model can hire the services of a specialized provider which, for a price, trains the model on their behalf. Consider for example a hospital purchasing a classifier for deciding between hospitalization and outpatient treatment when triaging patients. The provider invests in curating, cleaning and annotating training data, and delivers a trained model in return to payment from the hospital. Having a budget to expend on outsourced learning [51], we model the client as aiming to obtain the best possible predictive model. At first glance, it is tempting to assume that the optimal strategy is simply to pay the provider the maximal feasible amount—and hope to get a high-end model in return. After all, if the client were to spend the budget directly on learning, investing the maximal available sum would yield the best possible results. But this neglects to account for the incentives of the provider, who is interested in maximizing profit. Since the actions of the provider remain private, it is in his best interest to (secretly) minimize efforts, which in turn can result in his delivering a suboptimally-trained model. In our example, the provider can cut costs by annotating only a subset of the data, obtaining cheaper low-quality annotations, or neglecting to meticulously remove all outliers. Outsourced learning is hence susceptible to moral hazard, an economic situation which might occur under information asymmetry, and to the detriment of the client. Motivated by this observation, in this paper we initiate the study of delegated learning, and aim to explore the economic, algorithmic, and statistical implications that occur when learning is delegated to a specialized provider. Our key novelty is in instantiating delegated learning as a problem ofoptimal contract design [10, 42, 47, 48]. Broadly, contracts are an important monetary device that allows the client to establish a payment scheme 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.11475v2  [cs.LG]  5 Dec 2023Observe initial data; Design contract Strategically choose training set size !Sample data at cost \"!, and learn ℎSample validation set $∼&\"Count correct predictions '=)⋅acc#ℎ Private PrivateInteraction Classifier ℎ- Principal Agent Receive .'monetary units Contract .:0,…,)→ℝ$% Payment .' Figure 1: Delegated classification interaction sequence. The principal examines initial information, and designs a contract t : {0, . . . , m} →R≥0. Having observed t, the agent strategically selects a dataset size n that will maximize his expected utility. He samples a training set S ∼ Dn, incurs cost cn, then trains the classifier h ∈ Hand sends it to the principal. Upon receiving h, the principal evalu- ates its accuracy on a random validation set V ∼ Dm, and pays the agent according to the contract t. which, if properly set, serves to align incentives and guarantee that both parties are well-off. Our main challenge is to design effective contracts specialized to the task of delegated learning on a budget. Towards this, we begin with a conventional supervised classification setup, and impose economic structure by assuming that acquiring training examples is costly. We then conceptually “split” the conventional self-sufficient learner into two rational entities: a principal, who controls the budget and is interested in maximizing predictive accuracy; and an agent, who controls learning (in particular the training set) and is interested in maximizing profit. This allows us to model principal-agent relations as a Stackelberg game, in which the principal commits to acontract t, determining a priori the amount to be paid for every possible (stochastic) level of obtained accuracy. The agent best-responds to the contract by choosing the profit-maximizing number of samples n, and training the predictive model. Under this setting, we study the algorithmic problem of designing an optimal contract. As is standard in economic analysis, we begin with the assumption that the principal has full information on the distribution of possible outcomes for each of the agent’s possible actions. In our setting, actions correspond to the number of training samples, and outcomes to the empirical classifier accuracy; thus, the main object of interest for contract design in delegated learning settings is the learning curve, which describes the (stochastic) performance of learning per sample size. Under certain plausible conditions on the learning curve, namely MLRP and a certain notion of concavity, our main result here is that optimal contracts are simple, and in particular, take on the form of simple threshold functions. Simple contracts are appealing because they are straightforward to understand and communicate; in our setting, they are also easy to compute, and we give a closed-form solution for the optimal threshold contract. Providing an interpretation of the closed-form solution, our findings establish a new connection between contracts and the renowned Neymon-Pearson lemma [ 45], which we consider as one of our central theoretical contributions. We then switch gears and turn to empirically studying the construction of contracts from partial information. In particular, we consider a setting where the principal can only estimate the learning curve from small available data (e.g., by bootstrapping on small n and extrapolating). Using the recent LCDB dataset of learning curves [43], we show that threshold contracts generally perform well on estimated curves despite the inherent uncertainty. We also explore the role of different parameters of our setup, consider various tradeoffs in curve-fitting and contract design, and discuss limitations by pointing out certain failure modes to which contracts may be susceptible. Taken together, our results shed light on why and how simple contracts for delegated learning work to correctly balance between the incentives of both delegator and delegatee in outsourced learning. 1.1 Related work Previous works have considered delegation of ML-related tasks that differ from our task of training a classifier: labeling of data points in [ 14], gathering information in [16], and computing a costly high-dimensional function in [5]. In the delegated task of [24], the agent provides a classifier and the principal verifies its near-optimality withinH. A fundamental difference is that their agent is assumed to be adversarial rather than rational, and so interactive proofs are used instead of economic incentives. The Neyman-Pearson lemma has recently been connected to economic design by [8] in the context of adverse selection rather than moral hazard. The agent has a hidden type (e.g., whether a new 2drug is effective), and the optimal menu to offer this agent is designed based on the theory of e-values. When the hidden type has binary support, the method is equivalent to Neyman-Pearson. A “moral” link between the design of contracts (for a non-budgeted principal) and statistical inference (in particular likelihood ratios) was observed already in [ 25], but no connection was made to the power of hypothesis tests. Other intersections of ML and contracts that do not involve delegation of learning-related tasks include strategic classification [e.g., 36, 37, 3] and online learning of optimal contracts [29, 17, 52] Contract settings with a binary action and/or outcome space have been studied in [e.g., 20, 7, 22]. Not to be confused with our notion of delegation, there is a growing computational literature on delegation without monetary payments [e.g., 35]. To extrapolate from partial data, our work builds upon recent advancements in the study of learning curves, which characterize the expected generalization of learning as a function of dataset size and other exogenous factors [49]. There is growing empirical evidence that performance of modern neural networks can be predicted using simple scaling laws [e.g., 34, 41, 50, 23, 2, 46, 30], and theoretical results that back these findings in simplified settings [11, 12, 33, 9]. Perhaps closest to ours is the concurrent work [4], which analyzes a similar setting under different assumptions: Rather than assuming budget constraints, they assume that the principal’s utility is linear in accuracy and payout, and the learning curve takes a specific functional form. Based on these assumptions, they derive approximately optimal linear contracts which are robust to adverse selection. In contrast, we obtain globally optimal simple contracts based on hypothesis testing, and present data-driven methods to handle partial information. Together, the two studies demonstrate the important role of simple contracts in the growing ecosystem of machine learning delegation. 2 Problem Setup The core of our setting is based on a standard supervised classification task. Let x ∈ Xbe features and y ∈ Ybe labels, and assume there is some unknown joint distribution D over (x, y) pairs. Given a sample set S = {(xi, yi)}n i=1 ∼ Dn, the goal in learning is to use S to find a classifier h : X → Y from a class H that maximizes expected accuracy, accD(h) = P(x,y)∼D[h(x) = y]. Because the underlying distribution D is unknown, expected performance is estimated by the empirical average on an additional held-out validation set V ∼ Dm of size m, as accV (h) = 1 m Pm i=1 1 [h(xi) = yi], which is a consistent and unbiased estimator of accD(h). We will assume throughout that both the learning algorithm and the validation set size m are known and fixed. Learning on a budget. We will be interested in studying learning when certain resources are limited or costly. Our main focus will be on the setting where the main cost of learning is the number of labeled examples n, but we note that our approach can in principle extend to other forms of learning ‘effort’.1 We assume the learner has a monetary budget B to spend on samples, and is interested in maximizing accuracy under budget constraints. Let cn ≥ 0 be the cost of n samples (assumed to be increasing in n), then the learner aims to solve: n∗ = argmaxn Ehn[accD(hn)] s.t. cn ≤ B (1) where hn is a classifier learned from a random dataset of size |S| = n. Note that hn is a ran- dom variable with distribution depending on n. We denote the out-of-sample accuracy of hn by αn = accD(hn). When the learner is a self-sufficient entity, and when the expected αn improves monotonically in n, then n∗ in Eq. (1) in naturally the largest affordable n (see Sec. 3). However, as we will see, when learning is delegated—this seemingly straightforward observation can break. Delegation. We model the delegation of learning as a conceptual partition of the learner into two distinct entities: an agent, who controls learning; and a principal, who controls the validation process. The principal outsources the learning task to the agent, who in turn uses the training set S to train the classifier h; once delivered, the principal validates the performance of h using the validation set V . Whereas the classifier’s accuracy benefits the principal alone, the cost of learning (i.e., the cost of acquiring S) is born exclusively by the agent. Importantly, the amount of invested effort remains private to the agent; in our example, the principal cannot know how many examples received quality labeling. Because the agent seeks to maximize profit, the principal can use her budget as a source of monetary payment to incentivize the agent to invest in larger|S| = n. Intuitively, one could expect 1For example, [34] argue that not only training-set size, but also computation time and model size are related to accuracy through scaling laws, which have tight connections to the assumptions we discuss in Sec. 3. 30.0 0.5 1.0 Outcome (validation-set accuracy) Action n (training set size) 16 32 64 128 256 512 1024 2048 4096 8192 Outcome distributions fn Delegated classiﬁcation setting 0.00 0.25 0.50 0.75 1.00 Outcome (validation-set accuracy) 0 200 400 600 800 1000Payment t(j) Diﬀerent contracts with identical budget constant linear threshold 100 1000 Action n (training set size) 0.5 1.0 Utility un(t) Agent’s perspective: Maximize utility constant linear threshold Contract 0.0 0.5 E[accD(hn(t))] Principal’s perspective: Accuracy 101 102 103 Costcn(data acquisition cost) Figure 2: A delegated classification setting (data from Sec. 4). (Left) Each costly action taken by the agent (training set size n) induces a distribution fn of possible outcomes (classifier accuracy). The principal seeks to construct a contract t that incentivizes a profit-maximizing agent to take actions entailing favorable outcomes. Note thefn exhibit increasing expectation, but decreasing variance, inn. (Center) Three contracts for a given budget B, mapping outcomes to payments. (Top-right) Agent’s utilities un(t) and best responses n(t) (stars) for each contract t. (Bottom-right) Expected accuracies for principal resulting from each contract; here the threshold contract is optimal (see Sec. 3). larger payments to entail larger n, and therefore higher-accuracy h. However, as we will see, this is not always the case, and careful planning is required in order to fully utilize a given budget. 2.1 Delegation as contract design As the training set remains private to the agent, there is an information gap between the two parties. This creates a conflict of interest for the agent known asmoral hazard [10], in which the agent may be tempted to invest sub-par effort, while claiming that efforts were in fact his honest best. In economics, the celebrated solution to moral hazard are contracts [31]: pay-per-performance rules that a-priori determine future payments for every possible outcome, which we formally describe next. Contract design. A contract setting is defined by a set of actions A = {a1, . . . , aN } that can be taken by the agent, and a set of possible outcomes j ∈ {0, . . . , m}. Each action ai is associated with a cost ci, and w.l.o.g. we assume c1 ≤ ··· ≤cN so that actions correspond to increasing effort levels. The agent’s choice to perform action a ∈ Ayields a random outcome j ∼ fa for the principal, where fa describes a distribution over the possible outcomes associated with action a. The principal, who does not observe the agent’s chosen action, can incentivize the agent through acontract, t : {0, . . . , m} →R≥0, according to which she pays the agent t(j) ≥ 0 when the materialized outcome is j. Given contract t, let ua(t) be the agent’s expected utility from taking action a ∈ Aat cost ca (via stochastic outcomes j ∼ fa), and let a(t) be the agent’s best response—an action that maximizes his expected utility (and following standard tie-breaking assumptions as in [18]). Then: ua(t) = Ej∼fa[t(j)] − ca, a (t) ∈ argmaxa∈Aua(t). (2) Every action a∗ that is the best response a∗ = a(t) to some contract t is called implementable. In economic terms, the principal and agent are playing a Stackelberg game, in which the principal commits to a contract t and the agent best-responds by choosing action a(t) that maximizes his expected utility ua(t). The goal of the principal is to design a contract t which incentivizes the agent to take best-response actions yielding favorable outcomes for the principal. Contracts for delegated learning. We propose to formulate delegated learning as a problem of optimal contract design, instantiated as follows. First, we relate agent actions a with the number of samples n, and denote A = {n1, . . . , nN } as the possible sizes of S that the learning agent can work with. The cost of acquiring samples naturally maps as ca = cn, and agent’s best response is a(t) = n(t). Next, we associate outcomes j with accuracy for the principal by defining j as the number of validation samples (out of the possible m) on which h is correct; note this implies accV (h) = j/m, and we will therefore use j and accV (h) as ‘outcomes’ interchangeably. Finally, for an action n, we set fn to be the distribution over possible accuracies obtained when learning with n samples, namely fn(j) = Phn,V [accV (hn) = j/m] ∀j. We will also use the matrix form Fnj = fn(j), where F ∈ [0, 1]N×(m+1). Note that F admits two sources of variation: (i) a-priori variation in hn due to stochasticity in S ∼ Dn; and (ii) a-posteriori variation in j for any fixed hn 4due to stochasticity in V ∼ Dm. When hn is fixed, the outcome distribution admits a simple binomial form, namely j ∼ Binomial(m, αn). Empirically, we observe this to be the dominant component. 2.2 Delegation as an optimization problem Budget-optimal contracts. Recall that the principal seeks to maximize accuracy under budget constraints (Eq. (1)). Once learning is delegated to an agent and framed as a contract design problem, the principal’s objective becomes: t∗ = argmaxt∈[0,B]m Ehn(t) [accD(hn(t))] (3) Contract t∗ is chosen to incentivize the agent to invest effort n(t) (via eq. (2)) such that the training of hn(t) yields high dividends for the principal in terms of expected accuracy. We will refer to t∗ as a budget-optimal contract, and to the general task of finding t∗ as budget-optimal contract design. Information structure. Delegated learning settings have actions n and costs cn known to both sides, and outcome distribution Fnj known to the agent. For the principal, we explore varying levels of knowledge: In Sec. 3, we assume (as in the classic contract design literature) that the principal has full information of F (i.e., knows the learning curve), and focus on characterizing the optimal contract. In Sec. 4 we relax this assumption, and explore a partial-information setting in which the principal relies instead on an empirically-estimated curves ˆF. 3 Budget-Optimal Contract Design 3.1 The problem Why agents cut corners. The conceptual challenge in designing contracts lies in that agents cannot reliably report what they did. For example, consider a principal who, after delegation, received a classifier attaining 0.74 (validation) accuracy. Should she be happy? The crux is that there are two ways that this could have happened: (i) the agent invested high effort in learning (largen), but received an uninformative S by chance, and delivered a low-quality h as a result; and (ii) the agent invested low effort (small n). Since the agent’s actions are private, and because outcomes are stochastic, the principal can never know for certain which is the true underlying cause. In other words, a ‘lazy’ (or rather strategic) agent can hide behind the uncertainty that is inherent in learning outcomes.2 Contract types. To overcome this informational gap, the principal must devise a contract to align in- centives and encourage the agent to prefer certain actions over others. But not all contracts are equally effective. Fig. 2 illustrates for a budget B three contract types and their economic implications: • Constant contract (t(j) = B): The agent is paid B regardless of the outcome. His best-response in this case is to choose the least-costly action—to the detriment of the principal. • Linear contract (t(j) = Bj/m): The agent is paid a fraction of B, linear in the resulting accuracy. Linear contracts are a popular and extensively-studied class of contracts [e.g.,32, 15]. Nonetheless, and though seemingly sensible, linear contracts turn out to be sub-optimal for our setting. • Threshold contract (t(j) = B1 [j ≥ j0] for some j0): The agent is paid B provided the empirical accuracy surpasses a threshold j0. In the example in Fig. 2, the threshold contract is optimal. Rather than committing a-priori to some type of contract, we seek to find the best budget-optimal contract by solving Eq. (3). For this it is useful to have structure. Stochastic learning curves (and where to find them). Our approach uses the observation that there is a tight connection between the set of distributions {fn} encoded in F, and learning curves, which describe the anticipated accuracy of a classifier as a function of the size of its training set. Learning curves typically depict only expected accuracy, but there is also inherent variation in outcomes. We will therefore broadly use the term ‘stochastic learning curve’ to describe both mean trendand variation in accuracy as a function of n; formally, a stochastic learning curve is defined precisely by F. This connection is useful because learning curves have structure: First, expected learning curves are typically monotone [34, 11]; when not [43, 49], they can be monotonized [12]. Second, stochastic learning curves are likely to satisfy the monotone likelihood ratio property (MLRP), which states that the better the performance of a classifier, the more likely it was trained on more data (see Def. 1). 2The agent can also hide in the uncertainty due toV , particularly when m is small; see experiment in Sec. 4.2. 5Table 1: Characterization of simple min-budget contracts in different settings. Simple contract forms include all-or-nothing contracts and their subclass of threshold contracts. The table specifies for each configuration either the simple form that is optimal (in one case through equivalence to the Neyman-Pearson lemma), or that the simple form is non-optimal or intractable. Problem size Structural assumptions Actions Outcomes No assumptions MLRP Concave-MLRP |A| = 2 any size All-or-nothing (T1) Threshold (B.7.1) |A| > 2 m + 1 = 2 All-or-nothing (B.5.1) Threshold (B.7.2) m + 1 > 2 Simple is NP-hard (T3) ∃ non-threshold (B.7.3) Threshold (T4) 3.2 Optimization via min-budget contracts Our main technique for solving Eq. (3) relies on a reduction to what we refer to as min-budget contracts. Given an (implementable) target actionn∗ ∈ A, a min-budget contract forn∗ is a contract t that incentivizes the agent to employ precisely the actionn∗, while minimizing the maximum payment by the principal ∥t∥∞ = maxj∈{0,...,m}{t(j)}; i.e., t implements n∗ at minimum budget. Formally: t∗ = argmint ∥t∥∞ s.t. n(t) = n∗ (4) Our reduction relies on the following claim (Proof in Appendix B.1): Proposition 1. Every budget-optimal contract design problem has an optimal solution which is also min-budget. Using Prop. 1, a solution to Eq. (3) can be obtained by iteratively solving Eq. (4): For allni ∈ A, solve Eq. (4) with target actionn∗ = ni, and return t∗(ni) for the best implementable ni whose budget does not exceed B. The budget-optimal problem thus reduces to solving multiple min-budget problems. To compute each t∗(ni) in Eq. (4), we formulate a novel MIN-BUDGET linear program (LP), 3 detailed in Appx. B.2. One way to solve this LP is with generic solvers—an approach which is valid, but can be costly. One of our contributions is in identifying natural cases where min-budget contracts take on simple forms, which are easier to optimize, and have practical merit. In particular, we show that binary-action contracts have all-or-nothing structure (t(j) ∈ {0, B}), and plausible structural assumptions on the learning curve give rise to threshold contracts (t(j) = B1 [j ≥ j0] for some j0). Our theoretical results are summarized in Table 1, and detailed in the rest of the section. 3.3 All-or-nothing contracts: Binary action space and the statistical connection We begin with a simple delegated learning setting in which the agent can choose one of two actions, A = {n1, n2}, e.g., a ‘small’ vs. ‘large’ training set, and the principal seeks to incentivize training with more data.4 This reduced case will be useful as a building block for the general case (which we re- turn to in Sec. 3.4), and for making a precise connection between contract design and hypothesis tests. Simple min-budget contracts for binary action space. Our first result shows that optimal binary-action contracts are all-or-nothing contracts whose budget is determined by the total variation distance between outcome distributions f2 and f1, namely ∥f2 − f1∥TV = 1 2 Pm j=0 |f2,j − f1,j|. Theorem 1 (Optimal binary-action contract) . In a binary-action contract setting with outcome distributions f1, f2 and costs c1, c2, the min-budget contract is an all-or-nothing contract, given by: t∗(j) = B1 [f2(j) ≥ f1(j)] ∀j ∈ {0, . . . , m}, where B = (c2−c1)/∥f2−f1∥TV. (5) The proof (in Appendix B.4.2) is by LP duality. Intuitively, the optimal contract pays the agent for outcomes that are more likely to come from f2 than from f1. Moreover, it requires a higher budget the smaller the distance is between the two distributions f1, f2. At the extremes, if their distance is 1 (i.e. no overlap among their supports), the required budget for incentivizing n2 is c2 − c1, whereas if their distance is 0 (i.e. f1 = f2) it becomes impossible to incentivize n2. 3The MIN-BUDGET LP is closely related to the well-known MIN-PAY LP from non-budgeted contract design [e.g., 19], but with a different objective, and hence very different optimal solutions (see Appx. B.3). 4When n2 is not the target or when it is not implementable, then the solution is immediate: always pay c1. 6Formal connection to optimal hypothesis testing. Theorem 1 also uncovers a direct corre- spondence between optimal contracts and optimal hypothesis tests. Intuitively, given the outcome distributions {f1, f2}, and in order to incentivize n2, the principal wishes to pay the agent if the observed outcome j ∈ {0, . . . , m} is more likely to have originated from f2. The principal can attempt to identify whether the outcome j is drawn from distribution f1 or f2 through hypothesis testing, where a hypothesis test ψ : {0, . . . , m} → {0, 1} maps a sample j to either f2 (indicated by 1) or to the null hypothesis f1 (indicated by 0). For our purpose it is convenient to allow tests to be non-integral, in which case ψ : {0, . . . , m} →[0, 1] maps j to a probability with which it originates from f2. The quality of a hypothesis test is measured by summing its type-1 and type-2 errors: Pm j=0 f1,jψj + Pm j=0 f2,j(1 − ψj). The test that minimizes this sum is known as the most powerful hypothesis test, and has been characterized by Neyman and Pearson [45, 4.3]. We now turn to formally establishing the connection. For a fixed B, observe that every contract with budget B can be mapped to a hypothesis test via the bijection ψ(j) = t(j)/B. Then: Theorem 2 (Optimal contract vs. test). Consider binary-action contract design with distributions f1, f2 and costs c1, c2. A contract t with budget B is optimal if and only if its corresponding hypothesis test ψ = t/B is maximum power with type-1 and type-2 errors summing to 1 − c2−c1 B . The proof (Appendix B.4.2) is by a non-linear variable transformation to the MIN-BUDGET LP. Theorem 2 implies that the optimal contract for the binary-action case (Theorem 1) is equivalent to the well-known Neyman-Pearson lemma characterizing the most powerful hypothesis test: Lemma 1 (Neyman-Pearson [e.g., 45]). Let f1, f2 be two discrete probability distributions. Then the most powerful hypothesis test for f1, f2 is the likelihood ratio test ψ(j) = 1 [f2(j) ≥ f1(j)], which attains the optimal bound 1 − ∥p − q∥TV on the sum of type-1 and type-2 errors. Theorem 2 establishes a new formal connection between the two domains of contract design and hypothesis testing. In the context of contract design, it provides a statistical interpretation: A min- budget contract can be interpreted as an optimal hypothesis test, and the ability to distinguish between the two hypotheses determines the required budget. In the converse direction, it enables a new proof for the Neyman-Pearson using the min-budget contract given by Theorem 1 (see Appendix B.4.2). 3.4 All-or-nothing contracts: Beyond binary action For general action spaces, optimal contracts are not guaranteed to be all-or-nothing. In fact, we show that determining whether there exists an all-or-nothing contract that is optimal is NP-hard: Theorem 3 (Hardness). Finding a min-budget all-or-nothing contract is NP-hard. The proof is by reduction from 3SAT, and appears in Appx. B.5.2. Nonetheless, there are special but important cases—notably the binary outcome case5— in which results from Sec. 3.3 hold, suggesting that ideas from Thm. 1 apply more broadly. The following algorithm makes use of these ideas, showing good empirical performance, and provable performance under an MLRP condition (Sec. 3.5). Single binding action algorithm. Revisiting the closed-form all-or-nothing contract in Eq. (5), we observe that the result is based on the fact that binary action spaces have only one alternative action. Building upon this observation, we propose the single binding action (SBA) algorithm, which computes a solution to Eq. (4) in the general (many-actions) case: Given target action n∗ ∈ A, loop over all actions n ̸= n∗, and apply the closed-form formula in Eq. (5) to obtain an (all-or-nothing) contract t∗(n, n∗). If the agent’s best response (Eq. (2)) satisfies n (t∗(n, n∗)) = n∗, return t∗. If the loop ends without returning a contract, return ‘fail’. The following claim shows the algorithm is sound: Proposition 2 (Soundness of SBA). When the single binding action algorithm terminates successfully, it returns an optimal contract which is an all-or-nothing contract. Proof in Appendix B.6. Prop. 2 ensures that if SBA succeeds, then the returned all-or-nothing contract t∗ is optimal. Moreover, failure does not preclude the existence of an optimal all-or-nothing contract. If SBA fails, we solve Eq. (4) with a generic LP solver. Empirically, SBA was successful in more than 85% of cases, and is ∼103 times faster than the LP solver (Appendix C.3). Thus, this optimistic ‘try SBA first’ approach typically succeeds, adds negligible overhead if not, and guarantees correctness. 5In this case there are binary outcomes (m = 1), such as when the agent’s efforts can result in either success or failure [6, 28, 21]. For this case, we show that the optimal contract is also all-or-nothing (Appx. B.5.1). 73.5 Threshold contracts: MLRP assumption In this section we provide sufficient conditions, in the form of natural structural properties of (stochas- tic) learning curves, that guarantee the optimality of even simpler contracts—namely threshold contracts—and the success of SBA. With inspiration from hypothesis testing [40] and contract theory [26, 19], it is natural to consider the monotone likelihood ratio property (MLRP) assumption: Definition 1 (MLRP [e.g., 26]). A contract design setting satisfies MLRP if for every pair of actions a, a′ such that ca < ca′, the likelihood ratio fa′(j)/fa(j) is monotonically increasing in j. In our context, MLRP states that the better the validation-set performance of a classifier, the more likely it was trained on more data. This holds in particular for monotone learning curves with binomial outcome distribution [19, B.1]. For a binary action space, MLRP ensures that n2 is always implementable,6 and that the optimal contract is a threshold contract: t∗(j) = B1 [j ≥ j0]. This is by Theorem 1, and by the fact that ∃j0 such that f2(j)/f1(j) ≥ 1 iff j ≥ j0 (see also Appendix B.7.1).7 Interestingly, this is similar to the relation between the Neyman-Pearson lemma and the Karlin-Rubin theorem, which characterizes the most powerful hypothesis test under monotone likelihood ratio [40]. MLRP for general action space. MLRP does not guarantee threshold contracts in general: In Appendix B.7.3, we give a constructive counterexample satisfying MLRP, but for which the optimal contract is not threshold. However, refining MLRP to also capture ‘diminishing returns’ turns out to be sufficient for recovering guarantees generally. For target actionaN , denote the survival probability of an action ai by si = Pj∼fi[j ≥ j∗], where j∗ is the minimal outcome at which action aN is more likely than aN−1. These will serve as formal means for capturing the concavity of learning curves. Definition 2 (C-MLRP). A contract design setting satisfies Concave-MLRP (C-MLRP) if it satisfies MLRP , and additionally the actions’ survival probability is concave as a function of the actions’ cost. Our final result shows that C-MLRP guarantees optimality of threshold contracts, and success of SBA: Theorem 4 (Sufficiency for threshold). Consider a contract design setting with C-MLRP. Then the optimal contract is a threshold contract, and is recovered by the SBA algorithm. We prove this claim by showing that concavity implies that only one alternative action is binding in the linear program equivalent to Eq. (4), reducing the problem to the two-action case. By applying Theorem 1, we obtain optimality of threshold contracts in this case as well (proof in Appendix B.7.3). This also implies that SBC always terminates successfully on inputs that satisfy C-MLRP. In practice, we believe that C-MLRP is a reasonable assumption for realistic learning curves: in Appendix B.8.1, we prove it is satisfied by a standard theoretical model of learning curves, and in Ap- pendix C.3, we empirically demonstrate that it is (approximately) satisfied in settings where threshold contracts are optimal. Interestingly, in our empirical study, threshold contracts were often optimal even when C-MLRP did not hold—suggesting the condition is sufficient, but not necessary (see Sec. 4.1). 4 Experiments We now turn to our empirical investigation of delegated learning under full and partial information. We base our experiments on the recently curated Learning Curves Database(LCDB) [ 43], which includes a large collection of stochastic learning curves for multiple classification datasets and methods. For each dataset and method, the database includes held-out accuracy measurements obtained for increasing sample sizes n ∈ \b 24, 24.5, . . . ,215\t , with multiple repetitions per n; these provide us with stochastic learning curves. Here we focus primarily on the popular MNIST dataset [39] as our case study, and on MLP and GBDT as representative classifiers, but we refer the reader to Appendix C for further experiments on additional datasets and methods. Code is available at: https://github.com/edensaig/delegated-classification. 4.1 Full information We begin with the full information setting to explore in a clean environment how different parameters of the learning setting and environment affect predictive performance and economic outcomes. 6As a corollary of a similar result for min-pay contracts [19, Lemma 7]. 7This also holds for binary outcomes in an arbitrary action space, see Appendix B.7.2. 8102 103 104 Size of training set n 0.4 0.6 0.8 1.0 Accuracy accD(hn) MNIST learning curves MLP GBDT 0 20 40 Size of validation set m 10000 20000 30000Required budget B∗ Required budget for 85% accuracy MLP GBDT cn∗ MLPGBDT 0.5 0.6 0.7 0.8 0.9 Target accuracy E[accD(hn)] −105 −102 0 102 105 ∆B∗ = B∗ GBDT − B∗ MLP Which algorithm is less costly to delegate? ∆B Figure 3: Delegating with full information. (Left) Typical learning curves for two learning algorithms on MNIST. (Center) Required budget for target accuracy of 0.85 per validation set size m. (Right) Different cost regimes, indicating per accuracy region which of the two methods is cheaper to delegate. Validation set size. Fig. 3 (left) presents typical stochastic learning curves for two learning algo- rithms: Multi-Layered Perceptron (MLP) and Gradient-Boosted Decision Trees (GBDT). We take an arbitrary accuracy point on the curve at acc(n) = 0 .85 (dotted line) to examine the effects of validation set size m on min-budget contracts. Notice that MLP requires larger n to obtain 0.85; Fig. 3 (center) shows how this translates to a larger required budget B∗, which holds for all m. As m increases, required budgets and the difference between them both decrease. Larger validation sets are therefore useful for reducing required budget. Nonetheless, even for reasonable m, obtained budgets still remain higher than their theoretical lower bounds (target action costs cn∗). Budget regimes. Fig. 3 (left) also indicates two points in which the learning curves cross (dashed lines), at ∼0.74 and ∼0.94 accuracy. These correspond to sample sizes n for which both methods obtain matching accuracies (in expectation). For a self-sufficient learner, the implication is that at each of these points, both methods are equally costly, i.e., both cost cn. Interestingly, and in contrast, delegation can entail different required budgets despite equal accuracies. Fig. 3 (right) shows for each target accuracy the gap in required budgets between both methods, ∆B∗ = B∗ GBDT − B∗ MLP . As can be seen, each method is comparatively more (or less) costly in different accuracy regimes (up to 0.6; between 0.6 and 0.92; and above 0.92). Crucially, the budget gap can be large even when accuracies match (dashed lines). For example, even though both MLP and GBDT require n=362≈217/2 samples to obtain ∼0.74 accuracy, GBDT is cheaper (∆B∗= − 102); for ∼ 0.94 which requires n=23170≈229/2 from both, GBDT is significantly more expensive (∆B∗=105). The reason for this is that optimal budgets are determined by the ability to distinguish between distributions (Sec. 3.3). Prevalence of simple contracts. To understand the applicability of our theoretical findings, in Appendix C.3 we conduct an empirical prevalence evaluation on additional learning algorithms, and across target actions. We observed that min-budget contracts assume a threshold form and the SBC algorithm returns correct results in more than 85% of cases overall. Restricting optimization to simple contracts, budget requirements were generally less than 1% higher than that of a min-budget contract, suggesting that simple contracts may provide a good approximation even when min-budget contracts do not assume a simple form. 4.2 Partial information We now turn to consider delegation under partial information, in which the principal must rely on an estimated learning curve. We instantiate this idea by assuming that the principal has access to a small ‘pilot’ dataset of size k, where k is considered small. Using this set, the principal creates an estimated learning curve ˆF by fitting a curve to accuracies obtained for up to some n0 ≤ k, and extrapolating to larger n > n0. In particular, we experiment with fitting parametric power-law curves of the form E[αn] = a − bn−c, which have been shown to provide good fit in various scenarios both empirically and theoretically [49, 34, 11]. Since power-law curves are monotone, composition with binomial distributions increasing in p provably results in MLRP stochastic curves [19, B.1]. Bias-variance tradeoff. Given k pilot examples, there are different ways in which the principal can use them to construct an estimated curve. Here we consider a simple tradeoff: setting n0 to be small but with more samples per n < n0 (low variance), or setting n0 to be large but with few samples per n < n0 (low bias). We definer as the number of samples pern (so low r means larger n0). Then, for a 90.5 1.0 Diﬀerent sampling strategies r = 1 0.5 1.0 Expected accuracy r = 3 102 103 104 Training set size n 0.5 1.0 r = 5 102 103 Exploration set size k 0.6 0.7 0.8 0.9 Attained accuracy accD(hn(ˆt)) Extrapolation convergence 102 103 Pilot dataset size k 0 10 20 30Size multiplier µ(k) = n(ˆt)/k Cost-eﬃciency −0.6 −0.4 −0.2 0.0 0.2 Extrapolation residual at n(t∗) -100% -80% -60% -40% -20% 0% Rel. accuracy loss in delegation Eﬀective loss Observs. Median Figure 4: Delegating with partial information. (Left) Extrapolated learning curves for different r. (Center-left) Accuracy obtained via delegation per pilot set sizek. (Center-right) Multiplicative gain in effective number of samples due to delegation. (Right) Implications of over vs. under-estimation. given r, we set n0 such that P n≤n0 r·n ≤ k (i.e., such that the total number of used samples does not exceed k). Fig. 4 (left) shows different curve fits forr ∈ {1, 3, 5}, and corresponding n0. Then, Fig. 4 (center-left) shows for a certain fixed budget the accuracy level that can be attained for increasing k, and as a function of r. As can be seen, having sufficient points k for constructing ˆF is important, but performance grows quickly with k (note log-scale x-axis). It is also apparent in our example that low bias (via larger n0) is much more important than low variance for constructing useful ˆF. Cost-efficiency tradeoff. Because the pilot set provides the principal a basic means for obtaining minimal accuracy, we can ask: given k examples, and for a fixed budget B, what is the added benefit of delegating learning? For this, we define µ(k) = n(ˆt)/k to be the sample-size multiplier, i.e., the multiplicative gain in the effective number of samples due to delegation. Fig. 4 (center-right) shows µ(k) for increasing k and across r. For r = 1 (which is superior in terms of performance and outcomes), µ begins at ∼10, increases to ∼30 at around k = 190, and slowly decreases back to ∼10 towards k = 1, 000. For r > 1, we observe that µ ≈ 1, i.e., there is effectively no gain from delegation, until around k = 100, only after which some gain is restored. This highlights the importance of obtaining an accurate estimate ˆF in terms of the economic consequences of delegation. Over vs. under-estimation. Typically in curve-fitting, over and under-estimation are treated equally, since both types of error can negatively affect goodness of fit and extrapolation quality. However, for delegation, the implications of over vs. under-estimation on contract outcomes are highly asymmetric. Fig. 4 (right) shows for a target incentivized number of samplesn(t∗) the relation between the (theoretical) signed extrapolation error n(t∗) (i.e., over- or under-estimate, measured in accuracy points) and the eventual loss in accuracy obtained through delegation, relative to perfect estimation. Each point in the plot corresponds to one curve-fitting instance, with points shown for varying k, n0, and r, and with multiple independent repetitions. Results show that in the under- estimation regime (i.e., negative extrapolation error), loss in accuracy degrades gracefully with the estimation error. In stark contrast, even minimal over-estimation (positive extrapolation error) causes accuracy to plummet dramatically, as the agent’s rational response in those cases was to use the smallest dataset possible. We interpret this as a consequence of ‘setting the bar too high’—a rational decision to minimize effort in response to unrealistic expectations. This has important implications for the choice of how to fit and extrapolate learning curves, suggesting that contracts can be tolerant to under-estimation, while over-estimation should be avoided at all costs. 5 Discussion Motivated by the increasingly-common practice of outsourcing learning tasks, this paper sets out to introduce and study the novel problem of delegated classification. Our findings suggest that conflict of interests should not be overlooked, and that contracts hold potential as a means for aligning them. Our analysis relies on a set of assumptions, which should be carefully considered by practitioners and empiricists alike; we also believe that there are likely further fruitful connections to explore between contracts and statistical hypothesis testing. As a problem of contract design, and when the learning task is reasonably well-behaved, delegated learning manifests in the form simple threshold contracts. A natural question for future work is whether simplicity also implies robustness to partial knowledge—as is often the case [19]. 10Acknowledgements. The authors would like to thank Ruth Heller, Shafi Goldwasser, Jonathan Shafer, Ohad Einav, and anonymous reviewers for their insightful remarks and valuable suggestions. Nir Rosenfeld is supported by the Israel Science Foundation grant no. 278/22. Eden Saig is supported by the Israel Council for Higher Education PBC scholarship for Ph.D. students in data science. Funded by the European Union (ERC, ALGOCONTRACT, 101077862, PI: Inbal Talgam-Cohen). References [1] Milton Abramowitz, Irene A Stegun, and Robert H Romer. Handbook of mathematical functions with formulas, graphs, and mathematical tables, 1988. [2] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [3] Tal Alon, Magdalen Dobson, Ariel D. Procaccia, Inbal Talgam-Cohen, and Jamie Tucker-Foltz. Multiagent evaluation mechanisms. In AAAI 2020, pages 1774–1781, 2020. [4] Nivasini Ananthakrishnan, Stephen Bates, Michael I Jordan, and Nika Haghtalab. Delegating data collection in decentralized machine learning. arXiv preprint arXiv:2309.01837, 2023. [5] Pablo D Azar and Silvio Micali. Computational principal–agent problems. Theoretical Eco- nomics, 13(2):553–578, 2018. [6] Moshe Babaioff, Michal Feldman, and Noam Nisan. Combinatorial agency. In Proceedings of the 7th ACM Conference on Electronic Commerce, pages 18–28, 2006. [7] Moshe Babaioff, Michal Feldman, Noam Nisan, and Eyal Winter. Combinatorial agency. Journal of Economic Theory, 147(3):999–1034, 2012. [8] Stephen Bates, Michael I Jordan, Michael Sklar, and Jake A Soloff. Principal-agent hypothesis testing. arXiv preprint arXiv:2205.06812, 2022. [9] Devansh Bisla, Apoorva Nandini Saridena, and Anna Choromanska. A theoretical-empirical approach to estimating sample complexity of dnns. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3270–3280, 2021. [10] Patrick Bolton and Mathias Dewatripont. Contract theory. MIT press, 2004. [11] Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon Van Handel, and Amir Yehudayoff. A theory of universal learning. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 532–541, 2021. [12] Olivier J Bousquet, Amit Daniely, Haim Kaplan, Yishay Mansour, Shay Moran, and Uri Stemmer. Monotone learning. In Conference on Learning Theory, pages 842–866. PMLR, 2022. [13] Michael L. Bynum, Gabriel A. Hackebeil, William E. Hart, Carl D. Laird, Bethany L. Nicholson, John D. Siirola, Jean-Paul Watson, and David L. Woodruff.Pyomo–optimization modeling in python, volume 67. Springer Science & Business Media, third edition, 2021. [14] Yang Cai, Constantinos Daskalakis, and Christos H. Papadimitriou. Optimum statistical estimation with strategic data sources. In COLT 2015, pages 280–296, 2015. [15] Gabriel Carroll. Robustness and linear contracts. American Economic Review, 105(2):536–563, 2015. [16] Junjie Chen, Minming Li, and Haifeng Xu. Selling data to a machine learner: Pricing via costly signaling. In International Conference on Machine Learning, pages 3336–3359. PMLR, 2022. [17] Alon Cohen, Argyrios Deligkas, and Moran Koren. Learning approximately optimal contracts. In SAGT 2022, pages 331–346, 2022. [18] Paul Dütting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In EC 2019, pages 369–387, 2019. 11[19] Paul Dütting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In Proceedings of the 2019 ACM Conference on Economics and Computation, pages 369–387, 2019. [20] Paul Dütting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Combinatorial contracts. In FOCS 2021, pages 815–826, 2021. [21] Paul Dütting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Combinatorial contracts. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 815–826. IEEE, 2022. [22] Paul Dütting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Multi-agent contracts. In STOC 2023, 2023. To appear. [23] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=hR_SMu8cxCV. [24] Shafi Goldwasser, Guy N Rothblum, Jonathan Shafer, and Amir Yehudayoff. Interactive proofs for verifying machine learning. In12th Innovations in Theoretical Computer Science Conference (ITCS 2021). Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2021. [25] Sanford J. Grossman and Oliver D. Hart. An analysis of the principal-agent problem. Econo- metrica, 51(1):7–45, 1983. [26] Sanford J Grossman and Oliver D Hart. An analysis of the principal-agent problem. In Foundations of insurance economics, pages 302–340. Springer, 1992. [27] William E Hart, Jean-Paul Watson, and David L Woodruff. Pyomo: modeling and solving mathematical programs in python. Mathematical Programming Computation, 3(3):219–260, 2011. [28] Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. In Proceedings of the fifteenth ACM conference on Economics and computation, pages 359–376, 2014. [29] Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. Journal of Artificial Intelligence Research, 55:317–359, 2016. [30] Derek Hoiem, Tanmay Gupta, Zhizhong Li, and Michal Shlapentokh-Rothman. Learning curves for analysis of deep networks. In International conference on machine learning, pages 4287–4296. PMLR, 2021. [31] Bengt Holmström. Moral hazard and observability. The Bell Journal of Economics , 10(1): 74–91, 1979. [32] Bengt Holmstrom and Paul Milgrom. Aggregation and linearity in the provision of intertemporal incentives. Econometrica: Journal of the Econometric Society, pages 303–328, 1987. [33] Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021. [34] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [35] Jon Kleinberg and Robert Kleinberg. Delegated search approximates efficient search. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 287–302, 2018. [36] Jon Kleinberg and Manish Raghavan. How do classifiers induce agents to invest effort strategi- cally? In EC 2019, pages 825–844, 2019. [37] Jon M. Kleinberg and Manish Raghavan. Algorithmic classification and strategic effort. SIGe- com Exch., 18(2):45–52, 2020. 12[38] Sébastien Lahaie. How to take the dual of a linear program. Columbia University, New York, 2008. [39] Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. [40] Erich Leo Lehmann, Joseph P Romano, and George Casella. Testing statistical hypotheses, volume 3. Springer, 1986. [41] Rafid Mahmood, James Lucas, David Acuna, Daiqing Li, Jonah Philion, Jose M Alvarez, Zhiding Yu, Sanja Fidler, and Marc T Law. How much more data do I need? Estimating requirements for downstream tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 275–284, 2022. [42] David Martimort and Jean-Jacques Laffont. The theory of incentives: the principal-agent model. Princeton University Press, 2009. [43] Felix Mohr, Tom J Viering, Marco Loog, and Jan N van Rijn. Lcdb 1.0: An extensive learning curves database for classification tasks. Machine Learning and Knowledge Discovery in Databases, ECMLPKDD. p. accepted. Lecture Notes in Computer Science, Springer, 2022. [44] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. [45] Phillippe Rigollet and Jan-Christian Hütter. High dimensional statistics. Lecture notes for course 18S997, 813(814):46, 2015. [46] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=ryenvpEKDr. [47] Royal Swedish Academy of Sciences. Scientific background on the 2016 Nobel Prize in Economic Sciences, 2016. [48] Bernard Salanie. The Economics of Contracts: A Primer. MIT press, 2017. [49] Tom Viering and Marco Loog. The shape of learning curves: a review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [50] Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Veselin Stoyanov. Training trajectories of language models across scales. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages 13711–13738. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL -LONG.767. [51] Xinyi Zhao, Weixin Liang, and James Zou. Data budgeting for machine learning.arXiv preprint arXiv:2210.00987, 2022. [52] Banghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I. Jordan. The sample complexity of online contract design. In Proceedings of the 24th ACM Conference on Economics and Computation, EC 2023, London, United Kingdom, July 9-12, 2023, page 1188. ACM, 2023. doi: 10.1145/3580507.3597673. 13A Broader implications In this paper, we set out to formalize and study the task of delegating classification through the lens of contract design. Given that our work is largely motivated by the increasingly common practice of outsourcing learning tasks to specialized service providers, we believe our algorithm, analysis, and empirical observations carry meaningful implications for practitioners and decision-makers alike. At the same time, it is important to remember that our work—as others considering economic aspects of learning—studies delegation in a simplified setting and under certain assumptions. As such, and since devising and agreeing to legally-binding contracts can have concrete implications on real-world out- comes, care should be taken when applying ideas or conclusions that derive from our work in practice. For example, consider that our formalism relies on the assumption that the learning agent is all- knowing and rational. Yet in reality, agents must act under partial information, face irreducible (and often unquantifiable) uncertainty, and—being human—are subject to common behavioral biases. It is unclear a-priori if and how our statements and conclusions carry over to this setting. As another ex- ample, notice that our formalism considers a one-shot setting where a single contract between a single principal-agent pair is instantiated once. But in reality, competition and long-term reputation may play a significant role in determining the agent’s incentive structure, and consequently, her behavior. In such cases, naïvely applying our framework without careful inspection of the appropriate incentives on both ends can result in sub-optimal contracts, possibly to the detriment of all involved parties. Nonethe- less, given that our work aims to take an initial step towards establishing delegated learning, we view its extension to non-rational agents and temporal and competitive settings as intriguing future work. One message that our work conveys is that in delegation, simplicity, in the form of threshold contracts, has merit. This draws connections to other works that similarly argue for simplicity as an important and useful property of effective delegation mechanisms. Our work shows that simple contracts are, under reasonable conditions, computationally feasible and theoretically optimal. Economically, threshold contracts are practically appealing since they are easy to understand, communicate, and regulate. Given that contracts are in essence social constructs, we believe these properties are key for establishing threshold contracts as effective building blocks for machine learning markets. B Min-budget contract design – deferred proofs Notation. To align with traditional contract design notation, in this section, we denote the action space by A = [n] = {1, . . . , n}, and the outcome space by Ω = {0, . . . , m}. We denote by ∆ (X) the set of distributions over a given set X. For contract design problems, we denote the outcome probabilities by Fi,j, such that Fi ∈ ∆ (Ω)is the outcome distribution associated with action i, and also denote the cost of each action by ci. Given x ∈ R, we denote x+ = ReLU(x) = max {0, x}. We denote the indicator function by 1 [·], the total variation distance between distributions P, Qby ∥P − Q∥TV (see Definition 4), and the survival function of P ∈ ∆ (Ω)by SP (·) (see Definition 14). A note on individual rationality. In the contract design literature, a contract is said to be incentive compatible (IC) with respect to some action a∗ if it satisfies ua∗(t) ≥ ua(t) for all actions a ∈ A. In the delegated classification setting, this corresponds to the constraint n(t) = n∗ in eq. (4). As an additional constraint, contracts in which the agent’s expected utility is always non-negative (un∗(t) ≥ 0) are said to be individually rational (IR). In the case of delegated classification, it is natural to assume that there always exists a valid action that the agent can take at zero cost — for example, returning a dummy classifier that always abstains from prediction (thus having zero accuracy), or a classifier which makes a prediction at random. As such, contracts in the delegated classification setting can be assumed to be individually rational without loss of generality, as any action n that is chosen by the agent has utility which is weakly larger than the utility of the zero-cost action, which is always non-negative. Moreover, even in cases where a zero-loss action does not exist, we show that individual rationality (IR) can be attained in a straightforward manner, by adding the minimal cost c1 to each entry of an incentive compatible (IC) contract (tj 7→ tj + c1): Claim 1. Given an IC contract t with c1 > 0, the contract t + c1 (add c1 to every coordinate of the contract) is both IC and IR. Proof. An IC contract implementing action a∗ satisfies ua∗(t) ≥ ua(t), for all actions a ∈ A. In particular, this inequality holds in relation to the least-costly action, denoted by 1 ∈ A. Plugging the definition of the agent’s expected utility (eq. (2)), we obtain ua∗(t) ≥ Ej∼f1 [t] − c1. Thus, under the 14mapping t′ j = tj + c1, it holds that: ua∗(t′) = ua∗(t + c1) ≥ Ej∼f1 [t + c1] − c1 = Ej∼f1 [t] ≥ 0 and therefore the contract t′ = t + c1 is individually rational and still implements action a∗. B.1 Relation between budget-optimal and min-budget contracts Proof of Proposition 1. Given budget B >0, denote by tBO the budget-optimal contract, and denote the action it implements by n∗. By definition, tBO is a feasible solution to the min-budget contract design problem implementing action n∗. Denote by tMB the corresponding optimal solution to the min-budget problem implementing action n∗ (eq. (4)). tMB implements the same action n∗ by definition, and satisfies ∥tMB∥∞ ≤ ∥tBO∥∞ ≤ B due to the optimization objective. Hence, tMB is a budget-optimal contract for the given budget B, which is also min-budget. Iterative min-budget. To find the budget-optimal contract using iterative applications of eq. (4), we observe that any budget-limited contract t : Ω → [0, B] has bounded expected pay Efn[t] ≤ B for any distribution fn. Hence, actions n′ with cost cn′ > Bcannot be implemented using budget B, as the agent’s utility un′ = Efn′ [t] − cn′ < 0 will be smaller than the utility of the zero-cost action u0 = Efn[t] − 0 ≥ 0. Define the reduced action set: A′ = {n ∈ A |cn ≤ B ∧ n is implementable} A′ is finite when A is finite, or when the data cost is unbounded and the learning curve is monotone. To find n∗ within this space, go over all n ∈ A′, calculate the minimal budget Bn required for implementation, and take argminn∈A′ Bn. This is possible within a finite number of steps. B.2 The min-budget linear program and equivalent forms The min-budget contract (eq. (4)) implementing action i ∈ Ais given by the MIN-BUDGET linear program: min t∈R|Ω| ≥0 ,B∈R≥0 B s.t. ∀j ∈ Ω : tj ≤ B (BUDGET) ∀i′ ̸= i : X j∈Ω Fi′,jtj − ci′ ≤ X j∈Ω Fi,jtj − ci (IC) (6) The dual of the min-budget LP is given by: Claim 2. The dual linear program of Equation (6) is given by: max λ∈Rn−1 ≥0 ,µ∈R|Ω| ≥0 X i′̸=i (ci − ci′) λi′ s.t. ∀j ∈ Ω : X i′̸=i (Fi,j − Fi′,j) λi′ ≤ µj X j∈Ω µj ≤ 1 (7) Proof. We take the dual by translating the optimization problem into canonical form. The canonical form we target: min x≥0 cT x s.t. Cx ≤ d and its dual, as given by Lahaie [38], is: max y≥0 −dT y s.t. CT y ≥ −c To translate Equation (6), we note that in our case the components c,C,d are given by: 15cT = (0 . . .0 1 ) ∈ R|Ω|+1 C =   F1,0 − Fi,0 . . . F 1,|Ω| − Fi,|Ω| ... ... ... Fi′,j − Fi,j ... Fn,0 − Fi,0 Fn,m − Fi,m 0 ... 0 Im+1 −1 ... −1   ∈ R(n−1+|Ω|)×(|Ω|+1) dT = (c1 − ci . . . cn − ci 0 . . .0) ∈ Rn−1+|Ω| To simplify formulation, we denote the dual optimization variable y as follows: yT = (λ1 . . . λi−1 λi+1 . . . λn µ0 . . . µm) ∈ Rn−1+|Ω| Under this formulation, the dual’s objective is: −dT y = X i̸=i′ (ci − ci′) λi′ (8) The constraints given by the first m rows of CT are: ∀j ∈ Ω : X i′̸=i (Fi′,j − Fi,j) + µj ≥ 0 and equivalently: ∀j ∈ Ω : X i′̸=i (Fi,j − Fi′,j) ≤ µj (9) The constraint corresponding to the last row of CT is given by: X j∈Ω −µm ≥ −1 and equivalently: X j∈Ω µm ≤ 1 (10) Combining equations (8, 9, 10) yields the linear program given by Equation (7). To map between contracts and hypothesis tests, we introduce the following variable transformation: Definition 3 (Statistical representation of contracts) . For a given contract t : Ω → R≥0, denote B = maxj tj. The statistical representation of t is given by: tj = ϕjβ−1 where ϕj ∈ [0, 1] and β = B−1. Note that the transformation (t, B) 7→ (ϕ, β) is non-linear, and well-defined for all B >0. Under this variable transformation, the MIN-BUDGET transforms to an equivalent linear program: 16Lemma 2. For a feasible design problem, the min-budget contract design LP (Equation (6)) is equivalent to: max ϕ∈[0,1]|Ω|,β∈R≥0 β s.t. ∀i′ ̸= i : X j∈Ω Fi,j(1 − ϕj) + X j∈Ω Fi′,jϕj ≤ 1 − (ci − ci′)β (11) Proof. Given Equation (6), define ϕ ∈ [0, 1]|Ω|, β ≥ 0 according to definition 3: B = β−1 tj = ϕjB = ϕjβ−1 (12) Under the transformation defined by Equation (12), the (BUDGET) constraint in Equation (6) transforms as follows: tj ≤ B ⇔ ϕjβ−1 ≤ β−1 ⇔ ϕj ≤ 1 (13) and the (IC) constraint in Equation (6) transforms as: X j Fi,jtj − ci ≥ X j Fi′,jtj − ci′ ⇔ X j (Fi,j − Fi′,j) ϕjβ−1 ≥ ci − ci′ ⇔ X j Fi,jϕj − X j Fi′,jϕj ≥ (ci − ci′) β ⇔ 1 − X j Fi,j(1 − ϕj) − X j Fi′,jϕj ≥ (ci − ci′) β ⇔ X j Fi,j(1 − ϕj) + X j Fi′,jϕj ≤ 1 − (ci − ci′) β (14) where the first equivalence is by Equation (12), and the third equivalence is valid as P j Fi′,j = 1 for all i′. Finally, the objective of Equation (6) transforms as: min B ⇔ max β (15) Combining equations (13, 14, 15) yields the linear program in Equation (11). B.3 Relation to min-pay contract design Min-pay contract design aims to design a contract which minimizes the expected pay under the implemented action n∗: t∗ = argmint Ej∼fn∗ [tj] s.t. n(t) = n∗ (16) In contrast to the min-budget contract (eq. (4)), the∥t∥∞ objective measuring maximal pay is replaced with the Ej∼fn∗ [tj] objective measuring expected pay. Equation (16) is equivalent to the MIN-PAY linear program: min t∈R|Ω| ≥0 X j∈Ω Fi,jtj s.t. ∀i′ ̸= i : X j∈Ω Fi′,jtj − ci′ ≤ X j∈Ω Fi,jtj − ci (IC) (17) B.3.1 Characterization of implementability The implementatbility of min-pay contracts is characterized in Dütting et al. [19]. We cite the main result for completeness: Proposition 3 (Min-pay implementability; [ 19], A.2). An action i ∈ Ais implementable (up to tie-breaking) if and only if there is no convex combination of the other actions that results in the same distribution fi = P i′̸=i αi′fi′, but lower cost ci > P i′̸=i αi′ci′. 170.0 2.5 5.0 7.5 10.0 12.5 15.0 Outcome j 0.00 0.05 0.10 0.15 0.20 0.25 Outcome distributions f1 f2 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Outcome j 0 10 20Payment tj Min-pay and min-budget contracts MIN-PAY MIN-BUDGET Figure 5: Qualitative comparison of min-pay and min-budget contracts under MLRP. (Left) The contract design setting, representing two possible actions with binomial outcome distributions (p1 = 0.5, p2 = 0.8, m = 15). Action 1 has zero cost c1 = 0, and action 2 has unit cost c2 = 1. (Right) The resulting min-pay (red) and min-budget (purple) contracts, obtained by solving eq. (17) and eq. (6), respectively. The min-pay awards payment only for the highest outcome, in alignment with proposition 4; the min-budget is a threshold contract, in alignment with claim 11. We leverage to proposition 3 to characterize the implementability of min-budget contracts. This is possible due to the following connection: Claim 3 (Implementability equivalence). A contract t is feasible solution of MIN-BUDGET if and only if it is a feasible solution of MIN-PAY. Proof. A contract t which satisfies MIN-BUDGET (eq. (6)) satisfies the (IC) constraint, and thus also satisfies MIN-PAY (eq. (17)). Conversely, a contractt which satisfies MIN-PAY satisfies the(IC) constraint. Set B = maxj tj and obtain a feasible solution to MIN-BUDGET. B.3.2 Min-pay contracts under MLRP The optimal min-pay contracts under the MLRP assumption are characterized in Dütting et al. [19]: Proposition 4 (Optimal min-pay contract under MLRP; [19], Lemma 7). Consider a contract design setting for which MLRP holds. If the highest-cost action n is implementable, then there is a min-pay contract has a single nonzero-payment, which is rewarded for the highest outcomem. See fig. 5 for a qualitative comparison of min-pay and min-budget contracts. Min-pay contracts in delegated learning settings with MLRP. While simple, the min-pay contract given by proposition 4 is impractical in realistic delegated classification scenarios. In a delegated classification setting, the highest outcome corresponds to 100% validation set accuracy (i.e all validation set samples classified correctly). As m grows, the highest outcome becomes exponentially less likely, and thus the min-pay contract awards increasingly high payments with increasingly lower probabilities (See fig. 6). In such settings, even a slight degree of risk-averseness is likely to affect decisions: From the agent’s perspective, the probability of any receiving payment from a min-pay contract may become small to a degree where even a slight degree of agent risk-averseness will manifest itself. From the principal’s side, the exponential rate of growth in required budget B.4 Min-budget contracts with two actions In this section, we explore min-budget settings with two actions, and prove Theorem 2. When n = 2, assume without loss of generality that the contract implements action i = 2, and denote c = c2 − c1 > 0. We recall the definition of total variation distance: Definition 4 (Total variation distance). Given two distributions P, Q∈ ∆ (Ω), the total variation distance between P and Q is: ∥P − Q∥TV = 1 2 ∥P − Q∥1 The following equivalent definition is useful: 1810 20 30 40 50 Validation set size m 100 1.2 × 100 1.4 × 100 1.6 × 100 1.8 × 100 2 × 100 2.2 × 100 Ej∼f2 [tj] Expected pay MIN-PAY MIN-BUDGET c2 = 1 10 20 30 40 50 Validation set size m 100 101 102 103 104 B Required budget 10 20 30 40 50 Validation set size m 0.0 0.2 0.4 0.6 0.8 1.0 Pj∼f2 (tj > 0) Probability of any payment Figure 6: Comparison of min-pay and min-budget contracts under MLRP for varying validation set size m. The delegation setting is similar to the one depicted in Figure 5: two binomial-outcome actions (p1 = 0.5, p2 = 0.8) and varying m. Action 1 has zero cost c1 = 0, and action 2 has unit cost c2 = 1. In the left and center plots, the cost of action 2 is represented by an orange line (c2 = 1) representing the lower bound as in fig. 3 (Center). (Left) Expected pay Ej∼f2 [tj]. (Center) Required budget ∥t∥∞. (Right) Probability of getting any payment Pj∼f2 [tj > 0]. Claim 4. Let P, Q∈ ∆ (Ω). It holds: ∥P − Q∥TV = X j∈Ω (Qj − Pj)+ where x+ = max {x, 0}. Proof. By definition: ∥P − Q∥TV = 1 2 ∥P − Q∥1 Decompose the L1 norm: 1 2 ∥P − Q∥1 = 1 2 X j∈Ω \u0010 (Qj − Pj)+ + (Pj − Qj)+ \u0011 As P j∈Ω Pj = P j∈Ω Qj = 1, it holds that: X j∈Ω (Qj − Pj)+ = X j∈Ω (Pj − Qj)+ and therefore ∥P − Q∥TV = P j∈Ω (Qj − Pj)+ as required. B.4.1 Optimal two-action contract Theorem 5 (Two-action min-budget contract; formal statement of Theorem 1). When n = 2, the optimal min-budget contract t∗ is given by: t∗ j = c ∥F2 − F1∥TV 1 [F2,j ≥ F1,j] (18) Proof. We prove this claim using LP duality, by showing that the optimal primal objective corre- sponding to t∗ is identical to a feasible solution of the dual LP. The primal LP (Equation (6)) for two actions is given by: min t∈R|Ω| ≥0 ,B∈R≥0 B (19) s.t. ∀j ∈ Ω : tj ≤ B (BUDGET) X j∈Ω (F2,j − F1,j) tj ≥ c (IC) 19As t∗ is bounded, the optimal objective B∗ corresponds to the maximal possible payout of t∗: B∗ = max j∈Ω t∗ j = c ∥F2 − F1∥TV (20) and therefore the (BUDGET) constraint is satisfied. For the (IC) constraint, denote by Ω≥ the following set: Ω≥ = {j ∈ Ω | F2,j ≥ F1,2} using this notation, we note that t∗ j > 0 if and only if j ∈ Ω≥. Plugging into the constraint, we obtain: X j∈Ω (F2,j − F1,j) t∗ j = X j∈Ω≥ (F2,j − F1,j) B∗ = c P j∈Ω≥ (F2,j − F1,j) ∥F2 − F1∥TV = c Where ∥F2 − F1∥TV = P j∈Ω≥ (F2,j − F1,j) by definition. This shows that t∗ is a feasible solution for the primal LP. To prove optimality, we show that the the dual linear program attains an identical objective. By Claim 2, the dual LP for two actions is given by: max λ∈R≥0,µ∈R|Ω| ≥0 cλ s.t. ∀j ∈ Ω : (F2,j − F1,j) λ ≤ µj X j∈Ω µj ≤ 1 (21) Denote the vector ⃗ vj(λ) ∈ R|Ω| ≥0: ⃗ vj(λ) = λ (F2,j − F1,j)+ We note that outcomes j′ for which F2,j′ ≤ F1,j′ (formally j′ ∈ Ω \\ Ω≥) correspond to constraints which are satisfied for any λ ≥ 0. Otherwise j ∈ Ω≥, and in these cases λ can be increased until ⃗ v(λ) saturates the simplex constraint P j∈Ω µj = 1. The simplex constraint is binding for a value λ∗ > 0 satisfying: X j∈Ω λ∗ (F2,j − F1,j)+ = 1 and therefore the optimal value λ∗ of the dual LP (eq. (21)) is: cλ∗ = cP j∈Ω (F2,j − F1,j)+ = c ∥F2 − F1∥TV (22) Where the second equality is given by claim 4. The dual objective in eq. (22) is identical to the primal objective attained by the contract t∗ in Equation (20), and therefore t∗ is an optimal contract by strong LP duality. B.4.2 Contracts and hypothesis tests We recall the formal definition of the Neyman-Pearson lemma: Lemma 3 (Neyman-Pearson, [e.g., 45, 4.3]). Let P, Q∈ ∆ (X) be two probability measures over an arbitrary set X. Then for any hypothesis test ψ : X → {0, 1}, it holds: P(ψ(x) = 1) + Q(ψ(x) = 0) ≥ 1 − ∥P − Q∥TV . (23) Moreover, equality holds for the Likelihood Ratio testψ∗(x) = 1 [q(x) ≥ p(x)]. 20In our analysis, we will assume that the space X is finite. We also note that Lemma 3 is stated for decision rules with binary output, but its domain can be extended to fractional decision functions ψ : X →[0, 1] without loss of generality: As the sum of errors is linear in ψ, the optimal fractional decision rule is a solution to the linear program minψ∈[0,1]X (P(ψ = 1) + Q(ψ = 0)) where P(ψ = 1) = P x∈X pxψ(x) and Q(ψ = 0) = P x∈X qx(1 − ψ(x)). The feasible region of this linear program is the hypercube [0, 1]X and its vertices are the set of binary decision rules {0, 1}X , which also includes the optimal binary rule ψ∗ given by Lemma 3. As every feasible linear program attains its optimum on a vertex, the binary ψ∗ is also the optimal among fractional rules. Proof of Theorem 2. For a given two-action contract, denote Pj = F1,j, Qj = F2,j, and c = c2 −c1. We recall that the statistical min-budget LP for two actions is given by Lemma 2: max ϕ∈[0,1]|Ω|,β∈R≥0 β s.t. X j∈Ω Pjϕj + X j∈Ω Qj(1 − ϕj) ≤ 1 − cβ (24) and the Neyman-Pearson lemma is given by Equation (23). Given a min-budget contract design problem, apply Theorem 1 to obtain its optimal solution, as given by eq. (18): B∗ = c ∥P − Q∥TV t∗ j = c ∥P − Q∥TV 1 [Qj ≥ Pj] Applying the transformation from definition 3 on the optimal contract, we obtain equivalently: β∗ = (B∗)−1 = ∥P − Q∥TV c ϕ∗ j = t∗ j /B∗ = 1 [Qj ≥ Pj] Note that ϕ∗ j is a maximum-likelihood decision rule, similar to the optimal critical function in the Neyman-Pearson lemma. Additionally, by optimality of the contract-design optimization variable β∗, any feasible solution ϕ′ of Equation (24) satisfies: mX j=1 Pjϕ′ j + mX j=1 Qj(1 − ϕ′ j) ≥ 1 − cβ∗ = 1 − ∥P − Q∥TV with equality satisfied by the maximum likelihood rule ϕ∗. Therefore, the min-budget optimality of the contract t∗ implies the power optimality of the hypothesis test ϕ∗. Conversely, let ϕ : Ω → [0, 1] be an maximal-power hypothesis test. This provides a lower bound on the constraint in Equation (24): X j∈Ω Pjϕj + X j∈Ω Qj(1 − ϕj) ≥ 1 − ∥P − Q∥TV By the Neyman-Pearson lemma, the bound is tight for the maximum likelihood rule ϕ⋆ = 1 [Q ≥ P], and therefore the optimal objective β⋆ satisfies: 1 − cβ⋆ = 1 − ∥P − Q∥TV and hence β⋆ = ∥P−Q∥TV c . Applying the transformation in definition 3 yields the optimal contract t⋆ j = c ∥P−Q∥TV 1 [Qj ≥ Pj], showing that the corresponding contract is min-budget optimal if the corresponding hypothesis has optimal statistical power. Remark 1. Since the proof of Theorem 1 is independent of the Neyman-Pearson lemma, the argument proving the optimality of ϕ∗ in the proof above implies the Neyman-Pearson lemma for finiteX. 21B.5 More than two actions Definition 5 (All-or-nothing contract). Let B >0. An all-or-nothing contract t : Ω → R≥0 satisfies tj ∈ {0, B} for all j ∈ Ω. B.5.1 Binary outcomes In this section, we show that every binary-outcome min-budget contract is all-or-nothing. This is a corollary of a more general lemma: Lemma 4. For any feasible min-budget contractt∗, there exists j0 ∈ Ω such that t∗ j0 = 0. Proof. By contradiction, assume that t∗ j > 0 for all j, and denote j0 = argmin j t∗ j . Denote a = minj tj, and note that a >0. Define the contract ˜t as follows: ∀j : ˜tj = tj − a By definition, ˜tj ≥ 0 for all j, and ˜tj0 = 0. Since t∗ is a feasible min-budget contract, it satisfies the min-budget LP in eq. (6), and in particular the (IC) constraint: ∀i ∈ [n − 1] : X j Fi,jt∗ j − ci ≤ X j Fn,jt∗ j − cn Plugging in the definition ˜t = t∗ − a and using the fact that P j Fi,j = 1 for all i ∈ [n], we obtain: ∀i ∈ [n − 1] : X j Fi,j˜tj − ci ≤ X j Fn,j˜tj − cn and therefore ˜t satisfies the (IC) constraint as well. This is a contradiction, since ˜t is a feasible contract with a lower required budget. From this we conclude that the optimal contract must satisfy tj = 0 for some j. Corollary 1. In any min-budget contract design setting with two outcomes, the optimal contract is all-or-nothing. B.5.2 Hardness: Basic definitions and construction In this section, we show that finding optimal all-or-nothing contracts is NP-hard in the general case. Definition 6 (Maximin contract design matrix). For a given contract design problem targeting action n and satisfying ci < cn for all i ∈ [n − 1], the maximin design matrix A is defined as: Aij = Fn,j − Fi,j cn − ci (25) Claim 5. An optimal all-or-nothing contract is a solution of the following optimization problem: max ϕ∈{0,1}|Ω| min i∈[n−1] (Aϕ)i (26) Where A is the corresponding maximin design matrix, as given by Definition 6. Proof. By Lemma 2, Equation (11), the min-budget contract design LP is equivalent to: max ϕ∈[0,1]|Ω|,β∈R≥0 β s.t. ∀i < n: X j∈Ω Fn,j − Fi,j cn − ci ϕj ≥ β As Aij = Fn,j−Fi,j cn−ci , it holds that P j∈Ω Fn,j−Fi,j cn−ci ϕj = Aϕ. The LP above is therefore equivalent to: 22max ϕ∈[0,1]|Ω|,β∈R≥0 β s.t. ∀i < n: Aiϕ ≥ β as β is a lower bound for every constraint, its optimal value is the maximal minimum attainable through variation of ϕ: max ϕ∈[0,1]|Ω| min i∈[n−1] (Aϕ)i and restricting the optimization space of ϕ to {0, 1}|Ω| yields the desired result. Definition 7 (3-CNF – Conjunctive normal form). A 3-CNF formula over m variables and n clauses is a boolean-valued function ψ : {0, 1}m → {0, 1} of the form: ψ(x1, . . . , xm) = n^ i=1 (zi1 ∨ zi2 ∨ zi3) where zik ∈ {x1, . . . , xm, ¬x1, . . . ,¬xm}. Definition 8 (Number of positives in clause i). Given a 3-CNF ψ and an assignment x ∈ {0, 1}m, denote by σi(ψ, x) the number of variables zik in clause i which evaluate to 1 under assignment x. Claim 6. A formula ψ is satisfiable if and and only if there exists an assignment x ∈ {0, 1}m such that mini∈[n] σi(ψ, x) ≥ 1. Proof. If ψ is satisfiable then there exists x ∈ {0, 1}m such that ψ(x) = 1 . Since ψ is a 3-CNF, every clause i must evaluate to 1, and therefore σi(ψ, x) ≥ 1 for all i ∈ [n]. Conversely, if there exist an assignment x such that mini∈[n] σi(ψ, x) ≥ 1 then by definition x satisfies every clause, and therefore the whole formula ψ. B.5.3 Hardness: Reduction from 3SAT Given a 3-CNF ψ with n clauses and m variables, we define a min-budget contract design problem over n + 1 actions and m + 3 variables: The target action is i = n + 1. The cost associated with action i is: ci = \u001a1 i = n + 1 0 otherwise (27) Contract outcomes are denoted Ω = {1, . . . , m,pos, neg, const}. For simplicity of notations, let ε > 0, which will be assigned a suitable value later in the proof. The distribution of the target outcome is denoted by Q and defined as: ∀j ∈ [m] : Qj = ε m Qpos = 1 − ε \u0012 1 + 3 m \u0013 Qneg = 0 Qconst = 3ε m (28) 23For each i ∈ [n], denote the number of negated variables in clause i by ki ∈ {0, . . . ,3}. The distribution corresponding to the i-th action is denoted by P(i) and defined as: ∀j ∈ [m] : P(i) j =    0 xj exists in clause i and is not negated 2ε m xj exists in clause i and is negated ε m otherwise P(i) pos = 0 P(i) neg = 1 − ε \u0012 1 + 9 − 4ki m \u0013 P(i) const = (3 − ki) ε m (29) These distributions are well-defined whenever ε <1 4 and m ≥ 3. For concreteness, set: ε = 1 10 (30) Using Definition 6 and combining equations (27, 28, 29), the maximin design matrix Aψ ∈ R(n+1)×(m+3) corresponding to the contract design problem above is given by: ∀j ∈ [m] : Aψ i,j =    ε m xj exists in clause i and is not negated − ε m xj exists in clause i and is negated 0 otherwise Aψ i,pos = Qpos Aψ i,neg = −P(i) neg Aψ i,const = ki ε m (31) Definition 9. Given an assignment x ∈ {0, 1}m, the corresponding contract vector ϕx ∈ {0, 1}m+3 is defined as: ∀j ∈ [m] : ϕx j = xj ϕx pos = 1 ϕx neg = 0 ϕx const = 1 (32) Claim 7. Let ψ be a 3-CNF , andx ∈ {0, 1}m an assignment. It holds that: \u0000 Aψϕx\u0001 i = ε mσi(ψ, x) + Qpos Proof. To prove this claim, plug ϕx from Definition 9 into Aψ defined in Equation (31). Here we denote A = Aψ, ϕ = ϕx for simplicity. We obtain: (Aϕ)i = X j∈[m] Ai,jϕj | {z }P xj in clause ϕj−P ¬xj′ in clause ϕj′ + Ai,pos| {z } =Qpos ϕpos|{z} =1 +Ai,neg ϕneg|{z} =0 + Ai,const| {z } =ki ε m ϕconst| {z } =1 = ε m   X xj in clause i ϕj + X ¬xj′ in clause i (1 − ϕj′)   + Qpos = ε mσi(ψ, x) + Qpos Claim 8. A 3-CNF ψ is satisfiable if and only if the corresponding optimization problem in Equa- tion (26) attains a value of at least Qpos + ε m . 24Proof. If ψ is satisfiable by assignment x ∈ {0, 1}m, then σi(ψ, x) ≥ 1 for all i ∈ [n] by Claim 6. Let ϕx denote the vector corresponding to the satisfying assignment, according to Definition 9. Apply Claim 7 to obtain: \u0000 Aψϕx\u0001 i = ε mσi(ψ, x) + Qpos ≥ Qpos + ε m Conversely, assume the optimal solution to Equation (26) is at least Qpos + ε m , and denote the vector attaining the optimal solution by ϕopt. As the corresponding matrix Aψ, defined in Equation (31), satisfies the following for all i ∈ [n]: Ai,pos = Qpos ≥ 0 Ai,neg = −P(i) neg ≤ 0 Ai,const = ki ε m ≥ 0 it can be assumed that the optimal solution ψopt satisfies: ϕopt pos = 1 ϕopt neg = 0 ϕopt const = 1 because otherwise the value of any \u0000 Aψϕ \u0001 i would increase by setting these entries in ϕ to the values above. Hence, we can denote ϕopt = ϕx according to Definition 9, and apply Claim 7 to obtain: \u0000 Aψϕx\u0001 i = ε mσi(ψ, x) + Qpos as the value attained is at least ε m + Qpos, it must hold that σi(ψ, x) ≥ 1 for all i ∈ [n], and therefore x satisfies ψ according to Claim 6. B.5.4 Proof of hardness We are now ready to prove Theorem 3: Proof of Theorem 3. By reduction from 3SAT. Given a 3-CNF ψ with n clauses and m variables, construct in polynomial time the corresponding matrix Aψ as defined by eq. (31), and apply an all-or-nothing min-budget contract solver according to eq. (26) to obtain the optimal solution. The validity of the LP is given by eq. (26). According to Claim 8, the formula ψ is satisfiable if and only if the value attained in the optimization is at least ε m + Qpos, where m is the number of variables in x, and ε, Qpos are constants defined in eq. (30), eq. (28) respectively. B.6 The single binding action algorithm To prove the soundness of SBA, we first make the following definition: Definition 10 ((i′-IC) relaxation). Consider a MIN-BUDGET LP with target action i ∈ A, as given by eq. (6). For any action i′ ̸= i, the ( i′-IC) relaxation of the min-budget problem is given by eliminating all (IC) constraints except for the one corresponding to action i′. Proof of Proposition 2. Denote the target action by i ∈ A. On each iteration of the loop, the algorithm considers some action i′ ̸= i, and applies Theorem 1 on the ( i′-IC) relaxation of the original MIN-BUDGET LP. Since the relaxed LP has only one (IC) constraint, its optimal solution, denoted by t∗(i′, i), is given by eq. (5). By Definition 10, the feasible region of the original LP lies within feasible region corresponding to its (i′-IC) relaxation. Thus, if an optimal solution of the relaxed LP lies within the feasible region of the original LP, then it is also a global optimum of the original LP.t∗(i′, i) lies within the feasible region of the original LP is it satisfies the remaining (IC) constraints—a condition equivalent to the notation a(t∗(i′, i)) = i by eq. (2). The SBA algorithm terminates successfully only in such cases, and therefore it is sound. 25A note on ties in SBA. Denote the target action by i. To simplify presentation, the algorithm presentation in Section 3.4 implicitly assumes that required budgets∥t∗(i′, i)∥∞ are distinct for every pair of actions (i′, i), and therefore the return value of SBA is well-defined. In case of ties, the return value is not well-defined (as the iteration order is not well-defined), but small a modification to the algorithm allows ties to be broken explicitly without affecting other properties of the algorithm: In case of ties in optimal required budget, the soundness of the algorithm and the all-of-nothing property of the returned contracts is not affected. However, the exact functional form of the returned contract may be affected by the iteration order. In case such issue becomes relevant, the SBA algorithm can be modified to first collect a set of optimal contracts (instead of immediately returning when an optimal feasible contract is found), and then select one of the optimal contracts based on the desired criteria. As the proof of Proposition 2 does not depend on iteration order, soundness will not be affected, and worst-case time complexity will remain the same. B.7 MLRP In this section, we explore contract design under the MLRP assumption, and prove Theorem 4. We first recall the statistical notion of monotone likelihood ratio: Definition 11 (Mononote Likelihood Ratio – MLR). The distributions P, Q∈ ∆ (Ω)have Monotone Likelihood Ratio when Qj Pj is monotonically increasing for all j ∈ Ω = {0, . . . , m}. We denote this by P ≺ Q. An introduction to MLR and its relation to statistical hypothesis testing is provided in Lehmann et al. [40, Section 3.4]. Using this notation, we can reformulate Definition 1: Definition 12 (MLRP; reformulation of Def. 1 using MLR notation) . A principal-agent problem satisfies the Monotone Likelihood Ratio Property if Fi′ ≺ Fi for every pair of actions i, i′ such that ci′ < ci. Claim 9. Let P, Q∈ ∆ ([m]) such that P ≺ Q. Then P0 > Q0 and Pm < Qm. Proof. For the first inequality, assume by contradiction that Q0 ≥ P0. Combining with the MLR property, this assumption implies that Qj ≥ Pj for all j ∈ {0, . . . , m}. As the outcome distributions P, Q are distinct, there exists at least onej for which Qj > Pj. Summing over j yields: P j∈Ω Qj >P j∈Ω Pj, which is a contradiction, as the inequality is strict while both sides sum to 1. The proof for the second inequality follows in the same way. Definition 13 (MLR crossing point j∗ P,Q). Let P, Q∈ ∆ (Ω)such that P ≺ Q. The crossing point of Q over P is the outcome j∗ ∈ {0, . . . , m} such that Qj∗−1 < Pj∗−1 and Qj∗ ≥ Pj∗. When distribution are not clear from context, we denote j∗ = j∗ P,Q In words, the crossing point j∗ is the outcome such that for every lower outcome, P is strictly more likely than Q, and starting with this outcome Q is weakly more likely. By claim 9, the MLR crossing point j∗ is uniquely defined for any P ≺ Q, and it holds that j∗ > 0. Definition 14 (Survival function). Given P ∈ ∆ ([m]), the survival function SP (·) : Ω → [0, 1] is defined as: SP (j) = Pj′∼P [j′ > j] = mX j′=j+1 Pi Informally, the survival function is 1 minus the distribution’s CDF. This function is useful in our context, since the total variation distance between two distributions (as appearing in theorem 5) can be represented as the difference between their survival functions when they satisfy MLR. The intuitive reason for this is that MLR means the distributions are “single-crossing”. Claim 10 (Total variation under MLR). Let P, Q∈ ∆ (Ω)such that P ≺ Q. It holds: ∥P − Q∥TV = SQ(j∗ P,Q − 1) − SP (j∗ P,Q − 1) 26Proof. By Claim 4, it holds that: ∥P − Q∥TV = X j∈{0,...,m} (Qj − Pj)+ Using the j∗ notation (see definition 13), we obtain: X j∈{0,...,m} (Qj − Pj)+ = mX j=j∗ P,Q Qj − Pj And the result is obtained by applying the definition of survival function (see definition 14). B.7.1 Two-action contracts with MLRP Combining Theorem 5 with Claim 9 leads to a characterization of threshold contracts in the case of two actions: Claim 11. For any two-action contract design problem satisfying MLRP , the optimal contract is a threshold contract: t∗ j = c ∥F2 − F1∥TV 1 \u0002 j ≥ j∗ F1,F2 \u0003 where j∗ F1,F2 = min {j ∈ {0, . . . , m} |F2,j ≥ F1,j} as in Definition 13. Proof. Combining the result of Claim 9 with the monontonicity assumption, the likelihood ratio F2,j/F1,j crosses 1 exactly once. Denote the crossing point by j∗, and apply Theorem 5 to obtain the optimal contract. B.7.2 Two-outcome contracts with MLRP When there are more than two actions, assume without loss of generality that the contract aims to implement the last action n. The following claim establishes the existence of theshold contracts in the two-outcome setting |Ω| = 2. In contrast to corollary 1, the proof in constructive, and yields a concrete contract: Claim 12. For any contract design problem satisfying MLRP with n > 2 actions and m = 2 outcomes, the optimal min-budget contract is a threshold contract. Proof. By Claim 2, the dual LP for two outcomes is given by: max λ∈Rn−1 ≥0 ,µ∈R2 ≥0 n−1X i=1 (cn − ci)λi s.t. ∀j ∈ {1, 2} : n−1X i=1 (Fn,j − Fi,j) λi ≤ µj X j∈{1,2} µj ≤ 1 (33) From Claim 9, we obtain that Fn,1 − Fi,1 < 0 and Fn,2 − Fi,2 > 0 for all i ∈ [n − 1], and therefore the first constraint in Equation (33) is always satisfied for j = 1. Simplifying Equation (33) we obtain: max λ∈Rn−1 ≥0 ,µ∈R≥0 n−1X i=1 (cn − ci)λi s.t. n−1X i=1 (Fn,2 − Fi,2) λi ≤ 1 (34) 27which is maximized by allocating all budget to the λi maximizing the “bang for buck”. The dual objective is therefore given by: B∗ = max λ∈Rn−1 ≥0 ,µ∈R≥0 X i∈[n−1] (cn − ci)λi = max i∈[n−1] cn − ci Fn,2 − Fi,2 (35) To see that a threshold contract is optimal, let t∗ j = B∗ · 1 [j = 1]. Primal objective is B∗, and the contract is feasible if the primal LP is feasible. The (IC) constraint in Equation (6) can be written as: ∀i ∈ [n − 1] : P2 j=1 (Fn,j − Fi,j) tj cn − ci ≥ 1 and indeed for every action i ∈ [n − 1]: P2 j=1 (Fn,j − Fi,j) tj cn − ci = Fn,2 − Fi,2 cn − ci B∗ = Fn,2 − Fi,2 cn − ci max i∈[n−1] cn − ci Fn,2 − Fi,2 ≥ 1 And therefore t∗ j is feasible in the primal problem, and also optimal by strong LP duality. Also note that the resulting contract coincides exactly with the optimal min-pay contract as attained by Dütting et al. [19, Lemma 7]. B.7.3 General contracts with MLRP In this section, we explore min-budget contracts in MLRP settings with more than two actions and more than two outcomes. We start with a negative example, showing that the MLRP assumption is not sufficient for establishing the optimality of threshold contracts: Claim 13. For|Ω| > 2, there exists a design problem satisfying MLRP for which the optimal contract is not a threshold. Proof. Consider the following contract design problem: F1 ∼ Binomial(10, 0.5) F2 ∼ Binomial(10, 0.65) F3 ∼ Binomial(10, 0.8) c1 = 0 c2 = 0.45 c3 = 1 The distributions are binomial, and therefore the contract satisfies MLRP. Numerically solving Equation (11) yields the following fractional contract: t∗ LP = (0, 0, 0, 0, 0, 0, 0, 1.1, 1.46, 1.46, 1.46) Numerically solving Equation (11) while imposing integer constraints ϕj ∈ {0, 1} yields the follow- ing contract, which has higher max payout: t∗ IP = (0, 0, 0, 0, 0, 0, 0, 1.51, 1.51, 1.51, 1.51) Thus for this case any threshold contract is min-budget suboptimal. A graphical illustration of the proof is provided in Figure 7. Remark 2. The proof of Claim 13 is provided with 10 outcomes (|Ω| = 10) for ease of graphical interpretation, however a minimal counterexample may also be constructed using only3 outcomes. For example, consider the following design problem: F1 = (0.5, 0.3, 0.2) F2 = (0.3, 0.4, 0.3) F3 = (0.1, 0.35, 0.55) c1 = 0 c2 = 0.45 c3 = 1 The proof for this case follows in the same way, where numerical computation yields the contracts: t∗ LP = (0, 1.9, 2.6) t∗ IP = (0, 2.7, 2.7) 280.0 2.5 5.0 7.5 10.0 Outcome j 0.0 0.1 0.2 0.3 Probability mass Outcome distributions Fi F1 F2 F3 j∗ 0.0 2.5 5.0 7.5 10.0 Outcome j 0.0 0.5 1.0 1.5 Payout tj Min-budget - LP vs. integer solutions LP MIP 0.00 0.25 0.50 0.75 1.00 Cost ci 0.2 0.4 0.6 Crossing-point survival si Non-concavity of crossing-point survival Figure 7: Graphical illustration of Claim 13. (Left) Outcome distributions F1, F2, F3 and MLR crossing point j∗ F2,F3 . (Center) The min-budget contract t∗ j , given by numerically solving eq. (11) (purple), and numerically solving eq. (11) while imposing integer constraints ϕj ∈ {0, 1} (cyan). Note that the fractional LP solution achieves lower max payout. (Right) Crossing-point survival si = SFi(j∗ Fi,Fn − 1) as a function of costci (see definition 15). Note that the function is not concave, thus the sufficient condition given in Theorem 4 does not hold in this case. Remark 3. We also note that a counterexample with2 outcomes is not possible due to Claim 12. The negative example shows that even with MLRP, the optimal contract incentivizing the highest implementable action needs to rule out deviations of the agent to all other actions, and not just to the second-highest one. This makes the contract complex. We identify a natural economic condition that is sufficient for considering only a single deviation (to the second-highest action), resulting in a simple contract. Note that considering a single such deviation is equivalent to restricting the action space to actions {n − 1, n}. B.8 Sufficiency condition for more than two actions For the following definition, recall the definition of MLR crossing point j∗ (Def. 13), and survival function S (Def. 14): Definition 15 (Concave-MLRP; Formal statement of Definition 2). For a contract design setting sat- sifying MLRP , denote byj∗ = j∗ Fn,Fn−1 the crossing point of the outcome distribution corresponding to the highest action Fn, and the outcome distribution corresponding to the second-highest action Fn−1. For any action i ∈ [n], the crossing-point survival si is defined as: si = SFi(j∗ − 1) In words, si is the probability to get an outcome at or above crossing-point j∗ according to outcome distribution Fi. From an economic perspective, for every action i ∈ [n], si is the agent’s probability to receive any nonzero payment from choosing action i, if the contract is designed by restricting the action-space to actions {n − 1, n}. Note however that the definition of si only depends on the outcome distribution Fij, and does not require solving the contract design problem. B.8.1 Binomial power-law curves satisfy Concave-MLRP Recent theoretical work on learning curves has focused mainly on power-law expected learning curves of the form an = 1 − βn−γ [11, 33]. In addition, these curves were also found to provide a good fit for certain practical applications [49, POW2]. In this section, we show that a stochastic generalization of these curves satisfies the Concave-MLRP property: Definition 16 (Power-law stochastic learning curve). Let β, γ∈ R≥0, and m ∈ N. A delegated learning setting with actions A has a realizable power-law stochastic learning curve with parameters β, γif Fi = Binomial \u0000 1 − βn−γ i , m \u0001 for all ni ∈ A. For the proof, we also recall the definition of the regularized incomplete beta function: Definition 17 (Regularized incomplete beta function). For k1, k2 ≥ 1, the regularized incomplete beta function is defined as: Ip(k1, k2) = Rp 0 tk1−1(1 − t)k2−1dt R1 0 tk1−1(1 − t)k2−1dt 290 20000 40000 60000 Action cost cn = n 0.4 0.6 0.8 1.0 Expected accuracy - MLP LCDB data Power-law ﬁt 0 20000 40000 60000 Action cost cn = n 0.0 0.2 0.4 Survival sn Survival at likelihood transition ˜n n0 0 20000 40000 60000 Action cost cn = n 0 1 2d2sn/dn2 ×10−8 Second derivative of sn ˜n n0 Figure 8: Graphical intuition for the sufficiency condition in Claim 14. (Left) Expected accuracy curve for MNIST-784 MLP. Blue dots represent empirical data from the LCDB dataset, cyan curve represents power-law fit (an = 1 − βn−γ, with ˆβ = 1.89, ˆγ = 0.33). (Center) Crossing point survival si as a function of cost ci (see definition 15), for m = 30. Cyan dot represents inflection point ˜n ≈ 2022. Red vertical line represents the inflection point bound n0 ≈ 3957 suggested by the proof. It can be observed that the curve is concave for all n >˜n. (Right) Second derivative of the survival function sn, illustrating the position of the bound n0 in relation to the inflection point ˜n. We also recall that Ip is related to the survival function of the binomial distribution [e.g., 1, 6.6.4]: Ip(k + 1, n− k) = Px∼Binomial(p,n)[x > k] Claim 14. Let β, γ∈ R≥0, m ∈ N. A delegated learning setting with a power-law stochastic learning curve Fi = Binomial(1 − βn−γ i , m), action costs ci = ni, and mini ni ≥ \u0000 β(m+γ−1)/1+γ−1 \u00011 γ satisfies the Concave-MLRP assumption. Proof. Denote the expected accuracy of each action by an = 1 − βn−γ. The survival function of the binomial distribution is given by: sn = Ian (j∗, m+ 1 − j∗) Where Ian is the regularized incomplete beta function (definition 17). With slight abuse of notation, we treat an and sn as continuous functions of n. Taking the derivative by n and ignoring the constant denominator, we obtain: dsn dn ∝ aj∗−1 n (1 − an)m−j∗ dan dn Plugging the functional form of an: dsn dn ∝ \u0000 1 − βn−γ\u0001j∗−1 \u0000 βn−γ\u0001m−j∗ βγn−(γ+1) ∝ \u0000 βn−γ\u0001m−j∗+1+γ−1 \u0000 1 − βn−γ\u0001j∗−1 As a function of βn−γ, the function dsn dn attains its extremum at: β˜n−γ = m + 1 − j∗ + γ−1 m + γ−1 ˜n = \u0012 β(m + γ−1) m + 1 − j∗ + γ−1 \u00131 γ And therefore the function sn has an inflection point at ˜n (see fig. 8). From the upper bound j∗ ≤ m we obtain: ˜n ≤ \u0012β(m + γ−1) 1 + γ−1 \u00131 γ = n0 And as mini ni ≥ n0, the function sn is convex as a function of cn for all n ∈ A. The proof is illustrated in Figure 8. 30B.8.2 Proof of sufficiency theorem Theorem 6 (Concave-MLRP implies threshold contracts; formal statement of Theorem 4). For a contract design problem satisfying MLRP , considersi as a function of cost ci. If this function is concave, then the optimal contract is a threshold contract. Furthermore, the contract is successfully recovered by the SBA algorithm. Proof. To prove this claim, we construct a relaxed version of the min-budget LP (eq. (6)), and apply Theorem 5 in order to solve it. We then show that this solution is also feasible for the original LP. Given the min-budget LP, construct a relaxed LP by eliminating (IC) constraints for alli ≤ n − 2: min t∈R|Ω| ≥0 ,B∈R≥0 B s.t. ∀j ∈ Ω : tj ≤ B X j∈Ω Fn−1,jtj − cn−1 ≤ X j∈Ω Fn,jtj − cn (36) Equation (36) only depends on Fn and Fn−1, and therefore claim 11 can be applied to obtain the optimal min-budget contract: t∗ j = cn − cn−1 ∥Fn − Fn−1∥TV 1 [Fn,j ≥ Fn−1,j] (37) Let j∗ = min {j ∈ {0, . . . , m} |Fn,j ≥ Fn−1,j} as in definition 13, and denote si = SFi(j∗ − 1). By claim 10, we obtain: ∥Fn − Fn−1∥TV = sn − sn−1 For all i < n, denote: bi = cn − ci sn − si Using the definition of bi, eq. (37) can be rewritten as: t∗ j = bn−11 [j ≥ j∗] By assumption, si is a concave function of ci, and therefore the function bi is monotonically non- decreasing for all i < n: bn−1 ≥ bi Dividing both sides by bi, we obtain: bn−1 bi ≥ 1 Plugging in the definition of bi, and multiplying both sides by (cn − ci): bn−1 (sn − si) ≥ cn − ci (38) By definition of t∗ j , the expected payout of contract t∗ j under action i′ ∈ [n] is given by: mX j=0 t∗ j Fi′,j = bn−1 mX j=j∗ Fi′,j = bn−1si′ (39) Plugging eq. (38) into eq. (39) yields: mX j=0 tjFn,j − mX j=0 tjFi,j ≥ cn − ci (40) As eq. (40) is identical to the (IC) constraint in eq. (6), we obtain that t∗ j satisfies all the (IC) constraints in the original LP. From this we conclude that t∗ j , which is a threshold contract, is also an optimal contract with the respect to the full min-budget LP. For the second part of the claim, note that the SBA algorithm also attempts to solve eq. (36) on the iteration corresponding to action N − 1. As the solution of eq. (36) is guaranteed to be feasible for the original LP under Concave-MLRP, the SBA successfully recovers it. 310 5 10 15 Outcome j 0.00 0.05 0.10 0.15 0.20 0.25 Probability Fi,j Outcome distributions and crossing point j∗ F1 F2 F3 F4 F5 j∗ F4,F5 0 2500 5000 7500 10000 Cost ci 0.0 0.2 0.4 0.6 si = SFi(j∗ − 1) Crossing-point survival si 0 5 10 15 Outcome j 0 10000 20000 30000 40000Payment tj Optimal min-budget contract Figure 9: Graphical intuition for the sufficiency condition in Theorem 4 (restated in Theorem 6). (Left) Outcome distribution in a contract design setting satisfying MLRP, generated by a series of binomial distributions with power-law expectationaccD(hn) = 0.9−0.4 \u0000 n 100 \u0001−0.3 . The gray dotted line represents the MLR crossing point j∗ F4,F5 (see definition 13). (Center) Crossing point survival si as a function of cost ci (see definition 15). It can be observed that the curve is concave. (Right) The min-budget contract implementing action 5. It is a threshold contract, as guaranteed by theorem 4. Compare to fig. 7, where the sufficiency condition does not hold. B.9 Min-budget contracts with guaranteed minimum payout Our problem setting assumes that the agent selects its action rationally, as described in eq. (2). However, in some practical settings the agent may be risk averse to some extent, and require guaranteed minimum payment from the contract. In this section, we show that the rationality assumption is made without loss of generality in such cases: Lemma 5, which we prove below, shows that any optimal contract with guaranteed minimum payout can be represented as the sum of a min-budget contract without guaranteed payout, and a constant representing the payout guarantee. Definition 18 (Guaranteed minimum payout). Let δ ≥ 0. A contract t ∈ R|Ω| ≥0 has guaranteed payout of size δ if tj ≥ δ for all j ∈ Ω. Lemma 5. A contract t is a min-budget contract with guaranteed payout δ requiring budget B if any only if there exist a min-budget contract t′ (without guaranteed payout) requiring budget B′ such that t = t′ + δ and B = B′ + δ. Proof. When agents require a guaranteed minimum payout δ ≥ 0, we add a constraint to the MIN-BUDGET linear program defined in eq. (6), such that an optimal min-budget contract t with guaranteed payout δ is an optimal solution to the following linear program: min t∈R|Ω| ≥0 ,B∈R≥0 B s.t. ∀j ∈ Ω : tj ≤ B (BUDGET) ∀i′ ̸= i : X j∈Ω Fi′,jtj − ci′ ≤ X j∈Ω Fi,jtj − ci (IC) ∀j ∈ Ω : tj ≥ δ (MINW AGE) (41) To prove the first direction of the equivalence, assume thatt is a min-budget contract with guaranteed payout δ and budget B, and thus an optimal solution of eq. (41). Define the variable transformation: t′ = t − δ B′ = B − t0 (42) By eq. (42), the (BUDGET) constraint in eq. (41) transforms into: ∀j ∈ Ω :tj ≤ B ⇔ t′ j + δ ≤ B′ + δ ⇔ t′ j ≤ B′ (43) 32The (IC) constraint transforms into: ∀i′ ̸= i : X j∈Ω Fi′,jtj − ci′ ≤ X j∈Ω Fi,jtj − ci ⇔ X j∈Ω Fi′,j(t′ j + δ) − ci′ ≤ X j∈Ω Fi,j(t′ j + δ) − ci ⇔ X j∈Ω Fi′,jt′ j − ci′ + δ X j∈Ω Fi′,j | {z } =1 ≤ X j∈Ω Fi,jt′ j − ci + δ X j∈Ω Fi,j | {z } =1 ⇔ X j∈Ω Fi′,jt′ j − ci′ ≤ X j∈Ω Fi,jt′ j − ci (44) And the (MINW AGE) constraint transforms into: ∀j ∈ Ω :tj ≥ δ ⇔ t′ j ≥ 0 (45) Plugging back the transformed constraints (44, 43, 45) into eq. (41), we obtain that (t′, B′) is an optimal solution of the original MIN-BUDGET linear program eq. (6), and therefore t′ is an optimal min-budget contract without minimum guaranteed payout. Conversely, assume that t′ is an optimal min-budget contract (without minimum guaranteed payout), satisfying the MIN-BUDGET linear program in eq. (17) with budget B′. Apply the inverse transfor- mation t = t′ + δ and B = B′ + δ, and the equivalence relations (44, 43, 45) in the inverse direction to obtain that t, B is an optimal solution to eq. (41). C Experimental details C.1 Data LCDB. We base our main experimental environment on the LCDB dataset [43], which includes a large collection of stochastic learning curves for multiple learning algorithms and classifica- tion benchmark datasets. For each learning method and benchmark dataset, the database in- cludes held-out accuracy measurements, obtained for exponentially-increasing training set sizes n ∈ \b 24 = 16, . . . ,round(2k/2), . . . ,215 = 32, 768 \t , with multiple repetitions per n obtained by cross-validation. For all learning curves we consider in our analysis, and for each n, the number of repetitions provided in the dataset is in the range R ∈ {5, . . . ,125}, where the specific number of repetitions in LCDB depends on the algorithm and benchmark dataset (see the dataset documentation for additional details). For each trained classifier, each accuracy point on the learning curve is estimated using 5,000 held-out samples. Formally, and using our notation, for each learning algorithm Alg (e.g. MLP), dataset D (e.g. MNIST), and training set size n, LCDB provides a set of accuracy estimates \b a1,Alg,D n , . . . , aR,Alg,D n \t , such that each an is distributed according to an ∼ accD(hn), where hn is a classifier trained using training set S ∼ Dn and learning algorithm Alg (see section 2 for definition of accD(hn)). Benchmark dataset and algorithms. In our main analysis, we focus on the popular MNIST dataset [39, OpenML 554], and on multilayer perceptrons (MLP) and gradient-boosted decision trees (GBDT) as representative classifiers. For all classifiers, results are obtained for the respective default scikit-learn [44] implementations (e.g. MLPClassifier and GradientBoostingClassifier for MLP and GBDT, respectively). Delegated learning settings from empirical data. For a given validation set sizem where m = |V |, we instantiate a contract design task (A, c,Ω, F) as follows: (i) the set of actions with the set of training set sizes provided by LCDB ( A = \b 24, 24.5, . . . ,215\t ); (ii) action costs are set to fixed per-unit cost, i.e., cn = n; and (iii) the distribution F over outcomes Ω is associated with a binomial mixture distribtuion, resulting from applying bootstrap sampling to empirical error measurements: fn = 1 R RX r=1 Binomial(m, ar,Alg,D n ) 33where an are the accuracy estimates defined above. Figure 2 (Left) shows an example of such a setting, obtained by applying the above procedure to data describing learning curves for the MLP algorithm trained on the MNIST benchmark dataset. C.2 Implementation details Code. We implement our code in Python. Our code relies on Pyomo [13, 27] and GLPK for solving linear and mixed-integer programs. Code is available at: https://github.com/edensaig/delegated-classification. Hardware. All experiments were run on a single laptop, with 16GB of RAM, M1 Pro processor, and with no GPU support. Runtime. A single run consisting the entire pipeline takes roughly 5 minutes. The main bottleneck is running the LP solvers, taking roughly 70% of runtime to compute. C.2.1 Contract design solvers To find the optimal contracts, we implement and compare several different solvers: • LP solver: To find min-budget contracts given outcome distributions {fn} and costs {cn}, we solve the MIN-BUDGET LP (Eq. (6)) directly using Pyomo and GLPK. Given budget B, budget- optimal contracts are obtained by iterating through the actions set A, invoking the min-budget solver on each target action n ∈ A, enforcing incentive compatibility against all actions n′ which satisfy αn′ < αn, and taking the action yielding the maximal expected accuracy within budget (see appendix B.1). In our code, the LP solver is implemented within the MinBudgetContract class. Typical running time for a single problem instance: 10−1s. • SBA solver: Implementation of the single binding action algorithm presented in Section 3.4. The local solver is implemented within the MinBudgetSingleBindingConstraintContract class. Typical running time for a single problem instance: 10−4s. • Hybrid solver: A meta-solver implementing the ‘try SBA first’ computational approach. The solver starts by invoking the SBA solver, and applies the LP solver if the former fails. In our code, the LP solver is implemented within the MinBudgetHybridContract class. Running time depends on whether SBA is applicable. • MIP solver: To find optimal all-or-nothing contracts, we solve the MIN-BUDGET LP in its statistical formulation (Eq. (11)) while restricting ϕj ∈ {0, 1}. This turns the LP into a mixed- integer program, which can also be solved using GLPK. In our code, the IP solver is implemented within the MinBudgetStatisticalContract class. Typical running time for a single problem instance: 10−1s. • Min-pay LP solver: To compare between min-budget and min-pay contracts (Appendix B.3), min-pay contracts were obtained by solving the MIN-PAY LP (Eq. (17)) usingPyomo and GLPK, similar to the full min-budget LP solver. In our code, the min-pay is implemented within the MinPayContract class. Running time is similar to the other LP-based methods. • Full enumeration solver: To find all-or-nothing contracts for low-dimensional problems and avoid numerical instabilities, we implement a solver which performs full enumeration of all-or-nothing contracts. By the statistical variable transformation in definition 3, the (IC) constraint in the MIN-BUDGET LP translates to P j∈Ω (Fi,j − Fi′,j) ϕj ≤ (ci −ci′)β, where i is the target action, i′ ∈ A \\ {i}, and the objective is minimize β (see eq. (11)). Thus, when ϕ ∈ {0, 1}Ω is fixed, the optimal β is given by: β∗(ϕ) = min i′∈A\\{i} P j∈Ω(Fi,j−Fi′,j)ϕj ci−ci′ and optimizing β∗ over ϕ ∈ {0, 1}Ω yields an optimal all-or-nothing contract. The full enumaration solver is implemented within the FullEnumerationContract class. For for enumeration of all- or-nothing contracts, this solver is mostly applicable for small problem instances ( m < 20) due to its exponential complexity. In contrast, for threshold contracts the number of possible ϕ configurations is linear in m, and therefore enumeration is more efficient. • Fixed-budget solver: To find budget-optimal threhsold contracts given a fixed budgetB, we imple- ment a simple solver which iterates through all possible threshold contracts tj0 (j) = B1 [j ≥ j0] for all j0 ∈ {0, . . . , m}. The solver then simulates the agent’s rational choice (Eq. (2)) and selects 34the value of j0 which incentivizes the best action. In case of ties between possible values of j0, they are broken in favor of larger values, as this was shown to lead to greater numerical stability. The fixed-budget solver is implemented within the FixedBudgetThresholdContract class. C.3 Empirical prevalence estimation Since our theoretical analysis relies on MLRP assumptions, we would like to understand whether the results are applicable to real-world learning curves. Towards this, we run an empirical prevalence evaluation on the MNIST dataset. For each learning algorithm Alg, and training set size n, we construct a delegated learning setting with m = 10 and collect the following statistics: • Structural properties: – Is MLRP? (% MLRP): Check whether all pairs n1, n2 ≤ n such that cn1 < cn2 satisfy the monotone likelihood ratio property (see definition 1). • Computational properties: – SBA successful? (% SBA): Check whether the SBA algorithm was successful on the given instance. • Functional form of optimal contract: – Is monotone? (% M): Check whether the resulting min-budget contract satisfies tj ≥ tj−1 for all j ∈ [m]. – Is all or nothing? (% AoN): Check whether the resulting min-budget contract is an all-or- nothing contract (i.e. whether there exists j0 ∈ Ω,B >0 such that tj = 0 for all j < j0 and tj = B otherwise). – Is threshold? (% T): Check whether the resulting min-budget contract is a threshold contract (i.e. whether there exists j0 ∈ Ω,B >0 such that tj = 0 for all j < j0 and tj = B otherwise). • Excess cost: – Min-budget: Optimal objective BLP of MIN-BUDGET LP without additional restrictions, implemented using the full LP solver. – All-or-nothing budget: Optimal objective of MIN-BUDGET LP, when the resulting contract is restricted to all-or-nothing form tj ∈ {0, BAoN}. Implemented using the full enumeration solver. – Threshold budget: Optimal objective of MIN-BUDGET LP, when the resulting contract is restricted to threshold form tj = BThr1 [j ≥ j0]. Implemented using the full enumeration solver. – The excess cost columns in Table 2 indicate the relative excess cost incurred by restricting the min-budget optimization space to simple contracts ( B{AoN,Thr}−BLP BLP ). As all-or-nothing contracts are a superset of threshold contracts, we expect this excess cost to be smaller than the one associated with threshold contracts. The averaging in table 2 is performed on problem instances where both all-or-nothing and threshold contracts are feasible. Averaging over all implementable actions, we obtain the results in Table 2. The results show that threshold contracts are relatively common in this dataset (more than 85%), and that excess cost of simple contracts is relatively low (around 1%), indicating that threshold contracts may provide good approximation to the optimal min-budget contracts in some cases. All simple optimal contracts in our dataset had a threshold functional form, and some simple optimal contracts were not recovered by SBA, indicating that a tighter characterization of simple contracts may be possible. Figure 10 provides qualitative interpretation of the analysis for a selected subset of learners. While the survival functions are not perfectly concave (and thus don’t satisfy Concave-MLRP), the SBA algorithm successfully terminates in three cases (MLP, Logistic, KNN). The binding action is aN−1 for two of the classifiers (MLP, KNN), similar to the condition implied by Theorem 4. Note that SBA also terminated successfully on Logistic Regression data, despite the learning curve having a distinctive non-concave shape. In the case of GBDT, the survival function is not concave, and the optimal contract does not assume a threshold form. 35Table 2: Empirical robustness estimation on the MNIST dataset. Each row corresponds to a learning algorithm, and columns are specified in appendix C.3. Results are averaged across all implementable actions. Structure Compute Functional form of opt. contract Excess cost % MLRP % SBA % M % AoN % T AoN T MLP 100% 94.4% 100% 94.4% 94.4% 0.04% 0.04% GBDT 100% 76.5% 100% 82.4% 82.4% 0.71% 0.71% Logistic 93.8% 81.2% 93.8% 93.8% 93.8% 0.00% 0.00% Perceptron 68.8% 75% 93.8% 93.8% 93.8% 0.00% 0.00% Linear SVM 71.4% 78.6% 85.7% 85.7% 85.7% 0.95% 1.45% Poly SVM 100% 94.4% 100% 94.4% 94.4% 0.24% 0.24% RBF SVM 100% 94.4% 100% 94.4% 94.4% 0.04% 0.04% KNN 100% 100% 100% 100% 100% 0.00% 0.00% Overall 92.6% 87.4% 97% 92.6% 92.6% 0.22% 0.26% 0.25 0.50 0.75 MLP Expected accuracy E[accD(hni)] 0.0 0.5 Qualitative comparison of delegation problems Survival at transition si 0 200000 Optimal contract t∗ j 0.25 0.50 0.75 GBDT 0.0 0.5 0 200000 0.25 0.50 0.75 Logistic 0.0 0.5 0 200000 0 5000 10000 15000 Action cost ci 0.25 0.50 0.75 KNN 0 5000 10000 15000 Action cost ci 0.0 0.5 0 5 10 15 20 25 Outcome j 0 200000 Learning algorithm Figure 10: Qualitative comparison of optimal contract computation on LCDB MNIST data ( n∗ = 16384, m = 25). Each row represents a learning algorithm. (Left column) Expected accuracy curves αn. (Center column) Survival function at transition si, whenever SBA is successful, the binding actions are marked with triangles. (Right column) Optimal contract t∗ j . 36",
      "references": [],
      "meta_data": {
        "arxiv_id": "2306.11475v2",
        "authors": [
          "Eden Saig",
          "Inbal Talgam-Cohen",
          "Nir Rosenfeld"
        ],
        "published_date": "2023-06-20T11:59:03Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces the formal problem of delegated classification, where a budget-limited principal outsources model training to a self-interested agent whose effort (training-set size) is unobservable. Frames the interaction as a principal-agent Stackelberg game and studies how the principal can design performance-based payment contracts that incentivize accurate learning at minimum cost. Provides a theoretical characterization of budget-optimal contracts, proving they have simple threshold (all-or-nothing) structure under monotone likelihood-ratio and concavity conditions, and shows equivalence to the Neyman–Pearson lemma in the binary-action case. Supplies efficient algorithms (single-binding-action heuristic with LP fallback) and NP-hardness results when assumptions fail. Demonstrates empirically, using synthetic data and the LCDB learning-curve corpus, that such contracts can be estimated reliably from small pilot data and improve economic and accuracy outcomes.",
        "methodology": "1. Model delegation as a principal-agent game: agent chooses costly training-set size n; principal offers contract t(j) mapping validation accuracy (j correct out of m) to payment.\n2. Define budget-optimal contract problem, reduce to series of min-budget problems solved via linear programming.\n3. Derive closed-form optimal contracts for two actions using LP duality; connect to most-powerful hypothesis tests via statistical variable change.\n4. Prove that with monotone likelihood-ratio property (MLRP) and concave survival of learning curves (C-MLRP) the optimal contract is a single accuracy threshold.\n5. Provide Single-Binding-Action (SBA) algorithm that tries closed-form contracts, falls back to LP; prove soundness and analyze complexity.\n6. Show NP-hardness for all-or-nothing design without assumptions.",
        "experimental_setup": "• Datasets: Synthetic binomial outcome distributions derived from theoretical learning curves; real curves from Learning Curves DataBase (LCDB), focusing mainly on MNIST-784 with MLP and GBDT classifiers; additional experiments on other datasets/algorithms in appendix.\n• For each action size n (16–32 768) LCDB supplies multiple accuracy estimates; validation set sizes m varied (e.g., 10–50, up to 5 000 in LCDB). Costs set proportional to n.\n• Evaluate principal’s expected accuracy and required budget under different contract types: constant, linear, threshold, min-budget LP, SBA.\n• Metrics: budget required to implement target accuracy, sample-size multiplier µ, excess cost of simple contracts, prevalence of MLRP and threshold optimality.\n• Experiments study effect of validation set size, budget regimes, partial information (estimating learning curves from small pilot data via power-law fits), bias-variance trade-off, and robustness to over/under-estimation.",
        "limitations": "1. Relies on strong assumptions for simple optimality: monotone likelihood-ratio and concave survival; fails and becomes NP-hard in general multi-action, multi-outcome settings.\n2. Principal often assumed to know full stochastic learning curve; partial-information treatment uses parametric extrapolation that may mis-estimate and degrade incentives.\n3. Analysis focuses on classification accuracy; other performance metrics, model complexity or multi-objective utilities not considered.\n4. Single-shot interaction; ignores repeated dealings, reputation, competition, and risk-aversion which could alter incentives.\n5. Agent action limited to dataset size; ignores other costly dimensions (compute, model architecture) and possibility of adversarial behaviour beyond rational profit maximization.",
        "future_research_directions": "• Develop contract mechanisms that remain optimal or approximately optimal without MLRP/concavity, or provide approximation algorithms with guarantees.\n• Extend framework to repeated or competitive markets with multiple principals/agents and dynamic learning.\n• Incorporate multi-dimensional efforts (compute, data quality, model tuning) and richer performance metrics (AUC, fairness) into contract design.\n• Study robust contract design under severe uncertainty, non-parametric curve estimation, and adversarial or risk-averse agents.\n• Explore practical deployment: automated contract generation tools, empirical validation on commercial ML-as-a-service platforms, and integration with legal enforceability.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Classification with Rejection Based on Cost-sensitive Classification",
      "full_text": "Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Nontawat Charoenphakdee 1 2 Zhenghang Cui 1 2 Yivan Zhang1 2 Masashi Sugiyama 2 1 Abstract The goal of classiﬁcation with rejection is to avoid risky misclassiﬁcation in error-critical ap- plications such as medical diagnosis and prod- uct inspection. In this paper, based on the re- lationship between classiﬁcation with rejection and cost-sensitive classiﬁcation, we propose a novel method of classiﬁcation with rejection by learning an ensemble of cost-sensitive classiﬁers, which satisﬁes all the following properties: (i) it can avoid estimating class-posterior probabil- ities, resulting in improved classiﬁcation accu- racy, (ii) it allows a ﬂexible choice of losses in- cluding non-convex ones, (iii) it does not require complicated modiﬁcations when using different losses, (iv) it is applicable to both binary and mul- ticlass cases, and (v) it is theoretically justiﬁable for any classiﬁcation-calibrated loss. Experimen- tal results demonstrate the usefulness of our pro- posed approach in clean-labeled, noisy-labeled, and positive-unlabeled classiﬁcation. 1. Introduction In ordinary classiﬁcation, a classiﬁer learned from training data is expected to accurately predict a label of every possi- ble test input in the input space. However, when a particular test input is difﬁcult to classify, forcing a classiﬁer to always predict a label can lead to misclassiﬁcation, causing seri- ous troubles in risk-sensitive applications such as medical diagnosis, home robotics, and product inspection (Cortes et al., 2016a; Geifman & El-Yaniv, 2017; Ni et al., 2019). To cope with this problem, classiﬁcation with rejection was proposed as a learning framework to allow a classiﬁer to ab- stain from making a prediction (Chow, 1957; 1970; Bartlett & Wegkamp, 2008; El-Yaniv & Wiener, 2010; Geifman & El-Yaniv, 2017; Cortes et al., 2016a;b; Yuan & Wegkamp, 2010; Franc & Prusa, 2019; Pietraszek, 2005; Gangrade 1The University of Tokyo, Tokyo, Japan 2RIKEN AIP, Tokyo, Japan. Correspondence to: Nontawat Charoenphakdee <nontawat@ms.k.u-tokyo.ac.jp>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). et al., 2021), so that we can prevent misclassiﬁcation in critical applications. A well-known framework for classiﬁcation with rejection that has been studied extensively is called the cost-based framework (Chow, 1970; Bartlett & Wegkamp, 2008; Yuan & Wegkamp, 2010; Cortes et al., 2016a;b; Franc & Prusa, 2019; Ni et al., 2019). In this setting, we set a pre-deﬁned rejection cost to be less than the misclassiﬁcation cost. As a result, a classiﬁer trained in this framework prefers to reject than making a risky prediction, where there currently exist two main approaches as the following. The ﬁrst approach is called the conﬁdence-based approach, where we train a classiﬁer then use an output of the clas- siﬁer as a conﬁdence score (Bartlett & Wegkamp, 2008; Grandvalet et al., 2009; Herbei & Wegkamp, 2006; Yuan & Wegkamp, 2010; Ramaswamy et al., 2018; Ni et al., 2019). In this approach, we manually set a conﬁdence threshold as a criterion to refrain from making a prediction, if the conﬁdence score of a test input is lower than the thresh- old. Most conﬁdence-based methods rely on a loss that can estimate class-posterior probabilities (Yuan & Wegkamp, 2010; Reid & Williamson, 2010; Ni et al., 2019), which can be difﬁcult to estimate especially when using deep neu- ral networks (Guo et al., 2017). Although there are some exceptions that can avoid estimating class-posterior proba- bilities, most of them are only applicable to binary classiﬁ- cation (Bartlett & Wegkamp, 2008; Grandvalet et al., 2009; Manwani et al., 2015). The second approach is called the classiﬁer-rejector ap- proach, where we simultaneously train a classiﬁer and a rejector (Cortes et al., 2016a;b; Ni et al., 2019). It is known that this approach has theoretical justiﬁcation in the binary case only for the exponential and hinge-based losses (Cortes et al., 2016a;b). This is because the proof technique highly relies on the function form of the loss (Cortes et al., 2016a;b). In the multiclass case, Ni et al. (2019) argued that this ap- proach is not suitable both theoretically and experimentally since the multiclass extension of Cortes et al. (2016b) is not calibrated and the conﬁdence-based softmax cross-entropy loss can outperform this approach in practice. The goal of this paper is to develop an alternative approach to classiﬁcation with rejection that achieves the follow- ing four design goals. First, it can avoid estimating class- arXiv:2010.11748v5  [stat.ML]  29 Sep 2021Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation posterior probabilities, since this often yields degradation of classiﬁcation performance. Second, the choice of losses is ﬂexible and does not require complicated modiﬁcations when using different losses, which allows a wider range of applications. Third, it is applicable to both binary and multiclass cases. Fourth, it can be theoretically justiﬁed. In this paper, we show that this goal can be achieved by bridg- ing the theory of cost-sensitive classiﬁcation (Elkan, 2001; Scott, 2012; Steinwart, 2007) and classiﬁcation with rejec- tion. The key observation that allows us to connect the two problems is based on the fact that one can mimic the Bayes optimal solution of classiﬁcation rejection by only know- ing arg maxyp(y|x) and whether maxyp(y|x) > 1 −c, where cis the rejection cost. Based on this observation, we propose the cost-sensitive approach, which calibration can be guaranteed for any classiﬁcation-calibrated loss (Zhang, 2004; Bartlett et al., 2006). Classiﬁcation-calibration is known to be a minimum requirement for a loss in ordinary classiﬁcation (Bartlett et al., 2006). This suggests that the loss choices of our proposed approach are as ﬂexible as that of ordinary classiﬁcation. To emphasize the importance of having a ﬂexible loss choice, we explore the usage of our approach for classiﬁcation from positive and unlabeled data (PU-classiﬁcation) (du Plessis et al., 2014; 2015; Kiryo et al., 2017) and classiﬁcation from noisy labels (Angluin & Laird, 1988; Ghosh et al., 2015). Our experimental results show that a family of sym- metric losses, which are the losses that cannot estimate class-posterior probabilities (Charoenphakdee et al., 2019), can be advantageous in these settings. We also provide experimental results of clean-labeled classiﬁcation with re- jection to illustrate the effectiveness of the cost-sensitive approach. 2. Preliminaries In this section, we introduce the problem setting of clas- siﬁcation with rejection. Then, we review cost-sensitive binary classiﬁcation, which will be essential for deriving the proposed cost-sensitive approach for classiﬁcation with rejection. 2.1. Classiﬁcation with Rejection Our problem setting follows the standard cost-based frame- work classiﬁcation with rejection (Chow, 1970; Cortes et al., 2016b; Ni et al., 2019). Let X be an input space and Y = {1,...,K }be an output space, where K de- notes the number of classes. Note that we adopt a conven- tional notation Y = {−1,+1}when considering binary classiﬁcation (Bartlett et al., 2006). In this problem, we are given the training input-output pairs {xi,yi}n i=1 drawn i.i.d. from an unknown probability distribution with density p(x,y). A classiﬁcation rule of learning with rejection is f: X →{1,...,K, ®}, where ® denotes rejection. Let c∈(0,0.5) be the rejection cost. Unlike ordinary classiﬁca- tion, where the zero-one loss ℓ01(f(x),y) =1[f(x)̸=y]1 is the performance measure, we are interested in an extension of ℓ01, which is called the zero-one- closs ℓ01c deﬁned as follows (Ni et al., 2019): ℓ01c(f(x),y) = { c f (x) =®, ℓ01(f(x),y) otherwise. The goal is to ﬁnd a classiﬁcation rule f that minimizes the expected risk with respect to ℓ01c, i.e., Rℓ01c(f) = E (x,y)∼p(x,y) [ℓ01c(f(x),y)]. (1) In classiﬁcation with rejection, a classiﬁcation rule f is al- lowed to refrain from making a prediction and will receive a ﬁxed rejection loss c. In this paper, following most existing studies (Cortes et al., 2016a;b; Ramaswamy et al., 2018; Ni et al., 2019), we consider the case where c< 0.5. Intu- itively, this case implies that it is strictly better to reject if a classiﬁer has less than half a chance to be correct. Thus, the case where c< 0.5 is suitable if the goal is to avoid harm- ful misclassiﬁcation. We refer the readers to Ramaswamy et al. (2018) for more discussion on the case where c≥0.5 and how it is fundamentally different from the case where c< 0.5. Next, let us deﬁne η(x) = [η1(x),...,η K(x)]⊤, where ηy(x) = p(y|x) denotes the class-posterior proba- bility of a class y. The optimal solution for classiﬁcation with rejection f∗= arg minf Rℓ01c(f) known as Chow’s rule (Chow, 1970) can be expressed as follows: Deﬁnition 1 (Chow’s rule (Chow, 1970)) . The optimal solution of multiclass classiﬁcation with rejection f∗ = arg minf Rℓ01c(f) can be expressed as f∗(x) = { ® maxyηy(x) ≤1 −c, arg maxyηy(x) otherwise. Chow’s rule suggests that classiﬁcation with rejection is solved if we have the knowledge of η(x). Therefore, one approach is to estimate η(x) from training examples. This method is in a family of the conﬁdence-based approach, which has been extensively studied in both the binary (Yuan & Wegkamp, 2010) and multiclass cases (Ni et al., 2019). Figure 1 illustrates the conﬁdence-based approach. 2.2. Cost-sensitive Binary Classiﬁcation Consider binary classiﬁcation where y ∈ {−1,+1}. In ordinary classiﬁcation, the false positive and false negative costs are treated equally. On the other hand, in cost-sensitive 11[·] denotes an indicator function.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Figure 1. Illustration of the conﬁdence-based approach. Figure (a) denotes a prediction function. The rejector in ﬁgure (b) has a rejection region spreads from the decision boundary of the prediction function. The width of the rejection region depends on the choice of the rejection threshold parameter. Data points in purple are rejected. classiﬁcation, the false positive and false negative costs are generally unequal (Elkan, 2001; Saerens et al., 2002; Scott, 2012). Without loss of generality, we deﬁne α ∈(0,1) to be the false positive cost and 1 −αto be the false negative cost. Then, the expected cost-sensitive risk can be expressed as Rℓ01 α (f) = (1−α)π E x∼p(x|y=+1) [ℓ01(f(x),+1)] + α(1 −π) E x∼p(x|y=−1) [ℓ01(f(x),−1)], where π= p(y= +1)denotes the class prior. It is known that the Bayes optimal cost-sensitive binary classiﬁer can be expressed as follows: Deﬁnition 2 (Scott (2012)). The optimal cost-sensitive clas- siﬁer f∗ α = arg minf Rα(f) can be expressed as f∗ α(x) = { +1 p(y= +1|x) >α, −1 otherwise. Note that when α= 0.5, the Bayes optimal solution f∗ 0.5(x) coincides with that of ordinary binary classiﬁcation. More- over, when αis known, cost-sensitive binary classiﬁcation is solved if we have access to the class-posterior probability p(y= +1|x). 3. Cost-sensitive Approach In this section, we propose a cost-sensitive approach for classiﬁcation with rejection. We begin by describing our motivation and analyzing the behavior of the Bayes optimal solution of classiﬁcation with rejection. Then, we show that this problem can be solved by simultaneously solving multiple cost-sensitive classiﬁcation problems. 3.1. Motivation As suggested by Chow’s rule (Chow, 1970), classiﬁcation with rejection can be solved by estimating the class-posterior probabilities. However, an important question arises as: Is class-posterior probability estimation indispensable for solving classiﬁcation with rejection? This question is fun- damentally motivated by Vapnik’s principle (Vapnik, 1998), which suggests not to solve a more general problem as an intermediate step when solving a target problem if we are given a restricted amount of information. In our context, the general problem is class-posterior prob- ability estimation. In fact, knowing class-posterior prob- abilities can also solve many other problems (Qin, 1998; Bickel et al., 2007; Sugiyama et al., 2012; Dembczynski et al., 2013; Koyejo et al., 2014). However, many of such problems are also known to be solvable without estimating the class-posterior probabilities (Kanamori et al., 2012; Bao & Sugiyama, 2020). Note that class-posterior probability estimation can be unreliable when the model is misspeci- ﬁed (Begg & Lagakos, 1990; Heagerty & Kurland, 2001) or highly ﬂexible (Guo et al., 2017; Hein et al., 2019). To ﬁnd a more direct solution for classiﬁcation with rejec- tion, we seek for a general approach that it may not be able to estimate class-posterior probabilities, but its opti- mal solution coincides with the optimal Chow’s rule (Chow, 1970). Although the idea of directly solving classiﬁcation with rejection without class-posterior estimation itself is not novel, most existing methods are only applicable to binary classiﬁcation with rejection (Bartlett & Wegkamp, 2008;Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Figure 2. Illustration of Chow’s rule in binary classiﬁcation with rejection and the unnecessity of knowing the class-posterior proba- bility to solve this problem. If the rejection cost c= 0.2, as long as we know p(y = 1|x) > 0.8, knowing the exact value of the class-posterior probability does not change our ﬁnal decision to predict a positive label. Grandvalet et al., 2009; Manwani et al., 2015; Cortes et al., 2016b;a), or focus on speciﬁc types of losses (Ramaswamy et al., 2018). For the multiclass case, Zhang et al. (2018) proposed to modify a loss by bending it to be steeper (for the hinge loss) and positively unbounded, but there exist hyperparameters to be tuned such as the rejection threshold and the bending slope. Also, Mozannar & Sontag (2020) recently proposed a classiﬁer-rejector approach by augment- ing a rejection class in the model’s prediction, but their loss choice is limited to the modiﬁed cross-entropy loss. More discussion on related work is provided in Appendix A. 3.2. A Closer Look at Chow’s Rule Here, we analyze the behavior of Chow’s rule (Chow, 1970). We discuss the minimal knowledge required for a classi- ﬁcation rule to mimic Chow’s rule, which illustrates that the class-posterior probabilities need not to be known. For simplicity, we begin by considering binary classiﬁcation with rejection. In binary classiﬁcation with rejection, Chow’s rule in Deﬁ- nition 1 can be expressed as f∗(x) =    1 p(y= +1|x) >1 −c, ® c≤p(y= +1|x) ≤1 −c, −1 p(y= +1|x) <c. (2) To solve binary classiﬁcation with rejection, there are only three conditions to verify, which are p(y= +1|x) >1 −c, p(y = +1|x) < c, and p(y = +1|x) ∈[c,1 −c]. We can see that if we knowp(y= +1|x) >1−c, we do not need to know the exact value of p(y= +1|x) to predict the label as positive. For example, if c= 0.2, knowing p(y= +1|x) > 0.8 is already sufﬁcient to predict a label, i.e., knowing whether p(y = +1|x) > 0.88 or p(y = +1|x) > 0.96 does not change the decision of Chow’s rule. Figure 2 illustrates this fact, which is the key intuition why it is possible to develop a method that can avoid estimating the class-posterior probabilities for solving this problem. 3.3. Binary Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Here, we show that by solving two cost-sensitive binary classiﬁcation problems, binary classiﬁcation with rejection can be solved. The following proposition shows the relation- ship between the optimal solutions of cost-sensitive binary classiﬁcation and that of binary classiﬁcation with rejection. Proposition 3. In binary classiﬁcation with rejection, Chow’s rule can be expressed as f∗(x) =    1 f∗ 1−c(x) = 1, −1 f∗ c(x) =−1, ® otherwise. (3) Proof. We assert that if we can verify the following two conditions: p(y= +1|x) >1 −c, (4) p(y= +1|x) >c, (5) then binary classiﬁcation with rejection is solved. Based on Chow’s rule(2), if Ineq. (4) holds, Ineq. (5) must also hold since c< 0.5. Then we should predict a positive label. On the other hand, we should predict a negative label if Ineq.(5) does not hold. Next, if Ineq. (5) holds but Ineq. (4) does not hold, we should reject x. As a result, based on Deﬁnition 2, knowing f∗ 1−c(x) and f∗ c(x) is sufﬁcient to verify Ineqs. (4) and Ineq. (5). This concludes the proof. Proposition 3 suggests that by solving two binary cost- sensitive classiﬁcation with α= cand α= 1−cto obtain f∗ c(x) and f∗ 1−c(x), binary classiﬁcation with rejection can be solved. 3.4. Multiclass Extension Here, we show that our result in Section 3.3 can be naturally extended to the multiclass case. More speciﬁcally, we show that multiclass classiﬁcation with rejection can be solved by learning an ensemble of Kbinary cost-sensitive classiﬁers. Let us deﬁne the Bayes optimal solution for one-versus-rest cost-sensitive binary classiﬁer f∗,y α , where yis the positive class and y′∈Y,y′̸= yare the negative classes: f∗,y α (x) = { +1 ηy(x) >α, −1 otherwise. Then, we obtain the following proposition (its proof can be found in Appendix B.1). Proposition 4. Chow’s rule in multiclass classiﬁcation with rejection can be expressed as f∗(x) = { ® maxyf∗,y 1−c(x) =−1, arg maxyf∗,y 1−c(x) otherwise .Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 1.Classiﬁcation-calibrated binary surrogate losses and their properties including the convexity, symmetricity (i.e., φ(z) +φ(−z) is a constant), and its capability to estimate the class posterior probability η1(x) in binary classiﬁcation. The column “conﬁdence-based” indicates that a loss is applicable to the conﬁdence-based approach and it satisﬁes all conditions required in order to use the previous work to derive its excess risk bound (Yuan & Wegkamp, 2010; Ni et al., 2019). On the other hand, our cost-sensitive approach can guarantee the existence of the excess risk bound as long as a loss φis classiﬁcation-calibrated (Bartlett et al., 2006). Loss name φ(z) Convex Symmetric Estimating η1(x) Conﬁdence-based Squared (1 −z)2 ✓ × ✓ ✓ Squared hinge max(0,1 −z)2 ✓ × ✓ ✓ Exponential exp(−z) ✓ × ✓ ✓ Logistic log(1 + exp(−z)) ✓ × ✓ ✓ Hinge max(0,1 −z) ✓ × × × Savage [ (1 + exp(2z))2]−1 × × ✓ × Tangent (2arctan(z) −1)2 × × ✓ × Ramp max(0,min(1,0.5 −0.5z)) × ✓ × × Sigmoid [1 + exp(z)]−1 × ✓ × × Proposition 4 suggests that by learning cost-sensitive clas- siﬁers f∗,y 1−c for y∈Y, it is possible to obtain Chow’s rule without estimating the class-posterior probabilities. Note that when c< 0.5, there exists at most one y′∈Y such that f∗,y′ 1−c(x) = 1. This is because it implies that ηy′ (x) >1−c, which is larger than 0.5. Related work: The similar idea of constructing cost- sensitive classiﬁers for solving classiﬁcation with rejec- tion has also been recently explored in the bounded- improvement framework by Gangrade et al. (2021). Unlike the cost-based framework considered in this paper, where our goal is to minimize the expected risk with respect toℓ01c in Eq. (1), the goal of the bounded-improvement framework is to learn a classiﬁer that minimizes the number of rejec- tions (i.e., maximizing coverage) while achieving at least the pre-deﬁned accuracy on non-rejected data (Pietraszek, 2005; El-Yaniv & Wiener, 2010; Geifman & El-Yaniv, 2017; Liu et al., 2019; Franc & Prusa, 2019). Gangrade et al. (2021) proposed to solve classiﬁcation with rejection under the bounded-improvement framework by solving multiple one- sided classiﬁcation problems. Then, they further relaxed one-sided classiﬁcation problems to cost-sensitive classiﬁ- cation problems. Since the optimal solution in the bounded- improvement framework is not necessarily Chow’s rule, their ﬁnal learning objective is different from ours. Please see Gangrade et al. (2021) for more details. 4. A Surrogate Loss for the Cost-sensitive Approach In this section, we propose a surrogate loss for the cost- sensitive approach for classiﬁcation with rejection. It is known that given training data, directly minimizing the empirical risk with respect to ℓ01c is computationally infeasible (Bartlett & Wegkamp, 2008; Ramaswamy et al., 2018). Therefore, many surrogate losses have been pro- posed to learn a classiﬁer with rejection in practice (Yuan & Wegkamp, 2010; Cortes et al., 2016b; Ni et al., 2019). Here, we propose the cost-sensitive surrogate loss for clas- siﬁcation with rejection. Let g(x) = [g1(x),...,g K(x)]⊤, where gy(x): X →R is the score function for a class y. Let φ: R →R be a binary margin surrogate loss. A margin loss is a class of loss functions for binary classiﬁcation that takes only one real-valued argument (Bartlett et al., 2006; Reid & Williamson, 2010). Table 1 illustrates examples of binary margin surrogate losses. With a choice of φ, we can deﬁne our proposed cost-sensitive surrogate loss as follows. Deﬁnition 5. Given a binary margin surrogate loss φand a pre-deﬁned rejection cost c, the cost-sensitive surrogate loss for classiﬁcation with rejection is deﬁned as Lc,φ CS (g; x,y) =cφ ( gy(x) ) + (1−c) ∑ y′̸=y φ ( −gy′ (x) ) . Next, following the empirical risk minimization frame- work (Vapnik, 1998), a learning objective function can be straightforwardly obtained as follows: ˆRLc,φ CS (g) = 1 n n∑ i=1 Lc,φ CS (g; xi,yi). (6) Note that regularization can also be applied in practice to avoid overﬁtting. Moreover, we want to emphasize that although it is theoretically suggested to learn an ensemble of classiﬁers to solve classiﬁcation with rejection, in practice, by using linear-in-parameter models or neural networks with K-dimensional vectorial outputs, we can conveniently learn all Kcost-sensitive binary classiﬁers together at once, which is g. After learning gby minimizing Eq. (6), we have to design how to reject x. Following the optimal rejection rule in ourClassiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Figure 3. Illustration of the cost-sensitive approach. Figure (a) denotes a prediction function. Unlike the conﬁdence-based approach (Fig- ure 1), the prediction function is not designed to predict all data points in the space and the rejection region does not spread from the decision boundary. The decision boundary is based on an ensemble of cost-sensitive classiﬁers for blue, red, and green classes, respectively. Then, the rejector in ﬁgure (b) is constructed based on the rejection rule in Cond. (7) by aggregating the prediction result of each cost-sensitive classiﬁer. Data points in purple are rejected. Proposition 4, i.e., maxyf∗,y 1−c(x) = −1, we can directly obtain the following rejection rule: max y gy(x) ≤0. (7) Intuitively, Cond. (7) suggests to reject xif all gy(x) have low prediction conﬁdence. One may interpret this type of rejection as distance rejection (Dubuisson & Masson, 1993), where the rejection is made when gis uncertain whether x belongs to any of the known classes. Note that this does not necessarily imply that xbelongs to an unknown class, e.g., xmay be located close to the decision boundary, causing none of gy(x) to be conﬁdent enough to predict a class y. Next, we also consider the following rejection rule: ∃y,y′s.t. y̸= y′and gy(x),gy′ (x) >0. (8) Cond. (8) suggests to reject xbecause there exists a predic- tion conﬂict among at least two binary classiﬁers, i.e., gy(x) suggests to predict a class ybut gy′ (x) suggests to predict another class y′. Note that if we succeed to obtain the opti- mal classiﬁer g∗, this condition is impossible to be satisﬁed. Recall that in Section 3.4, for g∗, at most one g∗ y(x) can be more than zero since it implies ηy >1 −c> 0.5. Nev- ertheless, Cond. (8) may hold in practice due to empirical estimation. This rejection condition can be interpreted as ambiguity rejection (Dubuisson & Masson, 1993), where the rejection is made when ginterprets xto be associated with more than one class. More discussion on the proposed rejection conditions is provided in Appendix. D. To sum up, we employ the following classiﬁcation rule for the cost-sensitive approach: f(x; g) = { ® Conds. (7) or (8), arg maxygy(x) otherwise . (9) Figure 3 illustrates the cost-sensitive approach. It is worth mentioning that our rejection condition is different from that of Zhang et al. (2018). In their rejection rule, an input xis rejected if all binary classiﬁers’ outputs are close to zero. In our case, Cond. (7) rejects xas long as all gy(x)’s are negative, e.g., xis also rejected if all prediction outputs are much smaller than zero. Also, their method can predict a set of labels when at least two classiﬁers predict positively, which is different from our problem setting, where it is only allowed to predict one label or refrain from making a prediction. 5. Theoretical Analysis In this section, we show that the classiﬁcation rule f(x; g) in Eq. (9) can achieve Chow’s rule and also provide excess risk bounds. 5.1. Calibration We begin by introducing the well-known notion of classiﬁcation-calibrated loss in binary classiﬁcation. Let us deﬁne the pointwise conditional surrogate risk for a ﬁxed input xin binary classiﬁcation with its class-posterior prob- ability of a positive class η1: Cφ η1 (v) =η1φ(v) + (1−η1)φ(−v), (10)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation for v ∈R. Next, a classiﬁcation-calibrated loss can be deﬁned as follows. Deﬁnition 6 (Bartlett et al. (2006)) . We say a loss φ is classiﬁcation-calibrated if for any η1 ̸= 1 2 , we have inf v(2η1−1)≤0 Cφ η1 (v) >inf v Cφ η1 (v). Intuitively, classiﬁcation-calibration ensures that minimiz- ing a loss φ can give the Bayes optimal binary classi- ﬁer sign(2η1 −1). It is known that a convex loss φ is classiﬁcation-calibrated if and only if it is differentiable at 0 and φ′(0) <0 (Bartlett et al., 2006). Analogously, in classiﬁcation with rejection, calibration is also an important property that has been used to verify if a surrogate loss is appropriate (Bartlett & Wegkamp, 2008; Yuan & Wegkamp, 2010; Cortes et al., 2016b;a; Ni et al., 2019). Calibration guarantees that by replacing the zero- one-closs ℓ01c with a surrogate loss L, the optimal Chow’s rule can still be obtained by minimizing the surrogate risk. Verifying the calibration condition in classiﬁcation with rejection has not been as well-studied as ordinary binary classiﬁcation. We are only aware of the works by Yuan & Wegkamp (2010) and Ni et al. (2019), which provided a condition to verify calibration of general loss functions for the conﬁdence-based approach. Nevertheless, their condi- tion can only verify a convex loss. Note that losses that are calibrated in our cost-sensitive approach may not be cali- brated in the conﬁdence-based approach, e.g., the sigmoid loss. See Table 1 for more details. Now we are ready to show that the calibration condition of our proposed approach is equivalent to the classiﬁcation- calibration condition of φ. Let us deﬁne the pointwise conditional surrogate risk WLof an input xwith its class- posterior probability η(x) for the multiclass case: WL ( g(x)); η(x) ) = ∑ y∈Y ηy(x)L ( g; x,y ) . (11) By analyzing the classiﬁcation rule with respect to the con- ditional risk minimizer, we obtain the following theorem (its proof can be found in Appendix B.2). Theorem 7. Let g∗be a conditional risk minimizer that minimizes the pointwise conditional surrogate risk g∗(x) = arg mingWLc,φ CS ( g(x); η(x))). The surrogate loss Lc,φ CS is calibrated for classiﬁcation with rejection, that is, f(x; g∗) = f∗(x) for all x ∈ X, if and only if φ is classiﬁcation-calibrated. Theorem 7 suggests that the condition to verify if our cost- sensitive surrogate lossLc,φ CS is calibrated is equivalent to the condition of whether φis classiﬁcation-calibrated. As long as a binary surrogate loss φis classiﬁcation-calibrated, min- imizing the surrogate risk w.r.t. Lc,φ CS can lead to the optimal Chow’s rule. As a result, Theorem 7 successfully borrows the knowledge in the literature of binary classiﬁcation to help prove calibration in classiﬁcation with rejection for the cost-sensitive approach. 5.2. Excess Risk Bound While calibration ensures that the optimal solution w.r.t. a surrogate loss agrees with the optimal Chow’s rule, an ex- cess risk bound provides a regret bound relationship between the zero-one-closs ℓ01c and a surrogate loss L. Let Rφ,i 1−c(gi) be the cost-sensitive binary surrogate risk for class iand RL,∗be the minimum risk w.r.t. to the loss L. We prove the following theorem, which is our main result to use for deriving the excess risk bound of the cost-sensitive approach for any classiﬁcation-calibrated loss (its proof can be found in Appendix B.3). Theorem 8. Consider a cost-sensitive surrogate loss Lc,φ CS . Let f be a classiﬁcation rule of the cost-sensitive approach with respect to the score function g, that is, f = f(x; g) for an input x. If a binary surrogate loss φis classiﬁcation- calibrated, the excess risk bound can be expressed as fol- lows: Rℓ01c(f) −Rℓ01c,∗≤RLc,ℓ01 CS (g) −RLc,ℓ01 CS ,∗ (12) ≤ K∑ i=1 ψ−1 φ,1−c(Rφ,i 1−c(gi) −Rφ,i,∗ 1−c ), (13) where ψφ,1−c: R →R is a non-decreasing invertible func- tion and ψφ,1−c(0) = 0. Ineq. (12) suggests that the regret of the classiﬁcation with rejection problem can be bounded by the regret of the cost- sensitive surrogate with respect to the zero-one loss ℓ01. This inequality allows us to borrow the existing ﬁndings of cost-sensitive classiﬁcation to give excess risk bounds for classiﬁcation with rejection. Next, Ineq. (13) suggests that RLc,ℓ01 CS (g) −RLc,ℓ01 CS ,∗is bounded by the sum of an invertible function of the regret of the cost-sensitive binary classiﬁcation risk. The invertible function ψφ,1−c is a well- studied function in the literature of cost-sensitive classiﬁ- cation, which is guaranteed to exist for any classiﬁcation- calibrated loss (Steinwart, 2007; Scott, 2012). For example, ψ−1 φ,1−c(ϵ) = ϵ2 2c(1−c)−(ϵ)(1−2c) for the squared loss, where ϵ ≥0. Examples of ψφ,1−c for more losses and how to derive ψφ,1−c can be found in Steinwart (2007) and Scott (2012). Since ψφ,1−c is non-decreasing and ψφ,1−c(0) = 0, the regret with respect to the zero-one-closs will also get smaller and eventually become zero if the surrogate risk is successfully minimized. As an example to demonstrate how to obtain an excess risk bound with our Theorem 8, we prove that the followingClassiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.04 0.06 0.08 0.10Zero-one-c risk Clean Gisette 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12Zero-one-c risk Phishing 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.06 0.07 0.08 0.09 0.10 0.11 0.12Zero-one-c risk Spambase 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.08 0.10 0.12 0.14 0.16Zero-one-c risk Subj 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.02 0.04 0.06 0.08 0.10Zero-one-c risk Twonorm 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.10 0.15 0.20 0.25 0.30Zero-one-c risk Noisy 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.075 0.100 0.125 0.150 0.175 0.200 0.225Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.075 0.100 0.125 0.150 0.175 0.200 0.225 0.250Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.025 0.050 0.075 0.100 0.125 0.150 0.175Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.10 0.15 0.20 0.25 0.30 0.35 0.40Zero-one-c risk PU 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.06 0.08 0.10 0.12 0.14 0.16Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.100 0.125 0.150 0.175 0.200 0.225 0.250 0.275 0.300Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.100 0.125 0.150 0.175 0.200 0.225 0.250 0.275Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.02 0.04 0.06 0.08 0.10 0.12 0.14Zero-one-c risk SCE DEFER ANGLE CS-hinge CS-sigmoid Figure 4.Mean and standard error of the test empirical zero-one-crisk over ten trials with varying rejection costs (Binary classiﬁcation). Each column indicates the performance with respect to one dataset. (Top) clean-labeled classiﬁcation with rejection. (Middle) noisy-labeled classiﬁcation with rejection. (Bottom) PU-classiﬁcation with rejection. excess risk bound holds for the hinge loss φhin, which is the loss that cannot estimate the class-posterior probabili- ties (Cortes & Vapnik, 1995), and its optimal solution for the conﬁdence-based approach cannot mimic Chow’s rule. The bound can be straightforwardly derived based on our Theo- rem 8 and the known fact that ψ−1 φhin,1−c(ϵ) = ϵ(Steinwart, 2007). Corollary 9. Let us consider the hinge loss φhin(z) = max(0,1 −z). The excess risk bound for the cost-sensitive surrogate Lc,φhin CS can be given as follows: Rℓ01c(f) −Rℓ01c,∗≤RL c,φhin CS (g) −RL c,φhin CS ,∗. 6. Experimental Results In this section, we provide experimental results of clas- siﬁcation with rejection. The evaluation metric is the test empirical zero-one- c risk over ten trials. We also reported the rejection ratio, accuracy of accepted data, and the full experimental results in the table format in Appendix D. The varying rejection costs ranged from {0.1,0.15,0.20,0.25,0.30,0.35,0.40}for all settings. For noisy-labeled classiﬁcation, we used the uniform noise (An- gluin & Laird, 1988; Ghosh et al., 2015), where the ran- domly selected 25% of the training labels were ﬂipped. 6.1. Experiment Setup Datasets and models: For binary classiﬁcation, we used the subjective-versus-objective classiﬁcation (Subj), which is a text dataset (Pang & Lee, 2004). Moreover, we used Phishing and Spambase, which are tabular datasets, and Twonorm, which is a synthetic dataset drawn from different multivariate Gaussian distributions (Lichman et al., 2013). We also used the Gisette dataset, which is the problem of separating the highly confusible digits 4 and 9 with noisy features (Guyon et al., 2005). Linear-in-input models were used for all binary datasets. For multiclass classiﬁcation, we used Gas-Drift (Vergara et al., 2012) and Human activity recognition (HAR) (Anguita et al., 2013), which are tabular datasets. Multilayer perceptrons were used for both datasets. We also used the image datasets, which are MNIST (Le- Cun, 1998), Kuzushiji-MNIST (KMNIST) (Clanuwat et al., 2018), and Fashion-MNIST (Xiao et al., 2017). Convo- lutional neural networks were used for all image datasets. The implementation was done using PyTorch (Paszke et al., 2019). More details on the datasets and implementation canClassiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.005 0.010 0.015 0.020 0.025 0.030 0.035Zero-one-c risk Clean Gas-Drift 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.01 0.02 0.03 0.04 0.05 0.06Zero-one-c risk HAR 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.006 0.008 0.010 0.012 0.014 0.016Zero-one-c risk MNIST 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.04 0.05 0.06 0.07 0.08Zero-one-c risk Fashion-MNIST 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.03 0.04 0.05 0.06 0.07Zero-one-c risk KMNIST 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40Zero-one-c risk Noisy 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.05 0.10 0.15 0.20 0.25 0.30Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.06 0.08 0.10 0.12 0.14 0.16 0.18Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.050 0.075 0.100 0.125 0.150 0.175 0.200Zero-one-c risk SCE DEFER ANGLE CS-hinge CS-sigmoid Figure 5.Mean and standard error of the test empirical zero-one-crisk over ten trials with varying rejection cost (Multiclass classiﬁcation). Each column indicates the performance with respect to one dataset. (Top) clean-labeled classiﬁcation with rejection. (Bottom) noisy- labeled classiﬁcation with rejection. For MNIST, Fashion-MNIST, and KMNIST, we found that ANGLE failed miserably and has zero-one-crisk more than 0.5 and thus it is excluded from the ﬁgure for readability. be found in Appendix C. Methods: For the conﬁdence-based approach, based on Ni et al. (2019), we used the softmax cross-entropy loss (SCE). For the classiﬁer-reject approach, we used the proposed method by Mozannar & Sontag (2020) (DEFER). We also used the method by Zhang et al. (2018) with the bent hinge loss (ANGLE). For our cost-sensitive approach, we used the hinge (CS-hinge), and sigmoid (CS-sigmoid) losses. Hyperparameter tuning: We provided additional training data for SCE and ANGLE to tune their hyperparameters. For SCE, we also added temperature scaling (Guo et al., 2017) to improve the prediction conﬁdence. For ANGLE, we chose the bending slope paramater according to Zhang et al. (2018) and tuned the rejection threshold. In PU-classiﬁcation, it is difﬁcult to tune hyperparameters for them. Thus, we pro- vided clean-labeled data for them only for hyperparameter tuning. Both rejection threshold of ANGLE and the tem- perature parameter for SCE are chosen from the following candidate set of twenty numbers spaced evenly in a log scale from 0 to 1 (inclusively) and nine integers from 2 to 10. Since only SCE and ANGLE require additional data to tune hyperparameters, it is not straightforward to provide a fair comparison because our methods and DEFER do not use validation data. Nevertheless, with less data, our meth- ods are still competitive and can outperform the baselines in several settings. 6.2. Binary Classiﬁcation with Rejection Here, we compare the performance of all methods in clean- labeled, noisy-labeled, and positive-unlabeled classiﬁcation with rejection. For PU-classiﬁcation, we implemented all methods based on the empirical risk minimization frame- work proposed by Kiryo et al. (2017) (more detail can be found in Appendix C). Figure 4 shows the performance with varying rejection costs for all settings. In clean-labeled classiﬁcation, it can be observed that CS-hinge and ANGLE are most preferable in this setting. In noisy-labeled and PU classiﬁcation, CS- sigmoid outperformed other methods in most cases. This illustrates the usefulness of having a ﬂexible choice of loss functions. We also found that noise can degrade the per- formance to be worse than always reject for some methods. Moreover, we found that DEFER rejected data more often than other methods, which may sometimes lead to worse performance. In PU-classiﬁcation, SCE and ANGLE did not perform well although clean labeled data were used for hyperparameter tuning. This could be due to a steep loss can suffer severely from the negative risk problem (Kiryo et al., 2017), causing them to be ineffective in PU-classiﬁcation. 6.3. Multiclass Classiﬁcation with Rejection Figure 5 illustrates the performance of all methods in the clean-labeled and noisy-labeled settings. It can be observed that SCE had almost the same performance for all rejection costs. Although temperature scaling is applied, it seems that SCE still suffered from overconﬁdence (Guo et al., 2017) and failed to reject the ambiguous data points. This could be due to SCE has the high accuracy on the validation set (more than 90%) and thus temperature scaling could not smoothen the prediction conﬁdence to reject the ambigu- ous data effectively. Interestingly, DEFER did not sufferClassiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation from such overconﬁdence although it is also based on the cross-entropy loss and it rejected the data more than other methods. For ANGLE, we found that although it can per- form competitively in Gas-drift and HAR, it failed miserably in the image datasets. For ﬁgure’s readability, we report the performance of ANGLE in a table format in Appendix D. In noisy-labeled classiﬁcation, CS-sigmoid outperformed other methods in most cases. 7. Conclusions We have proposed a cost-sensitive approach to classiﬁcation with rejection, where any classiﬁcation-calibrated loss can be applied with theoretical guarantee. Our theory of excess risk bounds explicitly connects the classiﬁcation with re- jection problem to the cost-sensitive classiﬁcation problem. Our experimental results using clean-labeled, noisy-labeled, and positive and unlabeled training data demonstrated the advantages of avoiding class-posterior probability estima- tion and having a ﬂexible choice of loss functions. Acknowledgements We would like to thank Han Bao, Takeshi Teshima, Chenri Ni, and Junya Honda for helpful discussion, and also the Supercomputing Division, Information Technology Center, The University of Tokyo, for providing us the Reedbush supercomputer system to conduct the experiments. NC was supported by MEXT scholarship, JST AIP Challenge, and Google PhD Fellowship program. ZC was supported by JST AIP Challenge. MS was supported by the International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study. References Angluin, D. and Laird, P. Learning from noisy examples. Machine Learning, 2(4):343–370, 1988. Anguita, D., Ghio, A., Oneto, L., Parra, X., and Reyes- Ortiz, J. L. A public domain dataset for human activity recognition using smartphones. In Esann, volume 3, pp. 3, 2013. Bao, H. and Sugiyama, M. Calibrated surrogate maximiza- tion of linear-fractional utility in binary classiﬁcation. AISTATS, 2020. Bartlett, P. L. and Wegkamp, M. H. Classiﬁcation with a reject option using a hinge loss. JMLR, 9:1823–1840, 2008. Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. Convexity, classiﬁcation, and risk bounds. JASA, 101(473):138–156, 2006. Begg, M. D. and Lagakos, S. On the consequences of model misspeciﬁcation in logistic regression. Environmental health perspectives, 87:69–75, 1990. Bickel, S., Br ¨uckner, M., and Scheffer, T. Discriminative learning for differing training and test distributions. In ICML, pp. 81–88, 2007. Charoenphakdee, N. and Sugiyama, M. Positive-unlabeled classiﬁcation under class prior shift and asymmetric error. In SDM, pp. 271–279. SIAM, 2019. Charoenphakdee, N., Lee, J., and Sugiyama, M. On sym- metric losses for learning from corrupted labels. ICML, 2019. Chow, C. K. An optimum character recognition system using decision functions. IRE Transactions on Electronic Computers, EC-6(4):247–254, 1957. Chow, C. K. On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory, 16 (1):41–46, 1970. Clanuwat, T., Bober-Irizar, M., Kitamoto, A., Lamb, A., Yamamoto, K., and Ha, D. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718 , 2018. Cortes, C. and Vapnik, V . Support-vector networks. Ma- chine learning, 20(3):273–297, 1995. Cortes, C., DeSalvo, G., and Mohri, M. Boosting with ab- stention. In Advances in Neural Information Processing Systems, pp. 1660–1668, 2016a. Cortes, C., DeSalvo, G., and Mohri, M. Learning with rejection. In ALT, pp. 67–82, 2016b. Dembczynski, K., Jachnik, A., Kotlowski, W., Waegeman, W., and H ¨ullermeier, E. Optimizing the f-measure in multi-label classiﬁcation: Plug-in rule approach versus structured loss minimization. In ICML, pp. 1130–1138, 2013. du Plessis, M. C., Niu, G., and Sugiyama, M. Analysis of learning from positive and unlabeled data. In Advances in Neural Information Processing Systems, pp. 703–711, 2014. du Plessis, M. C., Niu, G., and Sugiyama, M. Convex formulation for learning from positive and unlabeled data. In ICML, pp. 1386–1394, 2015. Dubuisson, B. and Masson, M. A statistical decision rule with incomplete knowledge about classes. Pattern recog- nition, 26(1):155–165, 1993.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation El-Yaniv, R. and Wiener, Y . On the foundations of noise- free selective classiﬁcation. JMLR, 11(May):1605–1641, 2010. Elkan, C. The foundations of cost-sensitive learning. In IJCAI, volume 17, pp. 973–978. Lawrence Erlbaum As- sociates Ltd, 2001. Franc, V . and Prusa, D. On discriminative learning of pre- diction uncertainty. In ICML, pp. 1963–1971, 2019. Gangrade, A., Kag, A., and Saligrama, V . Selective classi- ﬁcation via one-sided prediction. In AISTATS, pp. 2179– 2187. PMLR, 2021. Geifman, Y . and El-Yaniv, R. Selective classiﬁcation for deep neural networks. In Advances in Neural Information Processing Systems, pp. 4878–4887, 2017. Ghosh, A., Manwani, N., and Sastry, P. Making risk min- imization tolerant to label noise. Neurocomputing, 160: 93–107, 2015. Grandvalet, Y ., Rakotomamonjy, A., Keshet, J., and Canu, S. Support vector machines with a reject option. In Advances in Neural Information Processing Systems, pp. 537–544, 2009. Guo, C., Pleiss, G., Sun, Y ., and Weinberger, K. Q. On calibration of modern neural networks. In ICML, pp. 1321–1330. JMLR. org, 2017. Guyon, I., Gunn, S., Ben-Hur, A., and Dror, G. Result analysis of the nips 2003 feature selection challenge. In Advances in Neural Information Processing Systems, pp. 545–552, 2005. Heagerty, P. J. and Kurland, B. F. Misspeciﬁed maximum likelihood estimates and generalised linear mixed models. Biometrika, 88(4):973–985, 2001. Hein, M., Andriushchenko, M., and Bitterwolf, J. Why ReLU networks yield high-conﬁdence predictions far away from the training data and how to mitigate the prob- lem. In CVPR, pp. 41–50, 2019. Herbei, R. and Wegkamp, M. H. Classiﬁcation with reject option. Canadian Journal of Statistics, 34(4):709–721, 2006. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pp. 448–456. PMLR, 2015. Kanamori, T., Suzuki, T., and Sugiyama, M. Statistical anal- ysis of kernel-based least-squares density-ratio estimation. Machine Learning, 86(3):335–367, 2012. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. ICLR, 2015. Kiryo, R., Niu, G., du Plessis, M. C., and Sugiyama, M. Positive-unlabeled learning with non-negative risk esti- mator. In Advances in Neural Information Processing Systems, pp. 1674–1684, 2017. Koyejo, O. O., Natarajan, N., Ravikumar, P. K., and Dhillon, I. S. Consistent binary classiﬁcation with generalized performance metrics. In Advances in Neural Information Processing Systems, pp. 2744–2752, 2014. LeCun, Y . The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. Lichman, M. et al. UCI machine learning repository, 2013. Liu, Z., Wang, Z., Liang, P. P., Salakhutdinov, R. R., Morency, L.-P., and Ueda, M. Deep gamblers: Learn- ing to abstain with portfolio theory. Advances in Neural Information Processing Systems, 32:10623–10633, 2019. Manwani, N., Desai, K., Sasidharan, S., and Sundararajan, R. Double ramp loss based reject option classiﬁer. In Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, pp. 151–163. Springer, 2015. Mozannar, H. and Sontag, D. Consistent estimators for learning to defer to an expert. ICML, 2020. Nair, V . and Hinton, G. E. Rectiﬁed linear units improve restricted boltzmann machines. In ICML, pp. 807–814, 2010. Ni, C., Charoenphakdee, N., Honda, J., and Sugiyama, M. On the calibration of multiclass classiﬁcation with re- jection. In Advances in Neural Information Processing Systems, pp. 2582–2592, 2019. Pang, B. and Lee, L. A sentimental education: Sentiment analysis using subjectivity summarization based on mini- mum cuts. arXiv preprint cs/0409058, 2004. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8026–8037, 2019. Pennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing, pp. 1532–1543, 2014. Pietraszek, T. Optimizing abstaining classiﬁers using roc analysis. In ICML, pp. 665–672, 2005.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Qin, J. Inferences for case-control and semiparametric two- sample density ratio models. Biometrika, 85(3):619–630, 1998. Ramaswamy, H. G., Tewari, A., Agarwal, S., et al. Con- sistent algorithms for multiclass classiﬁcation with an abstain option. Electronic Journal of Statistics , 12(1): 530–554, 2018. Reid, M. D. and Williamson, R. C. Composite binary losses. JMLR, 11:2387–2422, 2010. Saerens, M., Latinne, P., and Decaestecker, C. Adjusting the outputs of a classiﬁer to new a priori probabilities: a simple procedure. Neural computation, 14(1):21–41, 2002. Scott, C. Calibrated asymmetric surrogate losses. Electronic Journal of Statistics, 6:958–992, 2012. Steinwart, I. How to compare different loss functions and their risks. Constructive Approximation, 26(2):225–287, 2007. Sugiyama, M., Suzuki, T., and Kanamori, T. Density ratio estimation in machine learning. Cambridge University Press, 2012. Vapnik, V . Statistical learning theory. 1998 , volume 3. Wiley, New York, 1998. Vergara, A., Vembu, S., Ayhan, T., Ryan, M. A., Homer, M. L., and Huerta, R. Chemical gas sensor drift compen- sation using classiﬁer ensembles. Sensors and Actuators B: Chemical, 166:320–329, 2012. Xiao, H., Rasul, K., and V ollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. Yuan, M. and Wegkamp, M. Classiﬁcation methods with reject option based on convex risk minimization. JMLR, 11:111–130, 2010. Zhang, C., Wang, W., and Qiao, X. On reject and reﬁne op- tions in multicategory classiﬁcation. Journal of the Amer- ican Statistical Association, 113(522):730–745, 2018. Zhang, T. Statistical behavior and consistency of classiﬁca- tion methods based on convex risk minimization. Annals of Statistics, pp. 56–85, 2004.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation A. Related work In this section, we provide more discussion on the relationship of our work with Zhang et al. (2018) and Mozannar & Sontag (2020). A.1. Our work and Zhang et al. (2018) Zhang et al. (2018) considers to tackle classiﬁcation with rejection using angle-based classiﬁcation approach. Given xand y. Deﬁne yy = { (K−1)−1 2 1K−1 y= 1, −(1 +K 1 2 )/{(K−1) 3 2 }1K−1 + {K/(K−1)} 1 2 ey−1 2 ≤y≤K, where ey−1 denotes the one-hot vector with one at the index y−1 and 1K−1 ∈Rk−1 denotes a vector of all ones. Next, let a∈R be a positive scalar. Zhang et al. (2018) proposed the following bent hinge loss: Lhin ANGLE(g; x,y) = ∑ y′̸=y φhin ( −y⊤ y g(x) ) , where φhin(u) =    1 −au u< 0 1 −u 0 ≤u≤1, 0 otherwise . and the bent distance weighted discrimination loss: Ldwd ANGLE(g; x,y) = ∑ y′̸=y φhin ( −y⊤ y g(x) ) , where φdwd(u) =    1 −au u< 0 1 −u 0 ≤u≤0.5, 1 4u otherwise. Let Sδ(v) = sign(v) max(|v|−δ,0). After training a classiﬁer with their proposed loss function, the decision rule can be expressed as f(x; g) = { ® ∀j s.t. Sδ(y⊤ j g(x)) = 0, arg maxyy⊤ y g(x) otherwise . In their rejection rule, an input xis rejected if all binary classiﬁers’ outputs are close to zero. In our cost-sensitive approach, Cond. (7) rejects xas long as all gy(x)’s are negative, e.g.,xis also rejected if the all prediction outputs are much smaller than zero. Moreover, we do not have any hyperparameter in our rejection rule. There are two hyperparameters that are needed to be tuned for the angle-based method, which are a bending slope aand a rejection threshold δ2. Zhang et al. (2018) deﬁned the following quantities given the rejection cost cand the number of clases K: a1 = K−1 −c Kc−c , a 2 = (K−1)(1 −c) c . It was suggested that the choice of of acan be chosen by either a = a1 or a = a2. For δ, it needs to be tuned with the validation set with respect to the validation empirical zero-one-crisk. 2There is also a hyperparameter for determining the regularization strength, which is omitted here for brevity.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation It can be observed that our approach and Zhang et al. (2018) are different. The modiﬁcation of Zhang et al. (2018) is based on angle-based classiﬁcation while it is based on the cost-sensitive one-vs-rest loss in our case. Moreover, no additional hyperparameter is introduced and we do not modify the loss by bending it to be steeper (for the hinge loss). Note that the proposed bending scheme of Zhang et al. (2018) will lead any loss to be positively unbounded, including the symmetric losses, which may cause them to lose their favorable properties. Also, it is not straightforward to design a bending scheme for any loss to the best of our knowledge. In our approach, any classiﬁcation-calibrated loss can be straightforwardly applied in the surrogate loss in Deﬁnition 5. Finally, our Theorem 8 made it possible to transfer the excess risk bound from cost-sensitive classiﬁcation (Steinwart, 2007; Scott, 2012) to classiﬁcation with rejection. We are not aware of other works that discuss a theory that can explicitly show the excess risk bound relationship between cost-sensitive classiﬁcation and cost-sensitive classiﬁcation. It is worth noting that Zhang et al. (2018) also considered a different setting called classiﬁcation from reﬁne options, where a classiﬁer is also allowed to predict a set of labels instead of one label. f(x; g) =    ® ∀j s.t. Sδ(y⊤ j g(x)) = 0, {j : Sδ(y⊤ j g(x)) >0} ∃j s.t. Sδ(y⊤ j g(x)) >0, {j : Sδ(y⊤ j g(x)) = 0} otherwise. (14) It can be observed that the second condition in Eq. (14), i.e., ∃j s.t. Sδ(y⊤ j g(x)) >0 is similar to our Cond. (8), which occurs when a classiﬁer wants to predict more than one classes. Nevertheless, in our problem, it is not allowed to predict a set and we propose to reject a data point in this scenario. It is also interesting to explore the problem of learning with reﬁne options with our proposed cost-sensitive approach and we leave it for future work. A.2. Our work and Mozannar & Sontag (2020) Recently, Mozannar & Sontag (2020) has proposed a method for classiﬁcation with rejection based on a reduction to cost-sensitive learning. However, the reduction scheme is different to ours since they proposed to augment a rejection class in the model and the loss choice is ﬁxed to the cross-entropy loss. The main idea is to augment a rejection class K+ 1in the score function g. Rejection will be made if the maximum score of gis at index K+ 1. Given the rejection cost cand a score function g: Rd →RK+1, the loss function proposed by Mozannar & Sontag (2020) can be expressed as Lc DEFER(g; x,y) = log ( exp(gy(x))∑K+1 j=1 exp(gj(x)) ) + (1−c) log ( exp(gK+1(x))∑K+1 j=1 exp(gj(x)) ) . It can be seen that our cost-sensitive approach gives a different form of loss function, that is, their loss function is not a special case of our cost-sensitive approach and vice versa. Moreover, the theoretical analysis in Mozannar & Sontag (2020) is based on analyzing this speciﬁc loss. Unlike our work, it may not be straightforward to borrow the theory of cost-sensitive classiﬁcation (Steinwart, 2007; Scott, 2012) to justify the theoretical properties of a general surrogate loss function for classiﬁcation with rejection in their approach. It is worth pointing out that one advantage of the loss function proposed by Mozannar & Sontag (2020) over our approach is that it is applicable to the situation where the rejection cost can be different for each x(see Mozannar & Sontag (2020) for more detail). Nevertheless, in our problem setting, the rejection cost is assumed to be a constant. B. Proofs In this section, we provide proofs for the theoretical results in the main body. B.1. Proof of Proposition 4 Based on the following Chow’s rule: f∗(x) = { ® maxyηy(x) ≤1 −c, arg maxyηy(x) otherwise. It is straightforward to see that we can mimic Chow’s rule by only knowing whether ηy(x) ≤1 −c for all y∈Y, (15)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation and arg max y ηy(x). This is because if Condition (15) is true, then a classiﬁer refrains from making a prediction, otherwise a classiﬁer predicts a class arg maxyηy(x), which matches Chow’s rule. To verify whether ηy(x) ≤1 −cfor a class y, it sufﬁces to learn a cost-sensitive binary classiﬁer where α = 1−cto classify between a target class yand other classes (i.e., one-versus-rest classiﬁer), as suggested in Deﬁnition 2. We deﬁne such optimal cost-sensitive binary classiﬁer as f∗,y 1−c. As a result, we can construct K cost-sensitive binary classiﬁcation problems where α= 1−cto verify Condition (15), that is, Condition (15) is true if and only if f∗,y 1−c = −1 for all y∈Y. Next, we show that if Condition (15) is false based on learning K cost-sensitive binary classiﬁers, then it is sufﬁcient to verify arg maxyηy(x). This is because of the rejection cost cis less 0.5. Thus, if Condition (15) is false, the only possibility is that there must exists one y′such that η′ y(x) >1 −c. The reason it can have at most one y′to have f∗,y′ 1−c = 1is because it indicates that η′ y >0.5. Thus, the following rule can mimic Chow’s rule. f∗(x) = { ® maxyf∗,y 1−c(x) =−1, arg maxyf∗,y 1−c(x) otherwise . This concludes the proof. Remark: we note that if Condition (15) is true, it may not be possible to know arg maxyηy(x) given K optimal cost- sensitive binary classiﬁers in general. However, in classiﬁcation with rejection, it is not important to know arg maxyηy(x) if Condition (15) is true since a classiﬁer will refrain from making a prediction. B.2. Proof of Theorem 7 Let g∗be a conditional risk minimizer that minimizes the pointwise conditional surrogate risk: g∗(x) = arg min g WLc,φ CS ( g(x); η(x))) Recall that the cost-sensitive surrogate loss is deﬁned as Lc,φ CS (g; x,y) =cφ ( gy(x) ) + (1−c) ∑ y′̸=y φ ( −gy′ (x) ) . Thus, WLc,φ CS ( g(x); η(x))) = ∑ y∈Y ηy(x)Lc,φ CS ( g; x,y ) = ∑ y∈Y ηy(x)  cφ ( gy(x) ) + (1−c) ∑ y′̸=y φ ( −gy′ (x) )  . (16) We can rewrite Eq. (16) as follows based on the perspective ofgy: WLc,φ CS ( g(x); η(x))) = ∑ y∈Y ηy(x)  cφ ( gy(x) ) + (1−c) ∑ y′̸=y φ ( −gy′ (x) )  . = ∑ y∈Y [ ηy(x)cφ ( gy(x) ) + (1−ηy(x))(1 −c)φ ( −gy(x) )] .Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation It can be seen that ηy(x)cφ ( gy(x) ) + (1−ηy(x))(1 −c)φ ( −gy(x) ) is a pointwise conditional risk of a cost-sensitive binary classiﬁer gy. Thus, minimizing WLc,φ CS can be viewed as independently minimizing the pointwise conditional risk in cost-sensitive binary classiﬁcation for each gy. Thus g∗ y corresponds to the conditional risk minimizer of the cost-sensitive binary classiﬁcation where yis a positive class and y′̸= yis a negative class. Recall the deﬁnition of f(x; g∗): f(x; g∗) =    ® maxyg∗ y(x) ≤0, ® ∃y,y′s.t. y̸= y′ and g∗ y(x),g∗ y′ (x) >0, arg maxyg∗ y(x) otherwise and f∗(x): f∗(x) = { ® maxyηy(x) ≤1 −c, arg maxyηy(x) otherwise. First, we prove that f(x; g∗) =f∗(x) if φis classiﬁcation-calibrated. The proof is based on the deﬁnition of α-classiﬁcation calibration (α-CC) proposed by (Scott, 2012) and its relationship with ordinary classiﬁcation calibration (Bartlett et al., 2006), which is equivalent to 0.5-CC. More speciﬁcally, it is known that a margin loss φmust also be classiﬁcation-calibrated if it is α-CC, i.e., its conditional risk minimizer matches the Bayes optimal classiﬁer of cost-sensitive binary classiﬁcation when using the weighted risk minimization based on α(Scott, 2012). For a classiﬁcation-calibrated margin loss φ, sign(g∗ y) matches the Bayes optimal solution of the cost-sensitive binary classiﬁcation (Scott, 2012), that is, g∗ y(x) > 0 if ηy(x) > 1 −cand g∗ y(x) < 0 otherwise. Thus if g∗is obtained, the condition ∃y,y′s.t. y̸= y′and g∗ y(x),g∗ y′ (x) >0 (17) is impossible to occur since it is impossible to have g∗ y(x) >0 and g∗ y′ >0 simultaneously (because that can occur only if 1 −c< 0.5 which is impossible if c< 0.5.). Thus, it sufﬁces to look at f(x; g∗) = { ® maxyg∗ y(x) ≤0, arg maxyg∗ y(x) otherwise . maxyg∗ y(x) ≤0 indicates that ηy(x) ≤1 −cfor all y ∈Y and thus coincides with the rejection criterion of Chow’s rule. On the other hand, if maxyg∗ y(x) >0, then there exists only one ysuch that g∗ y(x) >0 and ηy(x) >1 −c. Thus, f(x; g∗) =f∗(x) if φis classiﬁcation-calibrated. Next, we prove the converse of our theorem, that is, if a margin lossφis not classiﬁcation-calibrated, then there must exist the case where g*w disagrees with Chow’s rule, whereg*w denotes an optimal solution for φthat is not classiﬁcation-calibrated. Here, let us drop xand only concerns η, which is a probability simplex for simplicity, which sufﬁces to prove our statement. If a margin lossφis not classiﬁcation-calibrated, all sign(g*w y ) does not match the Bayes-optimal solution of the cost-sensitive classiﬁer with respect to the rejection cost c, which suggests that there exists ηthat makes at least one g*w y (x) has the wrong sign compared with g∗ y(x). We divide our analysis of ηinto two cases. First, we analyze the case where Chow’s rule suggests to predict the input with the most probable class, which is the case wheremaxyηy >1 −c. Second, we analyze the case where Chow’s rule suggests to refrain from making a prediction, i.e., maxyηy ≤1 −c. Note that both cases cover all possibilities of η. Moreover, we note that if f(x; g*w) disagrees with Chow’s rule in at least one of the cases, it sufﬁces to prove thatg*w does not achieve calibration in classiﬁcation with rejection. Case 1: maxyηy >1 −c In this case, Chow’s rule suggests to accept and predict the most probable classarg maxyηy. It is straightforward to see that for a decision rule f(x; g) to match Chow’s rule in this case, the sign of allgy must match the Bayes optimal classiﬁer of binary cost-sensitive classiﬁcation g∗ y, that is gy >0 only for ηy >1 −cand g′ y <0 for otherClassiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation less probable classes. Based on f(x; g), this is the only possible conﬁguration of gto have the same decision as Chow’s rule in the case where maxyηy >1 −c, which is predicting the most probable class arg maxyηy. Thus, if the disagreement between of g*w and g∗arises in the case where maxyηy >1 −c, g*w must lead either predicting the wrong class (i.e., not the most probable class) or refrain from making a prediction, which both cases disagree with Chow’s rule and lead to higher zero-one-crisk. Thus, it sufﬁces to show that if φis not classiﬁcation-calibrated and the disagreement occurs when maxyηy >1 −c, then Lc,φ CS is not calibrated. Case 2: maxyηy ≤1 −c In this case, Chow’s rule suggests to refrain from making a prediction. We will show that if g*w agrees with Chow’s rule for all ηthat lie in Case 1, there must exist ηin Case 2 such that the decision of g∗w disagrees with Chow’s rule if no further restriction such as the number of classes is imposed. If f(x; g∗w) =f∗(x) everywhere in Case 1, then it is guaranteed thatg∗w y <0 for ηy <c, that is, g∗w y must predict negative correctly when maxyηy >1 −cand only one g∗w y >0 for ηy >1 −cfor all y∈Y. We will make use of these conditions to show that f(x; g∗w) will wrongly accept the data while Chow’s rule suggests to refrain from making a prediction. In this case where maxyηy ≤1 −c, there must exist ηsuch that at least one y, we have gy >0 although ηy ≤1 −cto make the sign of g∗w disagrees with g∗. Let β ≤1 −cbe a value that makes gy >0. There exists ηsuch that ηy = βfor one yand ηy′ ≤cfor all other classes. Since c> 0, it is always possible to make ∑ y∈Yηy = 1with a sufﬁcient number of classes. Thus, we have gy >0 where ηy = βand gy′ <0 for all other classes. This makes the decision of f(x; g∗w) to accept and predict the data while Chow’s rule suggests to refrain from making a prediction, which means Lc,φ CS is not calibrated. In summary, if the disagreement occurs in Case 1 , it sufﬁces to say Lc,φ CS is not calibrated. If disagreement does not occur in Case 1, there exists ηthat makes f(x; g∗w) disagrees with Chow’s rule. This concludes the proof of the converse case of our theorem. As a result, the surrogate loss Lc,φ CS is calibrated for classiﬁcation with rejection, that is, f(x; g∗) =f∗(x) for all x∈X, if and only if φis classiﬁcation-calibrated. This concludes the proof. B.3. Proof of Theorem 8 To prove that Rℓ01c(f) −Rℓ01c,∗≤ K∑ i=1 ψ−1 φ,1−c(Rφ,i 1−c(gi) −Rφ,i,∗ 1−c ), we divide the proof into two steps. The ﬁrst step is to prove that Rℓ01c(f) −Rℓ01c,∗≤RLc,ℓ01 CS (g) −RLc,ℓ01 CS ,∗ (18) and the second step is to prove that RLc,ℓ01 CS (g) −RLc,ℓ01 CS ,∗≤ K∑ i=1 ψ−1 φ,1−c(Rφ,i 1−c(gi) −Rφ,i,∗ 1−c ). (19) Proof of Ineq. (18): To prove this inequality, it sufﬁces to prove thatRℓ01c(f) ≤RLc,ℓ01 CS (g) and Rℓ01c,∗= RLc,ℓ01 CS ,∗. To prove that Rℓ01c(f) ≤RLc,ℓ01 CS (g), it sufﬁces to show that ℓ01c(f(x; g),y) ≤Lc,ℓ01 CS (g; x,y) holds for any choices of x∈X and y∈Y. Thanks to the discrete nature of both the zero-one-closs ℓ01c and the zero-one loss ℓ01, case analysis can be applied.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Case 1: ℓ01c(f(x; g),y) = 0 In this case, it suggests that f(x; g) predicts a label that matches a label y. This is possible only if gy >0 and gy′ <0 for y′̸= y. Recall the deﬁnition of the cost-sensitive surrogate loss: Lc,ℓ01 CS (g; x,y) =cℓ01 ( gy(x) ) + (1−c) ∑ y′̸=y ℓ01 ( −gy′ (x) ) . (20) It can be seen that Lc,ℓ01 CS can only be larger or equal to zero, that is , Lc,ℓ01 CS (g; x,y) ≥0. Thus, ℓ01c(f(x; g),y) ≤ Lc,ℓ01 CS (g; x,y) if ℓ01c(f(x; g),y) = 0. Nevertheless, we can show that they are in fact equal, i.e., Lc,ℓ01 CS (g; x,y) = 0. This is because cℓ01 ( gy(x) ) = 0and (1 −c)ℓ01 ( −gy′ (x) ) = 0. Thus, Lc,ℓ01 CS (g; x,y) = 0according to Eq. (20). Therefore, we have ℓ01c(f(x; g),y) =Lc,ℓ01 CS (g; x,y) = 0. Case 2: ℓ01c(f(x; g),y) = 1In this case, it can be shown that ℓ01c(f(x; g),y) =Lc,ℓ01 CS (g; x,y) = 1. It can be seen that f(x; g) wrongly predicts the label, which only occurs when gy′ (x) >0 where y′̸= yand gy′′ (x) <0 for y′′ ̸= y′, which also includes the correct label. Therefore, cℓ01 ( gy(x) ) = c, (1 −c)ℓ01 ( −gy′ (x) ) = 1−c, and (1−c)ℓ01 ( −gy′′ (x) ) = 0in the case wherey′′̸= yand y′′̸= y′. As a result, the sum of the penalty becomesc+(1 −c) = 1, which makes ℓ01c(f(x; g),y) =Lc,ℓ01 CS (g; x,y) = 1. Case 3: ℓ01c(f(x; g),y) =c Unlike the previous two cases, the bound can be loose in the case where f(x; g) decides to refrain from making a prediction. f(x; g) =cindicates that a decision rule decides to reject. This is possible only ifg∗ y(x) <0 for all y∈Y or Condition (17) holds. If g∗ y(x) <0 for all y∈Y, then Lc,ℓ01 CS (g∗; x,y) =cbecause cℓ01 ( gy(x) ) = cand (1 −c)ℓ01 ( −gy′ (x) ) = 0. If Condition (17) is true, we can show that Lc,ℓ01 CS (g∗; x,y) ≥c. We will show by using the fact that the minimum possible value of Lc,ℓ01 CS (g∗; x,y) when having a conﬂict is 1 −c. This is when there exists gy(x) > 0 and gy′ (x) > 0, where y is a correct label and y′is a wrong label. In this case, cℓ01 ( gy(x) ) = 0, (1 −c)ℓ01 ( −gy′ (x) ) = 1−cand (1 −c)ℓ01 ( −gy′′ (x) ) = 0for y′′̸= y′and y′′̸= y. If the conﬂicts of only two classiﬁers occur and both give the wrong labels, then we the penalty is 2(1 −c) +c. More conﬂicts only gain the higher penalty or nothing (if the conﬂict comes from the correct class), thus Lc,ℓ01 CS (g∗; x,y) ≥1 −c≥c. Therefore, Rℓ01c(f) ≤RLc,ℓ01 CS (g). Next, to prove that RLc,ℓ01 CS ,∗= RLc,ℓ01 CS ,∗, it sufﬁces to show that ℓ01c(f(x; g∗),y) =Lc,ℓ01 CS (g∗; x,y) for any choices of x∈X and y∈Y. Case 1: ℓ01c(f(x; g∗),y) = 0 From the previous analysis, in this case we have ℓ01c(f(x; g),y) =Lc,ℓ01 CS (g; x,y) = 0, for any g. Therefore, it must also hold when g= g∗. Case 2: ℓ01c(f(x; g∗),y) = 1 From the previous analysis, in this case we have ℓ01c(f(x; g),y) =Lc,ℓ01 CS (g; x,y) = 1, for any g. Therefore, it must also hold when g= g∗.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Case 3: ℓ01c(f(x; g∗),y) = cAs suggested in the proof of Theorem 7 that Condition (17) is impossible to occur for f(x; g∗). Thus, if ℓ01c(f(x; g∗) rejects, it means that g∗ y(x) <0 for all y∈Y. This makes Lc,ℓ01 CS (g∗; x,y) =cbecause cℓ01 ( gy(x) ) = cand (1 −c)ℓ01 ( −gy′ (x) ) = 0. Since ℓ01c(f(x; g∗),y) =Lc,ℓ01 CS (g∗; x,y) always holds, Rℓ01c,∗= RLc,ℓ01 CS ,∗. We have proven thatRℓ01c(f) ≤RLc,ℓ01 CS (g) and Rℓ01c,∗= RLc,ℓ01 CS ,∗. Thus, Ineq. (18) holds. Proof of Ineq. (19): In this part, the proof no longer involves with classiﬁcation with rejection but the well-studied cost-sensitive classiﬁcation. We borrow the existing result by Scott (2012) to prove this part. Recall the following pointwise conditional risk: WLc,φ CS ( g(x); η(x))) = ∑ y∈Y ηy(x)  cφ ( gy(x) ) + (1−c) ∑ y′̸=y φ ( −gy′ (x) )  . = ∑ y∈Y [ ηy(x)cφ ( gy(x) ) + (1−ηy(x))(1 −c)φ ( −gy(x) )] also holds when φ = ℓ01. This suggests that the risk of the cost-sensitive surrogate equals to the sum of the pointwise surrogate risks of Kcost-sensitive binary classiﬁcation problem, i.e., RLc,ℓ01 CS (g) =∑K i=1 Rℓ01,i 1−c(gi) for any g. Thus, we have RLc,ℓ01 CS (g) −RLc,ℓ01 CS ,∗= K∑ i=1 Rℓ01,i 1−c(gi) −Rℓ01,i,∗ 1−c . Next, since φis classiﬁcation-calibrated, Scott (2012) proved that there exists ψφ,1−c: R →R, which is a non-decreasing invertible function and ψφ,1−c(0) = 0such that Rℓ01,i 1−c(gi) −Rℓ01,i,∗ 1−c ≤ψ−1 φ,1−c(Rφ,i 1−c(gi) −Rφ,i,∗ 1−c ) By adding the excess risk bound of Kcost-sensitive binary classiﬁcation problems, we have Rℓ01c(f) −Rℓ01c,∗≤ K∑ i=1 ψ−1 φ,1−c(Rφ,i 1−c(gi) −Rφ,i,∗ 1−c ). Thus, Ineq. (19) holds. Since Ineq. (18) and Ineq. (19) hold, we concludes the proof of the excess risk bound of classiﬁcation with rejection based on cost-sensitive classiﬁcation for a general classiﬁcation-calibrated loss.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 2.Speciﬁcation of benchmark datasets: the number of features, the number of classes, the number of data. Name #features #classes #data Gisette 5000 2 7000 Phishing 30 2 11050 Spambase 57 2 4601 Subj 100 2 10000 Twonorm 20 2 7400 Gas-Drift 128 6 13910 HAR 561 6 10299 MNIST 28×28 10 70000 Fashion-MNIST 28×28 10 70000 KMNIST 28×28 10 70000 C. Experiment Details In this section, we provide more information on datasets and implementation details. C.1. Datasets We used the train and test data for MNIST, Fashion-MNIST, and KMNIST for training and testing, respectively. For MNIST, Fashion-MNIST, and KMNIST, we used the same test data as the one provided for testing for those datasets. For hyperparameter selection, we split ten percent of the training data to use as validation data (i.e., 6000). For the other ninety percent of training data, they were used for training all methods. Note that only ANGLE and SCE used validation data. For the datasets other than MNIST, Fashion-MNIST, and KMNIST, we randomly used half of the dataset for training all methods. For clean-labeled and noisy-labeled classiﬁcation, we used ten percent of all data for validation and the other fourty percent were used for testing. For PU-classiﬁcation, we used twenty percent of all data for validation and the other thirty percent were used for testing. Again, note that only ANGLE and SCE used validation data. Table 2 shows the speciﬁcation of the benchmark datasets used in this paper. Subj was preprocessed by using100-dimensional GloVe mean word embedding (Pennington et al., 2014). C.2. Implementation details We used a linear-in-input model for all binary classiﬁcation datasets. For MNIST, Fashion-MNIST, KMNIST, we used the same convolutional neural network (CNN) architecture. The CNN model consists of a sequence of two convolutional layers with 32 channels and two convolutional layers with 64 channels, followed by a max pooling layer and two linear layers with dimension 128. The kernel size of convolutional layers is 3, and the kernel size of max pooling layer is 2. Dropout with probability 0.5 is used between two linear layers. We used rectiﬁer linear units (ReLU) (Nair & Hinton, 2010) as the non-linear activation function. For HAR and Gas-Drift, we used the one hidden layer multilayer perceptron as a model (d−64 −1). We also applied batch normalization (Ioffe & Szegedy, 2015) at the ﬁnal layer to stabilize training. The objective functions were optimized using Adam (Kingma & Ba, 2015). The experiment code for implementing a model was written using PyTorch (Paszke et al., 2019). We ran 10 trials for each experiment. C.2.1. C LEAN -LABELED AND NOISY -LABELED CLASSIFICATION Data generation process For noise labels, the noise rate was 0.25, i.e., 25% of labels are randomly ﬂipped into other classes. No data augmentation was used for all experiments. Hyperparameters For all experiments, learning rate was set to 0.001, batch size was 256. The model was trained for 10 epochs for the convolutional neural networks and 100 epochs for both the linear-in-parameter model and multilayer perceptron.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation C.2.2. PU- CLASSIFICATION Problem setting PU-classiﬁcation considers situations when only positive and unlabeled data are available. We denote class conditional densities by p+(x) = p(x|y = +1) and p−(x) = p(x|y = −1) and the class prior probability by π= p(y= +1). Then the distribution for unlabeled data can be expressed as p(x) =πp+(x) + (1−π)p−(x). Let L(g; x,y) be a loss function. It is known that the expected classiﬁcation risk can be expressed as (du Plessis et al., 2015): E (x,y)∼p(x,y) [L(g; x,y)] =πEx∼p+(x) [L(g; x,+1)] −πEx∼p+(x) [L(g; x,−1)] +Ex∼p(x) [L(g; x,−1)] . Then, the unbiased risk estimator given positive examples {xp i}np i=1 i.i.d. ∼ p+(x) and unlabeled examples {xu j}nu j=1 i.i.d. ∼ p(x) can be expressed as π np np∑ i=1 [L(g; x,+1)] − π np np∑ i=1 [L(g; x,−1)] + 1 nu nu∑ j=1 [L(g; x,−1)] . (21) However, Kiryo et al. (2017) suggested that Eq.(21) is prone to overﬁtting because gmay treat all unlabeled data as negative to minimize the empirical risk. As a result, it was suggested to instead minimize the following empirical risk: ˆRL PU(g) = π np np∑ i=1 [L(g; x,+1)] + max  0, 1 nu nu∑ j=1 [L(g; x,−1)] − π np np∑ i=1 [L(g; x,−1)]  , (22) which is known to be a biased but still consistent estimator. With Eq.(22), the choice of loss functions is ﬂexible and we can easily apply any loss function to learn a classiﬁer with rejection, e.g., our cost-sensitive approach can be easily applied in PU-classiﬁcation by minimizing ˆR Lc,φ CS PU (g), which equals to solving two cost-sensitive PU-classiﬁcation. We refer the readers to du Plessis et al. (2014; 2015); Kiryo et al. (2017) for more detail about PU-classiﬁcation and Charoenphakdee & Sugiyama (2019) for more detail about cost-sensitive PU-classiﬁcation. Data generation process PU-classiﬁcation needs two set of data: positive data and unlabeled data. We ﬁrst decided the size of unlabeled data to be around the size of original training data, but truncated to be divisible by 200. Then, the size of positive data was set to be 1 5 of the size of unlabeled data. After deciding the size of two sets, we than sampled from the original training data. The positive data for PU is sampled from the original positive training data without replacement. Then, from the left positive training data and negative data, we sampled without replacement for unlabeled data according to the value of class prior. Hyperparameters The class prior for the positive class is set to be 0.7 throughout all PU-classiﬁcation experiments. Learning rate was set to 0.001, batch size was 64, and the number of epochs was 100. D. Additional Experiment Results In this section, we report full experimental results in a table format with varying rejection costs for clean-labeled classiﬁcation, noisy-labeled classiﬁcation, and PU-classiﬁcation, respectively. We also provide more discussion on our proposed rejection conditions, i.e., Conds. (7) and (8). D.1. Full experimental results in a table format We report the mean and standard error over ten trials of the test empirical zero-one-crisk, rejection ratio, and test error on the non-rejected data.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation TABLE INDEX : • Tables 3, 4 and 5: Test empirical 0-1-crisk, classiﬁcation error on accepted data, and rejection ratio for clean-labeled binary classiﬁcation with rejection, respectively. • Tables 6, 7 and 8: Test empirical 0-1-crisk, classiﬁcation error on accepted data, and rejection ratio for noisy-labeled binary classiﬁcation with rejection, respectively. • Tables 9, 10 and 11: Test empirical 0-1-crisk, classiﬁcation error on accepted data, and rejection ratio for positive- unlabeled binary classiﬁcation with rejection, respectively. • Tables 12, 13 and 14: Test empirical0-1-crisk, classiﬁcation error on accepted data, and rejection ratio for clean-labeled multiclass classiﬁcation with rejection, respectively. • Tables 15, 16 and 17: Test empirical0-1-crisk, classiﬁcation error on accepted data, and rejection ratio for noisy-labeled multiclass classiﬁcation with rejection, respectively.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 3.Mean and standard error of 0-1-crisk of the clean-labeled binary classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gisette 0.10 2.20(0.10) 5 .33(0.16) 2 .41(0.16) 2 .47(0.14) 2 .35(0.09) 0.15 2.72(0.10) 6 .41(0.34) 2 .94(0.32) 2 .69(0.13) 2 .65(0.11) 0.20 3.10(0.09) 7 .43(0.22) 3 .25(0.23) 2 .82(0.13) 2 .78(0.09) 0.25 3.36(0.16) 8 .53(0.25) 3 .59(0.43) 3 .04(0.10) 3 .09(0.16) 0.30 3.72(0.20) 9 .68(0.30) 3 .69(0.22) 3 .19(0.16) 3 .15(0.11) 0.35 3.95(0.30) 10 .46(0.19) 3 .79(0.24) 3 .41(0.16) 3 .25(0.07) 0.40 4.27(0.17) 11 .31(0.29) 3 .70(0.13) 3 .52(0.15) 3 .38(0.21) Phishing 0.10 4.27(0.31) 5 .49(0.02) 4 .02(0.27) 3 .59(0.09) 4 .28(0.15) 0.15 5.70(0.31) 7 .53(0.05) 5 .18(0.09) 4 .84(0.04) 5 .37(0.13) 0.20 6.47(0.09) 9 .26(0.06) 5 .88(0.16) 5 .73(0.06) 6 .34(0.09) 0.25 7.24(0.38) 10 .57(0.08) 6 .25(0.07) 6 .12(0.07) 7 .17(0.14) 0.30 7.82(0.35) 11 .32(0.03) 6 .78(0.22) 6 .64(0.11) 7 .72(0.13) 0.35 8.41(0.58) 11 .72(0.06) 7 .13(0.15) 7 .07(0.07) 7 .99(0.23) 0.40 9.10(0.75) 12 .01(0.06) 7 .41(0.20) 7 .30(0.04) 8 .30(0.13) Spambase 0.10 5.84(0.24) 7 .06(0.08) 6 .00(0.95) 5 .61(0.14) 6 .78(0.21) 0.15 7.08(0.35) 8 .93(0.08) 7 .07(0.46) 6 .95(0.21) 7 .99(0.16) 0.20 8.49(0.44) 10 .28(0.19) 7 .92(0.68) 7 .79(0.21) 8 .82(0.25) 0.25 9.43(0.65) 11 .32(0.24) 8 .42(0.22) 8 .34(0.14) 9 .00(0.32) 0.30 9.87(0.78) 12 .07(0.35) 8 .98(0.26) 8 .55(0.14) 9 .13(0.21) 0.35 10.92(1.16) 12 .43(0.40) 9 .21(0.28) 8 .79(0.22) 9 .40(0.20) 0.40 11.58(1.12) 12 .04(0.29) 9 .15(0.26) 9 .01(0.17) 9 .64(0.16) Subj 0.10 7.02(0.19) 6 .96(0.08) 6 .50(0.33) 6 .40(0.08) 8 .17(0.10) 0.15 8.39(0.26) 9 .56(0.04) 8 .11(0.23) 7 .97(0.07) 9 .21(0.13) 0.20 9.71(0.21) 11 .70(0.06) 9 .25(0.07) 9 .27(0.09) 9 .98(0.11) 0.25 10.96(0.20) 13 .46(0.07) 10 .35(0.36) 10 .26(0.11) 10 .60(0.06) 0.30 12.07(0.17) 14 .92(0.09) 10 .96(0.15) 10 .86(0.08) 11 .13(0.08) 0.35 13.09(0.19) 15 .91(0.07) 11 .38(0.20) 11 .33(0.10) 11 .48(0.07) 0.40 13.59(0.27) 16 .35(0.09) 11 .61(0.11) 11 .61(0.09) 11 .70(0.08) Twonorm 0.10 1.43(0.06) 5 .10(0.12) 1 .30(0.05) 1 .34(0.07) 1 .36(0.06) 0.15 1.86(0.10) 7 .03(0.18) 1 .73(0.22) 1 .62(0.07) 1 .58(0.06) 0.20 2.14(0.12) 8 .51(0.13) 1 .81(0.12) 1 .82(0.05) 1 .82(0.08) 0.25 2.47(0.22) 9 .93(0.18) 2 .05(0.08) 1 .99(0.05) 2 .01(0.10) 0.30 2.55(0.22) 10 .63(0.26) 2 .17(0.13) 2 .08(0.06) 2 .11(0.07) 0.35 2.67(0.19) 11 .13(0.11) 2 .38(0.15) 2 .23(0.09) 2 .26(0.08) 0.40 2.77(0.24) 11 .08(0.17) 2 .41(0.15) 2 .26(0.06) 2 .34(0.09)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 4.Mean and standard error of 0-1 risk for accepted data of the clean-labeled binary classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gisette 0.10 1.34(0.19) 3 .80(0.24) 1 .34(0.23) 2 .16(0.16) 2 .10(0.09) 0.15 1.60(0.33) 3 .58(0.41) 1 .73(0.68) 2 .21(0.15) 2 .32(0.13) 0.20 1.38(0.06) 3 .48(0.26) 2 .16(0.67) 2 .20(0.13) 2 .38(0.10) 0.25 1.67(0.33) 3 .43(0.33) 2 .35(0.83) 2 .27(0.12) 2 .66(0.18) 0.30 1.59(0.28) 3 .56(0.30) 2 .48(0.63) 2 .29(0.18) 2 .70(0.12) 0.35 1.81(0.37) 3 .39(0.19) 3 .29(0.51) 2 .32(0.11) 2 .81(0.07) 0.40 1.79(0.30) 3 .19(0.13) 2 .48(0.47) 2 .38(0.14) 2 .97(0.18) Phishing 0.10 2.75(0.45) 0 .33(0.05) 2 .23(0.79) 1 .26(0.23) 2 .73(0.29) 0.15 3.00(0.88) 0 .43(0.03) 2 .57(1.14) 2 .31(0.10) 3 .03(0.21) 0.20 3.40(0.56) 0 .72(0.04) 4 .03(0.95) 3 .37(0.13) 3 .66(0.15) 0.25 3.56(0.46) 1 .05(0.06) 4 .26(0.58) 3 .96(0.11) 4 .32(0.08) 0.30 4.40(0.81) 1 .51(0.07) 5 .08(1.05) 4 .80(0.25) 4 .89(0.11) 0.35 3.78(0.86) 1 .89(0.07) 5 .61(1.14) 5 .78(0.14) 5 .47(0.21) 0.40 4.72(0.84) 2 .55(0.08) 5 .94(1.19) 6 .44(0.04) 6 .44(0.18) Spambase 0.10 3.97(0.73) 4 .67(0.09) 4 .96(1.54) 2 .87(0.46) 5 .46(0.36) 0.15 4.48(0.55) 4 .90(0.15) 5 .87(1.11) 3 .63(0.40) 5 .65(0.27) 0.20 4.39(0.73) 5 .31(0.23) 6 .02(1.92) 4 .87(0.29) 6 .51(0.33) 0.25 4.41(0.82) 5 .70(0.27) 7 .15(1.11) 5 .79(0.19) 6 .56(0.25) 0.30 5.26(0.90) 6 .16(0.27) 8 .34(0.83) 6 .14(0.20) 6 .94(0.31) 0.35 5.31(0.73) 6 .55(0.22) 8 .67(0.80) 6 .95(0.14) 7 .53(0.25) 0.40 5.13(0.66) 6 .99(0.26) 7 .93(1.07) 7 .61(0.22) 8 .31(0.15) Subj 0.10 5.77(0.33) 2 .72(0.17) 4 .56(1.01) 4 .78(0.14) 7 .87(0.12) 0.15 5.76(0.45) 3 .15(0.10) 5 .58(1.02) 5 .76(0.11) 8 .40(0.18) 0.20 6.37(0.31) 3 .43(0.07) 6 .26(0.71) 6 .77(0.16) 8 .78(0.14) 0.25 6.22(0.39) 3 .78(0.08) 7 .18(1.47) 7 .65(0.15) 9 .13(0.08) 0.30 6.73(0.38) 4 .25(0.11) 8 .70(0.92) 8 .30(0.11) 9 .56(0.12) 0.35 7.25(0.21) 4 .98(0.08) 9 .72(0.86) 9 .05(0.14) 9 .91(0.10) 0.40 7.59(0.35) 5 .58(0.17) 9 .85(0.93) 9 .87(0.09) 10 .33(0.07) Twonorm 0.10 0.82(0.21) 0 .00(0.00) 0 .68(0.20) 0 .81(0.12) 0 .97(0.10) 0.15 0.94(0.23) 0 .00(0.00) 0 .98(0.58) 0 .96(0.15) 1 .03(0.14) 0.20 1.03(0.32) 0 .01(0.02) 0 .98(0.30) 1 .04(0.12) 1 .13(0.09) 0.25 1.11(0.22) 0 .02(0.02) 1 .35(0.42) 1 .20(0.12) 1 .16(0.16) 0.30 1.00(0.14) 0 .03(0.02) 1 .73(0.52) 1 .31(0.07) 1 .25(0.13) 0.35 1.24(0.24) 0 .04(0.01) 1 .53(0.57) 1 .57(0.13) 1 .46(0.11) 0.40 1.33(0.21) 0 .05(0.02) 1 .74(0.45) 1 .70(0.09) 1 .67(0.14)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 5.Mean and standard error of rejection ratio of the clean-labeled binary classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gisette 0.10 9.88(1.63) 24 .67(0.66) 12 .25(3.40) 3 .97(0.34) 3 .22(0.15) 0.15 8.32(2.52) 24 .78(0.70) 8 .93(3.72) 3 .73(0.16) 2 .63(0.16) 0.20 9.20(0.28) 23 .91(0.59) 5 .97(3.66) 3 .51(0.28) 2 .26(0.14) 0.25 7.21(1.87) 23 .63(0.69) 5 .37(3.74) 3 .39(0.24) 1 .90(0.21) 0.30 7.50(1.36) 23 .12(0.39) 4 .37(2.37) 3 .24(0.17) 1 .65(0.16) 0.35 6.45(1.81) 22 .36(0.71) 1 .57(1.34) 3 .34(0.36) 1 .37(0.13) 0.40 6.48(1.02) 22 .08(0.88) 3 .24(1.24) 3 .03(0.18) 1 .13(0.13) Phishing 0.10 20.96(2.29) 53 .33(0.27) 22 .53(5.75) 26 .60(1.52) 21 .26(1.37) 0.15 22.15(4.73) 48 .74(0.33) 20 .37(7.23) 19 .94(0.54) 19 .57(0.63) 0.20 18.36(2.79) 44 .30(0.32) 11 .30(5.28) 14 .21(0.58) 16 .43(0.77) 0.25 17.14(3.03) 39 .75(0.33) 9 .54(2.57) 10 .29(0.34) 13 .82(0.73) 0.30 13.28(2.69) 34 .44(0.20) 6 .71(3.36) 7 .26(0.52) 11 .28(0.41) 0.35 14.71(4.00) 29 .69(0.22) 5 .05(3.38) 4 .42(0.31) 8 .51(0.45) 0.40 12.33(3.81) 25 .26(0.22) 4 .19(3.66) 2 .57(0.14) 5 .56(0.30) Spambase 0.10 30.11(8.00) 44 .83(1.27) 17 .92(9.93) 38 .36(2.72) 29 .17(1.39) 0.15 24.47(5.80) 39 .94(0.93) 12 .34(6.33) 29 .12(1.98) 25 .08(1.61) 0.20 26.01(6.07) 33 .83(1.26) 12 .27(10.00) 19 .31(1.51) 17 .12(1.13) 0.25 24.18(5.81) 29 .15(1.22) 6 .82(4.84) 13 .25(0.64) 13 .19(0.94) 0.30 18.44(5.74) 24 .81(1.14) 2 .86(2.56) 10 .09(0.71) 9 .49(0.62) 0.35 18.75(5.78) 20 .68(1.40) 2 .00(2.02) 6 .56(0.47) 6 .81(0.62) 0.40 18.41(4.40) 15 .29(0.71) 3 .69(3.53) 4 .33(0.43) 4 .18(0.30) Subj 0.10 29.46(1.47) 58 .17(0.59) 34 .30(8.36) 31 .08(0.73) 14 .22(0.33) 0.15 28.39(1.45) 54 .10(0.37) 26 .19(6.17) 23 .90(0.44) 12 .21(0.47) 0.20 24.50(1.23) 49 .89(0.24) 21 .56(3.69) 18 .89(0.53) 10 .67(0.38) 0.25 25.24(1.41) 45 .62(0.31) 17 .27(6.22) 15 .04(0.20) 9 .26(0.25) 0.30 22.92(1.36) 41 .41(0.25) 10 .45(3.31) 11 .81(0.21) 7 .67(0.26) 0.35 21.03(0.61) 36 .42(0.32) 6 .48(2.96) 8 .79(0.27) 6 .28(0.26) 0.40 18.51(1.44) 31 .28(0.35) 5 .75(2.82) 5 .79(0.19) 4 .62(0.12) Twonorm 0.10 6.57(1.62) 50 .96(1.17) 6 .59(2.19) 5 .73(0.71) 4 .28(0.61) 0.15 6.48(1.56) 46 .87(1.18) 5 .21(2.96) 4 .68(0.62) 3 .94(0.64) 0.20 5.82(1.49) 42 .55(0.62) 4 .37(1.74) 4 .12(0.48) 3 .66(0.52) 0.25 5.69(1.52) 39 .66(0.75) 2 .92(1.60) 3 .30(0.33) 3 .59(0.40) 0.30 5.34(0.95) 35 .36(0.85) 1 .50(1.54) 2 .68(0.23) 3 .01(0.42) 0.35 4.23(1.20) 31 .72(0.33) 2 .50(1.82) 1 .97(0.25) 2 .38(0.26) 0.40 3.72(1.08) 27 .62(0.43) 1 .73(1.41) 1 .48(0.14) 1 .74(0.20)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 6.Mean and standard error of 0-1-crisk of the noisy-labeled binary classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gisette 0.10 10.05(0.28) 21 .29(0.70) 10 .03(0.14) 23 .98(0.54) 7 .64(0.64) 0.15 14.40(0.45) 22 .97(0.42) 14 .89(0.61) 26 .03(0.54) 9 .09(0.60) 0.20 18.24(0.46) 24 .36(0.95) 19 .03(0.33) 27 .23(0.69) 11 .26(0.64) 0.25 22.30(0.81) 26 .03(0.74) 22 .60(0.82) 27 .76(0.93) 12 .76(0.76) 0.30 25.39(0.50) 27 .75(0.62) 25 .80(0.81) 28 .47(0.80) 13 .52(1.06) 0.35 28.26(1.12) 29 .18(0.63) 27 .69(0.84) 29 .00(0.79) 14 .91(0.70) 0.40 31.01(1.00) 31 .39(0.66) 28 .78(1.07) 29 .72(0.81) 15 .88(0.68) Phishing 0.10 6.54(0.41) 9 .12(0.17) 9 .29(1.18) 10 .00(0.00) 8 .55(3.22) 0.15 8.61(0.63) 12 .77(0.21) 13 .74(1.37) 15 .00(0.00) 11 .12(3.68) 0.20 10.05(1.09) 15 .46(0.36) 13 .80(2.40) 19 .21(1.21) 10 .52(3.41) 0.25 11.66(1.29) 17 .05(0.33) 9 .89(1.07) 21 .69(0.05) 8 .91(0.57) 0.30 13.03(1.37) 17 .22(0.35) 8 .17(0.39) 20 .41(3.14) 9 .47(0.84) 0.35 14.54(1.13) 15 .81(0.44) 8 .13(0.37) 10 .38(1.23) 9 .06(0.29) 0.40 16.03(2.26) 13 .35(0.56) 8 .94(0.75) 9 .93(0.34) 9 .02(0.30) Spambase 0.10 9.25(0.93) 9 .70(0.15) 9 .95(0.52) 8 .62(0.39) 7 .44(0.24) 0.15 11.55(0.82) 14 .07(0.24) 13 .61(1.28) 12 .27(0.45) 9 .43(0.32) 0.20 13.84(0.85) 17 .93(0.35) 14 .95(1.41) 15 .45(0.61) 11 .70(0.62) 0.25 15.91(1.26) 20 .32(0.59) 14 .21(1.39) 18 .50(0.76) 12 .62(0.74) 0.30 17.07(1.52) 21 .43(0.83) 12 .87(1.36) 20 .16(0.82) 13 .11(0.88) 0.35 17.92(1.93) 19 .56(0.97) 12 .95(0.90) 18 .74(1.57) 12 .06(0.60) 0.40 19.67(2.44) 17 .24(0.82) 13 .37(0.73) 14 .29(0.87) 10 .75(0.49) Subj 0.10 10.05(0.41) 9 .14(0.07) 10 .26(0.43) 10 .00(0.00) 7 .58(0.13) 0.15 12.38(0.43) 12 .99(0.15) 14 .19(1.22) 15 .00(0.00) 9 .00(0.20) 0.20 14.32(0.71) 16 .28(0.29) 16 .15(1.70) 20 .00(0.00) 10 .11(0.25) 0.25 16.03(0.89) 18 .70(0.32) 13 .63(0.98) 25 .00(0.00) 11 .04(0.27) 0.30 17.68(0.62) 19 .95(0.46) 13 .74(0.80) 23 .61(2.42) 11 .73(0.21) 0.35 19.74(0.84) 19 .97(0.40) 13 .81(0.67) 17 .25(0.52) 12 .12(0.17) 0.40 20.97(1.10) 19 .12(0.42) 13 .94(0.55) 14 .97(0.44) 12 .51(0.17) Twonorm 0.10 4.26(0.52) 8 .61(0.19) 8 .99(1.59) 10 .00(0.00) 1 .58(0.15) 0.15 5.57(1.07) 12 .38(0.54) 8 .13(3.57) 14 .97(0.05) 2 .06(0.19) 0.20 6.13(0.90) 14 .97(0.85) 7 .69(3.16) 18 .28(1.42) 2 .12(0.16) 0.25 7.62(1.51) 16 .31(0.60) 5 .49(0.53) 15 .55(4.13) 2 .25(0.09) 0.30 8.44(2.13) 15 .84(0.61) 3 .61(0.76) 7 .39(1.03) 2 .48(0.22) 0.35 9.16(1.54) 14 .33(1.26) 2 .94(0.37) 4 .46(0.37) 2 .45(0.11) 0.40 10.67(2.60) 11 .53(0.27) 2 .80(0.27) 3 .26(0.16) 2 .51(0.13)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 7.Mean and standard error of 0-1 risk for accepted data of the noisy-labeled binary classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gisette 0.10 10.46(2.00) 27 .33(1.04) 19 .21(21.10) 26 .18(0.67) 6 .99(0.81) 0.15 12.35(1.86) 27 .30(0.74) 12 .58(3.53) 27 .47(0.62) 7 .70(0.69) 0.20 11.51(1.58) 26 .77(1.47) 16 .13(2.20) 28 .05(0.76) 9 .33(0.78) 0.25 12.87(1.78) 26 .61(1.15) 17 .27(3.11) 28 .04(1.03) 10 .28(0.81) 0.30 13.11(1.15) 26 .50(0.96) 20 .47(4.24) 28 .32(0.88) 10 .71(1.16) 0.35 13.96(2.14) 25 .88(1.04) 24 .33(2.90) 28 .47(0.86) 11 .79(0.75) 0.40 15.22(1.47) 26 .31(1.17) 25 .84(3.05) 28 .90(0.85) 12 .80(0.66) Phishing 0.10 4.67(0.77) 0 .07(0.20) 12 .13(18.57) 0 .00(0.00) 7 .60(9.63) 0.15 5.61(1.03) 0 .08(0.13) 8 .77(5.76) 0 .00(0.00) 7 .18(7.92) 0.20 5.74(1.04) 0 .13(0.13) 9 .44(3.28) 0 .00(0.00) 3 .70(4.76) 0.25 5.26(0.97) 0 .25(0.12) 6 .40(1.46) 0 .00(0.00) 2 .15(0.28) 0.30 6.24(0.91) 0 .45(0.17) 6 .06(1.41) 0 .10(0.24) 2 .83(0.64) 0.35 5.97(1.35) 0 .97(0.15) 6 .28(1.10) 5 .30(0.65) 3 .72(0.34) 0.40 5.52(1.32) 2 .23(0.37) 7 .26(0.96) 7 .58(0.79) 4 .63(0.34) Spambase 0.10 8.30(2.06) 5 .86(1.93) 13 .46(6.41) 3 .10(1.03) 5 .02(0.49) 0.15 8.40(1.75) 5 .83(1.79) 10 .67(2.96) 2 .27(0.46) 4 .65(0.57) 0.20 8.92(1.87) 5 .26(1.10) 12 .25(1.67) 2 .63(0.69) 5 .40(0.62) 0.25 8.11(1.20) 4 .56(1.23) 11 .81(2.43) 2 .49(0.43) 5 .49(0.60) 0.30 9.48(1.24) 5 .09(0.92) 10 .18(2.08) 2 .27(0.57) 5 .73(0.43) 0.35 9.05(1.60) 6 .12(0.76) 11 .31(1.69) 3 .26(0.76) 6 .61(0.50) 0.40 9.79(1.19) 7 .26(0.69) 10 .79(2.01) 6 .55(0.99) 7 .10(0.45) Subj 0.10 10.08(0.74) 1 .76(0.53) 23 .99(34.15) 0 .00(0.00) 6 .36(0.23) 0.15 10.38(0.55) 2 .13(0.41) 11 .35(4.78) 0 .00(0.00) 6 .75(0.29) 0.20 10.70(0.77) 2 .39(0.39) 12 .65(2.87) 0 .00(0.00) 7 .11(0.40) 0.25 10.26(1.22) 2 .89(0.34) 9 .78(2.03) 0 .00(0.00) 7 .81(0.48) 0.30 10.49(0.91) 3 .51(0.21) 10 .86(1.57) 2 .40(0.76) 8 .54(0.38) 0.35 10.79(0.89) 4 .57(0.34) 11 .10(1.39) 5 .06(0.45) 9 .10(0.34) 0.40 10.86(0.68) 5 .88(0.28) 12 .96(0.91) 7 .53(0.49) 9 .96(0.22) Twonorm 0.10 2.08(0.53) 0 .00(0.00) 21 .63(34.02) 0 .00(0.00) 0 .49(0.09) 0.15 2.03(0.50) 0 .00(0.00) 5 .03(3.63) 0 .00(0.00) 0 .54(0.12) 0.20 2.33(0.46) 0 .00(0.00) 4 .65(3.06) 0 .00(0.00) 0 .72(0.16) 0.25 2.06(0.40) 0 .00(0.00) 3 .95(0.86) 0 .03(0.04) 0 .78(0.13) 0.30 2.35(0.44) 0 .02(0.03) 2 .58(0.90) 0 .13(0.06) 0 .92(0.20) 0.35 2.11(0.39) 0 .04(0.04) 2 .10(0.76) 0 .44(0.07) 1 .10(0.10) 0.40 2.18(0.37) 0 .11(0.05) 2 .03(0.65) 0 .82(0.09) 1 .35(0.16)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 8.Mean and standard error of rejection ratio of the noisy-labeled binary classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gisette 0.10 84.89(5.15) 34 .86(0.94) 96 .31(3.59) 13 .58(0.60) 21 .90(1.23) 0.15 76.78(7.37) 35 .21(0.63) 82 .31(10.98) 11 .49(0.59) 19 .09(1.09) 0.20 78.23(7.98) 35 .62(0.71) 70 .45(8.83) 10 .17(0.39) 18 .17(1.26) 0.25 77.22(7.48) 35 .79(0.95) 62 .96(16.44) 9 .41(0.31) 16 .90(1.17) 0.30 72.54(4.04) 35 .76(0.39) 47 .33(19.82) 8 .84(0.58) 14 .59(1.32) 0.35 67.36(7.82) 36 .13(1.42) 28 .41(12.22) 8 .21(0.58) 13 .41(0.71) 0.40 63.43(5.69) 37 .05(1.03) 18 .55(11.06) 7 .41(0.33) 11 .32(0.83) Phishing 0.10 34.53(6.97) 91 .13(1.71) 77 .34(19.65) 100 .00(0.00) 62 .86(6.85) 0.15 31.53(6.13) 85 .04(1.37) 72 .12(14.58) 100 .00(0.00) 57 .96(8.23) 0.20 30.15(6.10) 77 .18(1.76) 42 .01(12.15) 96 .04(6.04) 44 .05(8.43) 0.25 32.27(7.31) 67 .89(1.33) 18 .49(5.68) 86 .75(0.20) 29 .58(2.91) 0.30 28.31(7.95) 56 .75(1.35) 8 .49(5.42) 67 .89(10.59) 24 .46(2.68) 0.35 29.32(5.64) 43 .62(1.29) 6 .31(3.94) 17 .01(5.61) 17 .06(1.36) 0.40 30.20(8.85) 29 .43(1.87) 5 .07(2.86) 7 .22(1.67) 12 .40(0.87) Spambase 0.10 46.18(10.07) 92 .89(1.21) 78 .50(24.15) 80 .13(4.62) 48 .50(2.35) 0.15 45.53(11.85) 89 .90(1.58) 58 .81(20.39) 78 .61(3.30) 46 .15(1.64) 0.20 43.21(9.73) 85 .95(2.02) 33 .30(18.65) 73 .85(3.05) 43 .18(2.86) 0.25 45.70(9.65) 77 .11(2.48) 17 .14(7.24) 71 .12(3.24) 36 .59(2.68) 0.30 36.53(9.99) 65 .59(3.27) 12 .74(9.60) 64 .49(3.36) 30 .41(3.54) 0.35 34.19(6.08) 46 .56(2.80) 6 .60(5.41) 48 .64(6.02) 19 .17(2.32) 0.40 32.59(8.55) 30 .48(1.99) 8 .51(5.24) 23 .07(3.57) 11 .07(1.90) Subj 0.10 44.78(4.04) 89 .50(0.77) 93 .12(10.68) 100 .00(0.00) 33 .50(1.54) 0.15 43.40(5.46) 84 .34(1.30) 75 .00(16.19) 100 .00(0.00) 27 .27(1.00) 0.20 39.16(3.43) 78 .89(1.60) 46 .94(13.23) 100 .00(0.00) 23 .24(1.50) 0.25 39.05(4.64) 71 .48(1.55) 24 .31(8.76) 100 .00(0.01) 18 .74(1.35) 0.30 36.76(3.25) 62 .07(1.67) 14 .68(4.86) 76 .61(9.20) 14 .82(1.29) 0.35 36.90(3.59) 50 .59(1.50) 11 .06(5.79) 40 .70(2.19) 11 .65(0.81) 0.40 34.71(3.48) 38 .82(0.96) 3 .52(3.75) 22 .89(1.39) 8 .47(0.64) Twonorm 0.10 27.32(6.19) 86 .07(1.90) 79 .71(25.94) 100 .00(0.00) 11 .44(2.23) 0.15 27.14(8.93) 82 .55(3.58) 37 .35(22.79) 99 .81(0.36) 10 .49(1.72) 0.20 21.40(5.79) 74 .87(4.25) 20 .48(7.83) 91 .38(7.10) 7 .27(1.32) 0.25 24.16(7.34) 65 .24(2.39) 7 .18(3.93) 62 .14(16.54) 6 .08(0.70) 0.30 21.98(8.06) 52 .76(2.03) 3 .69(3.04) 24 .29(3.53) 5 .34(1.16) 0.35 21.42(4.80) 40 .88(3.61) 2 .52(1.24) 11 .61(1.07) 3 .98(0.46) 0.40 22.46(6.83) 28 .62(0.76) 2 .02(1.63) 6 .23(0.45) 3 .01(0.46)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 9.Mean and standard error of 0-1-crisk of the positive-unlabeled classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gisette 0.10 11.18(0.21) 17 .90(1.22) 24 .85(2.86) 10 .63(0.62) 9 .28(0.79) 0.15 16.01(0.14) 20 .96(1.36) 30 .54(1.93) 15 .31(1.20) 11 .17(0.62) 0.20 20.87(0.18) 21 .63(2.20) 30 .83(2.65) 19 .87(1.37) 12 .11(0.69) 0.25 25.59(0.26) 22 .98(1.84) 34 .17(2.15) 22 .73(1.32) 14 .23(0.98) 0.30 30.16(0.81) 24 .69(2.80) 33 .99(2.17) 26 .46(1.38) 15 .41(0.93) 0.35 34.67(0.91) 26 .10(1.91) 36 .58(2.46) 29 .17(1.90) 18 .10(1.17) 0.40 40.04(0.17) 26 .76(2.57) 36 .25(2.42) 31 .08(1.01) 19 .69(0.93) Phishing 0.10 11.13(0.95) 5 .98(0.16) 9 .59(1.16) 6 .46(1.14) 4 .91(0.44) 0.15 12.45(0.61) 8 .40(0.19) 12 .62(1.22) 7 .30(0.84) 5 .96(0.25) 0.20 13.43(0.74) 10 .07(0.12) 13 .59(0.79) 9 .31(0.51) 7 .12(0.33) 0.25 14.64(0.62) 11 .36(0.25) 13 .56(0.88) 10 .22(0.41) 8 .26(0.38) 0.30 15.31(0.89) 12 .14(0.33) 13 .10(0.59) 11 .11(0.43) 9 .15(0.32) 0.35 16.42(1.15) 12 .81(0.26) 12 .56(0.38) 11 .74(0.37) 9 .86(0.39) 0.40 16.72(1.22) 13 .22(0.25) 12 .17(0.39) 11 .82(0.24) 10 .40(0.34) Spambase 0.10 21.59(2.37) 12 .03(0.75) 18 .98(2.07) 11 .26(0.56) 9 .91(0.72) 0.15 23.52(2.13) 15 .40(0.97) 22 .83(2.34) 15 .00(2.10) 12 .63(0.65) 0.20 24.97(1.50) 18 .13(0.35) 25 .04(1.74) 20 .22(0.93) 14 .82(1.03) 0.25 26.10(1.11) 19 .49(0.56) 25 .53(2.02) 22 .66(0.95) 17 .04(0.97) 0.30 26.97(1.52) 21 .55(0.36) 25 .73(1.10) 24 .60(0.71) 19 .60(1.29) 0.35 28.35(1.38) 22 .32(0.83) 26 .30(1.04) 25 .93(0.84) 21 .23(1.22) 0.40 29.26(1.11) 24 .17(0.58) 25 .47(0.98) 26 .43(0.66) 21 .71(1.18) Subj 0.10 18.61(0.81) 7 .84(0.13) 16 .76(2.40) 7 .88(0.16) 8 .87(0.35) 0.15 20.82(2.10) 10 .83(0.18) 20 .67(2.17) 10 .86(0.43) 10 .05(0.21) 0.20 22.33(0.88) 13 .10(0.31) 25 .37(1.48) 14 .74(0.50) 11 .19(0.45) 0.25 23.43(1.09) 15 .31(0.22) 26 .45(1.42) 17 .34(0.50) 12 .21(0.39) 0.30 24.91(0.95) 17 .34(0.33) 26 .50(1.14) 19 .27(0.34) 13 .21(0.40) 0.35 25.95(0.69) 18 .42(0.39) 25 .70(1.39) 20 .17(0.42) 13 .90(0.36) 0.40 27.11(0.59) 19 .57(0.36) 25 .27(0.80) 21 .20(0.30) 14 .42(0.31) Twonorm 0.10 8.59(1.44) 5 .29(0.11) 8 .27(2.20) 2 .38(0.53) 1 .51(0.05) 0.15 8.66(1.04) 7 .13(0.31) 12 .58(2.24) 2 .46(0.34) 1 .80(0.19) 0.20 9.76(0.84) 8 .77(0.35) 11 .88(2.21) 3 .37(0.66) 2 .19(0.22) 0.25 10.88(1.29) 9 .78(0.31) 10 .59(2.05) 4 .02(0.50) 2 .46(0.19) 0.30 11.09(1.03) 10 .37(0.37) 9 .20(0.78) 4 .89(0.46) 2 .63(0.18) 0.35 12.64(2.13) 10 .29(0.39) 8 .59(0.90) 5 .76(0.59) 3 .07(0.18) 0.40 12.90(1.51) 9 .99(0.42) 7 .55(0.59) 6 .63(0.58) 3 .21(0.15)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 10.Mean and standard error of 0-1 risk for accepted data of the positive-unlabeled classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gisette 0.10 45.39(9.08) 21 .52(1.57) 29 .98(2.93) 11 .04(0.98) 9 .03(1.06) 0.15 49.00(7.41) 23 .43(2.00) 32 .99(1.91) 15 .54(1.87) 9 .69(0.89) 0.20 46.40(7.76) 22 .30(3.05) 32 .12(2.69) 19 .82(2.05) 9 .35(1.01) 0.25 44.87(9.27) 22 .28(2.42) 34 .83(2.03) 21 .63(2.03) 10 .40(0.98) 0.30 39.79(10.70) 23 .05(3.51) 34 .13(2.23) 24 .77(2.12) 10 .47(1.07) 0.35 34.16(10.19) 23 .07(2.43) 36 .60(2.56) 26 .73(2.59) 11 .25(1.76) 0.40 41.78(4.27) 23 .30(2.76) 36 .17(2.47) 27 .11(1.75) 11 .53(0.98) Phishing 0.10 11.36(1.12) 1 .06(0.30) 9 .39(1.40) 0 .32(0.37) 3 .07(0.78) 0.15 11.52(1.16) 1 .60(0.33) 11 .82(1.77) 3 .08(1.05) 3 .79(0.48) 0.20 11.55(1.15) 1 .86(0.17) 12 .54(1.36) 5 .88(0.29) 4 .73(0.58) 0.25 11.82(1.17) 2 .29(0.25) 12 .05(1.72) 7 .24(0.28) 5 .99(0.56) 0.30 12.25(1.53) 2 .68(0.34) 11 .05(1.74) 7 .97(0.33) 7 .30(0.34) 0.35 11.65(1.38) 3 .61(0.45) 10 .92(1.15) 9 .12(0.36) 8 .41(0.36) 0.40 11.68(1.60) 4 .67(0.30) 10 .78(1.08) 9 .89(0.32) 9 .17(0.29) Spambase 0.10 24.75(2.41) 13 .79(1.21) 20 .75(1.98) 12 .12(0.87) 9 .84(1.09) 0.15 25.37(2.05) 15 .66(1.61) 23 .71(2.08) 14 .68(3.17) 11 .72(0.87) 0.20 26.10(1.56) 17 .10(0.59) 25 .37(1.73) 20 .25(1.13) 13 .23(1.38) 0.25 26.30(1.31) 17 .17(0.72) 25 .45(2.21) 22 .21(1.16) 15 .13(1.24) 0.30 26.32(2.01) 18 .63(0.59) 25 .43(1.26) 23 .84(0.96) 17 .92(1.68) 0.35 27.06(1.82) 18 .65(0.84) 25 .89(1.48) 24 .97(0.90) 19 .72(1.56) 0.40 26.77(1.55) 20 .34(0.51) 25 .04(1.25) 25 .43(0.70) 20 .15(1.44) Subj 0.10 22.08(0.97) 4 .37(0.31) 20 .04(2.78) 4 .04(0.98) 8 .62(0.45) 0.15 22.57(2.20) 4 .86(0.39) 22 .67(2.59) 7 .16(1.00) 9 .19(0.27) 0.20 23.10(1.12) 5 .26(0.49) 26 .43(1.64) 11 .75(0.77) 9 .88(0.59) 0.25 22.86(1.46) 6 .31(0.41) 26 .63(1.62) 14 .23(0.50) 10 .61(0.46) 0.30 23.43(1.41) 7 .83(0.52) 26 .09(1.37) 16 .00(0.49) 11 .56(0.48) 0.35 23.36(1.25) 8 .85(0.63) 24 .53(2.07) 17 .08(0.58) 12 .34(0.32) 0.40 23.12(1.28) 10 .87(0.52) 24 .36(1.35) 18 .70(0.33) 13 .04(0.38) Twonorm 0.10 8.08(1.92) 0 .03(0.05) 7 .91(2.49) 0 .52(0.22) 0 .95(0.17) 0.15 6.88(1.11) 0 .01(0.03) 12 .24(2.59) 1 .18(0.42) 1 .03(0.27) 0.20 7.50(1.47) 0 .03(0.08) 11 .06(2.82) 1 .84(0.47) 1 .37(0.22) 0.25 7.89(1.26) 0 .10(0.12) 9 .85(2.78) 2 .60(0.35) 1 .62(0.26) 0.30 7.46(1.96) 0 .11(0.14) 7 .87(1.71) 3 .33(0.48) 1 .82(0.20) 0.35 6.76(1.38) 0 .13(0.13) 7 .73(1.12) 3 .85(0.44) 2 .27(0.24) 0.40 7.84(1.06) 0 .33(0.09) 6 .61(1.40) 4 .50(0.47) 2 .52(0.18)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 11.Mean and standard error of rejection ratio of the positive-unlabeled classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gisette 0.10 96.36(1.36) 31 .50(4.21) 26 .12(6.31) 39 .56(3.48) 26 .76(2.69) 0.15 96.92(0.66) 29 .06(3.77) 13 .71(4.00) 35 .44(3.75) 27 .78(1.75) 0.20 96.50(0.93) 25 .94(3.29) 11 .09(3.93) 33 .52(2.22) 25 .88(1.35) 0.25 96.39(1.54) 26 .53(3.65) 7 .14(2.78) 31 .95(2.10) 26 .34(3.03) 0.30 95.30(2.98) 24 .51(3.87) 4 .46(2.48) 31 .83(2.13) 25 .29(2.97) 0.35 93.86(3.04) 25 .63(3.14) 3 .55(1.98) 29 .38(2.52) 28 .79(2.32) 0.40 96.08(0.65) 21 .09(2.85) 2 .11(2.06) 30 .58(3.38) 28 .63(2.62) Phishing 0.10 21.32(6.66) 55 .03(0.67) 21 .98(6.20) 63 .11(12.67) 26 .21(4.62) 0.15 23.95(8.43) 50 .74(0.78) 21 .17(7.93) 35 .15(7.50) 19 .23(1.63) 0.20 21.74(5.80) 45 .28(0.70) 13 .06(6.49) 24 .29(2.75) 15 .62(1.73) 0.25 21.02(5.39) 39 .91(1.18) 10 .90(5.60) 16 .79(1.32) 11 .92(1.83) 0.30 16.96(4.66) 34 .65(1.25) 10 .33(5.73) 14 .22(1.79) 8 .14(0.87) 0.35 20.09(7.37) 29 .33(0.58) 6 .62(3.63) 10 .10(1.32) 5 .44(0.78) 0.40 17.59(5.26) 24 .21(0.51) 4 .63(3.13) 6 .40(0.89) 3 .99(0.77) Spambase 0.10 21.96(4.71) 47 .17(2.78) 17 .02(5.16) 41 .88(4.09) 33 .29(3.08) 0.15 18.67(5.77) 40 .36(2.50) 11 .51(6.73) 30 .92(7.42) 27 .89(1.87) 0.20 19.56(5.30) 35 .27(1.80) 7 .29(5.40) 18 .39(2.35) 23 .28(1.81) 0.25 17.96(3.45) 29 .73(2.03) 7 .14(5.66) 15 .55(1.89) 19 .24(1.67) 0.30 15.63(4.62) 25 .58(1.71) 5 .98(3.57) 12 .02(2.58) 13 .66(1.98) 0.35 15.77(3.62) 22 .48(1.83) 3 .79(4.59) 9 .60(1.21) 9 .78(1.28) 0.40 18.58(3.12) 19 .45(2.00) 2 .76(2.74) 6 .85(1.37) 7 .79(1.35) Subj 0.10 28.78(3.10) 61 .62(0.40) 33 .86(5.22) 63 .66(5.24) 17 .23(1.89) 0.15 25.02(7.34) 58 .82(1.06) 27 .58(4.88) 46 .83(4.67) 14 .64(1.29) 0.20 25.83(3.27) 53 .20(1.48) 17 .18(3.65) 36 .16(2.67) 12 .92(1.03) 0.25 24.43(4.32) 48 .17(0.85) 13 .30(4.20) 28 .92(2.30) 11 .14(0.86) 0.30 21.99(3.93) 42 .89(0.61) 9 .49(4.04) 23 .34(1.20) 8 .95(0.99) 0.35 21.84(4.79) 36 .59(0.65) 10 .20(5.59) 17 .25(1.58) 6 .88(0.78) 0.40 23.42(4.08) 29 .86(0.39) 5 .50(4.21) 11 .72(0.99) 5 .13(0.52) Twonorm 0.10 19.36(8.49) 52 .73(1.01) 13 .25(4.94) 19 .65(5.11) 6 .15(1.69) 0.15 21.71(8.79) 47 .51(2.06) 6 .63(6.32) 9 .24(2.27) 5 .48(0.92) 0.20 17.54(5.54) 43 .74(1.74) 7 .56(5.89) 8 .44(2.82) 4 .40(0.74) 0.25 17.18(8.19) 38 .88(1.23) 4 .22(4.28) 6 .31(1.57) 3 .56(0.73) 0.30 15.69(6.22) 34 .33(1.17) 5 .68(5.18) 5 .84(1.33) 2 .87(0.58) 0.35 20.81(6.86) 29 .14(1.01) 3 .13(1.76) 6 .13(1.14) 2 .46(0.42) 0.40 15.58(6.44) 24 .36(1.15) 2 .69(3.28) 6 .01(1.14) 1 .84(0.49)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 12.Mean and standard error of 0-1-crisk of the clean-labeled multiclass classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gas-Drift 0.10 0.88(0.20) 2 .53(0.39) 1 .00(0.12) 1 .22(0.08) 0 .70(0.06) 0.15 1.07(0.40) 2 .84(0.50) 1 .13(0.19) 1 .39(0.13) 0 .70(0.07) 0.20 0.83(0.14) 3 .21(0.40) 1 .09(0.15) 1 .34(0.17) 0 .74(0.04) 0.25 0.89(0.25) 3 .49(0.51) 1 .12(0.15) 1 .27(0.22) 0 .85(0.10) 0.30 0.86(0.24) 2 .90(1.08) 1 .12(0.33) 1 .33(0.27) 0 .87(0.09) 0.35 1.13(0.76) 2 .64(0.59) 1 .10(0.27) 1 .24(0.27) 0 .91(0.12) 0.40 0.82(0.19) 2 .91(0.66) 1 .12(0.23) 1 .17(0.22) 0 .90(0.07) HAR 0.10 1.30(0.21) 2 .57(0.31) 1 .17(0.26) 1 .23(0.26) 1 .52(0.80) 0.15 1.42(0.44) 3 .45(0.68) 1 .32(0.20) 1 .50(0.19) 1 .25(0.14) 0.20 1.95(1.83) 3 .86(0.66) 1 .53(0.22) 1 .59(0.09) 1 .25(0.09) 0.25 1.29(0.21) 3 .87(0.42) 1 .53(0.11) 1 .78(0.27) 1 .42(0.39) 0.30 1.22(0.12) 4 .97(1.11) 1 .55(0.21) 1 .93(0.45) 1 .72(1.24) 0.35 1.79(0.77) 4 .49(0.77) 1 .56(0.12) 1 .94(0.19) 1 .44(0.19) 0.40 1.35(0.19) 4 .65(1.00) 1 .64(0.17) 2 .04(0.36) 1 .81(1.09) MNIST 0.10 0.63(0.04) 1 .03(0.09) 90 .09(0.40) 0 .46(0.03) 0 .47(0.05) 0.15 0.62(0.07) 1 .26(0.14) 90 .02(0.58) 0 .56(0.03) 0 .49(0.03) 0.20 0.63(0.06) 1 .44(0.09) 89 .86(0.46) 0 .62(0.05) 0 .56(0.04) 0.25 0.65(0.04) 1 .50(0.09) 90 .03(0.49) 0 .66(0.05) 0 .63(0.05) 0.30 0.61(0.05) 1 .64(0.09) 89 .82(0.43) 0 .74(0.04) 0 .67(0.04) 0.35 0.66(0.07) 1 .61(0.13) 89 .68(0.77) 0 .82(0.05) 0 .77(0.06) 0.40 0.65(0.04) 1 .65(0.15) 89 .82(0.46) 0 .86(0.04) 0 .82(0.05) Fashion-MNIST 0.10 7.82(0.20) 3 .79(0.21) 90 .00(0.00) 3 .82(0.21) 5 .47(0.17) 0.15 7.91(0.22) 4 .76(0.13) 90 .00(0.00) 4 .83(0.15) 5 .94(0.15) 0.20 7.95(0.20) 5 .77(0.16) 90 .00(0.00) 5 .82(0.17) 6 .42(0.18) 0.25 7.85(0.27) 6 .54(0.19) 90 .00(0.00) 6 .52(0.13) 6 .73(0.10) 0.30 7.96(0.20) 7 .16(0.27) 90 .00(0.00) 7 .19(0.21) 7 .25(0.22) 0.35 7.83(0.20) 7 .67(0.20) 90 .00(0.00) 7 .85(0.19) 7 .61(0.19) 0.40 7.86(0.16) 8 .02(0.23) 90 .00(0.00) 8 .29(0.26) 7 .97(0.18) KMNIST 0.10 4.31(0.16) 3 .53(0.14) 90 .00(0.00) 2 .43(0.12) 2 .39(0.13) 0.15 4.26(0.09) 4 .81(0.24) 90 .00(0.00) 3 .03(0.15) 2 .86(0.14) 0.20 4.14(0.20) 5 .70(0.33) 90 .00(0.00) 3 .36(0.17) 3 .21(0.17) 0.25 4.32(0.20) 6 .55(0.21) 90 .00(0.00) 3 .86(0.20) 3 .67(0.13) 0.30 4.34(0.30) 6 .94(0.19) 90 .00(0.00) 4 .25(0.23) 3 .96(0.18) 0.35 4.43(0.26) 7 .24(0.37) 90 .00(0.00) 4 .69(0.24) 4 .29(0.20) 0.40 4.27(0.32) 7 .36(0.20) 90 .00(0.00) 4 .92(0.26) 4 .67(0.34)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 13.Mean and standard error of 0-1 risk for accepted data of the clean-labeled multiclass classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gas-Drift 0.10 0.87(0.19) 0 .33(0.08) 0 .82(0.18) 0 .39(0.04) 0 .51(0.09) 0.15 1.07(0.40) 0 .39(0.09) 0 .88(0.16) 0 .40(0.05) 0 .46(0.06) 0.20 0.83(0.14) 0 .38(0.12) 0 .98(0.12) 0 .40(0.03) 0 .47(0.04) 0.25 0.88(0.25) 0 .42(0.13) 1 .01(0.17) 0 .44(0.05) 0 .56(0.08) 0.30 0.86(0.24) 0 .40(0.09) 0 .84(0.20) 0 .53(0.20) 0 .58(0.07) 0.35 1.13(0.76) 0 .41(0.09) 0 .95(0.13) 0 .61(0.17) 0 .56(0.07) 0.40 0.82(0.19) 0 .45(0.08) 1 .04(0.24) 0 .51(0.07) 0 .56(0.07) HAR 0.10 1.30(0.21) 0 .09(0.08) 0 .80(0.46) 0 .70(0.33) 1 .28(0.68) 0.15 1.42(0.44) 0 .10(0.12) 0 .84(0.31) 0 .83(0.26) 1 .07(0.15) 0.20 1.95(1.83) 0 .08(0.05) 1 .23(0.20) 0 .84(0.15) 1 .04(0.10) 0.25 1.29(0.21) 0 .25(0.18) 1 .32(0.18) 1 .00(0.28) 1 .10(0.16) 0.30 1.22(0.12) 0 .15(0.11) 1 .42(0.25) 1 .09(0.44) 1 .23(0.68) 0.35 1.79(0.77) 0 .14(0.07) 1 .56(0.10) 1 .14(0.22) 1 .09(0.20) 0.40 1.35(0.19) 0 .18(0.07) 1 .46(0.28) 1 .16(0.36) 1 .04(0.16) MNIST 0.10 0.63(0.04) 0 .01(0.01) 90 .09(0.40) 0 .14(0.05) 0 .35(0.05) 0.15 0.62(0.07) 0 .01(0.01) 90 .02(0.58) 0 .18(0.04) 0 .30(0.03) 0.20 0.63(0.06) 0 .02(0.01) 89 .86(0.46) 0 .20(0.05) 0 .31(0.03) 0.25 0.65(0.04) 0 .04(0.01) 90 .03(0.49) 0 .20(0.03) 0 .32(0.04) 0.30 0.61(0.05) 0 .04(0.01) 89 .82(0.43) 0 .22(0.04) 0 .30(0.05) 0.35 0.66(0.07) 0 .05(0.02) 89 .68(0.77) 0 .26(0.05) 0 .33(0.04) 0.40 0.65(0.04) 0 .07(0.02) 89 .82(0.46) 0 .25(0.03) 0 .32(0.03) Fashion-MNIST 0.10 7.82(0.20) 1 .29(0.15) 90 .00(0.00) 1 .69(0.36) 5 .03(0.19) 0.15 7.91(0.22) 1 .54(0.13) 90 .00(0.00) 1 .73(0.21) 5 .07(0.16) 0.20 7.95(0.20) 1 .84(0.15) 90 .00(0.00) 2 .24(0.23) 5 .17(0.21) 0.25 7.85(0.27) 2 .21(0.19) 90 .00(0.00) 2 .53(0.23) 5 .13(0.09) 0.30 7.96(0.20) 2 .65(0.26) 90 .00(0.00) 2 .95(0.26) 5 .26(0.24) 0.35 7.83(0.21) 3 .15(0.20) 90 .00(0.00) 3 .46(0.31) 5 .25(0.14) 0.40 7.86(0.16) 3 .81(0.28) 90 .00(0.00) 3 .75(0.29) 5 .09(0.19) KMNIST 0.10 4.31(0.16) 0 .11(0.03) 90 .00(0.00) 0 .91(0.16) 1 .75(0.13) 0.15 4.25(0.09) 0 .16(0.04) 90 .00(0.00) 0 .92(0.12) 1 .84(0.16) 0.20 4.13(0.20) 0 .26(0.04) 90 .00(0.00) 1 .02(0.16) 1 .80(0.22) 0.25 4.30(0.20) 0 .30(0.04) 90 .00(0.00) 1 .14(0.13) 1 .98(0.16) 0.30 4.33(0.30) 0 .36(0.07) 90 .00(0.00) 1 .35(0.14) 1 .92(0.24) 0.35 4.42(0.26) 0 .45(0.05) 90 .00(0.00) 1 .39(0.11) 2 .07(0.16) 0.40 4.26(0.32) 0 .50(0.09) 90 .00(0.00) 1 .44(0.12) 2 .06(0.17)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 14.Mean and standard error of rejection ratio of the clean-labeled multiclass classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gas-Drift 0.10 0.03(0.09) 22 .78(3.96) 1 .91(1.34) 8 .61(0.95) 2 .03(0.72) 0.15 0.00(0.00) 16 .79(3.17) 1 .77(0.99) 6 .77(0.94) 1 .68(0.46) 0.20 0.00(0.00) 14 .43(1.96) 0 .57(0.77) 4 .80(0.86) 1 .36(0.21) 0.25 0.01(0.02) 12 .47(2.14) 0 .47(0.35) 3 .35(0.86) 1 .18(0.30) 0.30 0.00(0.00) 8 .46(3.63) 0 .93(1.21) 2 .71(0.79) 0 .99(0.25) 0.35 0.00(0.01) 6 .44(1.56) 0 .43(0.72) 1 .83(0.57) 1 .01(0.26) 0.40 0.00(0.01) 6 .22(1.77) 0 .18(0.33) 1 .67(0.48) 0 .86(0.16) HAR 0.10 0.00(0.00) 24 .95(2.93) 3 .91(2.31) 5 .68(0.72) 2 .93(2.03) 0.15 0.00(0.00) 22 .49(4.38) 3 .35(1.33) 4 .71(0.97) 1 .34(0.30) 0.20 0.00(0.00) 18 .97(3.46) 1 .62(0.93) 3 .92(0.57) 1 .13(0.36) 0.25 0.00(0.00) 14 .62(1.97) 0 .87(0.84) 3 .23(0.58) 1 .35(1.03) 0.30 0.00(0.00) 16 .13(3.70) 0 .44(0.45) 2 .90(0.57) 1 .77(2.16) 0.35 0.00(0.00) 12 .46(2.25) 0 .02(0.07) 2 .37(0.30) 1 .04(0.21) 0.40 0.00(0.00) 11 .22(2.46) 0 .45(1.03) 2 .28(0.37) 1 .98(2.52) MNIST 0.10 0.01(0.01) 10 .19(0.91) 0 .00(0.00) 3 .24(0.24) 1 .24(0.06) 0.15 0.00(0.00) 8 .32(0.94) 0 .00(0.00) 2 .56(0.18) 1 .24(0.09) 0.20 0.00(0.00) 7 .09(0.44) 0 .00(0.00) 2 .12(0.11) 1 .30(0.09) 0.25 0.00(0.00) 5 .84(0.35) 0 .00(0.00) 1 .87(0.19) 1 .29(0.13) 0.30 0.00(0.00) 5 .36(0.32) 0 .00(0.00) 1 .76(0.10) 1 .24(0.08) 0.35 0.00(0.00) 4 .47(0.36) 0 .00(0.00) 1 .61(0.07) 1 .28(0.15) 0.40 0.00(0.00) 3 .95(0.33) 0 .00(0.00) 1 .53(0.09) 1 .26(0.09) Fashion-MNIST 0.10 0.01(0.01) 28 .67(2.42) 0 .00(0.00) 25 .64(1.80) 8 .78(0.45) 0.15 0.01(0.01) 23 .87(1.21) 0 .00(0.00) 23 .39(1.18) 8 .78(0.23) 0.20 0.00(0.01) 21 .65(0.89) 0 .00(0.00) 20 .16(1.45) 8 .43(0.34) 0.25 0.00(0.00) 18 .98(0.98) 0 .00(0.00) 17 .75(0.97) 8 .07(0.40) 0.30 0.00(0.00) 16 .48(1.25) 0 .00(0.00) 15 .67(1.00) 8 .04(0.35) 0.35 0.00(0.00) 14 .21(0.75) 0 .00(0.00) 13 .89(0.93) 7 .94(0.29) 0.40 0.00(0.00) 11 .64(0.86) 0 .00(0.00) 12 .52(1.22) 8 .26(0.33) KMNIST 0.10 0.12(0.05) 34 .53(1.47) 0 .00(0.00) 16 .72(1.31) 7 .81(0.49) 0.15 0.10(0.05) 31 .36(1.63) 0 .00(0.00) 14 .99(0.87) 7 .81(0.25) 0.20 0.08(0.03) 27 .53(1.73) 0 .00(0.00) 12 .32(0.60) 7 .77(0.53) 0.25 0.06(0.03) 25 .29(0.87) 0 .00(0.00) 11 .41(0.69) 7 .34(0.41) 0.30 0.04(0.01) 22 .21(0.64) 0 .00(0.00) 10 .14(0.64) 7 .25(0.47) 0.35 0.03(0.02) 19 .66(1.06) 0 .00(0.00) 9 .82(0.62) 6 .75(0.43) 0.40 0.02(0.01) 17 .38(0.61) 0 .00(0.00) 9 .03(0.61) 6 .88(0.60)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 15.Mean and standard error of 0-1-crisk of the noisy-labeled multiclass classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gas-Drift 0.10 2.44(0.66) 8 .29(0.35) 9 .66(0.32) 9 .16(0.19) 1 .88(0.18) 0.15 2.80(1.01) 11 .14(0.66) 13 .73(2.12) 12 .88(0.43) 2 .10(0.19) 0.20 3.13(0.83) 13 .54(1.26) 16 .95(4.65) 14 .49(0.98) 2 .08(0.18) 0.25 2.57(0.73) 13 .75(1.62) 22 .02(5.15) 11 .38(1.13) 1 .94(0.28) 0.30 3.34(1.46) 14 .43(1.13) 24 .32(7.80) 7 .33(0.74) 1 .67(0.37) 0.35 2.44(0.89) 12 .43(1.84) 31 .49(4.50) 5 .33(0.46) 1 .59(0.27) 0.40 2.48(0.46) 12 .92(2.28) 29 .77(11.38) 4 .42(0.43) 1 .52(0.27) HAR 0.10 24.36(3.18) 9 .05(0.74) 9 .25(0.82) 7 .21(0.50) 2 .68(0.37) 0.15 23.37(3.51) 11 .50(0.63) 11 .98(2.28) 9 .52(0.84) 3 .50(0.43) 0.20 25.47(3.55) 14 .26(1.00) 12 .07(3.00) 11 .14(0.73) 3 .58(0.34) 0.25 23.94(2.69) 17 .37(1.18) 13 .80(4.15) 12 .17(0.64) 3 .86(0.28) 0.30 23.85(3.93) 19 .39(1.30) 13 .67(5.36) 12 .29(1.81) 5 .23(0.96) 0.35 24.56(3.33) 22 .23(1.92) 10 .76(0.98) 11 .84(1.70) 5 .53(1.37) 0.40 26.99(3.28) 24 .87(1.36) 10 .31(2.12) 11 .06(1.68) 5 .96(0.86) MNIST 0.10 1.30(0.10) 7 .88(0.58) 90 .18(0.48) 7 .85(0.81) 0 .51(0.05) 0.15 1.28(0.10) 10 .41(0.87) 90 .11(0.59) 11 .43(0.50) 0 .64(0.05) 0.20 1.32(0.05) 11 .91(0.87) 89 .83(0.64) 14 .30(1.20) 0 .71(0.03) 0.25 1.36(0.09) 13 .13(1.32) 89 .98(0.79) 15 .14(1.47) 0 .78(0.07) 0.30 1.33(0.12) 11 .26(1.57) 89 .69(0.72) 12 .46(0.85) 0 .85(0.08) 0.35 1.36(0.06) 10 .72(1.59) 89 .92(0.58) 9 .36(0.76) 0 .90(0.06) 0.40 1.37(0.13) 9 .01(0.88) 90 .18(0.48) 6 .47(0.38) 0 .97(0.07) Fashion-MNIST 0.10 9.78(0.29) 8 .44(0.30) 90 .00(0.00) 9 .44(0.85) 5 .33(0.25) 0.15 9.84(0.24) 12 .11(0.63) 90 .00(0.00) 13 .77(1.33) 6 .20(0.21) 0.20 9.69(0.33) 14 .44(0.88) 90 .00(0.00) 16 .59(2.25) 6 .78(0.16) 0.25 9.91(0.25) 16 .19(0.96) 90 .00(0.00) 18 .42(0.72) 7 .51(0.24) 0.30 9.91(0.23) 16 .49(1.21) 90 .00(0.00) 18 .17(0.91) 7 .82(0.23) 0.35 9.88(0.21) 17 .00(1.82) 90 .00(0.00) 17 .45(1.00) 8 .33(0.18) 0.40 9.92(0.24) 16 .76(1.12) 90 .00(0.00) 16 .35(0.80) 8 .97(0.25) KMNIST 0.10 7.88(0.37) 8 .35(0.38) 90 .00(0.00) 8 .69(0.24) 2 .84(0.21) 0.15 8.14(0.49) 11 .70(0.58) 90 .00(0.00) 12 .77(0.81) 3 .55(0.25) 0.20 8.35(0.40) 14 .52(0.77) 90 .00(0.00) 15 .44(0.56) 4 .18(0.19) 0.25 8.50(0.31) 16 .79(1.01) 90 .00(0.00) 18 .01(0.65) 4 .54(0.20) 0.30 8.37(0.32) 18 .61(0.46) 90 .00(0.00) 18 .74(0.67) 4 .99(0.29) 0.35 8.55(0.42) 19 .79(1.04) 90 .00(0.00) 18 .18(0.81) 5 .53(0.28) 0.40 8.61(0.37) 20 .01(0.63) 90 .00(0.00) 16 .76(0.72) 5 .84(0.22)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 16.Mean and standard error of 0-1 risk for accepted data of the noisy-labeled multiclass classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gas-Drift 0.10 2.43(0.66) 1 .74(0.91) 11 .70(29.45) 0 .93(0.25) 0 .58(0.08) 0.15 2.80(1.00) 1 .42(0.67) 3 .51(3.95) 0 .71(0.37) 0 .58(0.09) 0.20 3.11(0.82) 1 .29(0.52) 2 .88(2.30) 0 .51(0.17) 0 .55(0.08) 0.25 2.56(0.73) 0 .85(0.28) 2 .38(1.30) 0 .38(0.14) 0 .59(0.12) 0.30 3.33(1.45) 0 .78(0.21) 2 .73(1.74) 0 .42(0.09) 0 .54(0.04) 0.35 2.43(0.89) 0 .70(0.17) 4 .61(2.84) 0 .46(0.05) 0 .60(0.12) 0.40 2.46(0.45) 0 .85(0.37) 3 .33(1.40) 0 .48(0.07) 0 .63(0.13) HAR 0.10 24.45(3.22) 7 .89(1.68) 5 .68(3.15) 3 .11(0.86) 1 .54(0.42) 0.15 23.41(3.53) 7 .33(0.78) 7 .98(1.45) 2 .69(0.76) 1 .63(0.31) 0.20 25.50(3.58) 7 .86(1.25) 9 .65(2.44) 2 .54(0.73) 1 .67(0.26) 0.25 23.94(2.70) 9 .10(1.30) 10 .54(3.32) 2 .34(0.39) 1 .66(0.21) 0.30 23.84(3.94) 9 .31(1.29) 9 .60(2.68) 2 .17(0.54) 2 .47(0.64) 0.35 24.54(3.33) 10 .91(2.21) 9 .58(0.86) 2 .10(0.34) 2 .39(0.70) 0.40 26.97(3.26) 12 .03(1.71) 9 .16(2.66) 2 .00(0.34) 2 .62(0.52) MNIST 0.10 1.28(0.10) 0 .00(0.00) 90 .18(0.48) 0 .70(0.41) 0 .29(0.05) 0.15 1.27(0.11) 0 .00(0.01) 90 .11(0.59) 0 .83(0.63) 0 .33(0.05) 0.20 1.31(0.05) 0 .00(0.01) 89 .83(0.64) 0 .70(0.30) 0 .33(0.05) 0.25 1.35(0.09) 0 .00(0.01) 89 .98(0.79) 0 .64(0.35) 0 .35(0.06) 0.30 1.32(0.11) 0 .00(0.01) 89 .69(0.72) 0 .35(0.16) 0 .36(0.05) 0.35 1.35(0.06) 0 .01(0.01) 89 .92(0.58) 0 .33(0.09) 0 .35(0.03) 0.40 1.37(0.13) 0 .01(0.01) 90 .18(0.48) 0 .24(0.08) 0 .36(0.04) Fashion-MNIST 0.10 9.78(0.29) 0 .50(0.22) 90 .00(0.00) 0 .95(1.45) 4 .56(0.33) 0.15 9.82(0.24) 0 .57(0.27) 90 .00(0.00) 1 .78(1.92) 4 .87(0.30) 0.20 9.67(0.33) 0 .62(0.12) 90 .00(0.00) 1 .62(1.22) 4 .91(0.31) 0.25 9.89(0.25) 0 .75(0.12) 90 .00(0.00) 2 .23(0.85) 5 .32(0.30) 0.30 9.89(0.23) 0 .94(0.16) 90 .00(0.00) 1 .82(0.26) 5 .29(0.30) 0.35 9.87(0.21) 1 .27(0.20) 90 .00(0.00) 2 .13(0.40) 5 .46(0.22) 0.40 9.91(0.24) 1 .32(0.20) 90 .00(0.00) 2 .12(0.25) 5 .66(0.18) KMNIST 0.10 7.84(0.38) 0 .09(0.06) 90 .00(0.00) 3 .01(0.79) 1 .68(0.25) 0.15 8.05(0.50) 0 .14(0.07) 90 .00(0.00) 2 .57(1.24) 1 .74(0.25) 0.20 8.25(0.42) 0 .11(0.05) 90 .00(0.00) 2 .89(0.84) 1 .85(0.11) 0.25 8.39(0.30) 0 .16(0.06) 90 .00(0.00) 2 .26(0.74) 1 .81(0.21) 0.30 8.29(0.32) 0 .17(0.06) 90 .00(0.00) 1 .78(0.42) 1 .94(0.24) 0.35 8.46(0.44) 0 .18(0.06) 90 .00(0.00) 1 .75(0.33) 2 .09(0.28) 0.40 8.55(0.37) 0 .24(0.07) 90 .00(0.00) 1 .64(0.31) 2 .09(0.21)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation Table 17.Mean and standard error of rejection ratio of the noisy-labeled multiclass classiﬁcation setting (rescaled to 0-100). Dataset c SCE DEFER ANGLE CS-hinge CS-sigmoid Gas-Drift 0.10 0.08(0.05) 79 .35(3.21) 95 .94(3.27) 90 .74(1.95) 13 .80(1.67) 0.15 0.04(0.07) 71 .61(4.47) 87 .60(21.06) 85 .17(2.82) 10 .56(1.05) 0.20 0.10(0.19) 65 .57(6.27) 79 .39(32.80) 71 .74(4.99) 7 .87(1.10) 0.25 0.03(0.05) 53 .48(6.20) 85 .90(26.03) 44 .69(4.55) 5 .52(1.14) 0.30 0.04(0.04) 46 .74(3.64) 77 .63(32.41) 23 .37(2.37) 3 .85(1.21) 0.35 0.02(0.02) 34 .20(5.23) 87 .36(18.73) 14 .11(1.34) 2 .88(0.71) 0.40 0.04(0.07) 30 .83(5.82) 71 .95(31.27) 9 .97(1.08) 2 .25(0.62) HAR 0.10 0.61(0.19) 58 .17(5.07) 70 .44(30.79) 59 .38(5.42) 13 .46(2.02) 0.15 0.38(0.11) 54 .58(5.37) 54 .29(35.03) 55 .73(4.66) 14 .01(2.05) 0.20 0.45(0.23) 53 .02(3.74) 21 .57(23.57) 49 .32(3.07) 10 .42(1.45) 0.25 0.30(0.18) 52 .25(3.96) 18 .63(26.08) 43 .37(2.41) 9 .42(1.45) 0.30 0.25(0.08) 48 .83(4.36) 17 .54(26.15) 36 .41(5.83) 10 .04(1.54) 0.35 0.14(0.04) 47 .23(4.40) 4 .60(3.37) 29 .61(5.03) 9 .66(2.52) 0.40 0.26(0.45) 45 .96(2.67) 3 .53(3.23) 23 .87(3.99) 8 .96(1.23) MNIST 0.10 0.18(0.06) 78 .81(5.78) 0 .00(0.00) 76 .61(9.03) 2 .30(0.25) 0.15 0.13(0.04) 69 .42(5.79) 0 .00(0.00) 74 .89(3.04) 2 .10(0.16) 0.20 0.08(0.04) 59 .56(4.36) 0 .00(0.00) 70 .45(6.35) 1 .95(0.19) 0.25 0.05(0.02) 52 .52(5.28) 0 .00(0.00) 59 .45(6.71) 1 .78(0.14) 0.30 0.05(0.02) 37 .52(5.24) 0 .00(0.00) 40 .82(2.83) 1 .64(0.18) 0.35 0.02(0.02) 30 .61(4.55) 0 .00(0.00) 26 .07(2.16) 1 .60(0.13) 0.40 0.01(0.01) 22 .51(2.21) 0 .00(0.00) 15 .67(0.92) 1 .55(0.16) Fashion-MNIST 0.10 0.36(0.07) 83 .60(3.09) 0 .00(0.00) 91 .82(12.52) 14 .12(0.92) 0.15 0.28(0.11) 79 .91(4.44) 0 .00(0.00) 89 .29(11.38) 13 .07(0.80) 0.20 0.18(0.07) 71 .30(4.59) 0 .00(0.00) 80 .71(12.75) 12 .41(1.06) 0.25 0.14(0.06) 63 .68(4.00) 0 .00(0.00) 71 .09(3.21) 11 .14(0.80) 0.30 0.08(0.02) 53 .51(4.18) 0 .00(0.00) 58 .00(3.47) 10 .21(0.63) 0.35 0.05(0.02) 46 .65(5.38) 0 .00(0.00) 46 .59(3.05) 9 .72(0.49) 0.40 0.02(0.01) 39 .91(2.93) 0 .00(0.00) 37 .56(2.28) 9 .62(0.42) KMNIST 0.10 1.45(0.26) 83 .36(3.81) 0 .00(0.00) 81 .34(2.64) 13 .92(1.34) 0.15 1.23(0.23) 77 .79(3.91) 0 .00(0.00) 81 .57(6.82) 13 .68(0.73) 0.20 0.84(0.15) 72 .46(3.89) 0 .00(0.00) 73 .39(2.59) 12 .83(0.96) 0.25 0.65(0.15) 66 .96(4.09) 0 .00(0.00) 69 .20(3.08) 11 .78(0.72) 0.30 0.39(0.11) 61 .83(1.54) 0 .00(0.00) 60 .11(2.23) 10 .87(0.78) 0.35 0.32(0.11) 56 .30(3.03) 0 .00(0.00) 49 .41(2.51) 10 .46(0.91) 0.40 0.17(0.04) 49 .72(1.58) 0 .00(0.00) 39 .41(2.04) 9 .87(0.43)Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation D.2. More discussion on the proposed rejection conditions In this section, we provide more discussion on our proposed rejection conditions. We provide additional experimental results without using Cond. (8). We found that using Cond. (8) slightly affects the performance of our cost-sensitive approach. This is because there is only a small portion of data that satisﬁes Cond. (8). Note that if we succeed to obtain the optimal classiﬁer g∗, this condition is impossible to be satisﬁed. Recall that in Section 3.4, for g∗, at most one g∗ y(x) can be more than zero since it implies ηy >1 −c> 0.5. Nevertheless, Cond. (8) may hold in practice due to empirical estimation. Figures 6 and 7 illustrate the performance of the proposed approach that uses only Cond. (7) as a rejection condition and other baselines in the binary and multiclass settings, respectively. Figures 8, 9, and 10 illustrate rejected test data based on different conditions for MNIST, Fashion-MNIST, and KMNIST, respectively.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.04 0.06 0.08 0.10Zero-one-c risk Clean Gisette 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12Zero-one-c risk Phishing 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.06 0.07 0.08 0.09 0.10 0.11 0.12Zero-one-c risk Spambase 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.08 0.10 0.12 0.14 0.16Zero-one-c risk Subj 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.02 0.04 0.06 0.08 0.10Zero-one-c risk Twonorm 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.10 0.15 0.20 0.25 0.30Zero-one-c risk Noisy 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.075 0.100 0.125 0.150 0.175 0.200 0.225Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.100 0.125 0.150 0.175 0.200 0.225 0.250Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.025 0.050 0.075 0.100 0.125 0.150 0.175Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.10 0.15 0.20 0.25 0.30 0.35 0.40Zero-one-c risk PU 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.06 0.08 0.10 0.12 0.14 0.16 0.18Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.100 0.125 0.150 0.175 0.200 0.225 0.250 0.275Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.100 0.125 0.150 0.175 0.200 0.225 0.250 0.275Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.02 0.04 0.06 0.08 0.10 0.12 0.14Zero-one-c risk SCE DEFER ANGLE CS-hinge CS-sigmoid Figure 6.Mean and standard error of the test empirical zero-one-crisk over ten trials with varying rejection costs (Binary classiﬁcation). Our approach in this ﬁgure only used Cond. (7) as a rejection condition. Each column indicates the performance with respect to one dataset. (Top) clean-labeled classiﬁcation with rejection. (Middle) noisy-labeled classiﬁcation with rejection. (Bottom) PU-classiﬁcation with rejection. It can be seen that a similar trend as Figure 4 can be observed. 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.005 0.010 0.015 0.020 0.025 0.030 0.035Zero-one-c risk Clean Gas-Drift 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.01 0.02 0.03 0.04 0.05 0.06Zero-one-c risk HAR 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.006 0.008 0.010 0.012 0.014 0.016Zero-one-c risk MNIST 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.04 0.05 0.06 0.07 0.08Zero-one-c risk Fashion-MNIST 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.03 0.04 0.05 0.06 0.07Zero-one-c risk KMNIST 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40Zero-one-c risk Noisy 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.05 0.10 0.15 0.20 0.25 0.30Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.06 0.08 0.10 0.12 0.14 0.16 0.18Zero-one-c risk 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Rejection cost c 0.050 0.075 0.100 0.125 0.150 0.175 0.200Zero-one-c risk SCE DEFER ANGLE CS-hinge CS-sigmoid Figure 7.Mean and standard error of the test empirical zero-one-crisk over ten trials with varying rejection cost (Multiclass classiﬁcation). Our approach in this ﬁgure only used Cond.(7) as a rejection condition. Each column indicates the performance with respect to one dataset. (Top) clean-labeled classiﬁcation with rejection. (Bottom) noisy-labeled classiﬁcation with rejection. For MNIST, Fashion-MNIST, and KMNIST, we found that ANGLE failed miserably and has zero-one- crisk more than 0.5 and thus it is excluded from the ﬁgure for readability. It can be seen that a similar trend as Figure 5 can be observed.Classiﬁcation with Rejection Based on Cost-sensitive Classiﬁcation index: 2742 given: 9 guess: 4 8 index: 4248 given: 2 guess: 2 4 index: 391 given: 8 guess: 8 4 index: 9679 given: 6 guess: 1 4 index: 2109 given: 3 guess: 2 4 index: 9698 given: 6 guess: 5 6 index: 6651 given: 0 guess: 0 8 index: 4078 given: 9 guess: 9 3 index: 3060 given: 9 guess: 4 9 index: 9634 given: 0 guess: 0 2 index: 9627 given: 6 guess: 6 4 index: 2129 given: 9 guess: 9 4 index: 2293 given: 9 guess: 6 4 index: 882 given: 9 guess: 9 4 index: 1414 given: 9 guess: 4 7 index: 2488 given: 2 guess: 1 4 index: 5936 given: 4 guess: 4 9 index: 4814 given: 6 guess: 6 0 index: 3005 given: 9 guess: 9 8 index: 543 given: 8 guess: 8 4 index: 2326 given: 0 guess: 5 0 index: 5832 given: 4 guess: 4 8 index: 1737 given: 5 guess: 3 5 index: 1911 given: 5 guess: 5 0 index: 4571 given: 6 guess: 8 6 index: 8287 given: 6 guess: 6 4 index: 4176 given: 2 guess: 7 2 index: 3778 given: 5 guess: 5 4 index: 4731 given: 8 guess: 8 7 index: 2118 given: 6 guess: 0 4 index: 4761 given: 9 guess: 9 8 index: 3859 given: 9 guess: 9 4 index: 4075 given: 8 guess: 8 0 index: 3062 given: 8 guess: 8 6 index: 1014 given: 6 guess: 0 5 index: 3727 given: 8 guess: 8 9 index: 3850 given: 9 guess: 4 9 index: 6172 given: 9 guess: 9 5 index: 1299 given: 5 guess: 5 7 index: 3225 given: 7 guess: 1 9 index: 6173 given: 9 guess: 9 0 index: 9620 given: 9 guess: 7 1 index: 3384 given: 2 guess: 2 6 index: 1393 given: 5 guess: 3 5 index: 3817 given: 2 guess: 2 4 index: 5887 given: 7 guess: 7 4 index: 1709 given: 9 guess: 9 5 index: 3808 given: 7 guess: 8 7 index: 4284 given: 9 guess: 5 9 index: 2107 given: 8 guess: 0 8 index: 3475 given: 3 guess: 3 7 index: 1828 given: 3 guess: 3 7 index: 3599 given: 2 guess: 2 7 index: 9883 given: 5 guess: 5 1 index: 3742 given: 3 guess: 3 9 index: 2975 given: 8 guess: 8 3 index: 1681 given: 3 guess: 7 3 index: 3780 given: 4 guess: 4 6 index: 1549 given: 4 guess: 6 4 index: 716 given: 1 guess: 1 7 index: 9754 given: 5 guess: 5 6 index: 9664 given: 2 guess: 7 2 index: 924 given: 2 guess: 2 7 index: 8059 given: 2 guess: 1 2 index: 4201 given: 1 guess: 1 7 index: 9638 given: 9 guess: 9 7 index: 2369 given: 5 guess: 5 3 index: 2053 given: 4 guess: 9 4 index: 8316 given: 7 guess: 7 2 index: 5937 given: 5 guess: 3 5 index: 1260 given: 7 guess: 7 1 index: 2972 given: 0 guess: 0 6 index: 4405 given: 9 guess: 9 4 index: 447 given: 4 guess: 4 9 index: 4860 given: 4 guess: 9 4 index: 4956 given: 8 guess: 4 8 index: 2016 given: 7 guess: 7 2 index: 1242 given: 4 guess: 4 9 index: 9123 given: 2 guess: 2 3 index: 4699 given: 6 guess: 1 6 index: 4876 given: 2 guess: 2 4 index: 1112 given: 4 guess: 4 6 index: 4763 given: 5 guess: 5 6 index: 4238 given: 7 guess: 7 3 index: 5278 given: 8 guess: 9 8 index: 3941 given: 4 guess: 6 4 index: 6393 given: 9 guess: 9 4 index: 9642 given: 9 guess: 7 9 index: 3647 given: 7 guess: 7 3 index: 6608 given: 9 guess: 9 5 index: 1226 given: 7 guess: 7 2 index: 3601 given: 1 guess: 1 6 index: 1609 given: 2 guess: 2 0 index: 3985 given: 9 guess: 4 9 index: 5842 given: 4 guess: 4 7 index: 8228 given: 0 guess: 0 4 index: 9669 given: 4 guess: 4 7 index: 7265 given: 8 guess: 8 3 index: 5888 given: 4 guess: 4 0 index: 1686 given: 8 guess: 8 6 Figure 8.Examples of rejected test data in MNIST. (Left) rejections that were made by Cond. (7) (distance rejection). (Right) rejections that were made by Cond. (8) (ambiguity rejection). The data rejected by Cond. (7) appeared to look more chaotic than the one rejected by Cond. (8). On the right ﬁgure, several images can be seen to be able to associated to more than one classes, e.g.,1 vs 7, 4 vs 9, and 3 vs 5. index: 2965 given: sneaker guess: shirt boot index: 8940 given: shirt guess: shirt coat index: 7526 given: dress guess: shirt dress index: 4505 given: shirt guess: shirt bag index: 8764 given: dress guess: shirt pullover index: 6023 given: shirt guess: shirt t-shirt index: 8298 given: pullover guess: shirt pullover index: 6230 given: bag guess: shirt bag index: 9306 given: shirt guess: shirt dress index: 1974 given: shirt guess: shirt t-shirt index: 9089 given: pullover guess: shirt pullover index: 9946 given: pullover guess: shirt pullover index: 490 given: dress guess: shirt dress index: 1852 given: dress guess: shirt bag index: 2940 given: shirt guess: shirt t-shirt index: 8066 given: coat guess: shirt coat index: 5266 given: shirt guess: shirt t-shirt index: 9969 given: bag guess: shirt t-shirt index: 8939 given: shirt guess: shirt dress index: 9349 given: t-shirt guess: shirt t-shirt index: 4251 given: coat guess: shirt t-shirt index: 7646 given: dress guess: shirt t-shirt index: 4152 given: shirt guess: shirt t-shirt index: 9449 given: pullover guess: shirt t-shirt index: 4668 given: coat guess: shirt t-shirt index: 7635 given: dress guess: shirt t-shirt index: 8933 given: coat guess: shirt t-shirt index: 1107 given: shirt guess: shirt t-shirt index: 9554 given: dress guess: shirt t-shirt index: 793 given: shirt guess: shirt t-shirt index: 976 given: pullover guess: shirt t-shirt index: 6832 given: shirt guess: shirt t-shirt index: 8005 given: shirt guess: shirt t-shirt index: 2817 given: dress guess: shirt t-shirt index: 8259 given: dress guess: shirt t-shirt index: 1462 given: coat guess: shirt t-shirt index: 851 given: pullover guess: shirt t-shirt index: 3706 given: dress guess: shirt t-shirt index: 1312 given: coat guess: shirt t-shirt index: 8420 given: t-shirt guess: shirt t-shirt index: 172 given: pullover guess: shirt t-shirt index: 2717 given: t-shirt guess: shirt t-shirt index: 2073 given: coat guess: shirt t-shirt index: 558 given: coat guess: shirt t-shirt index: 562 given: shirt guess: shirt t-shirt index: 5471 given: pullover guess: shirt t-shirt index: 5661 given: coat guess: coat shirt index: 4842 given: shirt guess: shirt dress index: 9059 given: dress guess: dress shirt index: 1557 given: bag guess: bag shirt index: 2351 given: shirt guess: pullover shirt index: 5504 given: shirt guess: shirt t-shirt index: 1701 given: shirt guess: shirt coat index: 3294 given: dress guess: coat dress index: 8787 given: shirt guess: shirt pullover index: 4213 given: t-shirt guess: shirt t-shirt index: 8906 given: shirt guess: t-shirt shirt index: 6742 given: t-shirt guess: shirt t-shirt index: 8649 given: t-shirt guess: t-shirt shirt index: 8693 given: coat guess: pullover coat index: 1496 given: shirt guess: shirt pullover index: 9644 given: shirt guess: shirt t-shirt index: 9743 given: pullover guess: coat pullover index: 1387 given: pullover guess: shirt pullover index: 6593 given: t-shirt guess: t-shirt shirt index: 4166 given: coat guess: coat pullover index: 5669 given: coat guess: shirt coat index: 5404 given: dress guess: dress coat index: 4399 given: shirt guess: shirt coat index: 5070 given: coat guess: dress coat index: 1744 given: shirt guess: shirt pullover index: 1629 given: shirt guess: pullover shirt index: 7194 given: shirt guess: shirt pullover index: 9364 given: shirt guess: shirt pullover index: 5107 given: coat guess: coat shirt index: 4845 given: shirt guess: shirt t-shirt index: 6160 given: shirt guess: shirt coat index: 6796 given: dress guess: coat dress index: 7411 given: coat guess: coat shirt index: 6929 given: t-shirt guess: t-shirt shirt index: 4484 given: coat guess: coat dress index: 5036 given: coat guess: coat dress index: 8462 given: coat guess: coat shirt index: 3257 given: shirt guess: shirt coat index: 6225 given: t-shirt guess: shirt t-shirt index: 1064 given: coat guess: coat dress index: 8260 given: shirt guess: coat shirt index: 7317 given: coat guess: coat pullover index: 9904 given: shirt guess: shirt coat index: 2464 given: shirt guess: t-shirt shirt index: 6953 given: t-shirt guess: t-shirt shirt index: 7723 given: coat guess: coat pullover index: 6955 given: shirt guess: shirt coat index: 9244 given: trouser guess: trouser dress index: 7584 given: sandal guess: boot sandal index: 4138 given: pullover guess: pullover shirt index: 4649 given: pullover guess: coat pullover index: 4742 given: sandal guess: boot sandal index: 3553 given: pullover guess: shirt t-shirt index: 2619 given: coat guess: coat shirt Figure 9.Examples of rejected test data in Fashion-MNIST. (Left) rejections that were made by Cond.(7) (distance rejection). (Right) rejections that were made by Cond. (8) (ambiguity rejection). The data rejected by Cond. (7) appeared to have more texture information than the one rejected by Cond. (8). index: 9394 given: ya guess: ha na index: 290 given: su guess: ha na index: 5605 given: ki guess: ha na index: 1055 given: ha guess: ha na index: 344 given: ki guess: ha na index: 9208 given: wo guess: ha na index: 3667 given: ya guess: ha na index: 6249 given: ma guess: ha na index: 6240 given: ma guess: ha na index: 4283 given: re guess: re ha index: 4055 given: na guess: na ha index: 8726 given: wo guess: ya ha index: 2038 given: ya guess: re ha index: 5318 given: ki guess: su ha index: 8846 given: ya guess: ya ha index: 4835 given: ki guess: wo ha index: 7696 given: su guess: su ha index: 6465 given: ma guess: re ha index: 1621 given: tsu guess: ha na index: 3367 given: ki guess: su ha index: 2527 given: na guess: na ha index: 2325 given: ya guess: na ha index: 5711 given: su guess: su ha index: 5671 given: su guess: ya ha index: 7799 given: na guess: na ha index: 4591 given: ki guess: na wo index: 7552 given: re guess: re wo index: 3567 given: tsu guess: tsu su index: 2039 given: ma guess: ma ki index: 4984 given: ya guess: ya ha index: 5352 given: tsu guess: su ma index: 8103 given: re guess: re wo index: 5812 given: ma guess: na ma index: 7199 given: wo guess: ma na index: 4649 given: o guess: wo su index: 1424 given: tsu guess: su ya index: 7804 given: na guess: ma su index: 1619 given: na guess: ya ha index: 4921 given: re guess: re ma index: 8339 given: su guess: ha na index: 8799 given: ya guess: ya ha index: 774 given: ha guess: o ha index: 8073 given: wo guess: ki wo index: 382 given: wo guess: wo ha index: 6758 given: re guess: tsu wo index: 8680 given: re guess: re o index: 6853 given: wo guess: su ha index: 2525 given: ya guess: na ya index: 187 given: ma guess: re ma index: 690 given: o guess: na re index: 9103 given: ki guess: ki na index: 2908 given: o guess: o ha index: 1638 given: o guess: ya o index: 1039 given: su guess: su re index: 7169 given: wo guess: wo na index: 5301 given: ki guess: ma ki index: 3799 given: su guess: su re index: 2609 given: o guess: o ya index: 2298 given: tsu guess: tsu na index: 5157 given: ki guess: ki re index: 819 given: na guess: o na index: 7196 given: ya guess: ya na index: 7115 given: o guess: ya o index: 6989 given: ha guess: su ma index: 5991 given: ki guess: re ki index: 2582 given: ha guess: ha wo index: 7747 given: re guess: ha re index: 7716 given: su guess: su re index: 3809 given: su guess: su re index: 1295 given: ki guess: ki ha index: 1960 given: ha guess: ha ma index: 568 given: su guess: su ya index: 8507 given: o guess: o ha index: 9976 given: tsu guess: tsu su index: 590 given: o guess: o ha index: 8437 given: wo guess: tsu wo index: 7993 given: ha guess: ha na index: 9838 given: wo guess: na wo index: 8426 given: re guess: su re index: 3665 given: ha guess: ha na index: 4919 given: na guess: ma na index: 7440 given: ya guess: ya wo index: 4122 given: o guess: ha o index: 7110 given: ki guess: ma ki index: 5174 given: ki guess: na ki index: 896 given: wo guess: wo ki index: 9855 given: na guess: na o index: 6811 given: tsu guess: tsu su index: 1322 given: ha guess: ha tsu index: 44 given: ki guess: ki ma index: 2466 given: wo guess: wo o index: 7081 given: su guess: su ma index: 4582 given: ma guess: o ma index: 3430 given: ma guess: ma su index: 9079 given: tsu guess: ma tsu index: 4291 given: su guess: ma su index: 9812 given: ya guess: ya ma index: 6042 given: ki guess: ki ma index: 514 given: su guess: tsu su index: 829 given: ha guess: ha na Figure 10.Examples of rejected test data in KMNIST. (Left) rejections that were made by Cond.(7) (distance rejection). (Right) rejections that were made by Cond. (8) (ambiguity rejection). The data rejected by Cond. (7) appeared to look more chaotic than the one rejected by Cond. (8).",
      "references": [],
      "meta_data": {
        "arxiv_id": "2010.11748v5",
        "authors": [
          "Nontawat Charoenphakdee",
          "Zhenghang Cui",
          "Yivan Zhang",
          "Masashi Sugiyama"
        ],
        "published_date": "2020-10-22T14:05:05Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces a theoretically-grounded \"cost-sensitive approach\" to classification with a reject option. By framing rejection as a set of K binary cost-sensitive classification problems, the method (i) avoids estimating class-posterior probabilities, (ii) works with any classification-calibrated loss (including non-convex or symmetric losses), (iii) handles both binary and multiclass tasks, and (iv) provides calibration and excess-risk guarantees. Extensive experiments on clean, noisy, and positive-unlabeled datasets show improved or competitive performance against confidence-based (SCE), classifier-rejector (DEFER), and bent-loss (ANGLE) baselines.",
        "methodology": "1) Theoretical reformulation: Show Chow’s optimal reject rule can be recovered by learning K one-vs-rest cost-sensitive classifiers with thresholds (c,1-c).\n2) Surrogate loss: Propose L_CS^{c,φ} that combines any margin loss φ with rejection cost c; minimised jointly over vector-valued score function g.\n3) Rejection rule: Reject if max_y g_y(x)≤0 (distance rejection) or if two classes predict positive scores (ambiguity).\n4) Theory: Prove calibration iff φ is classification-calibrated and derive excess-risk bound transferring from cost-sensitive to rejection setting.\n5) Implementation: learn linear models, MLPs, or CNNs with hinge or sigmoid losses, using ERM or non-negative PU risk for PU setting.",
        "experimental_setup": "Binary datasets: Gisette, Phishing, Spambase, Subj (text), Twonorm (synthetic). Multiclass: Gas-Drift, HAR, MNIST, Fashion-MNIST, KMNIST.\nSettings: (a) clean labels, (b) 25% uniform label noise, (c) positive-unlabeled (π=0.7).\nBaselines: Softmax cross-entropy with temperature scaling (SCE), DEFER, ANGLE; our method with hinge and sigmoid losses.\nModels: linear classifiers for tabular/text, MLP for HAR/Gas-Drift, CNN for image data.\nTraining: Adam, fixed LR 0.001, batch sizes 64-256, epochs 10-100. Hyper-parameters tuned on separate validation split for SCE/ANGLE; cost-sensitive approach uses no extra validation.\nMetrics: test empirical zero-one-c risk, accuracy on accepted instances, rejection ratio. 10 independent trials for each c∈{0.1,…,0.4}.",
        "limitations": "• Assumes constant rejection cost c<0.5 known at training time; does not handle instance-dependent or higher costs.\n• Needs training K cost-sensitive classifiers (one-vs-rest), which may scale poorly for very large K.\n• Experiments limited to medium-scale datasets; no large-scale or real industrial applications shown.\n• Comparative baselines required extra validation data, potentially biasing favorably; hyper-parameter search for proposed method minimal—performance may vary with tuning.\n• Rejection conditions rely on linear score sign; effectiveness with highly calibrated probabilistic models not explored.\n• Theoretical guarantees depend on loss being classification-calibrated and accurate estimation of class prior in PU case.",
        "future_research_directions": "1) Extend to heterogeneous or input-dependent rejection costs and decision-theoretic settings.\n2) Reduce computational overhead for large K (e.g., sharing representations or using all-in-one multiclass cost-sensitive formulations).\n3) Explore richer rejection criteria, integration with uncertainty quantification, and dynamic threshold learning.\n4) Apply framework to large-scale deep architectures and real safety-critical domains (medical imaging, autonomous driving).\n5) Combine with semi-supervised, domain-adaptation, or class-imbalance techniques; investigate robustness to adversarial noise.\n6) Theoretically analyze and design calibrated symmetric or non-convex losses tailored for rejection.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Enhancing Cross-lingual Transfer by Manifold Mixup",
      "full_text": "Published as a conference paper at ICLR 2022 ENHANCING CROSS -LINGUAL TRANSFER BY MANI - FOLD MIXUP Huiyun Yang1, Huadong Chen1, Hao Zhou∗1, Lei Li2 1ByteDance AI Lab 2University of California, Santa Barbara {yanghuiyun.11,chenhuadong.howard, zhouhao.nlp}@bytedance.com leili@cs.ucsb.edu ABSTRACT Based on large-scale pre-trained multilingual representations, recent cross-lingual transfer methods have achieved impressive transfer performances. However, the performance of target languages still lags far behind the source language. In this paper, our analyses indicate such a performance gap is strongly associated with the cross-lingual representation discrepancy. To achieve better cross-lingual transfer performance, we propose the cross-lingual manifold mixup (X-M IXUP ) method, which adaptively calibrates the representation discrepancy and gives compromised representations for target languages. Experiments on the XTREME benchmark show X-M IXUP achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and reduces the cross-lingual representation discrepancy signiﬁcantly. 1 I NTRODUCTION Many natural language processing tasks have shown exciting progress utilizing deep neural models. However, these deep models often heavily rely on sufﬁcient annotation data, which is not the case in the multilingual setting. The fact is that most of the annotation data are collected for popular languages like English and Spanish (Ponti et al., 2019; Joshi et al., 2020), while many long-tail languages could hardly obtain enough annotations for supervised training. As a result, cross-lingual transfer learning (Prettenhofer & Stein, 2011; Wan et al., 2011; Ruder et al., 2019) is crucial, transferring knowledge from the annotation-rich source language to low-resource or zero-resource target languages. In this paper, we focus on the zero-resource setting, where labeled data are only available in the source language. Recently, multilingual pre-trained models (Conneau & Lample, 2019; Conneau et al., 2020a; Xue et al., 2020) offer an effective way for cross-lingual transfer, which yield a universal embedding space across various languages. Such universal representations make it possible to transfer knowledge from the source language to target languages through the embedding space, signiﬁcantly improving the transfer learning performance (Chen et al., 2019; Zhou et al., 2019; Keung et al., 2019; Fang et al., 2020). Besides, Conneau et al. (2018) proposes translate-train, a simple yet effective cross- lingual data augmentation method, which constructs pseudo-training data for each target language via machine translation. Although these works have achieved impressive improvements in cross-lingual transfer (Hu et al., 2020; Ruder et al., 2021), signiﬁcant performance gaps between the source language and target languages still remain (see Table 1). Hu et al. (2020) refers to the gap as the cross-lingual transfer gap, a difference between the performance on the source and target languages. To investigate how the cross-lingual transfer gap emerges, we perform relevant analyses, demonstrat- ing that transfer performance correlates well with the cross-lingual representation discrepancy(see Section 3 for details). Here the cross-lingual representation discrepancy means the degree of differ- ence between the source and target language representations in the universal embedding space. As shown in Figure 1(a), in translate-train, the representation distribution of Spanish almost overlaps ∗Corresponding author. Code is available at https://github.com/yhy1117/X-Mixup. 1 arXiv:2205.04182v1  [cs.CL]  9 May 2022Published as a conference paper at ICLR 2022 20  10  0 10 20   15 10 5 0 5 10 15   Lang en es ar sw (a) Translate-train 20  10  0 10 20   15 10 5 0 5 10 15   Lang en es ar sw (b) X-M IXUP Figure 1: Representation visualization of four languages: English (en), Spanish (es), Arabic (ar) and Swahili (sw) based on XLM-R. We plot the sentence representation of the XNLI test set, which is parallel across 15 languages. We average hidden states of the last layer to get sentence representations and implement the dimensionality reduction by PCA. Obviously, the cross-lingual representation discrepancies are large in translate-train, but X-M IXUP reduces the discrepancy signiﬁcantly. with English, while Arabic shows a certain representation discrepancy compared with English and Swahili performs larger discrepancy, where translate-train achieves 88.6 accuracy on English, 85.7 on Spanish, 82.2 on Arabic and 77.0 on Swahili. Intuitively, a larger representation discrepancy could lead to a worse cross-lingual transfer performance. In this paper, we propose the Cross-Lingual Manifold Mixup (X-M IXUP ) approach to ﬁll the cross- lingual transfer gap. Based on our analyses, reducing the cross-lingual representation discrepancy is a promising way to narrow the transfer gap. Given the cross-lingual representation discrepancy is hard to remove, X-M IXUP directly faces the issue and explicitly accommodates the representation discrep- ancy in the neural networks, by mixing the representation of the source and target languages during training and inference. With X-M IXUP , the model itself can learn how to escape the discrepancy, which adaptively calibrates the representation discrepancy and gives compromised representations for target languages to achieve better cross-lingual transfer performance. X-M IXUP is motivated by robust deep learning (Vincent et al., 2008), while X-M IXUP adopts the mixup (Zhang et al., 2018) idea to handle the cross-lingual discrepancy. Speciﬁcally, X-M IXUP is designed upon the translate-train approach, faced with the exposure bias (Ranzato et al., 2016) problem and data noise problem. During training, the source sequence is a real sentence and the target sequence is a translated one, while situations are opposite during inference. Besides, the translated text often introduces some noises due to imperfect machine translation systems. To address them, we further impose the Scheduled Sampling (Bengio et al., 2015) and Mixup Ratio in X-M IXUP to handle the distribution shift problem and data noise problem, respectively. We verify X-M IXUP on the cross-lingual understanding benchmark XTREME (Hu et al., 2020), which includes several understanding tasks and covers 40 languages from diverse language families. Experimental results show X-M IXUP achieves 1.8% performance gains across different tasks and lan- guages, comparing with strong baselines. It also reduces the cross-lingual representation discrepancy signiﬁcantly, as Figure 1(b) shows. 2 R ELATED WORK Multilingual Representation Learning Recent studies have demonstrated the superiority of large- scale pre-trained multilingual representations on downstream tasks. Multilingual BERT (mBERT; Devlin et al., 2019) is the ﬁrst work to extend the monolingual pre- training to the multilingual setting. Then, several extensions achieve better cross-lingual performances by introducing more monolingual or parallel data and new pre-training tasks, such as Unicoder (Huang et al., 2019), XLM-R (Conneau et al., 2020a), ALM (Yang et al., 2020), MMTE (Siddhant et al., 2020), InfoXLM (Chi et al., 2020), HICTL (Wei et al., 2020), ERNIE-M (Ouyang et al., 2020), mT5 2Published as a conference paper at ICLR 2022 Table 1: Cross-lingual transfer performances of POS and NER tasks on languages with different data resources or different language families, where there are only labeled training data in English. The data resource refers to the resource of each language utilized in the pre-training process. For the language family, English belongs to the Germanic languages, so we divide languages into two types: Germanic one and others. Results show high-resource languages outperform low-resource ones signiﬁcantly and languages dissimilar to the source language tend to perform worse. Language TypeSource Language Resource Language Family High-resource Low-resource Germanic Other Language en es it pt eu kk mr af de nl ar hi ja mBERT POS 95.5 86.9 88.4 86.2 60.7 70.5 69.4 86.6 85.2 88.6 56.2 67.2 49.2 NER 85.2 77.4 81.5 80.8 66.3 45.8 58.2 77.4 78.0 81.8 41.1 65.0 29.0 XLM-R POS 96.1 88.3 89.4 87.6 72.5 78.1 80.8 89.8 88.5 89.5 67.5 76.4 15.9 NER 84.7 79.6 81.3 81.9 60.9 56.2 68.1 78.9 78.8 84.0 53.0 73.0 23.2 (Xue et al., 2020), nmT5 (Kale et al., 2021), AMBER (Hu et al., 2021) and VECO (Luo et al., 2021). They have been the standard backbones of current cross-lingual transfer methods. Cross-lingual Transfer Learning Cross-lingual transfer learning (Prettenhofer & Stein, 2011; Wan et al., 2011; Ruder et al., 2019) aims to transfer knowledge learned from source languages to target languages. According to the type of transfer learning (Pan & Yang, 2010), previous cross- lingual transfer methods can be divided into three categories: instance transfer, parameter transfer, and feature transfer. The cross-lingual transferability improves a lot when engaged with the instance transfer by translation (i.e. translate-train, translate-test) or other cross-lingual data augmentation methods (Singh et al., 2019; Bornea et al., 2020; Qin et al., 2020; Zheng et al., 2021). Chen et al. (2019) and Zhou et al. (2019) focus on the parameter transfer to learn a share-private model architecture. Besides, other works implement the feature transfer to learn the language-invariant features by adversarial networks (Keung et al., 2019; Chen et al., 2019) or re-alignment (Libovick´y et al., 2020; Zhao et al., 2020). X-M IXUP utilizes both the instance transfer and feature transfer, which is based on the translate-train data augmentation approach and implements the feature transfer by cross-lingual manifold mixup. Mixup and Its Variants Mixup (Zhang et al., 2018) proposes to train models on the linear interpo- lation at both the input level and label level, which is effective to improve the model robustness and generalization. Generally, the interpolated pair is selected randomly. Manifold mixup (Verma et al., 2019) performs the interpolation in the latent space by conducting the linear combinations of hidden states. Previous mixup methods (Chen et al., 2020; Jindal et al., 2020) focus on the monolingual setting. However, X-Mixup focuses on the cross-lingual setting and faces many new challenges (see Section 4 for details). Besides, in contrast to previous mixup methods, X-M IXUP mixes the parallel pairs, which share the same semantics across different languages. As a result, the choice of parallel pairs for interpolation can build a smart connection between the source and target languages. 3 A NALYSES OF THE CROSS -LINGUAL TRANSFER PERFORMANCE In this section 1, we concentrate on the cross-lingual transfer performance and ﬁnd it is strongly associated with the cross-lingual representation discrepancy. Firstly, we observe the cross-lingual transfer performance on different target languages and propose an assumption. Then we conduct qualitative and quantitative analyses to verify it. Although previous studies (Hu et al., 2020; Ruder et al., 2021) have shown impressive improvements on cross-lingual transfer, the cross-lingual transfer gap is still pretty large, more than 16 points in Hu et al. (2020). Furthermore, results in Table 1 show the performance of low-resource languages and dissimilar languages fall far behind other languages in cross-lingual transfer tasks. Compared with English, the representations of other languages, especially low-resource languages, are not well-trained (Lauscher et al., 2020; Wu & Dredze, 2020), because high-resource languages dominate the representation learning process, which results in the cross-lingual representation 1In our analyses, we take English as the source language, and the dissimilar language is the language which is dissimilar to English. 3Published as a conference paper at ICLR 2022 Table 2: Spearman’s rank correlationρbetween the CKA score and cross-lingual transfer performance on two XTREME tasks, where †denotes training on the source language, and ‡denotes the translate- train approach. ∗denotes the p-value is lower than 0.05. Results indicate the correlation is solid. Task XNLI † XNLI‡ PAWS-X† PAWS-X‡ ρ 0.76∗ 0.69∗ 0.90∗ 0.93∗ discrepancy. Besides, dissimilar languages often show differences in language characteristics (like vocabulary, word order), which also leads to the representation discrepancy. As a result, we assume that the cross-lingual transfer performance is closely related to the representation discrepancy between the source language and target languages. Following Conneau et al. (2020b), we utilize the linear centered kernel alignment (CKA; Kornblith et al., 2019) score to indicate the cross-lingual representation discrepancy CKA(X,Y ) = ||Y⊤X||2 F ||X⊤X||2 F||Y⊤Y||2 F , (1) where X and Y are parallel sequences from the source and target languages, respectively. A higher CKA score denotes a smaller cross-lingual representation discrepancy. To verify our assumption, we perform qualitative and quantitative analyses on the relationship between the CKA score and cross-lingual transfer performance. Figure 3 in Appendix B indicates a higher CKA score tends to induce better cross-lingual transfer performance. We also calculate the Spearman’s rank correlation between the CKA score and the transfer performance in Table 2, which shows a strong correlation between them. Both the trend and correlation score conﬁrm the cross- lingual transfer performance is highly related to the cross-lingual representation discrepancy. 4 M ETHODOLOGY : X-M IXUP Based on the aforementioned analyses, we believe that reducing the cross-lingual representation discrepancy is the key to ﬁlling the cross-lingual transfer gap. In this section, we propose X-M IXUP to explicitly reduce the representation discrepancy by implementing the manifold mixup between the source language and target language. With X-M IXUP , the model can adaptively calibrate the representation discrepancy and give compromised representations for target languages. This section will ﬁrst introduce the overall architecture of X-M IXUP and its details. After that, the training objectives and inference process will be shown. 4.1 O VERALL ARCHITECTURE Figure 2 illustrates the overall architecture of X-Mixup. Sequences from the source and target languages are ﬁrst encoded separately. Then within the encoder, X-M IXUP implements the manifold mixup between the paired sequences (original sequence and its translation) within a speciﬁc layer, where Mixup Ratio controls the degree of mixup andScheduled Sampling schedules the data sampling process during training. Notations We use Sto denote the source language and T to denote the target language. hl denotes the hidden states of a sequence in layer l. Ddenotes the real text data collection and ˜Ddenotes the translation data collection. For downstream understanding tasks, there are annotation data in the source language DTrain S = (XTrain S ,YTrain S ) and raw test data in the target language DTest T = (XTest T ). Through translate-train, we can get pseudo-training data in the target language ˜DTrain T = ( ˜XTrain T , ˜YTrain T ). Similarly, through translate-test, we can get pseudo-test data in the source language ˜DTest S = ( ˜XTest S ). During training, the Scheduled Sampling process uses translation data 2 ˜DTrain S = ( ˜XTrain S ) from the source language. Note that we use translation data ( ˜XTrain T and ˜XTest S ) and translate- train labels ( ˜YTrain T ) from the ofﬁcial XTREME repository, which is in the same setting as baselines. 2These data are acquired by forward translation (from Sto T) then backward translation (from T to S). 4Published as a conference paper at ICLR 2022 ×n×n Multi-head Attention LN Feed ForwardAdd & LN 𝐡! 𝐡\"QKVMulti-head Attention Feed ForwardAdd & LN QKV Add & LNMulti-head AttentionQKV Add & LN 𝐡\"|! 𝐡\" '𝐡\" Train:Inference: X-Mixup Output layer Output layer ℒ$%&'(') ℒ)*'+\" ℒ)*'+! 𝐱!∈+𝒟!\",') 𝐱\"∈+𝒟\"\"-*(&𝐱!∈𝒟!\"-*(&∪+𝒟!\"-*(&𝐱\"∈𝒟\"\",') λ Source language Target language𝐀 Share parameters Figure 2: The model architecture of X-M IXUP , where the cross-lingual manifold mixup process is in the green block. Note that the manifold mixup process is implemented only in a certain layer (the same layer of both sides), and in other layers the process is omitted. Basic Model We use mBERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020a) as the backbone model. Within each layer, there are two sub-layers: the multi-head attention layer and the feed-forward layer3, followed by the residual connection and layer norm. We use the same multi-head attention layer (see details in Appendix A.1) as BERT (Devlin et al., 2019), where inputs are query, key, and value respectively. In layer l+ 1, the hidden states of the source sequence xS and target sequence xT are acquired by the multi-head attention hl+1 S = MultiHead(hl S,hl S,hl S), hl+1 T = MultiHead(hl T ,hl T ,hl T ). (2) Manifold Mixup To reduce the cross-lingual representation discrepancy, a straightforward idea is to ﬁnd compromised representations between the source and target languages. It’s difﬁcult to ﬁnd such representations because of varying degrees of differences across languages, like vocabulary and word order. However, manifold mixup provides an elegant way to get intermediate representations by conducting linear interpolation on hidden states. To extract target-related information from the source hidden states, the target hidden states are used as the query, and the source hidden states are used as the key and value. This cross-attention process is computed as hl+1 T|S = MultiHead(hl+1 T ,hl+1 S ,hl+1 S ), (3) which shares parameters with the multi-head attention. The manifold mixup process mixes the target hidden states hl+1 T and the source-aware target hidden states hl+1 T|S based on the mixup ratio λ ˜hl+1 T = LN(λhl+1 T|S + (1 −λ)hl+1 T ), (4) where λis an instance-level parameter, ranging from 0 to 1, and indicates the degree of manifold mixup. LN denotes the layer norm operation. Mixup Ratio The machine translation process may change the original semantics and introduce data noises in varying degrees (Castilho et al., 2017; Fomicheva et al., 2020). Thus we introduce the 3In this section, the feed-forward layer is omitted for simpliﬁcation. 5Published as a conference paper at ICLR 2022 translation quality modeling in the mixup process to handle this problem. Following Fomicheva et al. (2020), we use the entropy of attention weights to measure the translation quality H(A) = −1 I I∑ i J∑ j AjilogAji, where Aij = softmax( hTi h⊤ Sj √n ). (5) I is the number of target tokens and J is the number of source tokens. Lower entropy implies better cross-lingual alignment and higher translation quality. To introduce the translation quality modeling into the manifold mixup process, we compute the mixup ratio as λ= λ0 ·σ[(H(A) +H(A⊤))W+ b], where σis the sigmoid function, and W, bare trainable parameters. λ0 is the max value of the mixup ratio, which is set to 0.5 in this paper. We consider two-way alignment in the translation quality modeling, i.e. H(A) and H(A⊤). Scheduled Sampling The source sequences utilized in training and inference are drawn from different distributions. During training, the source sequence is a real text from DTrain S , while during inference, the source sequence is a translation from ˜DTest S . This discrepancy, commonly called the exposure bias, leads to a gap between training and inference. Motivated by the scheduled sampling approach (Bengio et al., 2015) in NMT, we sample the source sequence dynamically during training. Speciﬁcally, the source sequence fed into the manifold mixup is either a real text from DTrain S or translation from ˜DTrain S with a certain probability p { p<= p∗, xs ∈DTrain S , p>p ∗, xs ∈ ˜DTrain S , (6) where p∗is decreasing during training to match the situation of inference. We utilize the inverse sigmoid decay (Bengio et al., 2015), which decreases p∗as a function of the index of mini-batch. 4.2 F INAL TRAINING OBJECTIVE The training loss is composed of two parts: the task loss and the consistency loss L= Ltask task loss + MSE(rS,rT ) + KL(pS,pT )   consistency loss . (7) where MSE(·) is Mean Squared Error and KL(·) is Kullback-Leibler divergence. r∗is the sequence representation4 and p∗is the predicted probability distribution of downstream tasks. The task loss Ltask is the sum of the source language task loss LS task and target language one LT task, weighted by the hyper-parameter α, which is utilized to balance the training process Ltask = αLS task + (1 −α)LT task. (8) For classiﬁcation, structured prediction, and span extraction tasks, the task loss is the cross-entropy loss (see details in Appendix A.2). For structured prediction tasks, it is non-trivial to implement the token-level label mapping across different languages. Thus we use the label probability distribution, predicted by the source language task model, as the pseudo-label for training, where tokens and labels are corresponding. The consistency loss is composed of two parts: the representation consistency loss and the prediction consistency loss. The ﬁrst loss is a regularization term and provides a way to align representations across different languages (Ruder et al., 2019). The second loss is to make better use of the supervision of downstream tasks. It only exists in the classiﬁcation task, as the translation process does not change the label of this task, while in other tasks, it does. 4.3 I NFERENCE During inference, the manifold mixup process is the same as training, except for the Scheduled Sampling process. Concretely, for the source language, only translation data are available in the 4We utilize the mean pooling of the last layer’s hidden states as the sequence representation, which is independent of the sequence length. 6Published as a conference paper at ICLR 2022 Table 3: Main results on the XTREME benchmark. †denotes using other data augmentation strategy in addition to machine translation. ‡denotes results from Ruder et al. (2021), which is an updated version of Hu et al. (2020). Model Pair Sentence Structured Prediction Question Answering Avg.XNLI PAWS-X POS NER XQuAD MLQA TyDiQA Metrics Acc Acc F1 F1 F1/EM F1/EM F1/EM - Based on XLM-R-large XLM-R (Hu et al., 2020) 79.2 86.4 73.8 65.4 76.6/60.8 71.6/53.2 65.1/45.0 70.1 Trans-train (Wei et al., 2020) 82.9 90.1 74.6 66.8 80.4/65.6 72.4/54.7 66.2/48.2 72.6 Filter (Fang et al., 2020) 83.9 91.4 76.2 67.7 82.4/68.0 76.2/57.7 68.3/50.9 74.4 XTUNE(Zheng et al., 2021) 84.8 91.6 79.3† 69.9† 82.5/69.0† 75.0/57.1† 75.4/60.8† 76.5 X-MIXUP 85.3 91.8 78.4 69.0 82.6/69.3 76.5/58.1 69.0/52.8 75.5 Based on mBERT mBERT (Hu et al., 2020) 65.4 81.9 71.5 62.2 64.5/49.4 61.4/44.2 59.7/43.9 63.2 Joint-Align (Zhao et al., 2020) 72.3 - - - - - - - Trans-train (Hu et al., 2020) 75.1 88.9 - - 72.4/58.3 67.6/49.8 59.5/45.8 ‡ - X-MIXUP 78.8 89.7 76.5 65.0 73.3/58.9 69.0/50.9 60.8/46.5 70.0 inference stage, without real data, so we use xs ∈ ˜DTest S . For classiﬁcation tasks, we synthesize the predictions of both the source and target sequences by taking the mean of the predicted probability distributions as the ﬁnal prediction. For structured prediction and QA tasks, we only consider the prediction of the target sequence. 5 E XPERIMENTS This section ﬁrst introduces the cross-lingual understanding benchmark, XTREME. Then brieﬂy introduces the conﬁgurations of downstream tasks and baselines. Finally, shows the main results of baselines and X-M IXUP on XTREME. 5.1 T ASKS AND SETTINGS Tasks In our experiments, we focus on three types of tasks in XTREME: (1) sentence pair classiﬁ- cation task: XNLI (Conneau et al., 2018) and PAWS-X (Yang et al., 2019); (2) structured prediction task: POS (Nivre et al., 2018) and NER (Pan et al., 2017); (3) question answering task: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020) and TyDiQA (Clark et al., 2020). The details of these datasets can refer to Hu et al. (2020). We utilize the translate-train and translate-test data from the XTREME repo5, which also provide the pseudo-label of translate-train data for classiﬁcation tasks and question answering tasks. The rest translation data are from Google Translate6. Models Experiments are based on two multilingual pre-trained models: mBERT and XLM-R. We use the pre-trained models of Huggingface Transformers7 as the backbone model. Hyper-parameters We select XNLI, POS, and MLQA as representative tasks to search for the best hyper-parameters. The ﬁnal model is selected based on the averaged performance of all languages on the dev set. We perform grid search over the balance training parameterαand learning rate from [0.2, 0.4, 0.6, 0.8] and [3e-6, 5e-6, 2e-5, 3e-5]. We also search for the best manifold mixup layer from [1, 4, 8, 12, 16, 20, 24]. In ﬁnal results, we implement mixup in the ﬁrst layer for classiﬁcation tasks, 4th layer for structured prediction tasks. For QA tasks, we implement mixup in the 16th layer for large model, 8th layer for base model. Concrete details of experiments are presented in Appendix C.1. 5.2 B ASELINES We conduct experiments on two strong multilingual pre-trained models to verify the generality of methods: (1) mBERT Multilingual BERT is a 12-layer transformer model pre-trained on the 5https://github.com/google-research/xtreme. 6https://translate.google.com/. 7We use bert-base-multilingual-cased for mBERT and xlm-roberta-large for XLM-R. 7Published as a conference paper at ICLR 2022 Table 4: Comparisons between X-M IXUP and XTUNE under the same setting: XLM-R-base model and machine translation data augmentation. Results of XTUNE are from Zheng et al. (2021) Table 4. Model XNLI POS MLQA XTUNE R1 (Zheng et al., 2021) 79.7 - - XTUNE R2 (Zheng et al., 2021) 78.9 76.6 68.7/51.1 X-M IXUP 80.4 77.8 71.2/53.1 Wikipedias of 104 languages. (2) XLM-R XLM-R-large is a 24-layer transformer model pre- trained on 2.5T data extracted from Common Crawl covering 100 languages. Based on them, these are some strong baselines: (1) Trans-train Abbreviation for Translate-train. The training set of the source language is machine-translated to each target language and then the model is trained on the concatenation of all training sets. (2) Joint-Align Zhao et al. (2020) aligns the monolingual sub-spaces of the source and target language by minimizing the distances of embeddings for matched word pairs. (3) Filter Fang et al. (2020) splices the representation of the target sequence and its translation in intermediate layers to extract multilingual knowledge. (4) XTUNE Zheng et al. (2021) uses two types of consistency regularization based on four types of data augmentation. 5.3 M AIN RESULTS Results on the XTREME benchmark are shown in Table 3. Concrete results for each task are presented in Appendix C.2. Compared with strong baselines, X-M IXUP shows its superiority across different backbones and tasks, which indicates its generality. X-M IXUP outperforms Trans-train by 2.2% based on mBERT, and X-M IXUP outperforms Filter by 1.5% based on XLM-R. The superiority of X-M IXUP over Filter is that X-M IXUP gives a calibrated representation for target languages, not just the concatenation of two representations. Besides, X-M IXUP considers the noise of translation data and limits the noise propagation by introducing mixup ratio. XTUNE achieves the best results on structured prediction tasks and the low-resource QA task TyDiQA (only 3.7k training data in English), but XTUNE uses three other data augmentation approaches in addition to machine translation. To make a fairer comparison, we conduct experiments under the same setting in Table 4, which indicates X-M IXUP outperforms XTUNE on three types of tasks with only machine translation data augmentation. Besides, X-M IXUP only needs one-stage training, while XTUNE implements a two-stage training algorithm. However, X-M IXUP and XTUNE are complementary, where the former focuses on ﬁnding better representations for target languages while the latter concentrates on the cross-lingual data augmentation and consistency regularization. 6 A NALYSIS AND DISCUSSION To better understand X-M IXUP and explore how X-M IXUP inﬂuences the cross-lingual transfer performance, we conduct analyses8 on several questions. Results show X-M IXUP achieves perfor- mance improvements across different languages and it also reduces the cross-lingual representation discrepancy obviously. Table 6 in Appendix B veriﬁes the effectiveness of X-Mixup on both seen and unseen languages. Besides, ablation results show the cross-lingual manifold mixup training contributes a lot to cross-lingual transfer. (Q1) How X-M IXUP inﬂuences the cross-lingual representation discrepancy? Language centroid (Rosenberg & Hirschberg, 2007) is the mean of the representations within each language. We plot the language centroid of different methods (see Figure 4 in Appendix B), which indicates X-M IXUP brings closer language centroids signiﬁcantly. We also calculate the CKA scores of the XNLI dataset (see Table 7 in Appendix B). Results show X-M IXUP reduces the cross-lingual representation discrepancy evenly across different target languages, improving the CKA score by 10.4% on average. In conclusion, both the language centroids visualization and the CKA score improvement indicate X-M IXUP reduces the cross-lingual representation discrepancy effectively. 8In this section, we utilize the XLM-R-large model as the backbone model. 8Published as a conference paper at ICLR 2022 Table 5: Ablation results on X-M IXUP , where w/o mixup denotes remove the cross-lingual manifold mixup during training and inference and λ= λ0 denotes a constant mixup ratio. Model XNLI POS MLQA X-M IXUP 85.3 78.4 76.5/58.1 w/o mixup 82.9 75.7 72.7/54.8 w/o mixup inference 84.0 77.6 75.6/57.3 w/o scheduled sampling 84.6 78.0 76.3/57.9 w/o consistency loss 84.2 78.0 76.5/58.0 λ= λ0 84.1 77.8 75.8/57.5 (Q2) How X-M IXUP inﬂuences the cross-lingual transfer gap? We compare the cross-lingual transfer gap in Appendix B Table 8. Compared with Trans-train, X-M IXUP reduces the averaged gap by 39.8% and shows its superiority across three types of tasks. Compared with state-of-the-art methods, X-M IXUP achieves the smallest cross-lingual transfer gap on four out of seven datasets, which suggests the effectiveness of X-MIXUP on classiﬁcation and QA tasks. (Q3) What is the essential component of X-M IXUP ? There are ﬁve major components of X- MIXUP : cross-lingual manifold mixup training, mixup inference, Mixup Ratio, Scheduled Sampling, and consistency loss. To better understand X-M IXUP , we implement ablation studies in Table 5. Comparisons between X-M IXUP and w/o mixup show the effectiveness of cross-lingual manifold mixup across different tasks, and even without mixup inference (translate-test data), the mixup training can also achieve 2.6% improvements on average. Besides, comparisons between X-M IXUP and λ = λ0 show the effectiveness of introducing the translation quality modeling in the mixup process. Scheduled sampling achieves more performance improvements on the classiﬁcation task, as the task shares labels across languages, and scheduled sampling can prevent the model from solely relying on the gold source sequence to make predictions. In addition, the consistency loss is also more effective on the classiﬁcation task, because there is additional prediction consistency loss which can transfer the task capability from the source language to target languages. Detailed ablation results on the consistency loss are shown in Appendix B Table 9, which shows the KL consistency loss contributes more than the MSE consistency loss on the classiﬁcation task. (Q4) Which layer is the best to implement the manifold mixup? We implement the cross-lingual manifold mixup in different layers (see Figure 5 in Appendix B for details) and ﬁnd different tasks prefer different mixup layers. Although different tasks have their own preferences, no matter which layer we mix, the cross-lingual transfer performance can be improved, except for mixing within a higher layer on classiﬁcation tasks. The drop in classiﬁcation task is mainly because the source and target sequences share the same task label. Performing mixup in a higher layer may make the model rely on the source sequence and ignore the target sequence. The structured prediction task is not sensitive to the mixup layer, mainly because this task relies on both the short and long dependence. For QA tasks, the cross-lingual transfer performance shows a trend from rise to decline as the mixup layer increases. The QA task needs higher-level understanding, but higher layers are more language-speciﬁc, where sequences from different languages have different gold answers. 7 C ONCLUSION This paper focuses on enhancing the cross-lingual transfer performance on understanding tasks. Considering the large cross-lingual transfer gap in recent works, this paper ﬁrst analyses related factors and ﬁnds this gap is strongly associated with the cross-lingual representation discrepancy. Then X-M IXUP is proposed to alleviate the discrepancy, which gives compromised representations for target languages by implementing the manifold mixup between the source and target languages. Empirical evaluations on XTREME verify the effectiveness of X-M IXUP across different tasks and languages. Besides, both the visualization and quantitative analyses show X-M IXUP reduces the cross-lingual representation discrepancy effectively. Furthermore, X-Mixup can also be applied to the multilingual pre-training process by implementing the cross-lingual manifold mixup on parallel data. Findings on the relationship between the cross-lingual transfer performance and representation discrepancy shed light on a promising way to boost cross-lingual transfer for future research. 9Published as a conference paper at ICLR 2022 REFERENCES Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolin- gual representations. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pp. 4623–4637, Online, July 2020. Association for Computational Linguis- tics. doi: 10.18653/v1/2020.acl-main.421. URL https://www.aclweb.org/anthology/ 2020.acl-main.421. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Informa- tion Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada , pp. 1171–1179, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/ e995f98d56967d946471af29d7bf99f1-Abstract.html. Mihaela Bornea, Lin Pan, Sara Rosenthal, Radu Florian, and Avirup Sil. Multilingual transfer learning for QA using translation as data augmentation. CoRR, abs/2012.05958, 2020. URL https://arxiv.org/abs/2012.05958. Sheila Castilho, Joss Moorkens, Federico Gaspari, Iacer Calixto, John Tinsley, and Andy Way. Is neural machine translation the new state of the art? Prague Bull. Math. Linguistics, 108:109–120, 2017. URL http://ufal.mff.cuni.cz/pbml/108/ art-castilho-moorkens-gaspari-tinsley-calixto-way.pdf . Jiaao Chen, Zichao Yang, and Diyi Yang. MixText: Linguistically-informed interpolation of hid- den space for semi-supervised text classiﬁcation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 2147–2157, Online, July 2020. Asso- ciation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.194. URL https: //aclanthology.org/2020.acl-main.194. Xilun Chen, Ahmed Hassan Awadallah, Hany Hassan, Wei Wang, and Claire Cardie. Multi-source cross-lingual model transfer: Learning what to share. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 3098–3112, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1299. URL https://www. aclweb.org/anthology/P19-1299. Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. Infoxlm: An information-theoretic framework for cross- lingual language model pre-training. CoRR, abs/2007.07834, 2020. URL https://arxiv. org/abs/2007.07834. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Lin- guistics, 8:454–470, 2020. doi: 10.1162/tacl a 00317. URL https://www.aclweb.org/ anthology/2020.tacl-1.30. Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 7057–7067, 2019. URL https://proceedings.neurips.cc/paper/ 2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2475–2485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1269. URL https://www.aclweb.org/anthology/D18-1269. 10Published as a conference paper at ICLR 2022 Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm ´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un- supervised cross-lingual representation learning at scale. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics , pp. 8440–8451, Online, July 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https://www.aclweb.org/anthology/2020.acl-main.747. Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. Emerging cross- lingual structure in pretrained language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6022–6034, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.536. URL https://www. aclweb.org/anthology/2020.acl-main.536. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //www.aclweb.org/anthology/N19-1423. Yuwei Fang, Shuohang Wang, Zhe Gan, Siqi Sun, and Jingjing Liu. FILTER: an enhanced fusion method for cross-lingual language understanding. CoRR, abs/2009.05166, 2020. URL https: //arxiv.org/abs/2009.05166. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Fr´ed´eric Blain, Francisco Guzm´an, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised quality estimation for neural machine translation. Trans. Assoc. Comput. Linguistics, 8:539–555, 2020. URL https: //transacl.org/ojs/index.php/tacl/article/view/1997. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization. CoRR, abs/2003.11080, 2020. Junjie Hu, Melvin Johnson, Orhan Firat, Aditya Siddhant, and Graham Neubig. Explicit alignment objectives for multilingual bidirectional encoders. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T¨ur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 3633–3643. Association for Computational Linguistics, 2021. URL https://doi.org/10.18653/v1/2021.naacl-main.284. Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. Unicoder: A universal language encoder by pre-training with multiple cross-lingual tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2485–2494, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1252. URL https://www.aclweb.org/anthology/D19-1252. Amit Jindal, Arijit Ghosh Chowdhury, Aniket Didolkar, Di Jin, Ramit Sawhney, and Rajiv Ratn Shah. Augmenting NLP models using latent feature interpolations. In Donia Scott, N ´uria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020 , pp. 6931–6936. International Committee on Computational Linguistics, 2020. doi: 10.18653/v1/2020.coling-main. 611. URL https://doi.org/10.18653/v1/2020.coling-main.611. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6282–6293, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL https://www.aclweb.org/anthology/2020.acl-main.560. 11Published as a conference paper at ICLR 2022 Mihir Kale, Aditya Siddhant, Rami Al-Rfou, Linting Xue, Noah Constant, and Melvin Johnson. nmt5 - is parallel data still relevant for pre-training massively multilingual language models? In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021, pp. 683–691. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021. acl-short.87. URL https://doi.org/10.18653/v1/2021.acl-short.87. Phillip Keung, Yichao Lu, and Vikas Bhardwaj. Adversarial learning with contextual embeddings for zero-resource cross-lingual classiﬁcation and NER. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1355–1360, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1138. URL https: //www.aclweb.org/anthology/D19-1138. Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neural net- work representations revisited. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceed- ings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 3519–3529. PMLR, 2019. URL http://proceedings.mlr.press/v97/kornblith19a.html. Anne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and Goran Glava ˇs. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4483– 4499, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.363. URL https://www.aclweb.org/anthology/2020.emnlp-main. 363. Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. MLQA: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7315–7330, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.653. URL https://www. aclweb.org/anthology/2020.acl-main.653. Jindˇrich Libovick´y, Rudolf Rosa, and Alexander Fraser. On the language neutrality of pre-trained multilingual representations. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1663–1674, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.150. URL https://www.aclweb.org/anthology/ 2020.findings-emnlp.150. Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang, and Luo Si. VECO: variable and ﬂexible cross-lingual pre-training for language understanding and generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 3980–3994. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.308. URL https://doi.org/10.18653/v1/2021. acl-long.308. Joakim Nivre, Mitchell Abrams, Zeljko Agic, Lars Ahrenberg, Lene Antonsen, and et al. Universal De- pendencies 2.2, 2018. URL https://hal.archives-ouvertes.fr/hal-01930733. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics ( ´UFAL), Faculty of Mathematics and Physics, Charles University. Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE- M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. CoRR, abs/2012.15674, 2020. URL https://arxiv.org/abs/2012.15674. Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22 (10):1345–1359, 2010. doi: 10.1109/TKDE.2009.191. URL https://doi.org/10.1109/ TKDE.2009.191. 12Published as a conference paper at ICLR 2022 Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1946–1958, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1178. URL https://www.aclweb.org/anthology/P17-1178. Edoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak, Ivan Vuli´c, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, and Anna Korhonen. Modeling language variation and universals: A survey on typological linguistics for natural language processing. Computational Linguistics, 45(3): 559–601, September 2019. doi: 10.1162/coli a 00357. URL https://www.aclweb.org/ anthology/J19-3005. Peter Prettenhofer and Benno Stein. Cross-lingual adaptation using structural correspondence learning. ACM Trans. Intell. Syst. Technol., 3(1):13:1–13:22, 2011. doi: 10.1145/2036264.2036277. URL https://doi.org/10.1145/2036264.2036277. Libo Qin, Minheng Ni, Yue Zhang, and Wanxiang Che. Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual NLP. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2020, pp. 3853–3860. ijcai.org, 2020. doi: 10.24963/ijcai.2020/533. URL https://doi.org/10.24963/ijcai. 2020/533. Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.06732. Andrew Rosenberg and Julia Hirschberg. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL) , pp. 410–420, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D07-1043. Sebastian Ruder, Ivan Vulic, and Anders Søgaard. A survey of cross-lingual word embedding models. J. Artif. Intell. Res. , 65:569–631, 2019. doi: 10.1613/jair.1.11640. URL https: //doi.org/10.1613/jair.1.11640. Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, and Melvin Johnson. XTREME-R: towards more challenging and nuanced multilingual evaluation. CoRR, abs/2104.07412, 2021. URL https://arxiv.org/ abs/2104.07412. Aditya Siddhant, Melvin Johnson, Henry Tsai, Naveen Ari, Jason Riesa, Ankur Bapna, Orhan Firat, and Karthik Raman. Evaluating the cross-lingual effectiveness of massively multilingual neural machine translation. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pp. 8854–8861. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6414. Jasdeep Singh, Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. XLDA: cross-lingual data augmentation for natural language inference and question answering. CoRR, abs/1905.11471, 2019. URL http://arxiv.org/abs/1905.11471. Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najaﬁ, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research, pp. 6438–6447. PMLR, 2019. URL http://proceedings.mlr.press/v97/verma19a.html. 13Published as a conference paper at ICLR 2022 Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096–1103, 2008. C. Wan, Rong Pan, and Jiefei Li. Bi-weighting domain adaptation for cross-language text classiﬁca- tion. In IJCAI, 2011. Xiangpeng Wei, Yue Hu, Rongxiang Weng, Luxi Xing, Heng Yu, and Weihua Luo. On learning universal representations across languages. CoRR, abs/2007.15960, 2020. URL https:// arxiv.org/abs/2007.15960. Shijie Wu and Mark Dredze. Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP , pp. 120–130, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.repl4nlp-1.16. URL https: //www.aclweb.org/anthology/2020.repl4nlp-1.16. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. CoRR, abs/2010.11934, 2020. URL https://arxiv.org/abs/2010.11934. Jian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi Wu, Zhoujun Li, and Ming Zhou. Alternating language modeling for cross-lingual pre-training. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelli- gence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 9386–9393. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6480. Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset for paraphrase identiﬁcation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3687–3692, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1382. URL https://www.aclweb. org/anthology/D19-1382. Hongyi Zhang, Moustapha Ciss´e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=r1Ddp1-Rb. Wei Zhao, Steffen Eger, Johannes Bjerva, and Isabelle Augenstein. Inducing language-agnostic multilingual representations. CoRR, abs/2008.09112, 2020. URL https://arxiv.org/abs/ 2008.09112. Bo Zheng, Li Dong, Shaohan Huang, Wenhui Wang, Zewen Chi, Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, and Furu Wei. Consistency regularization for cross-lingual ﬁne-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3403–3417, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.264. URL https://aclanthology.org/2021.acl-long.264. Joey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu, Meng Fang, Rick Siow Mong Goh, and Kenneth Kwok. Dual adversarial neural transfer for low-resource named entity recognition. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 3461–3471, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1336. URL https://www.aclweb.org/anthology/P19-1336. 14Published as a conference paper at ICLR 2022 A M ETHOD DETAILS A.1 M ULTI -HEAD ATTENTION In the multi-head attention layer, multiple attention heads are concatenated MultiHead(Q,K,V ) = Concat(head1,...,h)WO, (9) and each head is the scaled dot-product attention headi = Attention(QWq i ,KWk i ,V Wv i ), (10) Attention(Q,K,V ) = softmax(QK⊤ √ d )V, (11) where WO, Wq, Wk and Wv are trainable parameters. A.2 T RAINING OBJECTIVE For classiﬁcation tasks (e.g. NLI), the task loss is the cross-entropy loss Ltask = − C∑ j yjlogpj, (12) where Cis the size of the label set. For structured prediction tasks (e.g. POS) and span extraction tasks (e.g. QA), the task loss is also the cross-entropy loss Ltask = − n∑ i C∑ j yijlogpij. (13) where nis the sequence length. 15Published as a conference paper at ICLR 2022 B A NALYSIS RESULTS en es fr de ko zh ja Language 85 90 95Accuracy XLM-R Trans-train (a) PAWS-X enesdevi fr bgtr hi ar elswruurthzh Language 75 80 85Accuracy XLM-R Trans-train (b) XNLI Figure 3: Performances on PAWS-X and XNLI test set, where languages are sorted by decreasing CKA scores. The trend indicates the performance gets worse along with the CKA score decreasing. 0.4  0.2  0.0 0.2 0.4 0.6 0.8 0.2 0.0 0.2 0.4 0.6 af ar bg de el en eset eu fa fi fr he hi hu id it ja kk ko mr nlptru ta te th tl tr urvi zh (a) w/o X-M IXUP 0.4  0.2  0.0 0.2 0.4 0.6 0.8 0.2 0.0 0.2 0.4 0.6 afar bg deel enes eteu fa fi fr he hi huidit ja kk ko mr nlpt ruta te th tl tr ur vi zh (b) X-M IXUP Figure 4: Language centroids visualization of the POS test set, which indicates X-Mixup brings closer these centroids obviously. 1 4 8 12 16 20 24 Layer 72 74 76 78 80 82 84Performance XNLI POS MLQA Figure 5: Performances on implementing X-M IXUP (solid line) in different layers and Trans-train (dashed line) on three downstream tasks. 16Published as a conference paper at ICLR 2022 Table 6: Performances on XNLI test set, where Trans-train and X-M IXUP are trained on 8 seen languages and tested on both these seen languages and 7 unseen languages. ∆ is the performance difference between X-M IXUP and Trans-train. Results show X-M IXUP performs better than Trans- train by a large margin on both seen and unseen languages. Model Seen Languages Unseen Languages Avg.en bg el fr ru th ur zh ar de es hi sw tr vi Trans-train87.6 84.7 84.2 84.6 82.9 80.1 76.5 83.0 82.6 84.5 85.0 80.6 77.8 82.7 82.7 82.6 X-MIXUP 89.5 87.1 86.3 86.8 84.7 82.7 79.0 85.0 85.3 86.3 86.9 82.9 80.3 84.5 84.5 84.8 ∆ +1.9 +2.4 +1.9 +2.2 +1.8 +2.6 +2.5 +2.0 +2.7 +1.8 +1.9 +1.7 +2.5 +1.8 +1.8 +1.8 Table 7: CKA scores and performances on XNLI test set, where ∆ is the score or performance difference between X-M IXUP and Trans-train. Results show X-M IXUP improves the CKA scores evenly across different target languages and the performance improvements are diverse. There is no obvious correlation between the CKA score improvement and performance improvement. Model en es de vi fr bg tr el ru ar hi sw ur th zh Avg. CKA scoreTrans-train 1.00 0.81 0.80 0.78 0.78 0.78 0.76 0.75 0.75 0.74 0.74 0.73 0.72 0.72 0.72 0.77X-MIXUP 1.00 0.88 0.87 0.86 0.86 0.86 0.85 0.83 0.82 0.84 0.83 0.82 0.81 0.80 0.79 0.85∆ 0.00 0.07 0.07 0.08 0.08 0.08 0.09 0.08 0.07 0.10 0.09 0.09 0.09 0.08 0.07 0.08 PerformanceTrans-train 88.6 85.7 84.5 82.6 84.2 85.2 82.1 84.5 81.8 82.2 80.8 77.0 77.7 80.2 82.7 82.6X-MIXUP 89.9 87.7 86.9 85.4 87.1 87.3 84.9 86.8 85.1 85.2 83.5 81.2 79.6 83.2 85.2 85.3∆ +1.3 +2.0 +2.4 +2.8 +2.9 +2.1 +2.8 +2.3 +3.3 +3.0 +2.7 +4.2 +1.9 +3.0 +2.5 +2.7 Table 8: The cross-lingual transfer gap (lower is better) of different methods on the XTREME benchmark. For QA tasks, we only show EM scores. †denotes results from Wei et al. (2020). Overall, X-M IXUP achieves the smallest cross-lingual transfer gap on four out of seven datasets. Model XNLI PAWS-X POS NER XQuAD MLQA TyDiQA Avg. mBERT (Hu et al., 2020) 16.5 14.1 25.5 23.6 25.0 27.5 22.2 22.1 XLM-R (Hu et al., 2020) 10.2 12.4 24.3 19.8 16.3 19.1 13.1 16.5 Trans-train (Hu et al., 2020) 7.3 9.0 22.4 † 20.5† 17.6 22.2 24.2 17.6 Filter (Fang et al., 2020) 6.0 5.2 19.7 16.3 7.3 15.7 9.2 11.3 XTUNE (Zheng et al., 2021) 5.5 5.2 17.3 14.8 10.1 18.5 0.9 10.3 X-MIXUP 4.9 5.2 18.1 15.9 6.7 13.9 9.6 10.6 Table 9: Ablation results on the consistency loss, which show the KL consistency loss contributes more than the MSE consistency loss on the classiﬁcation task. Model XNLI POS MLQA X-M IXUP 85.3 78.4 76.5/58.1 w/o MSE consistency loss 84.6 78.0 76.5/58.0 w/o KL consistency loss 84.3 - - w/o both 84.2 - - 17Published as a conference paper at ICLR 2022 C E XPERIMENTAL DETAILS C.1 H YPER -PARAMETERS For all tasks, we ﬁne-tune on 8 Nvidia V100-32GB GPU cards with the batch size 64. For XQuAD and MLQA, we ﬁnetune 2 epochs. For other tasks, we ﬁnetune 4 epochs. There is no dev set in XQuAD, so we use the dev set of MLQA for the model selection. Table 10 shows hyper-parameters used for X-M IXUP . Table 10: Hyper-parameters used for X-M IXUP , where αis used for balanced training in Eq 8 and pk is the scheduled sampling decay rate. Parameter Classiﬁcation Structured Prediction QA α 0.4 0.8 0.2 pk 1000 1000 2000 C.2 D ETAILED RESULTS Detailed results of each tasks and languages are shown below. Results of mBERT, XLM, MMTE and XLM-R are from XTREME (Hu et al., 2020). Results of Filter is the best results of Fang et al. (2020). Model en ar bg de el es fr hi ru sw th tr ur vi zh Avg. mBERT 81.9 73.8 77.6 77.6 75.9 79.1 77.8 70.7 75.4 70.5 70.0 74.3 67.4 77.0 77.6 75.1 XLM 82.8 66.0 71.9 72.7 70.4 75.5 74.3 62.5 69.9 58.1 65.5 66.4 59.8 70.7 70.2 69.1 MMTE 79.6 64.9 70.4 68.2 67.3 71.6 69.5 63.5 66.2 61.9 66.2 63.6 60.0 69.7 69.2 67.5 XLM-R 88.6 82.2 85.2 84.5 84.5 85.7 84.2 80.8 81.8 77.0 80.2 82.1 77.7 82.6 82.7 82.6 Filter 89.5 83.6 86.4 85.6 85.4 86.6 85.7 81.1 83.7 78.7 81.7 83.2 79.1 83.9 83.8 83.9 XTUNE 89.9 84.0 87.0 86.5 86.2 87.4 86.6 83.2 85.2 80.0 82.7 84.1 79.6 84.8 84.3 84.8 X-MIXUP 89.9 85.2 87.3 86.9 86.8 87.7 87.1 83.5 85.1 81.2 83.2 84.9 79.6 85.4 85.2 85.3 Table 11: XNLI accuracy scores for each language. Model en de es fr ja ko zh Avg. mBERT 94.0 85.7 87.4 87.0 73.0 69.6 77.0 81.9 XLM 94.0 85.9 88.3 87.4 69.3 64.8 76.5 80.9 MMTE 93.1 85.1 87.2 86.9 72.0 69.2 75.9 81.3 XLM-R 94.7 89.7 90.1 90.4 78.7 79.0 82.3 86.4 Filter 95.9 92.8 93.0 93.7 87.4 87.6 89.6 91.5 XTUNE 96.1 92.6 93.1 93.9 87.8 89.0 88.8 91.6 X-M IXUP 96.3 93.2 93.6 94.6 87.3 88.2 89.5 91.8 Table 12: PAWS-X accuracy scores for each language. 18Published as a conference paper at ICLR 2022 Model af ar bg de el en es et eu fa ﬁ fr he hi hu id it mBERT 86.6 56.2 85.0 85.2 81.1 95.5 86.9 79.1 60.7 66.7 78.9 84.2 56.2 67.2 78.3 71.0 88.4XLM 88.5 63.1 85.0 85.8 84.3 95.4 85.8 78.3 62.8 64.7 78.4 82.8 65.9 66.2 77.3 70.2 87.4XLM-R 89.8 67.5 88.1 88.5 86.3 96.1 88.3 86.5 72.5 70.6 85.8 87.2 68.3 76.4 82.6 72.4 89.4Filter 88.7 66.1 88.5 89.2 88.3 96.0 89.1 86.3 78.0 70.8 86.1 88.9 64.9 76.7 82.6 72.6 89.8 XTUNE 90.4 72.8 89.0 89.4 87.0 96.1 88.8 88.1 73.1 74.7 87.2 89.5 83.5 77.7 83.6 73.2 90.5 X-MIXUP 89.4 70.1 88.8 88.7 86.7 96.0 89.0 88.3 76.2 72.5 87.0 88.2 82.4 78.0 83.8 72.4 90.3 Model ja kk ko mr nl pt ru ta te th tl tr ur vi yo zh Avg. mBERT 49.2 70.5 49.6 69.4 88.6 86.2 85.5 59.0 75.9 41.7 81.4 68.5 57.0 53.2 55.7 61.6 71.5XLM 49.0 70.2 50.1 68.7 88.1 84.9 86.5 59.8 76.8 55.2 76.3 66.4 61.2 52.4 20.5 65.4 71.3XLM-R 15.9 78.1 53.9 80.8 89.5 87.6 89.5 65.2 86.6 47.2 92.2 76.3 70.3 56.8 24.6 25.7 73.8Filter 40.4 80.4 53.3 86.4 89.4 88.3 90.5 65.3 87.3 57.2 94.1 77.0 70.9 58.0 43.1 53.1 76.9 XTUNE 65.3 79.8 56.0 85.5 89.7 89.3 90.8 65.7 85.5 61.4 93.8 78.3 74.0 57.5 27.9 68.8 79.3 X-MIXUP 62.7 79.0 55.3 84.8 89.6 88.8 90.1 63.6 87.4 59.9 93.1 77.1 72.4 59.4 27.3 68.3 78.4 Table 13: POS results (F1) for each language. Model en af ar bg bn de el es et eu fa ﬁ fr he hi hu id it ja jv mBERT 85.2 77.4 41.1 77.0 70.0 78.0 72.5 77.4 75.4 66.3 46.2 77.2 79.6 56.6 65.0 76.4 53.5 81.5 29.0 66.4XLM 82.6 74.9 44.8 76.7 70.0 78.1 73.5 74.8 74.8 62.3 49.2 79.6 78.5 57.7 66.1 76.5 53.1 80.7 23.6 63.0MMTE 77.9 74.9 41.8 75.1 64.9 71.9 68.3 71.8 74.9 62.6 45.6 75.2 73.9 54.2 66.2 73.8 47.9 74.1 31.2 63.9XLM-R 84.7 78.9 53.0 81.4 78.8 78.8 79.5 79.6 79.1 60.9 61.9 79.2 80.5 56.8 73.0 79.8 53.0 81.3 23.2 62.5Filter 83.5 80.4 60.7 83.5 78.4 80.4 80.7 74.0 81.0 66.9 71.3 80.2 79.9 57.4 74.3 82.2 54.0 81.9 24.3 63.5 XTUNE 85.0 80.4 59.1 84.8 79.1 80.5 82.0 78.1 81.5 64.5 65.9 82.2 81.9 62.0 75.0 82.8 55.8 83.1 30.5 65.9 X-MIXUP 84.5 79.0 58.4 84.0 81.4 80.6 81.4 73.8 81.5 65.7 61.6 80.4 80.3 64.4 74.7 82.0 53.4 82.2 38.8 63.5 Model ka kk ko ml mr ms my nl pt ru sw ta te th tl tr ur vi yo zh mBERT 64.6 45.8 59.6 52.3 58.2 72.7 45.2 81.8 80.8 64.0 67.5 50.7 48.5 3.6 71.7 71.8 36.9 71.8 44.9 42.7XLM 67.7 57.2 26.3 59.4 62.4 69.6 47.6 81.2 77.9 63.5 68.4 53.6 49.6 0.3 78.6 71.0 43.0 70.1 26.5 32.4MMTE 60.9 43.9 58.2 44.8 58.5 68.3 42.9 74.8 72.9 58.2 66.3 48.1 46.9 3.9 64.1 61.9 37.2 68.1 32.1 28.9XLMR 71.6 56.2 60.0 67.8 68.1 57.1 54.3 84.0 81.9 69.1 70.5 59.5 55.8 1.3 73.2 76.1 56.4 79.4 33.6 33.1Filter 71.0 51.1 63.8 70.2 69.8 69.3 59.0 84.6 82.1 71.1 70.6 64.3 58.7 2.4 74.4 83.0 73.4 75.8 42.9 35.4 XTUNE 76.3 56.9 67.1 72.6 71.5 72.5 66.7 85.8 82.1 75.2 72.4 66.0 61.8 1.1 77.5 83.7 75.6 80.8 44.9 36.5 X-MIXUP 76.5 51.7 63.9 69.8 71.2 70.4 67.9 84.5 83.1 73.5 70.7 65.6 59.3 4.4 75.0 81.8 73.1 78.2 41.6 47.8 Table 14: NER results (F1) for each language. Model en ar de el es hi ru th tr vi zh Avg. mBERT 83.5 / 72.2 61.5 / 45.1 70.6 / 54.0 62.6 / 44.9 75.5 / 56.9 59.2 / 46.0 71.3 / 53.3 42.7 / 33.5 55.4 / 40.1 69.5 / 49.6 58.0 / 48.3 64.5 / 49.4XLM 74.2 / 62.1 61.4 / 44.7 66.0 / 49.7 57.5 / 39.1 68.2 / 49.8 56.6 / 40.3 65.3 / 48.2 35.4 / 24.5 57.9 / 41.2 65.8 / 47.6 49.7 / 39.7 59.8 / 44.3MMTE 80.1 / 68.1 63.2 / 46.2 68.8 / 50.3 61.3 / 35.9 72.4 / 52.5 61.3 / 47.2 68.4 / 45.2 48.4 / 35.9 58.1 / 40.9 70.9 / 50.1 55.8 / 36.4 64.4 / 46.2XLM-R 86.5 / 75.7 68.6 / 49.0 80.4 / 63.4 79.8 / 61.7 82.0 / 63.9 76.7 / 59.7 80.1 / 64.3 74.2 / 62.8 75.9 / 59.3 79.1 / 59.0 59.3 / 50.0 76.6 / 60.8Filter 86.4 / 74.6 79.5 / 60.7 83.2 / 67.0 83.0 / 64.6 85.0 / 67.9 83.1 / 66.6 82.8 / 67.4 79.6 / 73.2 80.4 / 64.4 83.8 / 64.7 79.9 / 77.0 82.4 / 68.0XTUNE 88.8 / 78.1 79.7 / 63.9 83.7 / 68.2 83.0 / 65.7 84.7 / 68.3 80.7 / 64.9 82.2 / 66.6 81.9 / 76.1 79.3 / 65.0 82.7 / 64.5 81.3 / 78.0 82.5 / 69.0 X-MIXUP86.7 / 75.4 81.3 / 63.5 83.5 / 66.8 84.3 / 67.6 85.2 / 68.2 83.9 / 68.5 83.0 / 67.7 82.6 / 76.9 80.9 / 65.3 84.8 / 66.8 72.4 / 75.6 82.6 / 69.3 Table 15: XQuAD results (F1 / EM) for each language. Model en ar de es hi vi zh Avg. mBERT 80.2 / 67.0 52.3 / 34.6 59.0 / 43.8 67.4 / 49.2 50.2 / 35.3 61.2 / 40.7 59.6 / 38.6 61.4 / 44.2 XLM 68.6 / 55.2 42.5 / 25.2 50.8 / 37.2 54.7 / 37.9 34.4 / 21.1 48.3 / 30.2 40.5 / 21.9 48.5 / 32.6 MMTE 78.5 / – 56.1 / – 58.4 / – 64.9 / – 46.2 / – 59.4 / – 58.3 / – 60.3 / 41.4 XLM-R 83.5 / 70.6 66.6 / 47.1 70.1 / 54.9 74.1 / 56.6 70.6 / 53.1 74.0 / 52.9 62.1 / 37.0 71.6 / 53.2 Filter 84.0 / 70.8 72.1 / 51.1 74.8 /60.0 78.1 / 60.1 76.0 / 57.6 78.1 /57.5 70.5 / 47.0 76.2 / 57.7 XTUNE 85.3 / 72.9 69.7 / 50.1 72.3 / 57.3 76.3 / 58.8 74.0 / 56.0 76.5 / 55.9 70.8 / 48.3 75.0 / 57.1 X-MIXUP 83.1 / 70.0 71.9 / 51.1 74.5 / 59.4 77.7 / 60.0 76.3 / 57.7 78.0 / 57.5 73.7 / 51.1 76.5 / 58.1 Table 16: MLQA results (F1 / EM) for each language. Model en ar bn ﬁ id ko ru sw te Avg. mBERT 75.3 / 63.6 62.2 / 42.8 49.3 / 32.7 59.7 / 45.3 64.8 / 45.8 58.8 / 50.0 60.0 / 38.8 57.5 / 37.9 49.6 / 38.4 59.7 / 43.9XLM 66.9 / 53.9 59.4 / 41.2 27.2 / 15.0 58.2 / 41.4 62.5 / 45.8 14.2 / 5.1 49.2 / 30.7 39.4 / 21.6 15.5 / 6.9 43.6 / 29.1MMTE 62.9 / 49.8 63.1 / 39.2 55.8 / 41.9 53.9 / 42.1 60.9 / 47.6 49.9 / 42.6 58.9 / 37.9 63.1 / 47.2 54.2 / 45.8 58.1 / 43.8XLM-R 71.5 / 56.8 67.6 / 40.4 64.0 / 47.8 70.5 / 53.2 77.4 / 61.9 31.9 / 10.9 67.0 / 42.1 66.1 / 48.1 70.1 / 43.6 65.1 / 45.0Filter 72.4 / 59.1 72.8 / 50.8 70.5 / 56.6 73.3 / 57.2 76.8 / 59.8 33.1 / 12.3 68.9 / 46.6 77.4 / 65.7 69.9 / 50.4 68.3 / 50.9 XTUNE 73.8 / 61.6 77.8 / 60.2 73.5 / 61.1 77.0 / 62.2 80.8 / 68.1 66.9 / 56.5 72.1 / 51.9 77.9 / 65.3 77.6 / 60.7 75.3 / 60.8 X-MIXUP 73.9 / 61.4 73.8 / 54.2 67.4 / 49.6 75.4 / 60.6 78.8 / 65.0 32.9 / 12.0 69.1 / 52.2 78.0 / 66.9 72.0 / 53.5 69.0 / 52.8 Table 17: TyDiQA-GolP results (F1 / EM) for each language. 19",
      "references": [
        "On the cross-lingual transferability of monolin- gual representations.",
        "Scheduled sampling for sequence prediction with recurrent neural networks.",
        "Multilingual transfer learning for QA using translation as data augmentation.",
        "Is neural machine translation the new state of the art?",
        "MixText: Linguistically-informed interpolation of hid- den space for semi-supervised text classiﬁcation.",
        "Multi-source cross-lingual model transfer: Learning what to share.",
        "Infoxlm: An information-theoretic framework for cross- lingual language model pre-training.",
        "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages.",
        "Cross-lingual language model pretraining.",
        "XNLI: Evaluating cross-lingual sentence representations.",
        "Unsupervised cross-lingual representation learning at scale.",
        "Emerging cross- lingual structure in pretrained language models.",
        "BERT: Pre-training of deep bidirectional transformers for language understanding.",
        "FILTER: an enhanced fusion method for cross-lingual language understanding.",
        "Unsupervised quality estimation for neural machine translation.",
        "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization.",
        "Explicit alignment objectives for multilingual bidirectional encoders.",
        "Unicoder: A universal language encoder by pre-training with multiple cross-lingual tasks.",
        "Augmenting NLP models using latent feature interpolations.",
        "The state and fate of linguistic diversity and inclusion in the NLP world.",
        "nmt5 - is parallel data still relevant for pre-training massively multilingual language models?",
        "Adversarial learning with contextual embeddings for zero-resource cross-lingual classiﬁcation and NER.",
        "Similarity of neural net- work representations revisited.",
        "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers.",
        "MLQA: Evaluating cross-lingual extractive question answering.",
        "On the language neutrality of pre-trained multilingual representations.",
        "VECO: variable and ﬂexible cross-lingual pre-training for language understanding and generation.",
        "Universal De- pendencies 2.2",
        "ERNIE- M: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora.",
        "A survey on transfer learning.",
        "Cross-lingual name tagging and linking for 282 languages.",
        "Modeling language variation and universals: A survey on typological linguistics for natural language processing.",
        "Cross-lingual adaptation using structural correspondence learning.",
        "Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual NLP.",
        "Sequence level training with recurrent neural networks.",
        "V-measure: A conditional entropy-based external cluster evaluation measure.",
        "A survey of cross-lingual word embedding models.",
        "XTREME-R: towards more challenging and nuanced multilingual evaluation.",
        "Evaluating the cross-lingual effectiveness of massively multilingual neural machine translation.",
        "XLDA: cross-lingual data augmentation for natural language inference and question answering.",
        "Manifold mixup: Better representations by interpolating hidden states.",
        "Extracting and composing robust features with denoising autoencoders.",
        "Bi-weighting domain adaptation for cross-language text classiﬁcation.",
        "On learning universal representations across languages.",
        "Are all languages created equal in multilingual BERT?",
        "mt5: A massively multilingual pre-trained text-to-text transformer.",
        "Alternating language modeling for cross-lingual pre-training.",
        "PAWS-X: A cross-lingual adversarial dataset for paraphrase identiﬁcation.",
        "mixup: Beyond empirical risk minimization.",
        "Inducing language-agnostic multilingual representations.",
        "Consistency regularization for cross-lingual ﬁne-tuning.",
        "Dual adversarial neural transfer for low-resource named entity recognition."
      ],
      "meta_data": {
        "arxiv_id": "2205.04182v1",
        "authors": [
          "Huiyun Yang",
          "Huadong Chen",
          "Hao Zhou",
          "Lei Li"
        ],
        "published_date": "2022-05-09T10:49:07Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Cross-Lingual Manifold Mixup (X-Mixup), a training and inference strategy that linearly interpolates hidden representations of parallel source–target sentence pairs to explicitly reduce cross-lingual representation discrepancy, thereby narrowing the performance gap between high-resource source and low-resource target languages. Provides empirical analysis showing strong correlation between centered-kernel-alignment (CKA) discrepancy and transfer accuracy, and demonstrates 1.8% average improvement over strong baselines on the XTREME benchmark.",
        "methodology": "1. Start from multilingual encoders (mBERT or XLM-R). 2. For each parallel sentence pair produced by translate-train/test, compute source-aware target states via shared multi-head cross-attention; then perform manifold mixup: \\tilde h = λ h_{T|S} + (1-λ) h_T, followed by layer norm. 3. Mixup ratio λ is instance-wise and bounded (≤0.5); it is predicted from bidirectional attention entropy to down-weight noisy MT examples. 4. Scheduled Sampling replaces gold source sentences with their MT versions during training with a decaying probability to mitigate exposure bias. 5. Joint loss = α·L_task(source) + (1-α)·L_task(target) + representation MSE + prediction KL (for classification). 6. During inference, the same mixup is applied using translated source sentence plus real target sentence; classification probabilities from both views are averaged.",
        "experimental_setup": "Benchmarked on seven XTREME tasks covering 40 languages: sentence classification (XNLI, PAWS-X), sequence labeling (UD-POS, WikiAnn-NER), and extractive QA (XQuAD, MLQA, TyDiQA-GoldP). Source training data only in English; target pseudo-training sets produced with Google Translate (translate-train). Baselines include Translate-Train, Joint-Align, Filter, and XTUNE. Models fine-tuned on mBERT-base and XLM-R-large for 2–4 epochs, batch size 64, learning rates 3e-6–3e-5. Evaluation metrics: Accuracy for classification, F1 for labeling, F1/Exact-Match for QA. Correlation between CKA scores and performance also computed.",
        "limitations": "1. Relies on parallel data generated by machine translation; quality errors affect mixup despite entropy gating. 2. Additional computational overhead from encoding both source and target sentences and performing cross-attention/mixup. 3. Method evaluated only on understanding tasks; effectiveness for generation remains unverified. 4. Hyper-parameters such as mixup layer, λ_max, and α require task-specific tuning. 5. Assumes availability of English-centric MT systems; effectiveness for other source languages or truly zero-translation settings is unclear.",
        "future_research_directions": "1. Integrate manifold mixup into multilingual pre-training to produce inherently aligned representations. 2. Explore non-linear or data-driven mixup strategies and dynamic layer selection. 3. Extend technique to cross-lingual generation tasks (translation, summarization) and multimodal settings. 4. Reduce dependence on external MT by leveraging unsupervised or pivot-based alignment. 5. Combine X-Mixup with complementary consistency-regularization or augmentation methods (e.g., XTUNE) for further gains.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Federated Learning from Small Datasets",
      "full_text": "Published as a conference paper at ICLR 2023 FEDERATED LEARNING FROM SMALL DATASETS Michael Kamp Institute for AI in medicine (IKIM) University Hospital Essen, Essen Germany, and Ruhr-University Bochum, Bochum Germany, and Monash University, Melbourne, Australia michael.kamp@uk-essen.de Jonas Fischer Harvard T.H. Chan School of Public Health Department of Biostatistics Boston, MA, United States jfischer@hsph.harvard.edu Jilles Vreeken CISPA Helmholtz Center for Information Security Saarbr¨ucken, Germany vreeken@cispa.de ABSTRACT Federated learning allows multiple parties to collaboratively train a joint model without having to share any local data. It enables applications of machine learning in settings where data is inherently distributed and undisclosable, such as in the medical domain. Joint training is usually achieved by aggregating local models. When local datasets are small, locally trained models can vary greatly from a globally good model. Bad local models can arbitrarily deteriorate the aggregate model quality, causing federating learning to fail in these settings. We propose a novel approach that avoids this problem by interleaving model aggregation and permutation steps. During a permutation step we redistribute local models across clients through the server, while preserving data privacy, to allow each local model to train on a daisy chain of local datasets. This enables successful training in data-sparse domains. Combined with model aggregation, this approach enables effective learning even if the local datasets are extremely small, while retaining the privacy benefits of federated learning. 1 I NTRODUCTION How can we learn high quality models when data is inherently distributed across sites and cannot be shared or pooled? In federated learning, the solution is to iteratively train models locally at each site and share these models with the server to be aggregated to a global model. As only models are shared, data usually remains undisclosed. This process, however, requires sufficient data to be available at each site in order for the locally trained models to achieve a minimum quality—even a single bad model can render aggregation arbitrarily bad (Shamir and Srebro, 2014). In many relevant applications this requirement is not met: In healthcare settings we often have as little as a few dozens of samples (Granlund et al., 2020; Su et al., 2021; Painter et al., 2020). Also in domains where deep learning is generally regarded as highly successful, such as natural language processing and object detection, applications often suffer from a lack of data (Liu et al., 2020; Kang et al., 2019). To tackle this problem, we propose a new building block called daisy-chaining for federated learning in which models are trained on one local dataset after another, much like a daisy chain. In a nutshell, at each client a model is trained locally, sent to the server, and then—instead of aggregating local models—sent to a random other client as is (see Fig. 1). This way, each local model is exposed to a daisy chain of clients and their local datasets. This allows us to learn from small, distributed datasets simply by consecutively training the model with the data available at each site. Daisy-chaining alone, however, violates privacy, since a client can infer from a model upon the data of the client it received it from (Shokri et al., 2017). Moreover, performing daisy-chaining naively would lead to overfitting which can cause learning to diverge (Haddadpour and Mahdavi, 2019). In this paper, we propose to combine daisy-chaining of local datasets with aggregation of models, both orchestrated by the server, and term this method federated daisy-chaining (FEDDC). 1 arXiv:2110.03469v3  [cs.LG]  12 Oct 2023Published as a conference paper at ICLR 2023 Figure 1: Federated learning settings. A standard federated learning setting with training of local models at clients (middle) with aggregation phases where models are communicated to the server, aggregated, and sent back to each client (left). We propose to add daisy chaining (right), where local models are sent to the server and then redistributed to a random permutation of clients as is. We show that our simple, yet effective approach maintains privacy of local datasets, while it prov- ably converges and guarantees improvement of model quality in convex problems with a suitable aggregation method. Formally, we show convergence for FEDDC on non-convex problems. We then show for convex problems that FEDDC succeeds on small datasets where standard federated learning fails. For that, we analyze FEDDC combined with aggregation via the Radon point from a PAC-learning perspective. We substantiate this theoretical analysis for convex problems by showing that FEDDC in practice matches the accuracy of a model trained on the full data of the SUSY binary classification dataset with only 2 samples per client, outperforming standard federated learning by a wide margin. For non-convex settings, we provide an extensive empirical evaluation, showing that FEDDC outperforms naive daisy-chaining, vanilla federated learning FEDAVG (McMahan et al., 2017), FEDPROX (Li et al., 2020a), FEDADAGRAD , FEDADAM, and FEDYOGI (Reddi et al., 2020) on low-sample CIFAR10 (Krizhevsky, 2009), including non-iid settings, and, more importantly, on two real-world medical imaging datasets. Not only does FEDDC provide a wide margin of improve- ment over existing federated methods, but it comes close to the performance of a gold-standard (centralized) neural network of the same architecture trained on the pooled data. To achieve that, it requires a small communication overhead compared to standard federated learning for the additional daisy-chaining rounds. As often found in healthcare, we consider a cross-SILO scenario where such small communication overhead is negligible. Moreover we show that with equal communication, standard federated averaging still underperforms in our considered settings. In summary, our contributions are (i) FEDDC, a novel approach to federated learning from small datasets via a combination of model permutations across clients and aggregation, (ii) a formal proof of convergence for FEDDC, (iii) a theoretical guarantee that FEDDC improves models in terms of ϵ, δ-guarantees which standard federated learning can not, (iv) a discussion of the privacy aspects and mitigations suitable for FEDDC, including an empirical evaluation of differentially private FEDDC, and (v) an extensive set of experiments showing that FEDDC substantially improves model quality on small datasets compared to standard federated learning approaches. 2 R ELATED WORK Learning from small datasets is a well studied problem in machine learning. In the literature, we find both general solutions, such as using simpler models and transfer learning (Torrey and Shavlik, 2010), and more specialized ones, such as data augmentation (Ibrahim et al., 2021) and few-shot learning (Vinyals et al., 2016; Prabhu et al., 2019). In our scenario overall data is abundant, but the problem is that data is distributed into small local datasets at each site, which we are not allowed to pool. Hao et al. (2021) propose local data augmentation for federated learning, but their method requires a sufficient quality of the local model for augmentation which is the opposite of the scenario we are considering. Huang et al. (2021) provide generalization bounds for federated averaging via the NTK-framework, but requires one-layer infinite-width NNs and infinitesimal learning rates. Federated learning and its variants have been shown to learn from incomplete local data sources, e.g., non-iid label distributions (Li et al., 2020a; Wang et al., 2019) and differing feature distributions (Li et al., 2020b; Reisizadeh et al., 2020a), but fail in case of large gradient diversity (Haddadpour and Mahdavi, 2019) and strongly dissimilar label distribution (Marfoq et al., 2021). For small 2Published as a conference paper at ICLR 2023 datasets, local empirical distributions may vary greatly from the global distribution: the difference of empirical to true distribution decreases exponentially with the sample size (e.g., according to the Dvoretzky–Kiefer–Wolfowitz inequality), but for small samples the difference can be substantial, in particular if the distribution differs from a Normal distribution (Kwak and Kim, 2017). Shamir and Srebro (2014) have shown the adverse effect of bad local models on averaging, proving that even due to a single bad model averaging can be arbitrarily bad. A different approach to dealing with biased local data is by learning personalized models at each client. Such personalized FL (Li et al., 2021) can reduce sample complexity, e.g., by using shared representations (Collins et al., 2021) for client-specific models, e.g., in the medical domain (Yang et al., 2021), or by training sample-efficient personalized Bayesian methods (Achituve et al., 2021). It is not applicable, however, to settings where you are not allowed to learn the biases or batch effects of local clients, e.g., in many medical applications where this would expose sensitive client information. Kiss and Horvath (2021) propose a decentralized and communication-efficient variant of federated learning that migrates models over a decentralized network, storing incoming models locally at each client until sufficiently many models are collected on each client for an averaging step, similar to Gossip federated learing (Jelasity et al., 2005). The variant without averaging is similar to simple daisy-chaining which we compare to in Section 7. FEDDC is compatible with any aggregation operator, including the Radon machine (Kamp et al., 2017), the geometric median (Pillutla et al., 2022), or neuron-clustering (Yurochkin et al., 2019), and can be straightforwardly combined with approaches to improve communication-efficiency, such as dynamic averaging (Kamp et al., 2018), and model quantization (Reisizadeh et al., 2020b). We combine FEDDC with averaging, the Radon machine, and FedProx (Li et al., 2020a) in Sec. 7. 3 P RELIMINARIES We assume iterative learning algorithms (cf. Chp. 2.1.4 Kamp, 2019) A : X × Y × H → Hthat update a model h ∈ Husing a dataset D ⊂ X × Yfrom an input space X and output space Y, i.e., ht+1 = A(D, ht). Given a set of m ∈ N clients with local datasets D1, . . . , Dm ⊂ X × Ydrawn iid from a data distribution D and a loss function ℓ : Y × Y →R, the goal is to find a single model h∗ ∈ Hthat minimizes the risk ε(h) = E(x,y)∼D[ℓ(h(x), y)]. In centralized learning, datasets are pooled as D = S i∈[m] Di and A is applied to D until convergence. Note that applying A on D can be the application to any random subset, e.g., as in mini-batch training, and convergence is measured in terms of low training loss, small gradient, or small deviation from previous iterate. In standard federated learning (McMahan et al., 2017), A is applied in parallel for b ∈ N rounds on each client locally to produce local models h1, . . . , hm. These models are then centralized and aggregated using an aggregation operator agg : Hm → H, i.e., h = agg(h1, . . . , hm). The aggregated model h is then redistributed to local clients which perform another b rounds of training using h as a starting point. This is iterated until convergence of h. When aggregating by averaging, this method is known as federated averaging (FEDAVG). Next, we describe FEDDC. 4 F EDERATED DAISY-CHAINING We propose federated daisy chaining as an extension to federated learning in a setup with m clients and one designated sever.1 We provide the pseudocode of our approach as Algorithm 1. The client: Each client trains its local model in each round on local data (line 4), and sends its model to the server every b rounds for aggregation, where b is the aggregation period, and every d rounds for daisy chaining, where d is the daisy-chaining period (line 6). This re-distribution of models results in each individual model conceptually following a daisy chain of clients, training on each local dataset. Such a daisy chain is interrupted by each aggregation round. The server: Upon receiving models, in a daisy-chaining round (line 9) the server draws a random permutation π of clients (line 10) and re-distributes the model of clienti to client π(i) (line 11), while in an aggregation round (line 12), the server instead aggregates all local models and re-distributes the aggregate to all clients (line 13-14). 1This star-topology can be extended to hierarchical networks in a straightforward manner. Federated learning can also be performed in a decentralized network via gossip algorithms (Jelasity et al., 2005). 3Published as a conference paper at ICLR 2023 Algorithm 1:Federated Daisy-Chaining FEDDC Input: daisy-chaining period d, aggregation period b, learning algorithm A, aggregation operator agg, m clients with local datasets D1, . . . , Dm, total number of rounds T Output: final model aggregate hT 1 initialize local models h1 0, . . . , hm 0 2 Locally at client i at time t do 3 sample S from Di 4 hi t ← A(S, hi t−1) 5 if t mod d= d − 1 or t mod b= b − 1 then 6 send hi t to server 7 receive new hi t from server // receives either aggregate ht or some hj t 8 At serverat time t do 9 if t mod d= d − 1 then // daisy chaining 10 draw permutation π of [1,m] at random 11 for all i ∈ [m] send model hi t to client π(i) 12 else ift mod b= b − 1 then // aggregation 13 ht ← agg(h1 t , . . . , hm t ) 14 send ht to all clients Communication complexity: Note that we consider cross-SILO settings, such as healthcare, were communication is not a bottleneck and, hence, restrict ourselves to a brief discussion in the interest of space. Communication between clients and server happens in O(T d + T b ) many rounds, where T is the overall number of rounds. Since FEDDC communicates every dth and bth round, the amount of communication rounds is similar to FEDAVG with averaging period bFedAvg = min{d, b}. That is, FEDDC increases communication over FEDAVG by a constant factor depending on the setting of b and d. The amount of communication per communication round is linear in the number of clients and model size, similar to federated averaging. We investigate the performance of FEDAVG provided with the same communication capacity as FEDDC in our experiments and in App. A.3.6. 5 T HEORETICAL GUARANTEES In this section, we formally show that FEDDC converges for averaging. We, further, provide theoretical bounds on the model quality in convex settings, showing that FEDDC has favorable generalization error in low sample settings compared to standard federated learning. More formally, we first show that under standard assumptions on the empirical risk, it follows from a result of Yu et al. (2019) that FEDDC converges when using averaging as aggregation and SGD for learning—a standard setting in, e.g., federated learning of neural networks. We provide all proofs in the appendix. Corollary 1. Let the empirical risks Ei emp(h) = P (x,y)∈Di ℓ(hi(x), y) at each client i ∈ [m] be L-smooth with σ2-bounded gradient variance and G2-bounded second moments, then FEDDC with averaging and SGD has a convergence rate ofO(1/ √ mT), where T is the number of local updates. Since model quality in terms of generalization error does not necessarily depend on convergence of training (Haddadpour and Mahdavi, 2019; Kamp et al., 2018), we additionally analyze model quality in terms of probabilistic worst-case guarantees on the generalization error (Shalev-Shwartz and Ben-David, 2014). The average of local models can yield as bad a generalization error as the worst local model, hence, using averaging as aggregation scheme in standard federated learning can yield arbitrarily bad results (cf. Shamir and Srebro, 2014). As the probability of bad local models starkly increases with smaller sample sizes, this trivial bound often carries over to our considered practical settings. The Radon machine (Kamp et al., 2017) is a federated learning approach that overcomes these issues for a wide range of learning algorithms and allows us to analyze (non-trivial) quality bounds of aggregated models under the assumption of convexity. Next, we show that FEDDC can improve model quality for small local datasets where standard federated learning fails to do so. A Radon point (Radon, 1921) of a set of points S from a space X is—similar to the geometric median—a point in the convex hull of S with a high centrality (i.e., a Tukey depth (Tukey, 1975; 4Published as a conference paper at ICLR 2023 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 centralized (test) rounds accuracy train test (a) FEDDC with Radon point with d = 1, b = 50. 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 rounds (b) Federated learning with Radon point with b = 1. 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 rounds (c) Federated learning with Radon point with b = 50. Figure 2: Results on SUSY. We visualize results in terms of train (green) and test error (orange) for (a) FEDDC (d = 1, b= 50) and standard federated learning using Radon points for aggregation with (b) b = 1, i.e., the same amount of communication as FEDDC, and (c) b = 50, i.e., the same aggregation period as FEDDC. The network has 441 clients with 2 data points per client. The performance of a central model trained on all data is indicated by the dashed line. Gilad-Bachrach et al., 2004) of at least 2). For a Radon point to exist, S ⊂ Xhas to have a minimum size r ∈ N called the Radon number of X. For X ⊆Rd the radon number is d + 2. Here, the set of points S are the local models, or more precisely their parameter vectors. We make the following standard assumption (V on Luxburg and Sch¨olkopf, 2011) on the local learning algorithm A. Assumption 2((ϵ, δ)-guarantees). The learning algorithm A applied on a dataset drawn iid from D of size n ≥ n0 ∈ N produces a model h ∈ Hs.t. with probability δ ∈ (0, 1] it holds for ϵ >0 that P(ε(h) > ϵ) < δ. The sample size n0 is monotonically decreasing in δ and ϵ (note that typically n0 is a polynomial in ϵ−1 and log(δ−1)). Here ε(h) is the risk defined in Sec. 3. Now let r ∈ N be the Radon number of H, A be a learning algorithm as in assumption 2, and risk ε be convex. Assume m ≥ rh many clients with h ∈ N. For ϵ >0, δ∈ (0, 1] assume local datasets D1, . . . , Dm of size larger thann0(ϵ, δ) drawn iid from D, and h1, . . . , hm be local models trained on them using A. Let rh be the iterated Radon point (Clarkson et al., 1996) with h iterations computed on the local models (for details, see App. A.2). Then it follows from Theorem 3 in Kamp et al. (2017) that for all i ∈ [m] it holds that P(ε(rh) > ϵ) ≤ (r P(ε(hi) > ϵ))2h (1) where the probability is over the random draws of local datasets. That is, the probability that the aggregate rh is bad is doubly-exponentially smaller than the probability that a local model is bad. Note that in PAC-learning, the error bound and the probability of the bound to hold are typically linked, so that improving one can be translated to improving the other (V on Luxburg and Sch¨olkopf, 2011). Eq. 1 implies that the iterated Radon point only improves the guarantee on the confidence compared to that for local models ifδ < r−1, i.e. P(ε(rh) > ϵ) ≤ (r P(ε(hi) > ϵ))2h < (rδ)2h < 1 only holds for rδ < 1. Consequently, local models need to achieve a minimum quality for the federated learning system to improve model quality. Corollary 3. Let H be a model space with Radon number r ∈ N, ε a convex risk, and A a learning algorithm with sample size n0(ϵ, δ). Given ϵ >0 and any h ∈ N, if local datasets D1, . . . , Dm with m ≥ rh are smaller than n0(ϵ, r−1), then federated learning using the Radon point does not improve model quality in terms of (ϵ, δ)-guarantees. In other words, when using aggregation by Radon points alone, an improvement in terms of(ϵ, δ)- guarantees is strongly dependent on large enough local datasets. Furthermore, given δ > r−1, the guarantee can become arbitrarily bad by increasing the number of aggregation rounds. Federated Daisy-Chaining as given in Alg. 1 permutes local models at random, which is in theory equivalent to permuting local datasets. Since the permutation is drawn at random, the amount of permutation rounds T necessary for each model to observe a minimum number of distinct datasets k with probability 1 − ρ can be given with high probability via a variation of the coupon collector problem as T ≥ d m ρ 1 m (Hm − Hm−k), where Hm is the m-th harmonic number—see Lm. 5 in 5Published as a conference paper at ICLR 2023 App. A.5 for details. It follows that when we perform daisy-chaining withm clients and local datasets of size n for at least dmρ− 1 m (Hm − Hm−k) rounds, then each local model will with probability at least 1 −ρ be trained on at least kn distinct samples. For an ϵ, δ-guarantee, we thus need to set b large enough so that kn ≥ n0(ϵ, √ δ) with probability at least 1 − √ δ. This way, the failure probability is the product of not all clients observing k distinct datasets and the model having a risk larger than ϵ, which is √ δ √ δ = δ. Proposition 4.Let H be a model space with Radon numberr ∈ N, ε a convex risk , andA a learning algorithm with sample size n0(ϵ, δ). Given ϵ >0, δ ∈ (0, r−1) and any h ∈ N, and local datasets D1, . . . , Dm of size n ∈ N with m ≥ rh, then Alg. 1 using the Radon point with aggr. period b ≥ d m δ 1 2m \u0010 Hm − Hm−⌈n−1n0(ϵ, √ δ)⌉ \u0011 (2) improves model quality in terms of (ϵ, δ)-guarantees. This result implies that if enough daisy-chaining rounds are performed in-between aggregation rounds, federated learning via the iterated Radon point improves model quality in terms of (ϵ, δ)-guarantees: the resulting model has generalization error smaller than ϵ with probability at least 1 − δ. Note that the aggregation period cannot be arbitrarily increased without harming convergence. To illustrate the interplay between these variables, we provide a numerical analysis of Prop. 4 in App. A.5.1. This theoretical result is also evident in practice, as we show in Fig. 2. There, we compare FEDDC with standard federated learning and equip both with the iterated Radon point on the SUSY binary classification dataset (Baldi et al., 2014). We train a linear model on 441 clients with only 2 samples per client. After 500 rounds FEDDC daisy-chaining every round (d = 1) and aggregating every fifty rounds (b = 50) reached the test accuracy of a gold-standard model that has been trained on the centralized dataset (ACC= 0.77). Standard federated learning with the same communication complexity using b = 1 is outperformed by a large margin (ACC=0.68). We additionally provide results of standard federated learning withb = 50 (ACC=0.64), which shows that while the aggregated models perform reasonable, the standard approach heavily overfits on local datasets if not pulled to a global average in every round. More details on this experiment can be found in App. A.3.2. In Sec. 7 we show that the empirical results for averaging as aggregation operator are similar to those for the Radon machine. First, we discuss the privacy-aspects of FEDDC. 6 D ATA PRIVACY 0 5 · 104 10 · 104 15 · 104 20 · 104 0 0.2 0.4 0.6 0.8 rounds accuracy FEDDC DP-FEDDC (S= 2,σ= 0.01) DP-FEDDC (S= 2,σ= 0.02) DP-FEDDC (S= 4,σ= 0.05) Figure 3: Differential privacy results. Com- parison of FEDDC (top solid line) to FEDDC with clipped parameter updates and Gaussian noise (dashed lines) on CIFAR10 with 250 clients. A major advantage of federated over centralized learn- ing is that local data remains undisclosed to anyone but the local client, only model parameters are exchanged. This provides a natural benefit to data privacy, which is the main concern in applications such as healthcare. However, an attacker can make inferences about lo- cal data from model parameters (Ma et al., 2020) and model updates or gradients (Zhu and Han, 2020). In the daisy-chaining rounds of FEDDC clients receive a model that was directly trained on the local data of an- other client, instead of a model aggregate, potentially facilitating membership inference attacks (Shokri et al., 2017)—reconstruction attacks (Zhu and Han, 2020) remain difficult because model updates cannot be in- ferred since the server randomly permutes the order of clients in daisy-chaining rounds. Should a malicious client obtain model updates through additional attacks, a common defense is applying appropriate clipping and noise before sending models. This guarantees ϵ, δ-differential privacy for local data (Wei et al., 2020) at the cost of a slight-to-moderate loss in model quality. This technique is also proven to defend against backdoor and poisoning attacks (Sun et al., 2019). Moreover, FEDDC is compatible with standard defenses against such attacks, such as noisy or robust aggregation (Liu et al., 2022)—FEDDC with the Radon machine is an example of robust aggregation. We illustrate the effectiveness ofFEDDC 6Published as a conference paper at ICLR 2023 0 200 400 600 800 1,000 0 0.2 0.4 0.6 0.8 1 centralized (test) rounds accuracy train test (a) FEDDC with d = 1, b = 200. 0 200 400 600 800 1,000 0 0.2 0.4 0.6 0.8 1 rounds (b) FEDAVG with b = 1. 0 200 400 600 800 1,000 0 0.2 0.4 0.6 0.8 1 rounds (c) FEDAVG with b = 200. Figure 4: Synthetic data results. Comparison of FEDDC (a), FEDAVG with same communication (b) and same averaging period (c) for training fully connected NNs on synthetic data. We report mean and confidence accuracy per client in color and accuracy of central learning as dashed black line. with differential privacy in the following experiment. We train a small ResNet on250 clients using FEDDC with d = 2 and b = 10, postponing the details on the experimental setup to App. A.1.1 and A.1.2. Differential privacy is achieved by clipping local model updates and adding Gaussian noise as proposed by Geyer et al. (2017). The results as shown in Figure 3 indicate that the standard trade-off between model quality and privacy holds for FEDDC as well. Moreover, for mild privacy settings the model quality does not decrease. That is, FEDDC is able to robustly predict even under differential privacy. We provide an extended discussion on the privacy aspects ofFEDDC in App. A.7. 7 E XPERIMENTS ON DEEP LEARNING Our approach FEDDC, both provably and empirically, improves model quality when using Radon points as aggregation which, however, require convex problems. For non-convex problems, in particular deep learning, averaging is the state-of-the-art aggregation operator. We, hence, evaluate FEDDC with averaging against the state of the art in federated learning on synthetic and real world data using neural networks. As baselines, we consider federated averaging (FEDAVG) (McMahan et al., 2017) with optimal communication,FEDAVG with equal communication asFEDDC, and simple daisy-chaining without aggregation. We further consider the 4 state-of-the-art methods FEDPROX (Li et al., 2020a), FEDADAGRAD , FEDYOGI , and FEDADAM (Reddi et al., 2020). As datasets we consider a synthetic classification dataset, image classification in CIFAR10 (Krizhevsky, 2009), and two real medical datasets: MRI scans for brain tumors,2 and chest X-rays for pneumonia3. We provide additional results on MNIST in App. A.3.8. Details on the experimental setup are in App. A.1.1,A.1.2, code is publicly available at https://github.com/kampmichael/FedDC. Synthetic Data: We first investigate the potential ofFEDDC on a synthetic binary classification dataset generated by the sklearn (Pedregosa et al., 2011) make_classification function with 100 features. On this dataset, we train a simple fully connected neural network with 3 hidden layers on m = 50 clients with n = 10 samples per client. We compare FEDDC with daisy-chaining period d = 1 and aggregation period b = 200 to FEDAVG with the same amount of communication b = 1 and the same averaging period b = 200. The results presented in Fig. 4 show that FEDDC achieves a test accuracy of 0.89. This is comparable to centralized training on all data which achieves a test accuracy of 0.88. It substantially outperforms both FEDAVG setups, which result in an accuracy of 0.80 and 0.76. Investigating the training of local models between aggreation periods reveals that the main issue of FEDAVG is overfitting of local clients, where FEDAVG train accuracy reaches 1.0 quickly after each averaging step. With these promising results on vanilla neural networks, we next turn to real-world image classification problems typically solved with CNNs. CIFAR10: As a first challenge for image classification, we consider the well-known CIFAR10 image benchmark. We first investigate the effect of the aggregation periodb on FEDDC and FEDAVG, separately optimizing for an optimal period for both methods. We use a setting of 250 clients with 2kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection 3kaggle.com/praveengovi/coronahack-chest-xraydataset 7Published as a conference paper at ICLR 2023 a small version of ResNet, and 64 local samples each, which simulates our small sample setting, drawn at random without replacement (details in App. A.1.2). We report the results in Figure 5 and set the period for FEDDC to b = 10, and consider federated averaging with periods of both b = 1 (equivalent communication to FEDDC with d = 1, b= 10) and b = 10 (less communication than FEDDC by a factor of 10) for all subsequent experiments. 1 10 20 50 100 200 500 ∞ 0 0.2 0.4 0.6 0.8 averaging periodb accuracy FEDDC FEDAVG Figure 5: Averaging periods on CIFAR10. For 150 clients with small ResNets and 64 samples per client, we visualize the test accuracy (higher is better) of FEDDC and FEDAVG for different aggregation periods b. Next, we consider a subset of 9600 samples spread across 150 clients (i.e. 64 samples per client), which corresponds to our small sample setting. Now, each client is equipped with a larger, untrained ResNet18.4 Note that the com- bined amount of examples is only one fifth of the original training data, hence we cannot ex- pect typical CIFAR10 performance. To obtain a gold standard for comparison, we run cen- tralized learning CENTRAL , separately optimiz- ing its hyperparameters, yielding an accuracy of around 0.65. All results are reported in Ta- ble 1, where we report FEDAVG with b = 1 and b = 10, as these were the best performing set- tings and b = 1 corresponds to equal amounts of communication as FEDDC. We use a daisy chaining period of d = 1 for FEDDC throughout all experiments for consistency, and provide results for larger daisy chaining periods in App. A.3.5, which, depending on the data distribution, might be favorable. We observe that FEDDC achieves substantially higher accuracy over the baseline set by federated averaging. In App. A.3.7 we show that this holds also for client subsampling. Upon further inspection, we see that FEDAVG drastically overfits, achieving training accuracies of 0.97 (App. A.3.1), a similar trend as on the synthetic data before. Daisy-chaining alone, apart from privacy issues, also performs worse than FEDDC. Intriguingly, also the state of the art shows similar trends. FEDPROX, run with optimal b = 10 and µ = 0.1, only achieves an accuracy of 0.51 and FEDADAGRAD , FEDYOGI , and FEDADAM show even worse performance of around 0.22, 0.31, and 0.34, respectively. While applied successfully on large-scale data, these methods seem to have shortcomings when it comes to small sample regimes. To model different data distributions across clients that could occur in for example our healthcare setting, we ran further experiments on simulated non-iid data, gradually increasing the locally available data, as well as on non-privacy preserving decentralized learning. We investigate the effect of non-iid data on FEDDC by studying the “pathological non-IID partition of the data” (McMahan et al., 2017). Here, each client only sees examples from 2 out of the 10 classes of CIFAR10. We again use a subset of the dataset. The results in Tab. 2 show that FEDDC outperforms FEDAVG by a wide margin. It also outperforms FEDPROX, a method specialized on heterogeneous datasets in our considered small sample setting. For a similar training setup as before, we show results for gradually increasing local datasets in App. A.3.4. Most notably, FEDDC outperforms FEDAVG even with 150 samples locally. Only when the full CIFAR10 dataset is distributed across the clients, FEDAVG is on par with FEDDC (see App. Fig. 7). We also compare with distributed training through gradient sharing (App. A.3.3), which discards any privacy concerns, implemented by mini-batch SGD with parameter settings corresponding to our federated setup as well as a separately optimized version. The results show that such an approach is outperformed by both FEDAVG as well as FEDDC, which is in line with previous findings and emphasize the importance of model aggregation. As a final experiment on CIFAR10, we consider daisy-chaining with different combinations of aggregation methods, and hence its ability to serve as a building block that can be combined with other federated learning approaches. In particular, we consider the same setting as before and combine FEDPROX with daisy chaining. The results, reported in Tab. 2, show that this combination is not only successful, but also outperforms all others in terms of accuracy. Medical image data: Finally, we consider two real medical image datasets representing actual health related machine learning tasks, which are naturally of small sample size. For the brain MRI scans, we simulate25 clients (e.g., hospitals) with8 samples each. Each client is equipped with a CNN 4Due to hardware restrictions we are limited to training 150 ResNets, hence 9600 samples across 150 clients. 8Published as a conference paper at ICLR 2023 CIFAR10 MRI Pneumonia FEDDC (ours) 62.9 ±0.02 78.4 ±0.61 83.2 ±0.84 DC (baseline) 58.4 ±0.85 57.7 ±1.57 79.8 ±0.99 FEDAVG (b=1) 55.8 ±0.78 74.1 ±1.68 80.1 ±1.53 FEDAVG (b=10) 48.7 ±0.87 75.6 ±1.18 79.4 ±1.11 FEDPROX 51.1 ±0.80 76.5 ±0.50 80.0 ±0.36 FEDADAGRAD 21.8 ±0.01 45.7 ±1.25 62.5 ±0.01 FEDYOGI 31.4 ±4.37 71.3 ±1.62 77.6 ±0.64 FEDADAM 34.0 ±0.23 73.8 ±1.98 73.5 ±0.36 CENTRAL 65.1 ±1.44 82.1 ±1.00 84.1 ±3.31 Table 1: Results on image data, reported is the average test accuracy of the final model over three runs (± denotes maximum deviation from the average). CIFAR10 FEDDC 62.9 ±0.02 FEDDC +F EDPROX 63.2 ±0.38 Non-IID FEDDC 34.2 ±0.61 FEDAVG (b=1) 30.2 ±2.11 FEDAVG (b=10) 24.9 ±1.95 FEDPROX 32.8 ±0.00 FEDADAGRAD 11.7 ±0.00 FEDADAM 13.0 ±0.00 FEDYOGI 12.5 ±0.04 Table 2: Combination of FEDDC with FEDAVG and FEDPROX and non-iid results on CIFAR10. (see App. A.1.1). The results for brain tumor prediction evaluated on a test set of53 of these scans are reported in Table 1. Overall, FEDDC performs best among the federated learning approaches and is close to the centralized model. Whereas FEDPROX performed comparably poorly on CIFAR10, it now outperforms FEDAVG. Similar to before, we observe a considerable margin between all competing methods and FEDDC. To investigate the effect of skewed distributions of sample sizes across clients, such as smaller hospitals having less data than larger ones, we provide additional experiments in App. A.3.5. The key insight is that also in these settings, FEDDC outperforms FEDAVG considerably, and is close to its performance on the unskewed datasets. For the pneumonia dataset, we simulate 150 clients training ResNet18 (see App. A.1.1) with 8 samples per client, the hold out test set are 624 images. The results, reported in Table 1, show similar trends as for the other datasets, with FEDDC outperforming all baselines and the state of the art, and being within the performance of the centrally trained model. Moreover it highlights that FEDDC enables us to train a ResNet18 to high accuracy with as little as 8 samples per client. 8 D ISCUSSION AND CONCLUSION We propose to combine daisy-chaining and aggregation to effectively learn high quality models in a federated setting where only little data is available locally. We formally prove convergence of our approach FEDDC, and for convex settings provide PAC-like generalization guarantees when aggregating by iterated Radon points. Empirical results on the SUSY benchmark underline these theoretical guarantees, with FEDDC matching the performance of centralized learning. Extensive empirical evaluation shows that the proposed combination of daisy-chaining and aggregation enables federated learning from small datasets in practice.When using averaging, we improve upon the state of the art for federated deep learning by a large margin for the considered small sample settings. Last but not least, we show that daisy-chaining is not restricted to FEDDC, but can be straight-forwardly included in FEDAVG, Radon machines, and FEDPROX as a building block, too. FEDDC permits differential privacy mechanisms that introduce noise on model parameters, offering protection against membership inference, poisoning and backdoor attacks. Through the random permutations in daisy-chaining rounds, FEDDC is also robust against reconstruction attacks. Through the daisy-chaining rounds, we see a linear increase in communication. As we are primarily interested in healthcare applications, where communication is not a bottleneck, such an increase in communi- cation is negligible. Importantly, FEDDC outperforms FEDAVG in practice also when both use the same amount of communication. Improving the communication efficiency considering settings where bandwidth is limited, e.g., model training on mobile devices, would make for engaging future work. We conclude that daisy-chaining lends itself as a simple, yet effective building block to improve federated learning, complementing existing work to extend to settings where little data is available per client. F EDDC, thus, might offer a solution to the open problem of federated learning in healthcare, where very few, undisclosable samples are available at each site. 9Published as a conference paper at ICLR 2023 ACKNOWLEDGMENTS The authors thank Sebastian U. Stich for his detailed comments on an earlier draft. Michael Kamp received support from the Cancer Research Center Cologne Essen (CCCE). Jonas Fischer is supported by a grant from the US National Cancer Institute (R35CA220523). REFERENCES Idan Achituve, Aviv Shamsian, Aviv Navon, Gal Chechik, and Ethan Fetaya. Personalized feder- ated learning with gaussian processes. In Advances in Neural Information Processing Systems, volume 34. Curran Associates, Inc., 2021. 3 Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers. International Journal of Security and Networks, 10(3):137–150, 2015. 22 Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy physics with deep learning. Nature communications, 5(1):1–9, 2014. 6, 15 Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. In International Conference on Machine Learning, pages 634–643. PMLR, 2019. 22 Kenneth L Clarkson, David Eppstein, Gary L Miller, Carl Sturtivant, and Shang-Hua Teng. Approxi- mating center points with iterative radon points. International Journal of Computational Geometry & Applications, 6(03):357–377, 1996. 5, 15 Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representa- tions for personalized federated learning. In Marina Meila and Tong Zhang, editors,Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 2089–2099. PMLR, 18–24 Jul 2021. 3 Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557, 2017. 7 Ran Gilad-Bachrach, Amir Navot, and Naftali Tishby. Bayes and tukey meet at the center point. In International Conference on Computational Learning Theory, pages 549–563. Springer, 2004. 5 Kristin L Granlund, Sui-Seng Tee, Hebert A Vargas, Serge K Lyashchenko, Ed Reznik, Samson Fine, Vincent Laudone, James A Eastham, Karim A Touijer, Victor E Reuter, et al. Hyperpolarized mri of human prostate cancer reveals increased lactate with tumor grade driven by monocarboxylate transporter 1. Cell metabolism, 31(1):105–114, 2020. 1 Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated learning. arXiv preprint arXiv:1910.14425, 2019. 1, 2, 4 Weituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang, Changyou Chen, and Lawrence Carin Duke. Towards fair federated learning with zero-shot data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3310–3319, 2021. 2 Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel-based framework for federated learning analysis. In International Conference on Machine Learning, pages 4423–4434. PMLR, 2021. 2 Marwa Ibrahim, Mohammad Wedyan, Ryan Alturki, Muazzam A Khan, and Adel Al-Jumaily. Augmentation in healthcare: Augmented biosignal using deep learning and tensor representation. Journal of Healthcare Engineering, 2021, 2021. 2 M´ark Jelasity, Alberto Montresor, and Ozalp Babaoglu. Gossip-based aggregation in large dynamic networks. ACM Transactions on Computer Systems (TOCS), 23(3):219–252, 2005. 3 10Published as a conference paper at ICLR 2023 Michael Kamp. Black-Box Parallelization for Machine Learning. PhD thesis, Rheinische Friedrich- Wilhelms-Universit¨at Bonn, Universit¨ats-und Landesbibliothek Bonn, 2019. 3 Michael Kamp, Mario Boley, Olana Missura, and Thomas G ¨artner. Effective parallelisation for machine learning. In Advances in Neural Information Processing Systems , volume 30, pages 6480–6491. Curran Associates, Inc., 2017. 3, 4, 5, 14, 15 Michael Kamp, Linara Adilova, Joachim Sicking, Fabian H ¨uger, Peter Schlicht, Tim Wirtz, and Stefan Wrobel. Efficient decentralized deep learning by dynamic model averaging. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases , pages 393–409. Springer, 2018. 3, 4 Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object detection via feature reweighting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8420–8429, 2019. 1 P´eter Kiss and Tomas Horvath. Migrating models: A decentralized view on federated learning. In Proceedings of the Workshop on Parallel, Distributed, and Federated Learning. Springer, 2021. 3 Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, Toronto, 2009. 2, 7 Sang Gyu Kwak and Jong Hae Kim. Central limit theorem: the cornerstone of modern statistics. Korean journal of anesthesiology, 70(2):144, 2017. 3 Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 18 Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10713–10722, 2021. 3 Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Conference on Machine Learning and Systems, 2020a, 2020a. 2, 3, 7 Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. In International Conference on Learning Representations, 2020b. 2 Pei Liu, Xuemin Wang, Chao Xiang, and Weiye Meng. A survey of text data augmentation. In 2020 International Conference on Computer Communication and Network Security (CCNS), pages 191–195. IEEE, 2020. 1 Pengrui Liu, Xiangrui Xu, and Wei Wang. Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives. Cybersecurity, 5(1):4, 2022. 6, 22 Chuan Ma, Jun Li, Ming Ding, Howard H Yang, Feng Shu, Tony QS Quek, and H Vincent Poor. On safeguarding privacy and security in the framework of federated learning. IEEE network, 34(4): 242–248, 2020. 6, 22 Othmane Marfoq, Giovanni Neglia, Aur´elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2021. 2 Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli- gence and Statistics, pages 1273–1282, 2017. 2, 3, 7, 8 Peter Neal. The generalised coupon collector problem. Journal of Applied Probability , 45(3): 621–629, 2008. 20 11Published as a conference paper at ICLR 2023 Corrie A Painter, Esha Jain, Brett N Tomson, Michael Dunphy, Rachel E Stoddard, Beena S Thomas, Alyssa L Damon, Shahrayz Shah, Dewey Kim, Jorge G´omez Tejeda Za˜nudo, et al. The angiosar- coma project: enabling genomic and clinical discoveries in a rare cancer through patient-partnered research. Nature medicine, 26(2):181–187, 2020. 1 F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 7, 14 Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. IEEE Transactions on Signal Processing, 70:1142–1154, 2022. 3 Viraj Prabhu, Anitha Kannan, Murali Ravuri, Manish Chaplain, David Sontag, and Xavier Amatriain. Few-shot learning for dermatological disease diagnosis. In Machine Learning for Healthcare Conference, pages 532–552. PMLR, 2019. 2 Johann Radon. Mengen konvexer K¨orper, die einen gemeinsamen Punkt enthalten. Mathematische Annalen, 83(1):113–115, 1921. 4, 15 Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2020. 2, 7, 14 Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning: The case of affine distribution shifts. In Advances in Neural Information Processing Systems, volume 33, pages 21554–21565. Curran Associates, Inc., 2020a. 2 Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fedpaq: A communication-efficient federated learning method with periodic averaging and quan- tization. In International Conference on Artificial Intelligence and Statistics, pages 2021–2031. PMLR, 2020b. 3 Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. 4 Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 850– 857. IEEE, 2014. 1, 3, 4, 16 Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3–18. IEEE, 2017. 1, 6, 22 Xiaoping Su, Xiaofan Lu, Sehrish Khan Bazai, Eva Comp´erat, Roger Mouawad, Hui Yao, Morgan Rouprˆet, Jean-Philippe Spano, David Khayat, Irwin Davidson, et al. Comprehensive integrative profiling of upper tract urothelial carcinomas. Genome biology, 22(1):1–25, 2021. 1 Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019. 6, 22 Lisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research on machine learning applications and trends: algorithms, methods, and techniques, pages 242–264. IGI global, 2010. 2 John W Tukey. Mathematics and picturing data. In Proceedings of the International Congress of Mathematics, volume 2, pages 523–531, 1975. 4 Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, volume 29, pages 3630–3638. Curran Associates, Inc., 2016. 2 Ulrike V on Luxburg and Bernhard Sch¨olkopf. Statistical learning theory: Models, concepts, and results. In Handbook of the History of Logic, volume 10, pages 651–706. Elsevier, 2011. 5 12Published as a conference paper at ICLR 2023 Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Fed- erated learning with matched averaging. In International Conference on Learning Representations, 2019. 2 Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek, and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. IEEE Transactions on Information Forensics and Security, 15:3454–3469, 2020. 6, 22 Qian Yang, Jianyi Zhang, Weituo Hao, Gregory P. Spell, and Lawrence Carin. Flop: Federated learning on medical datasets using partial networks. InProceedings of the 27th ACM SIGKDD Con- ference on Knowledge Discovery and Data Mining, page 3845–3853. Association for Computing Machinery, 2021. 3 Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5693–5700, 2019. 4, 19 Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. InInternational Conference on Machine Learning, pages 7252–7261. PMLR, 2019. 3 Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. Batchcrypt: Effi- cient homomorphic encryption for cross-silo federated learning. In USENIX Annual Technical Conference, pages 493–506, 2020. 22 Ligeng Zhu and Song Han. Deep leakage from gradients. In Federated learning, pages 17–31. Springer, 2020. 6 13Published as a conference paper at ICLR 2023 A A PPENDIX A.1 D ETAILS ON EXPERIMENTAL SETUP In this section we provide all details to reproduce the empirical results presented in this paper. Furthermore, the implementation provided at https://github.com/kampmichael/FedDC allows to directly reproduce the result. Experiments were conducted on an NVIDIA DGX with six A6000 GPUs. A.1.1 N ETWORK ARCHITECTURES Here, we detail network architectures considered in our empirical evaluation. MLP for Synthetic DataA standard multilayer perceptron (MLP) with ReLU activations and three linear layers of size 100,50,20. Averaging round experiment For this set of experiments we use smaller versions of ResNet architectures with 3 blocks, where the blocks use 16, 32, 64 filters, respectively. In essence, these are smaller versions of the original ResNet18 to keep training of 250 networks feasible. CIFAR10 & Pneumonia For CIFAR10, we consider a standard ResNet18 architecture, where weights are initialized by a Kaiming Normal and biases are zero-initialized. Each client constructs and initializes a ResNet network separately. For pneumonia, X-ray images are resized to (224, 224). MRI For the MRI data, we train a small convolutional network of architecture Conv(32)-Batchnorm- ReLU-MaxPool-Conv(64)-Batchnorm-ReLU-MaxPool-Linear, where Conv(x) are convolutional layers with x filters of kernel size 3. The pooling layer uses a stride of 2 and kernel size of 2. The Linear layer is of size 2 matching the number of output classes. All scan images are resized to (150, 150). A.1.2 T RAINING SETUP In this section, we give additional information for the training setup for each individual experiment of our empirical evaluation. SUSY experiments SUSY is a binary classification dataset with18 features. We train linear models with stochastic gradient descent (learning rate 0.0001, found by grid-search on an independent part of the dataset) on 441 clients, aggregating every 50 rounds via the iterated Radon point (Kamp et al., 2017) with h = 2 iterations. F EDDC performs daisy-chaining with period d = 1. The test accuracy is evaluated on a test set with 1 000 000samples drawn iid at random. Synthetic Data The synthetic binary classification dataset is generated by the sklearn (Pedregosa et al., 2011) make_classification function with 100 features of which 20 are informative, 60 are redundant, and 5 are repeated. We generate 3 clusters per class with a class separation of 1.0, a shift of 1.0 and a scale of 3.0. Class labels are randomly flipped with probability 0.02. Averaging rounds parameter optimizationTo find a suitable number when averaging should be carried out, we explore b ∈ {1, 10, 20, 50, 100, 200, 500, ∞} on CIFAR10 using 250 clients each equipped with a small ResNet. We assign 64 samples to each client drawn at random (without replacement) from the CIFAR10 training data and use a batch size of 64. For each parameter, we train for 10k rounds with SGD using cross entropy loss and initial learning rate of 0.1, multiplying the rate by a factor of .5 every 2500 rounds. FedAdam, FedAdagrad, and FedYogiWe use the standard values forβ1 and β2, i.e., β1 = 0.9, β2 = 0.999, as suggested in Reddi et al. (2020). We optimized learning rate ηl and global learning rate η from the set {0.001, 0.01, 0.1, 1.0, 2.0} yielding optimal parameters ηl = 0.1 and η = 1.0. 14Published as a conference paper at ICLR 2023 CIFAR10 differential privacy and main experimentsWe keep the same experimental setup as for hyperparameter tuning, but now use 100 clients each equipped with a ResNet18. A.2 I TERATED RADON POINTS AND THE RADON MACHINE The Radon machine (Kamp et al., 2017) aggregates a set S = h1, . . . , hm of local models hi ∈ H via the iterated Radon point algorithm (Clarkson et al., 1996). For models with d ∈ N parameters, r = d + 2 many models are required to compute a single Radon point, where r is called the Radon number of H. Let m = rh for some h ∈ N, then the iterated Radon point aggregates models in h iterations. In each iteration, the set S is partitioned into subsets of size r and the Radon point of each subset is calculated. The final step of each iteration is to replace the set S of models by the set of Radon points. After h iterations, a single Radon point rh is obtained as the aggregate. Radon points can be obtained by solving a system of linear equations of size r × r (Kamp et al., 2017): In his main theorem, Radon (1921) gives the following construction of a Radon point for a set S = {s1, ..., sr} ⊆Rd. Find a non-zero solution λ ∈ R|S| for the following linear equations. rX i=1 λisi = (0, . . . ,0) , rX i=1 λi = 0 Such a solution exists, since |S| > d+ 1 implies that S is linearly dependent. Then, let I, Jbe index sets such that for all i ∈ I : λi ≥ 0 and for all j ∈ J : λj < 0. Then a Radon point is defined by r(λ) = X i∈I λi Λ si = X j∈J λj Λ sj , where Λ = P i∈I λi = −P j∈J λj. Any solution to this linear system of equations is a Radon point. The equation system can be solved in time r3. By setting the first element of λ to one, we obtain a unique solution of the system of linear equations. Using this solution λ, we define the Radon point of a set S as r(S) = r(λ) in order to resolve ambiguity. A.3 A DDITIONAL EMPIRICAL RESULTS In Sec. 7 we have shown that FEDDC performs well on benchmark and real-world datasets. In the following we provide additional empirical results, both to investigate the main results more closely, as well as to further investigate the properties of FEDDC. For the CIFAR10 experiment, we investigate training accuracy (App. A.3.1) and present results for distributed mini-batch SGD (App. A.3.3). For the SUSY experiment, we compare to FEDAVG (App. A.3.2). As additional experiments, we investigate the impact of local dataset size (App. A.3.4) and skewed dataset size distributions (App. A.3.5), and analyze the communication-efficiency of FEDDC (App. A.3.6). Finally, we present results on MNIST where FEDDC achieves state-of-the-art accuracy (App. A.3.8). A.3.1 T RAIN AND TEST ACCURACIES ON CIFAR10: In Table 3 we provide the accuracies on the entire training set for the final model, together with test accuracies, on CIFAR10. The high training accuracies of FEDAVG (≈ 0.97)—and to a lesser degree FEDPROX (0.96)—indicate overfitting on local data sets. The poor training performance of FEDADAGRAD , FEDYOGI , and FEDADAM hint at insufficient model updates. A possible explanation is that the adaptive learning rate parameter (which is proportional to the sum of past model updates) becomes large quickly, essentially stopping the training process. The likely reason is that due to large differences in local data distributions, model updates after each aggregation round are large. A.3.2 A DDITIONAL RESULTS ON SUSY In Sec. 5 we compared FEDDC to federated learning with the iterated Radon point. For completeness, we compare it to FEDAVG as well, i.e., federated learning using averaging on the same SUSY binary classification dataset (Baldi et al., 2014). The results shown in Fig. 6 are in line with the findings in Sec. 5: FEDDC with the iterated Radon point outperforms FEDAVG both with the same amount of communication (b = 1) and the same aggregation period (b = 50). The results for b = 50 show that 15Published as a conference paper at ICLR 2023 Test Train FEDDC (ours) 62.9 ±0.02 94.7 ±0.52 DC (baseline) 58.4 ±0.85 94.1 ±2.31 FEDAVG (b=1) 55.8 ±0.78 97.2 ±0.87 FEDAVG (b=10) 48.7 ±0.87 97.4 ±0.23 FEDPROX 51.1 ±0.80 95.9 ±0.42 FEDADAGRAD 21.8 ±0.01 31.7 ±0.25 FEDYOGI 31.4 ±4.37 72.4 ±0.90 FEDADAM 34.0 ±0.23 73.9 ±0.89 Table 3: Train and test accuracy on CIFAR10 of the final model over three runs (± denotes maximum deviation from the average). 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 centralized (test) rounds accuracy train test (a) FEDDC with Radon point with d = 1, b = 50. 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 rounds (b) Federated learning with averag- ing with b = 1. 0 100 200 300 400 500 0.4 0.5 0.6 0.7 0.8 0.9 1 rounds (c) Federated learning with averag- ing with b = 50. Figure 6: Results on SUSY. We visualize results in terms of train (green) and test error (orange) for FEDDC with the iterated Radon point (a) and FEDAVG with b = 1 (b) as well as FEDAVG with b = 50 (c). The network has 441 clients with 2 data points per client. The performance of a central model trained on all data is indicated by the dashed line. FEDAVG exhibits the same behavior on local training sets that indicates overfitting. Overall, FEDAVG performs comparably to federated learning with the iterated Radon point. That is, FEDAVG (b = 1 and b = 50) has an accuracy of 0.67, resp. 0.64, compared to 0.68, resp. 0.64 for federated learning with the iterated Radon point. A.3.3 C OMPARISON WITH DISTRIBUTED MINI -BATCH SGD ON CIFAR10 We compare to distributed mini-batch SGD, i.e., central updates where gradients are computed distributedly on CIFAR10. We use the same setup as for the other experiments, i.e., m = 150 clients and a mini-batch size of B = 64, so that the effective mini-batch size for each update is mB = 9600, the optimal learning rate is λ = 0.01. Here, mini-batch SGD achieves a test accuracy of 19.47 ±0.68. Since a plausible explanation for the poor performance is the large mini-batch size, we compare it to the setting with B = 1 to achieve the minimum effective mini-batch size of B = 150. The results are substantially improved to an accuracy of 50.14 ± 0.63, underlining the negative effect of the large batch size in line with the theoretical analysis of Shamir and Srebro (2014). Running it 64 times the number of rounds that FedDC uses improves the accuracy just slightly to 54.3. Thus, even with optimal B = 1 and a 64-times slower convergence, mini-batch SGD is outperformed by both FedAvg and FedDC, since it cannot use the more favorable mini-batch size of B = 64 on m = 150 clients. A.3.4 L OCAL DATASET SIZE In our experiments, we used dataset sizes common in the medical domain, e.g., for radiological images. To further investigate the impact of local dataset sizes on the performance of FEDDC wrt. FEDAVG, we evaluate the performance for local dataset sizes ranging from 2 to 256 (given the size of CIFAR10, 256 is the maximum without creating overlap between local datasets). The results in 16Published as a conference paper at ICLR 2023 Fig. 7 show that FEDDC outperforms all baselines for smaller local datasets. Only for as much as 256 examples FEDAVG performs as good as FEDDC. These results further confirm that FEDDC is capable of handling heterogeneous data: for n <10 the clients only see a subset of labels due to the size of their local datasets (with n=2, each client can at most observe two classes). We find this a more natural non-iid setting. These results indicate that the shuffle mechanism indeed mitigates data heterogeneity well. A further study of the impact of non-iid data, a comparison with personalized FL, and potential improvements to the shuffling scheme are interesting directions for future work Figure 7: Test accuracy wrt. local dataset size on CIFAR10 with 150 clients ( n = 2 i for i ∈ {1, . . . ,8}) with linear (left) and logarithmic (right) x-axis. A.3.5 R EALISTIC DATASET SIZE DISTRIBUTION In medical applications, a common scenario is that some hospitals, e.g., university clinics, hold larger datasets, while small clinics, or local doctors’ offices only hold very small datasets. To simulate such a scenario, we draw local dataset sizes for a dataset of size n so that a fraction c of the clients hold only a minimum number of samples nmin (the local doctor’s offices), and the other clients have an increasing local dataset size starting from nmin until all data is distributed. That is, for clients i = [1, . . . , m− ⌊cm⌋] the dataset sizes are given by nmin + ai with a = 2(n − (⌊cm⌋)nmin) ⌊(1 − c)m⌋(⌊(1 − c)m⌋ −1) . We use the MRI brainscan dataset with c = 0.3 and nmin = 2. The results presented in Tab. 4 show that FEDDC performs well in that setting. FEDDC (d = 4, b= 10) outperforms all other methods with an accuracy of around 0.81, is similar to FEDDC on equally distributed data (0.79), and is even close to the centralized gold-standard (0.82). MRI FEDDC (d=1, b=10) 74.7 ±0.61 FEDDC (d=2, b=10) 74.4 ±1.15 FEDDC (d=4, b=10) 80.6 ±0.66 FEDDC (d=5, b=10) 77.8 ±1.42 DC (baseline) 53.9 ±0.25 FEDAVG (b=1) 70.1 ±2.59 FEDAVG (b=10) 75.3 ±2.37 central 79.9 ±6.23 Table 4: Results for realistic dataset size distribution on MRI, reported is the average test accuracy of the final model over three runs (± denotes maximum deviation from the average). 17Published as a conference paper at ICLR 2023 A.3.6 C OMMUNICATION EFFICIENCY OF FEDDC Although communication is not a concern in cross-silo applications, such as healthcare, the commu- nication efficiency of FEDDC is important in classical federated learning applications. We therefore compare FEDDC with varying amounts of communication to FEDAVG on CIFAR10 in Tab. 5. The results show that FEDDC (d=2) outperforms FEDAVG (b=1), thus outperforming just using half the amount of communication, and FEDDC (d=5) performs similar to FEDAVG (b=1), thus outperforming using five times less communication. FEDDC with d = 10 and b = 10 significantly outperforms FEDAVG (b=10), which corresponds to the same amount of communication in this low-sample setting. CIFAR10 FEDDC (d=1,b=10) 62.9 ±0.02 FEDDC (d=2,b=10) 60.8 ±0.65 FEDDC (d=5,b=10) 55.4 ±0.11 FEDDC (d=10,b=20) 53.8 ±0.47 FEDAVG (b=1) 55.8 ±0.78 FEDAVG (b=10) 48.7 ±0.87 central 65.1 ±1.44 Table 5: Communication efficiency of FEDDC compared to FEDAVG., where FEDDC (d=1,b=10) and FEDAVG (b=1), respectively FEDDC (d=10,b=20) and FEDAVG (b=10) have the same amount of communication. A.3.7 C LIENT SUBSAMPLING A widely used technique to improve communication-efficiency in federated learning is to subsample clients in each communication round. For example, instead of averaging the models of all clients in vanilla FedAvg, only a subset of clients sends their models and receives the average of this subset. By randomly sampling this subset, eventually all clients will participate in the process. The fraction C ∈ (0, 1] of clients sampled is a hyperparameter. In cross-SILO applications, such as healthcare, communication-efficiency is not relevant and client participation is assumed to beC = 1.0. Client subsampling can naturally be used in FEDDC by sampling clients both in daisy-chaining and aggregation rounds. We conducted an experiment on CIFAR10 where we compare FEDDC using subsampling to FEDAVG (b = 10). The results in Table 6 show that FEDDC indeed works well with client subsampling and outperforms FEDAVG with C = 0.2, similar to full client participation (C=1.0). However, due to the restricted flow of information, the training process is slowed. By prolonging training from 10000 to 30000 rounds, FEDDC with C = 0.2 reaches virtually the same performance as in full client participation, but with higher variance. The same holds true forFEDAVG. T = 10 000 T = 30 000 C = 1.0 C = 0.2 C = 0.2 FEDDC (d=1,b=10) 62.9 ±0.02 53.2 ±3.42 61.0 ±1.07 FEDAVG (b=10) 48.7 ±0.87 45.9 ±6.94 49.3 ±5.23 Table 6: Client subsampling of FEDDC compared to F EDAVG on CIFAR10. A.3.8 A DDITIONAL RESULTS ON MNIST In order to further demonstrate the efficiency of FEDDC on clients that achieve state-of-the-art performance we perform experiments on the MNIST (LeCun et al., 1998) dataset. We use a CNN with two convolutional layers with max-pooling, followed by two linear layers with 1024, resp. 100 neurons. Centralized training on all 60 000training samples of MNIST achieves a test accuracy of 0.994 which is similar to the state-of-the-art. The results for m = 50 clients in Tab. 7 show that 18Published as a conference paper at ICLR 2023 FEDDC outperforms FEDAVG both with the same amount of communication, i.e., FEDAVG (b = 1) and FEDAVG (b = 10). In line with the results on CIFAR10 (cf. Fig. 7), the advantage of FEDDC shrinks with increasing local dataset size. Using n = 1200, i.e., the full training set distributed over m = 50 clients, results in virtually the same performance of FEDDC and F EDAVG, both reaching a test accuracy of around 0.96. FEDDC (d=1,b=10) F EDAVG (b=1) F EDAVG (b=10) n = 8 87 .6 84 .4 84 .9 n = 1200 96 .7 96 .3 96 .5 Table 7: Performance of FEDDC and F EDAVG on MNIST for varying local dataset sizes n. A.4 P ROOF OF CONVERGENCE Corollary. Let the empirical risks Ei emp(h) = P (x,y)∈Di ℓ(hi(x), y) at each client be L-smooth with σ2-bounded gradient variance and G2-bounded second moments, then FEDDC with averaging and SGD as learning algorithm has a convergence rate of O(1/ √ mT), where T ∈ N is the number of local updates. Proof. We assume that each client i ∈ [m] uses SGD as learning algorithm. In each iteration t ∈ [T], a client i computes the gradient Gi t = ∇ℓ(hi t(x), y) with x, y∈ Di drawn randomly and updates the local model hi t+1 = hi t − γGi t, where γ >0 denotes the learning rate. FEDDC with daisy-chaining period d and averaging period b, run for T local iterations, computes T/b local gradients at each of the m clients before averaging. Each local gradient is computed on an iid sample from D, independent of whether local models are permuted. Therefore, FEDDC with averaging and SGD is equivalent to parallel restarted SGD (PR-SGD) (Yu et al., 2019) withb/d times larger local datasets. Yu et al. (2019) analyze the convergence of PR-SGD with respect to the average ht of local models in round t. Since Ei emp(h) = P (x,y)∈Di ℓ(hi(x), y) at each client be L-smooth with σ2-bounded gradient variance and G2-bounded second moments, Theorem 1 in Yu et al. (2019) is applicable. It then follows from Corollary 1 (Yu et al., 2019) that forγ = √m/(L √ T) and b ≤ T 1 4 /m 3 4 it holds that 1 T E h TX t=1 E(x,y)∼D h Ei emp \u0000 ht(x), y \u0001ii ≤ 2L√ mT \u0010 E(x,y)∼D h Ei emp \u0000 h0(x), y \u0001i −E(x,y)∼D h Ei emp \u0010 h ∗ (x), y \u0011i\u0011 + 1√ mT \u0000 4G2 + σ2\u0001 ∈ O \u0012 1√ mT \u0013 . Here, the first expectation is over the draw of local datasets and h ∗ is given by h ∗ = arg min h∈H E(x,y)∼D h Eemp(h(x), y) i . Thus, FEDDC with averaging and SGD converges in O(1/ √ mT). A.5 P ROOF OF MODEL QUALITY IMPROVEMENT BY FEDDC In order to proof Prop. 4, we first need the following Lemma. Lemma 5. Given δ ∈ (0, 1], m ∈ N clients, and k ∈ [m], if Algorithm 1 with daisy chaining period d ∈ N is run for T ∈ N rounds with T ≥ d m ρ 1 m (Hm − Hm−k) where Hm is the m-th harmonic number, then each local model has seen at least k distinct datasets with probability 1 − ρ. 19Published as a conference paper at ICLR 2023 Note that Hm ≈ log m + γ + 1 2 + O \u0000 1 m \u0001 where γ ≈ 0.5772156649 denotes the Euler-Mascheroni- constant. Proof. For a single local model, it follows from the coupon collector problem (Neal, 2008) that the expected number of daisy-chaining rounds R required to see at least k out of m clients is at least m(Hm − Hm−k), where Hm = mX i=1 1 i is the m-th harmonic number. To see that, consider that for the first distinct client the chance to pick it is m m−1 , for the second m m−2 and for the k-th it is m m−k+1 , which sums up to m(Hm − Hm−k). Applying the Markov inequality yields P \u0012 R ≥ 1 ρm(Hm − Hm−k) \u0013 ≤ ρ . The probability for all local models to have seen at least k clients then is at most ρm. Thus, if we perform at least R ≥ m ρ 1 m m(Hm − Hm−k) daisy-chaining rounds, then the probability that each local model has not seen at least k distinct datasets is smaller than ρ. The result follows from the fact that the number of daisy-chaining rules is R = T/d. Note that Hk can be approximated as Hm ≈ log m + γ + 1 2 + O \u0012 1 m \u0013 where γ = limm→∞(Hm − ln m) ≈ 0.5772156649 denotes the Euler-Mascheroni-constant. From this it follows that Hm − Hm−k ≈ ln m m − k + O \u0012 1 m − 1 m − k \u0013 With this, we can now proof Prop. 4 that we restate here for convenience. Proposition. Let H be a model space with Radon number r ∈ N, ε a convex risk, and A a learning algorithm with sample size n0(ϵ, δ). Given ϵ >0, δ ∈ (0, r−1) and any h ∈ N, and local datasets D1, . . . , Dm of size n ∈ N with m ≥ rh, then Alg. 1 using the Radon point with aggr. period b ≥ d m δ 1 2m \u0010 Hm − Hm−⌈n−1n0(ϵ, √ δ)⌉ \u0011 improves model quality in terms of (ϵ, δ)-guarantees. Proof. For b ≥ d m δ 1 2m \u0010 Hm − Hm−⌈n−1n0(ϵ, √ δ)⌉ \u0011 it follows from Lemma 5 with k = l n−1n0 \u0010 ϵ, √ δ \u0011m that with probability 1 − √ δ all local models are trained on at least kn = n0 \u0010 ϵ, √ δ \u0011 samples. Thus an (ϵ, √ δ)-guarantee holds for each model with probability 1 − √ δ. It follows from Eq. 1 that the probability that the risk is higher than ϵ is P(ε(rh) > ϵ) < \u0010 r √ δ √ δ \u00112h = (rδ)2h . The result follows from δ < r−1 and Eq. (1). 20Published as a conference paper at ICLR 2023 A.5.1 N UMERICAL ANALYSIS OF PROPOSITION 4 The lower bound on the aggregation period b ≥ d m δ 1 2m \u0010 Hm − Hm−⌈n−1n0(ϵ, √ δ)⌉ \u0011 grows linearly with the daisy-chaining period and a factor depending on the number of clients m, the error probability δ, and the required number of hops l n−1n0 \u0010 ϵ, √ δ \u0011m . Applying the bound to the experiment using the Radon point on SUSY in Sec. 5 with m = 441 clients, daisy-chaining period d = 1, and local dataset size of n = 2 for local learners achieving an (ϵ = 0.05, δ= 0.01)-guarantee requires b ≥ 49.9 to improve model quality according to Prop. 4. For b = 50 like in our experiments, Prop. 4 thus predicts that model quality is improved, under the assumption that n0(ϵ, δ) = 1 ϵ log 1 δ . Even though, the experiments on CIFAR10 in Section 7 are non-convex, and thus Prop. 4 does not apply, we can still evaluate the predicted required aggregation period: with m = 150 clients, daisy-chaining period d = 1, local dataset size of n = 64, and local learners achieving an (ϵ = 0.01, δ= 0.01)-guarantee requires b ≥ 8.81. We now analyze the scaling behavior with the error probability δ for various local dataset sizes in Fig. 8a. The lower the error probability, the larger the required aggregation period b, in particular for small local datasets. If local datasets are sufficiently large, the aggregation period can be chosen very small. In Fig. 8b we investigate the required aggregation period b for local learners achieving an (ϵ = 0.01, δ= 0.01)-guarantee in relation to the local dataset size n. Indeed, the smaller the local dataset size, the larger the required aggregation period. We also see that for smaller numbers of clients, more aggregation rounds are required, since the chance of a model visiting the same client multiple times is larger. 0 0.2 0.4 0.6 0.8 1 0 20 40 60 80 error probabilityδ aggregation periodb n= 8 n= 16 n= 32 n= 64 n= 128 n= 256 (a) Aggregation period b for error probability δ for varying local dataset sizes with m = 150clients and ϵ = 0.01. 23 24 25 26 27 0 100 200 number local samplesn aggregation periodb m = 50 m = 100 m = 150 m = 500 m = 2000 (b) Aggregation period b for local dataset size n for varying numbers of clients m, with ϵ = 0.01 and δ = 0.01. A.6 N OTE ON COMMUNICATION COMPLEXITY In our analysis of the communication complexity, we assume the total amount of communication to be linear in the number of communication rounds. For some communication systems the aggregated model can be broadcasted to individual clients which is not possible in daisy-chaining rounds, reducing the communication complexity in FedAvg. In most scenarios, fiber or GSM networks are used where each model has to be sent individually, so there is no substantial difference between broadcasting a model to all clients and sending an individual model to each client. Therefore, also in this case the amount of communication rounds determines the overall amount of communication. 21Published as a conference paper at ICLR 2023 A.7 E XTENDED DISCUSSION ON PRIVACY Federated learning only exchanges model parameters, and no local data. It is, however, possible to infer upon local data given the model parameters, model updates or gradients (Ma et al., 2020). In classical federated learning there are two types of attacks that would allow such inference: (i) an attacker intercepting the communication of a client with the server, obtaining models and model updates to infer upon the clients data, and (ii) a malicious server obtaining models to infer upon the data of each client. A malicious client cannot learn about a specific other client’s data, since it only obtains the average of all local models. In federated daisy-chaining there is a third possible attack: (iii) a malicious client obtaining model updates from another client to infer upon its data. In the following, we discuss potential defenses against these three types of attacks in more detail. Note that we limit the discussion on attacks that aim at inferring upon local data, thus breaching data privacy. Poisoning (Bhagoji et al., 2019) and backdoor (Sun et al., 2019) attacks are an additional threat in federated learning, but are of less importance for our main setting in healthcare: there is no obvious incentive for a hospital to poison a prediction. It is possible that FEDDC presents a novel risk surface for those attacks, but such attack strategies are non-obvious. Robust aggregation, such as the Radon point, are suitable defenses against such attacks (Liu et al., 2022). Moreover, the standard mechanisms that guarantee differential privacy also defend against backdoor and poising attacks (Sun et al., 2019). A general and wide-spread approach to tackle all three possible attack types is to add noise to the model parameters before sending. Using appropriate clipping and noise, this guarantees ϵ, δ- differential privacy for local data (Wei et al., 2020) at the cost of a slight-to-moderate loss in model quality. We empirically demonstrated that FEDDC performs well under such noise in Sec. 6. Another approach to tackle an attack on communication (i) is to use encrypted communication. One can furthermore protect against a malicious server (ii) by using homomorphic encryption that allows the server to average models without decrypting them (Zhang et al., 2020). This, however, only works for particular aggregation operators and does not allow to perform daisy-chaining. Secure daisy-chaining in the presence of a malicious server (ii) can, however, be performed using asymmetric encryption. Assume each client creates a public-private key pair and shares the public key with the server. To avoid the malicious server to send clients its own public key and act as a man in the middle, public keys have to be announced (e.g., by broadcast). While this allows sending clients to identify the recipient of their model, no receiving client can identify the sender. Thus, inference on the origin of a model remains impossible. For a daisy-chaining round the server sends the public key of the receiving client to the sending client, the sending client checks the validity of the key and sends an encrypted model to the server which forwards it to the receiving client. Since only the receiving client can decrypt the model, the communication is secure. In standard federated learning, a malicious client cannot infer specifically upon the data of another client from model updates, since it only receives the aggregate of all local models. In federated daisy-chaining, it receives the model from a random, unknown client in each daisy-chaining round. Now, the malicious client can infer upon the membership of a particular data point in the local dataset of the client the model originated from, i.e., through a membership inference attack (Shokri et al., 2017). Similarly, the malicious client can infer upon the presence of data points with certain attributes in the dataset (Ateniese et al., 2015). The malicious client, however, does not know the client the model was trained on, i.e., it does not know the origin of the dataset. Using a random scheduling of daisy-chaining and aggregation rounds at the server, the malicious client cannot even distinguish between a model from another client or the average of all models. Nonetheless, daisy-chaining opens up new potential attack vectors (e.g., clustering received models to potentially determine their origins). These potential attack vectors can be tackled in the same way as in standard federated learning, i.e., by adding noise to model parameters as discussed above, since “[d]ifferentially private models are, by construction, secure against membership inference attacks” (Shokri et al., 2017). 22",
      "references": [
        "Personalized federated learning with gaussian processes.",
        "Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers.",
        "Searching for exotic particles in high-energy physics with deep learning.",
        "Analyzing federated learning through an adversarial lens.",
        "Approximating center points with iterative radon points.",
        "Exploiting shared representations for personalized federated learning.",
        "Differentially private federated learning: A client level perspective.",
        "Bayes and tukey meet at the center point.",
        "Hyperpolarized mri of human prostate cancer reveals increased lactate with tumor grade driven by monocarboxylate transporter 1.",
        "On the convergence of local descent methods in federated learning.",
        "Towards fair federated learning with zero-shot data augmentation.",
        "Fl-ntk: A neural tangent kernel-based framework for federated learning analysis.",
        "Augmentation in healthcare: Augmented biosignal using deep learning and tensor representation.",
        "Gossip-based aggregation in large dynamic networks.",
        "Black-Box Parallelization for Machine Learning.",
        "Effective parallelisation for machine learning.",
        "Efficient decentralized deep learning by dynamic model averaging.",
        "Few-shot object detection via feature reweighting.",
        "Migrating models: A decentralized view on federated learning.",
        "Central limit theorem: the cornerstone of modern statistics.",
        "Gradient-based learning applied to document recognition.",
        "Model-contrastive federated learning.",
        "Federated optimization in heterogeneous networks.",
        "Fedbn: Federated learning on non-iid features via local batch normalization.",
        "A survey of text data augmentation.",
        "Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives.",
        "On safeguarding privacy and security in the framework of federated learning.",
        "Federated multi-task learning under a mixture of distributions.",
        "Communication-efficient learning of deep networks from decentralized data.",
        "The generalised coupon collector problem.",
        "The angiosarcoma project: enabling genomic and clinical discoveries in a rare cancer through patient-partnered research.",
        "Scikit-learn: Machine learning in Python.",
        "Robust aggregation for federated learning.",
        "Few-shot learning for dermatological disease diagnosis.",
        "Mengen konvexer Körper, die einen gemeinsamen Punkt enthalten.",
        "Adaptive federated optimization.",
        "Robust federated learning: The case of affine distribution shifts.",
        "Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization.",
        "Understanding machine learning: From theory to algorithms.",
        "Distributed stochastic optimization and learning.",
        "Membership inference attacks against machine learning models.",
        "Comprehensive integrative profiling of upper tract urothelial carcinomas.",
        "Can you really backdoor federated learning?",
        "Transfer learning.",
        "Mathematics and picturing data.",
        "Matching networks for one shot learning.",
        "Statistical learning theory: Models, concepts, and results.",
        "Federated learning with matched averaging.",
        "Federated learning with differential privacy: Algorithms and performance analysis.",
        "Flop: Federated learning on medical datasets using partial networks.",
        "Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning.",
        "Bayesian nonparametric federated learning of neural networks.",
        "Batchcrypt: Efficient homomorphic encryption for cross-silo federated learning.",
        "Deep leakage from gradients."
      ],
      "meta_data": {
        "arxiv_id": "2110.03469v3",
        "authors": [
          "Michael Kamp",
          "Jonas Fischer",
          "Jilles Vreeken"
        ],
        "published_date": "2021-10-07T13:49:23Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces Federated Daisy-Chaining (FEDDC), a federated learning protocol that alternates model aggregation with permutation (daisy-chaining) of local models across clients, enabling effective training when each client holds only very small datasets. Provides (i) algorithm and communication scheme, (ii) convergence proof for non-convex SGD with averaging, (iii) PAC-style generalization guarantees for convex problems when combined with Radon-point aggregation, (iv) privacy discussion plus DP extension, and (v) extensive empirical evidence on synthetic, SUSY, CIFAR-10, MNIST and two medical imaging datasets, showing FEDDC matches centralized performance and outperforms FEDAVG, FedProx, FedAdam/Yogi/Adagrad especially in low-sample, non-IID settings.",
        "methodology": "Core idea: in addition to periodic global aggregation every b rounds, every d rounds clients send their locally updated model to the server, which randomly permutes and forwards each model to another client (daisy-chain). Thus each model is sequentially trained on many small local datasets, reducing variance from bad local models. Aggregation can be simple averaging or robust operators such as iterated Radon point. The authors analyze communication (O(T/d + T/b) rounds), prove O(1/√(mT)) convergence for SGD with averaging (using PR-SGD theory), derive probability bounds showing FEDDC improves (ε,δ) guarantees for convex risk with Radon aggregation, and discuss differential privacy via clipped noisy updates. Algorithm supplied in pseudocode.",
        "experimental_setup": "Synthetic: sklearn 100-feature classification, 50 clients, 10 samples each.\nSUSY binary dataset: 441 clients, 2 samples each, linear model, compare FEDDC (d=1,b=50) with baselines.\nImage tasks: CIFAR-10 with subsets (9,600 samples, 150 or 250 clients, ResNet18 or small ResNet) including IID and pathological non-IID partitions; MNIST, 50 clients; Real medical – Brain MRI tumor detection (25 clients, 8 scans each, small CNN) and Chest X-ray pneumonia (150 clients, 8 images each, ResNet18). Baselines: vanilla FedAvg (different b), Daisy-chain only, FedProx, FedAdam, FedYogi, FedAdagrad, centralized training, and decentralized SGD. Metrics: test accuracy; train accuracy analyzed for overfitting. Additional experiments vary local sample size, client subsampling, DP noise, skewed data sizes, communication budget. Code released on GitHub.",
        "limitations": "1. Communication overhead grows linearly with added daisy-chaining rounds; only feasible in cross-silo settings with reliable bandwidth.\n2. Privacy risk: passing raw models between clients can increase exposure to membership-inference unless defenses (DP, encryption) are applied; analysis mostly conceptual.\n3. Theoretical guarantees for generalization require convex losses and Radon-point aggregation; deep-learning results rely on empirical evidence only.\n4. Experiments use relatively small networks and datasets; scalability to very large models or millions of mobile clients not demonstrated.\n5. Security against poisoning/backdoor attacks and robustness to adversarial clients left to future work.",
        "future_research_directions": "1. Design communication-efficient variants (e.g., model compression, adaptive d/b scheduling) for bandwidth-limited cross-device scenarios.\n2. Extend theoretical analysis to non-convex settings and stochastic client participation.\n3. Integrate robust/secure aggregation and formal privacy proofs tailored to daisy-chaining, including homomorphic encryption or secure multiparty computation.\n4. Explore personalized or clustered extensions where daisy-chaining is combined with client-specific models.\n5. Test FEDDC at scale with larger models (e.g., transformers) and heterogeneous hardware, and in real clinical deployments.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Expanding Small-Scale Datasets with Guided Imagination",
      "full_text": "Expanding Small-Scale Datasets with Guided Imagination Yifan Zhang1∗ Daquan Zhou2∗ Bryan Hooi1 Kai Wang1 Jiashi Feng2 1National University of Singapore 2ByteDance Abstract The power of DNNs relies heavily on the quantity and quality of training data. However, collecting and annotating data on a large scale is often expensive and time- consuming. To address this issue, we explore a new task, termeddataset expansion, aimed at expanding a ready-to-use small dataset by automatically creating new labeled samples. To this end, we present a Guided Imagination Framework (GIF) that leverages cutting-edge generative models like DALL-E2 and Stable Diffusion (SD) to \"imagine\" and create informative new data from the input seed data. Specifically, GIF conducts data imagination by optimizing the latent features of the seed data in the semantically meaningful space of the prior model, resulting in the creation of photo-realistic images with new content. To guide the imagination towards creating informative samples for model training, we introduce two key criteria, i.e., class-maintained information boostingand sample diversity promotion. These criteria are verified to be essential for effective dataset expansion: GIF-SD obtains 13.5% higher model accuracy on natural image datasets than unguided expansion with SD. With these essential criteria, GIF successfully expands small datasets in various scenarios, boosting model accuracy by 36.9% on average over six natural image datasets and by 13.5% on average over three medical datasets. The source code is available at https://github.com/Vanint/DatasetExpansion. 1 Introduction A substantial number of training samples is essential for unleashing the power of deep networks [14]. However, such requirements often impede small-scale data applications from fully leveraging deep learning solutions. Manual collection and labeling of large-scale datasets are often expensive and time-consuming in small-scale scenarios [52]. To address data scarcity while minimizing costs, we explore a novel task, termed Dataset Expansion. As depicted in Figure 1 (left), dataset expansion aims to create an automatic data generation pipeline that can expand a small dataset into a larger and more informative one for model training. This task particularly focuses on enhancing the quantity and quality of the small-scale dataset by creating informative new samples. This differs from conventional data augmentations that primarily focus on increasing data size through transformations, often without creating samples that offer fundamentally new content. The expanded dataset is expected to be versatile, fit for training various network architectures, and promote model generalization. We empirically find that naive applications of existing methods cannot address the problem well (cf. Table 1 and Figure 4). Firstly, data augmentation [ 12, 15, 107], involving applying pre-set transformations to images, can be used for dataset expansion. However, these transformations primarily alter the surface visual characteristics of an image, but cannot create images with novel content (cf. Figure 5a). As a result, the new information introduced is limited, and the performance gains tend to saturate quickly as more data is generated. Secondly, we have explored pre-trained generative models (e.g., Stable Diffusion (SD) [59]) to create images for model training. However, ∗Corresponding to: yifan.zhang@u.nus.edu, zhoudaquan21@gmail.com 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2211.13976v6  [cs.CV]  10 Oct 2023+39 +26+56+67+20 +15+22 Coarse-grainedobjectFine-grainedobjectTexturePathologyUltrasound datasetexpansion Auto-createddatawithnewinformation Smalldataset Expandeddataset catdog catdog catdog Figure 1: Dataset Expansion aims to create data with new information to enrich small datasets for training DNNs better (left). As shown, the created images by our method are all class-relevant but diverse (e.g., new sitting postures of cats, new patterns of cushions). This enables ResNet-50 trained on our expanded datasets to perform much better than the one trained on the original datasets (right). these pre-trained generative models are usually category-agnostic to the target dataset, so they cannot ensure that the synthetic samples carry the correct labels and are beneficial to model training. Different from them, our solution is inspired by human learning with imagination. Upon observing an object, humans can readily imagine its different variants in various shapes, colors, or contexts, relying on their prior understanding of the world [ 74, 79]. Such an imagination process is highly valuable for dataset expansion as it does not merely tweak an object’s appearance, but leverages rich prior knowledge to create object variants infused with new information. In tandem with this, recent generative models like SD and DALL-E2 [56] have demonstrated exceptional abilities in capturing the data distribution of extremely vast datasets [4, 66] and generating photo-realistic images with rich content. This motivates us to explore their use as prior models to establish our computational data imagination pipeline for dataset expansion, i.e., imagining different sample variants given seed data. However, the realization of this idea is non-trivial and is complicated by two key challenges: how to generate samples with correct labels, and how to ensure the created samples boost model training. To handle these challenges, we conduct a series of exploratory studies (cf. Section 3) and make two key findings. First, CLIP [54], which offers excellent zero-shot classification abilities, can map latent features of category-agnostic generative models to the specific label space of the target dataset. This enables us to generate samples with correct labels. Second, we discover two informativeness criteria that are crucial for generating effective training data: 1) class-maintained information boosting to ensure that the imagined data are class-consistent with the seed data and bring new information; 2) sample diversity promotion to encourage the imagined samples to have diversified content. In light of the above findings, we propose a Guided Imagination Framework (GIF) for dataset expansion. Specifically, given a seed image, GIF first extracts its latent feature with the prior (generative) model. Unlike data augmentation that imposes variation over raw images, GIF optimizes the \"variation\" over latent features. Thanks to the guidance carefully designed by our discovered criteria, the latent feature is optimized to provide more information while maintaining its class. This enables GIF to create informative new samples, with class-consistent semantics yet higher content diversity, for dataset expansion. Such expansion is shown to benefit model generalization. As DALL-E2 [56] and SD are powerful in generating images, and MAE [24] excels at reconstructing images, we explore their use as prior models of GIF for data imagination. We evaluate our methods on small-scale natural and medical image datasets. As shown in Figure 1 (right), compared to ResNet- 50 trained on the original dataset, our method improves its performance by a large margin across various visual tasks. Specifically, with the designed guidance, GIF obtains 36.9% accuracy gains on average over six natural image datasets and 13.5% gains on average over three medical datasets. Moreover, the expansion efficiency of GIF is much higher than data augmentation, i.e., 5× expansion by GIF-SD outperforms even 20× expansion by Cutout, GridMask and RandAugment on Cars and DTD datasets. In addition, the expanded datasets also benefit out-of-distribution performance of models, and can be directly used to train various architectures (e.g., ResNeXt [84], WideResNet [95], and MobileNet [64]), leading to consistent performance gains. We also empirically show that GIF is more applicable than CLIP to handle real small-data scenarios, particularly with non-natural image domains (e.g., medicine) and hardware constraints (e.g., limited supportable model sizes). Please note that GIF is much faster and more cost-effective than human data collection for dataset expansion. 22 Related Work Learning with synthetic images. Training with synthetic data is a promising direction [ 26, 34, 108]. For example, DatasetGANs [ 42, 103] explore GAN models [17, 33] to generate images for segmentation model training. However, they require a sufficiently large dataset for in-domain GAN training, which is not feasible in small-data scenarios. Also, as the generated images are without labels, they need manual annotations on generated images to train a label generator for annotating synthetic images. Similarly, many recent studies [2, 3, 22, 39, 63, 77, 81, 92] also explored generative models to generate new data for model training. However, these methods cannot ensure that the synthesized data bring sufficient new information and accurate labels for the target small datasets. Moreover, training GANs from scratch [3, 39, 63, 77, 92], especially with very limited data, often fails to converge or produce meaningful results. In contrast, our dataset expansion aims to expand a small dataset into a larger labeled one in a fully automatic manner without involving human annotators. As such, our method emerges as a more effective way to expand small datasets. Data augmentation. Augmentation employs manually specified rules, such as image manipula- tion [89], erasing [15, 107], mixup [28, 98, 99], and transformation selection [11, 12] to boost model generalization [68]. Despite certain benefits, these methods enrich images by pre-defined transforma- tions, which only locally vary the pixel values of images and cannot generate images with highly diversified content. Moreover, the effectiveness of augmented data is not always guaranteed due to random transformations. In contrast, our approach harnesses generative models trained on large datasets and guides them to generate more informative and diversified images, thus resulting in more effective and efficient dataset expansion. For additional related studies, please refer to Appendix A. 3 Problem and Preliminary Studies Problem statement. To address data scarcity, we explore a noveldataset expansion task. Without loss of generality, we consider image classification, where a small-scale training datasetDo = {xi, yi}no i=1 is given. Here, xi denotes an instance with class label yi, and no denotes the number of samples. The goal of dataset expansion is to generate a set of new synthetic samples Ds = {x′ j, y′ j}ns j=1 to enlarge the original dataset, such that a DNN model trained on the expanded dataset Do ∪ Ds outperforms the model trained on Do significantly. The key is that the synthetic sample set Ds should be highly related to the original dataset Do and bring sufficient new information to boost model training. 3.1 A proposal for computational imagination models Given an object, humans can easily imagine its different variants, like the object in various colors, shapes, or contexts, based on their accumulated prior knowledge about the world [ 74, 79]. This imagination process is highly useful for dataset expansion, as it does not simply perturb the object’s appearance but applies rich prior knowledge to create variants with new information. In light of this, we seek to build a computational model to simulate this imagination process for dataset expansion. Deep generative models, known for their capacity to capture the distribution of a dataset, become our tool of choice. By drawing on their prior distribution knowledge, we can generate new samples resembling the characteristics of their training datasets. More importantly, recent generative models, such as Stable Diffusion (SD) and DALL-E2, have demonstrated remarkable abilities in capturing the distribution of extremely large datasets and generating photo-realistic images with diverse content. This inspires us to explore their use as prior models to construct our data imagination pipeline. Specifically, given a pre-trained generative modelG and a seed example (x, y) from the target dataset, we formulate the imagination of x′ from x as x′ = G(f(x) + δ). Here, f(·) is an image encoder of G to transform the raw image into an embedding for imagination with the generative model. δ is a perturbation applied to f(x) such that G can generate x′ different from x. A simple choice of δ would be Gaussian random noise, which, however, cannot generate highly informative samples. In the following subsection, we will discuss how to optimize δ to provide useful guidance. It is worth noting that we do not aim to construct a biologically plausible imagination model that strictly follows the dynamics and rules of the human brain. Instead, we draw inspiration from the imaginative activities of the human brain and propose a pipeline to leverage well pre-trained generative models to explore dataset expansion. 3original dataset Cutout expansion RandAugment expansion 30 35 40 45 50 55Model performance 35.0 43.2 44.8 46.7 48.0 Random Cutout expansion Selective Cutout expansion Random RandAugment expansion Selective RandAugment expansion (a) Input DALLEexpansion GuidedDALLEexpansion  (b) Figure 2: Effectiveness of the informativeness criteria for sample creation . (a) Comparison between random expansion and our selective expansion on CIFAR100-Subset. (b) Visualization of DALLE expansion with and without diversity guidance. 3.2 How to guide imagination for effective expansion? Our data imagination pipeline leverages generative models to create new samples from seed data. However, it is unclear what types of samples are effective and how to optimizeδ accordingly in the pipeline to create data that are useful for model training. Our key insight is that the newly created sample x′ should introduce new information compared to the seed sample x, while preserving the same class semantics as the seed sample . To achieve these properties, we explore the following preliminary studies and discover two key criteria: (1) class-maintained information boosting, and (2) sample diversity promotion. Class-maintained informativeness boosting. When enhancing data informativeness, it is non-trivial to ensure that the generated x′ has the same label y as the seed sample x, since it is difficult to maintain the class semantics after perturbation in the latent space f(x). To overcome this, we explore CLIP [54] for its well-known image-text alignment ability: CLIP’s image encoder can project an image x to an embedding space aligned with the language embedding of its class name y [71, 80]. Therefore, we can leverage CLIP’s embedding vectors of all class names as a zero-shot classifier to guide the generation of samples that maintain the same class semantics as seed data. Meanwhile, the entropy of the zero-shot prediction can serve as a measure to boost the classification informativeness of the generated data. To pinpoint whether the criteria of class-maintained information boosting helps to generate more informative samples, we start with exploratory experiments on a subset of CIFAR100 [41]. Here, the subset is built for simulating small-scale datasets by randomly sampling 100 instances per class from CIFAR100. We first synthesize samples based on existing data augmentation methods ( i.e., RandAugment and Cutout [ 15]) and expand CIFAR100-Subset by 5 ×. Meanwhile, we conduct selective augmentation expansion based on our criteria ( i.e., selecting the samples with the same zero-shot prediction but higher prediction entropy compared to seed samples) until we reach the required expansion ratio per seed sample. As shown in Figure 2a, selective expansion outperforms random expansion by 1.3% to 1.6%, meaning that the selected samples are more informative for model training. Compared to random augmentation, selective expansion filters out the synthetic data with lower prediction entropy and those with higher entropy but inconsistent classes. The remaining data thus preserve the same class semantics but bring more information gain, leading to better expansion effectiveness. Sample diversity promotion. To prevent the \"imagination collapse\" issue that generative models yield overly similar or duplicate samples, we delve further into the criterion of sample diversity promotion. To study its effectiveness, we resort to a powerful generative model (i.e., DALL-E2) as the prior model to generate images and expand CIFAR100-Subset by 5×, where the guided expansion scheme and the implementation of diversity promotion will be introduced in the following section. As shown in Figure 2b, the generated images with diversity guidance are more diversified: starfish images have more diverse object numbers, and motorbike images have more diverse angles of view and even a new driver. This leads to 1.4% additional accuracy gains on CIFAR100-Subset (cf. Table 20 in Appendix F.5), demonstrating that the criterion of sample diversity promotion is effective in bringing diversified information to boost model training. 4input CLIPimageencoder imaginedimages DALL-E2decoder informativenessguidanceCLIPtext encoderclass names perturbed zero-shot classifier  optimized latentfeaturerepeated “cat”“dog”“car” catdogcar ⊕ Figure 3: Illustration of the proposed GIF method based on DALL-E2 [ 56], which expands small datasets by creating informative new samples with guided imagination. Here, we resort to DALL-E2 as the prior model, in which the image/text encoders are CLIP image/text encoders while the decoder is the diffusion model of DALL-E2. Moreover, ⊕ denotes guided residual multiplicative perturbation. More implementation details of GIF-DALLE, GIF-SD and GIF-MAE are provided in Appendix D. 4 GIF: A Guided Imagination Framework for Dataset Expansion In light of the aforementioned studies, we propose a Guided Imagination Framework (GIF) for dataset expansion. This framework guides the imagination of prior generative models based on the identified criteria. Given a seed image x from a target dataset, we first extract its latent feature f(x) via the encoder of the generative model. Different from data augmentation that directly imposes variations over raw RGB images, GIF optimizes the \"variations\" over latent features. Thanks to the aforementioned criteria as guidance, the optimized latent features result in samples that preserve the correct class semantics while introducing new information beneficial for model training. Overall pipeline. To detail the framework, we use DALL-E2 [56] as a prior generative model for illustration. As shown in Figure 3, DALL-E2 adopts CLIP image/text encoders fCLIP-I and fCLIP-T as its image/text encoders and uses a pre-trained diffusion model G as its image decoder. To create a set of new images x′ from the seed image x, GIF first repeats its latent feature f = fCLIP-I(x) for K times, with K being the expansion ratio. For each latent feature f, we inject perturbation over it with randomly initialized noise z ∼ U(0, 1) and bias b ∼ N(0, 1). Here, to prevent out-of-control imagination, we conduct residual multiplicative perturbation on the latent feature f and enforce an ε-ball constraint on the perturbation as follows: f′ = Pf,ϵ((1 + z)f + b), (1) where Pf,ϵ(·) means to project the perturbed feature f′ to the ε-ball of the original latent feature, i.e., ∥f′ − f∥∞ ≤ ε. Note that each latent feature has independent z and b. Following our explored criteria, GIF optimizes z and b over the latent feature space as follows: z′, b′ ← −arg max z,b Sinf + Sdiv, (2) where Sinf and Sdiv correspond to the class-maintained informativeness score and the sample diversity score, respectively, which will be elaborated below. This latent feature optimization is the key step for achieving guided imagination. After updating the noise z′ and bias b′ for each latent feature, GIF obtains a set of new latent features by Eq. (1), which are then used to create new samples through the decoder G. Class-maintained informativeness. To boost the informativeness of the generated data without changing class labels, we resort to CLIP’s zero-shot classification abilities. Specifically, we first use fCLIP-T to encode the class name y of a sample x and take the embedding wy = fCLIP-T(y) as the zero-shot classifier of class y. Each latent feature fCLIP-I(x) can be classified according to its cosine similarity to wy, i.e., the affinity score of x belonging to class y is ˆsy = cos(f(x), wy), which forms a prediction probability vector s = σ([ˆs1, . . . ,ˆsC]) for the total C classes of the target dataset based on softmax σ(·). The prediction of the perturbed feature s′ can be obtained in the same way. We then design Sinf to improve the information entropy of the perturbed feature while maintaining its class semantics as the seed sample: Sinf = s′ j + (s log(s) − s′ log(s′)), s.t. j = arg max(s). Specifically, s′ j denotes the zero-shot classification score of the perturbed feature s′ regarding the predicted label of the original latent feature j = arg max(s). Here, the zero-shot prediction of the 5original data serves as an anchor to regularize the class semantics of the perturbed features in CLIP’s embedding space, thus encouraging class consistency between the generated samples and the seed sample. Moreover, s log(s) − s′ log(s′) means contrastive entropy increment, which encourages the perturbed feature to have higher prediction entropy and helps to improve the classification informativeness of the generated image. Sample diversity. To promote the diversity of the generated samples, we design Sdiv as the Kullback–Leibler (KL) divergence among all perturbed latent features of a seed sample: Sdiv = DKL(f′∥ ¯f), where f′ denotes the current perturbed latent feature and ¯f indicates the mean over the K perturbed features of the seed sample. To enable measuring KL divergence between features, inspired by [83], we apply the softmax function to transform feature vectors into probability vectors for KL divergence. Theoretical analysis. We then analyze our method to conceptually understand its benefits to model generalization. We resort to δ-cover [62] to analyze how data diversity influences the generalization error bound. Specifically, \"a dataset E is a δ-cover of a dataset S\" means a set of balls with radius δ centered at each sample of the dataset E can cover the entire dataset S. According to the property of δ-cover, we define the dataset diversity by δ-diversity as the inverse of the minimal δmin, i.e., δdiv = 1 δmin . Following the same assumptions of [67], we have the following result. Theorem 4.1. Let A denote a learning algorithm that outputs a set of parameters given a dataset D = {xi, yi}i∈[n] with n i.i.d. samples drawn from distribution PZ. Assume the hypothesis function is λη-Lipschitz continuous, the loss function ℓ(x, y) is λℓ-Lipschitz continuous for all y, and is bounded by L, with ℓ(xi, yi; A) = 0 for all i ∈ [n]. If D constitutes a δ-cover of PZ, then with probability at least 1 − γ, the generalization error bound satisfies: |Ex,y∼PZ [ℓ(x, y; A)] − 1 n X i∈[n] ℓ(xi, yi; A)| c ≤ λℓ + ληLC δdiv , where C is a constant, and the symbol c ≤ indicates \"smaller than\" up to an additive constant. Please refer to Appendix C for proofs. This theorem shows that the generalization error is bounded by the inverse of δ-diversity. That is, the more diverse samples are created, the more improvement of generalization performance would be made in model training. In real small-data applications, data scarcity leads the covering radius δ to be very large and thus the δ-diversity is low, which severely affects model generalization. Simply increasing the data number ( e.g., via data repeating) does not help generalization since it does not increase δ-diversity. In contrast, our GIF applies two key criteria (i.e., \"informativeness boosting\" and \"sample diversity promotion\") to create informative and diversified new samples. The expanded dataset thus has higher data diversity than random augmentation, which helps to increase δ-diversity and thus boosts model generalization. This advantage can be verified by Table 2 and Figure 4. Implementing GIF with different prior models. To enable effective expansion, we explore three prior models for guided imagination: DALL-E2 [56] and Stable Diffusion [59] are advanced image generative methods, while MAE [ 24] is skilled at reconstructing images. We call the resulting methods GIF-DALLE, GIF-SD, and GIF-MAE, respectively. We introduce their high-level ideas below, while their method details are provided in Appendix D. GIF-DALLE adheres strictly to the above pipeline for guided imagination, while we slightly modify the pipeline in GIF-SD and GIF-MAE since their image encoders are different from the CLIP image encoder. Given a seed sample, GIF-SD and GIF-MAE first generate the latent feature via the encoders of their prior models, and then conduct random noise perturbation following Eq. (1). Here, GIF-SD has one more step than GIF-MAE before noise perturbation, i.e., conducting prompt-guided diffusion for its latent feature, where the rule of the prompt design will be elaborated in Appendix D.2. Based on the perturbed feature, GIF-SD and GIF-MAE generate an intermediate image via their decoders, and apply CLIP to conduct zero-shot predictions for both the seed and intermediate images to compute the informativeness guidance (i.e., Eq. (2)) for optimizing latent features. Note that, in GIF-SD and GIF-MAE, we execute channel-level noise perturbation since we find it facilitates the generation of content-consistent samples with a greater variety of image styles (cf. Appendix B.2). 6Table 1: Accuracy of ResNet-50 trained from scratch on small datasets and their expanded datasets by various methods. Here, CIFAR100-Subset is expanded by 5 ×, Pets is expanded by 30 ×, and all other natural image datasets are expanded by 20 ×. All medical image datasets are expanded by 5×. Moreover, MAE, DALL-E2 and SD (Stable Diffusion) are the baselines of directly using them to expand datasets without our GIF. In addition, CLIP indicates its zero-shot performance. All performance values are averaged over three runs. Please see Appendix F for more comparisons. Dataset Natural image datasets Medical image datasets Caltech101 Cars Flowers DTD CIFAR100-S Pets Average PathMNIST BreastMNIST OrganSMNIST Average Original 26.3 19.8 74.1 23.1 35.0 6.8 30.9 72.4 55.8 76.3 68.2CLIP 82.1 55.8 65.9 41.7 41.6 85.4 62.1 10.7 51.8 7.7 23.4Distillation of CLIP 33.2 18.9 75.1 25.6 37.8 11.1 33.6 77.3 60.2 77.4 71.6 ExpandedCutout [15] 51.5 25.8 77.8 24.2 44.3 38.7 43.7 (+12.8) 78.8 66.7 78.3 74.6 (+6.4)GridMask [8] 51.6 28.4 80.7 25.3 48.2 37.6 45.3 (+14.4) 78.4 66.8 78.9 74.7 (+6.5)RandAugment [12] 57.8 43.2 83.8 28.7 46.7 48.0 51.4 (+20.5) 79.2 68.7 79.6 75.8 (+7.6)MAE [24] 50.6 25.9 76.3 27.6 44.3 39.9 44.1 (+13.2) 81.7 63.4 78.6 74.6 (+6.4)DALL-E2 [56] 61.3 48.3 84.1 34.5 52.1 61.7 57.0 (+26.1) 82.8 70.8 79.3 77.6 (+9.4)SD [59] 51.1 51.7 78.8 33.2 52.9 57.9 54.3 (+23.4) 85.1 73.8 78.9 79.3 (+11.1)GIF-MAE (ours) 58.4 44.5 84.4 34.2 52.7 52.4 54.4 (+23.5) 82.0 73.3 80.6 78.6 (+10.4)GIF-DALLE (ours) 63.0 53.1 88.2 39.5 54.5 66.4 60.8 (+29.9) 84.4 76.6 80.5 80.5 (+12.3)GIF-SD (ours) 65.1 75.7 88.3 43.4 61.1 73.4 67.8 (+36.9) 86.9 77.4 80.7 81.7 (+13.5) +5x +10x +20x 20 30 40 50 60 70 80Model performance 24.1 25.7 25.8 26.2 27.3 28.4 33.9 38.7 43.2 36.0 42.8 44.546.3 50.6 53.1 60.6 70.5 75.7 Standard Cars Cutout GridMask RandAugment GIF-MAE (ours) GIF-DALLE (ours) GIF-SD (ours) +5x +10x +20x 20 25 30 35 40 45 22.9 23.4 24.2 24.1 24.3 25.326.1 28.2 28.727.9 33.3 34.2 31.2 35.0 39.5 33.9 39.5 43.4 DTD +10x +20x +30x 30 40 50 60 70 80 37.7 38.6 38.7 37.1 37.5 37.6 45.3 46.7 48.0 50.7 51.7 52.4 59.7 64.5 66.465.67 70.7 73.4 Oxford-IIIT Pets Figure 4: Accuracy of ResNet-50 trained from scratch on the expanded datasets with different expansion ratios. The results on other datasets are reported in Appendix F.1. 5 Experiments Datasets. We evaluate GIF on six small-scale natural image datasets and three medical datasets. Nat- ural datasets cover a variety of tasks: object classification (Caltech-101 [18], CIFAR100-Subset [41]), fine-grained classification (Cars [40], Flowers [49], Pets [50]) and texture classification (DTD [10]). Here, CIFAR100-Subset is an artificial dataset for simulating small-scale datasets by randomly sampling 100 instances per class from the original CIFAR100. Moreover, medical datasets [ 88] cover a wide range of image modalities, such as breast ultrasound (BreastMNIST), colon pathology (PathMNIST), and Abdominal CT (OrganSMNIST). Please refer to Appendix E for data statistics. Compared methods. As there is no algorithm devoted to dataset expansion, we take representative data augmentation methods as baselines, including RandAugment, Cutout, and GridMask [8]. We also compare to directly using prior models (i.e., DALL-E2, SD, and MAE) for dataset expansion. Besides, CLIP has outstanding zero-shot abilities, and some recent studies explore distilling CLIP to facilitate model training. Hence, we also compare to zero-shot prediction and knowledge distillation (KD) of CLIP on the target datasets. The implementation details of GIF are provided in Appendix D. 5.1 Results of small-scale dataset expansion Expansion effectiveness. As shown in Table 1, compared with the models trained on original datasets, GIF-SD boosts their accuracy by an average of 36.9% across six natural image datasets and 13.5% across three medical datasets. This verifies the superior capabilities of GIF over other methods for enhancing small datasets, particularly in the data-starved field of medical image understanding. This also suggests that dataset expansion is a promising direction for real small-data applications. Here, the reason why GIF-SD outperforms GIF-DALLE is that GIF-DALLE only exploits the image-to-image variation ability of DALL-E2, while GIF-SD further applies text prompts to diversify samples. Expansion efficiency. Our GIF is more sample efficient than data augmentations, in terms of the accuracy gain brought by each created sample. As shown in Figure 4, 5 × expansion by GIF-SD outperforms even 20× expansion by various data augmentations on Cars and DTD datasets, implying 7Table 2: Corruption accuracy of ResNet-50 trained from scratch on CIFAR100-S and our 5× expanded dataset, under 15 types of corruption in CIFAR100-C with the severity level 3. More results regarding other severity levels are provided in Appendix F.2. Noise Blur Weather DigitalDataset Gauss. Shot Impul.Defoc. Glass Motion ZoomSnow Frost Fog Brit.Contr. Elastic Pixel JPEGAverage Original 12.8 17.0 12.530.5 31.7 25.2 28.6 26.5 19.0 18.6 28.311.5 29.5 33.6 28.8 23.65×-expandedby RandAugment16.7 21.9 27.542.2 42.5 35.8 40.2 36.9 31.9 30.0 43.120.4 41.2 44.7 37.634.2 (+10.6)5×-expandedby GIF-SD 29.7 36.4 32.751.9 32.4 39.2 46.0 45.3 38.1 47.1 55.737.3 48.6 53.2 49.443.3 (+19.7)20×-expandedby GIF-SD31.8 39.2 34.758.4 33.4 43.1 51.9 51.7 47.4 55.0 63.346.5 54.9 58.0 53.648.2(+24.6) Table 3: Accuracy of various architectures trained on 5 × expanded Cars. The results on other datasets are given in Appendix F.3. Dataset ResNeXt-50 WideResNet-50 MobilteNet-v2 Avg. Original 18.4±0.5 32.0±0.8 26.2±4.2 25.5 ExpandedRandAugment 29.6±0.8 49.2±0.2 39.7±2.5 39.5 (+14.0)GIF-DALLE 43.7±0.2 60.0±0.6 47.8±0.6 50.5 (+25.0)GIF-SD 64.1±1.3 75.1±0.4 60.2±1.6 63.5(+38.0) Table 4: Comparison between our methods and directly fine-tuning CLIP models on three medical image datasets. Dataset PathMNIST BreastMNIST OrganSMNIST Originaldataset 72.4 ±0.7 55.8±1.3 76.3±0.4 Linear-probing of CLIP 74.3±0.1 60.0±2.9 64.9±0.2 fine-tuning of CLIP 78.4±0.9 67.2±2.4 78.9±0.1 distillation of CLIP 77.3±1.7 60.2±1.3 77.4±0.8 5×-expanded by GIF-SD86.9±0.3 77.4±1.8 80.7±0.2 Table 5: Benefits of dataset expansion to CLIP fine-tuning on CIFAR100-S. Moreover, ID indicates in-distribution performance, while OOD indicates out-of-distribution performance on CIFAR100-C. Methods ID Accuracy OOD Accuracy Training from scratch on original dataset 35.0 23.6 Fine-tuning CLIP on original dataset 75.2 (+40.2) 55.4 (+31.8) Fine-tuning CLIP on 5x-expanded dataset by GIF-SD79.4(+44.4) 61.4(+37.8) our method is at least 4× more efficient than them. The limitations of these augmentations lie in their inability to generate new and highly diverse content. In contrast, GIF leverages strong prior models (e.g., SD), guided by our discovered criteria, to perform data imagination. Hence, our method can generate more diversified and informative samples, yielding more significant gains per expansion. Benefits to model generalization. Theorem 4.1 has shown the theoretical benefit of GIF to model generalization. Here, Table 2 demonstrates that GIF significantly boosts model out-of-distribution (OOD) generalization on CIFAR100-C [27], bringing 19.3% accuracy gain on average over 15 types of OOD corruption. This further verifies the empirical benefit of GIF to model generalization. Versatility to various architectures. We also apply the 5×-expanded Cars dataset by GIF to train ResNeXt-50, WideResNet-50 and MobileNet V2 from scratch. Table 3 shows that the expanded dataset brings consistent accuracy gains for all architectures. This underscores the versatility of our method: once expanded, these datasets can readily be applied to train various model architectures. Comparisons with CLIP.As our method applies CLIP for dataset expansion, one might question why not directly use CLIP for classifying the target dataset. In fact, our GIF offers two main advantages over CLIP in real-world small-data applications. First, GIF has superior applicability to the scenarios of different image domains. Although CLIP performs well on natural images, its transferability to non-natural domains, such as medical images, is limited (cf. Table 4). In contrast, our GIF is able to create samples of similar nature as the target data for dataset expansion, making it more applicable to real scenarios across diverse image domains. Second, GIF supplies expanded datasets suitable for training various model architectures. In certain scenarios like mobile terminals, hardware constraints may limit the supportable model size, which makes the public CLIP checkpoints (such as ResNet-50, ViT-B/32, or even larger models) unfeasible to use. Also, distilling from these CLIP models can only yield limited performance gains (cf. Table 1). In contrast, the expanded datasets by our method can be directly used to train various architectures (cf. Table 3), making our approach more practical for hardware-limited scenarios. Further comparisons and discussions are provided in Appendix F.4. Benefits to model fine-tuning. In previous experiments, we have demonstrated the advantage of dataset expansion over model fine-tuning on medical image domains. Here, we further evaluate the benefits of dataset expansion to model fine-tuning. Hence, we use the 5x-expanded dataset by GIF-SD to fine-tune the pre-trained CLIP VIT-B/32. Table 5 shows that our dataset expansion significantly improves the fine-tuning performance of CLIP on CIFAR100-S, in terms of both in-distribution and out-of-distribution performance. 8Input  OurGIF-DALLE OurGIF-SD GridMask RandAugment (a) InputFailurecases (b) Figure 5: Visualization. (a) Examples of the created samples for Caltech101 by augmentation and GIF. Please see Appendix G for the visualization of more datasets. (b) Failure cases by GIF-SD. original MAE fixed pixel noisefixed channel noiselearned token noiselearned channel noise 30 35 40 45 50 55Model performance35.0 44.3 45.3 48.8 46.2 52.7 Figure 6: Effects of different perturbation noise on MAE for expanding CIFAR100-Subset by 5×. Input RandAugment OurGIF-SD Figure 7: Visualization of the created medical images by GIF-SD, where SD is fine-tuned on medical data before dataset expansion (Please see the analysis of fine-tuning in Appendix B.6). 5.2 Analyses and discussions We next empirically analyze GIF. Due to the page limit, we provide more analyses of GIF ( e.g., mixup, CLIP, image retrieval, and long-tailed data) in Appendix B and Appendix F. Effectiveness of zero-shot CLIP in GIF.We start with analyzing the role of zero-shot CLIP in GIF. We empirically find (cf. Appendix B.5) that GIF with fine-tuned CLIP performs only comparably to that with zero-shot CLIP on medical datasets. This reflects that zero-shot CLIP is enough to provide sound guidance without the need for fine-tuning. Additionally, a random-initialized ResNet50 performs far inferior to zero-shot CLIP in dataset expansion, further highlighting the significant role of zero-shot CLIP in GIF. Please refer to Appendix B.5 for more detailed analyses. Effectiveness of guidance in GIF. Table 1 shows that our guided expansion obtains consistent performance gains compared to unguided expansion with SD, DALL-E2 or MAE, respectively. For instance, GIF-SD obtains 13.5% higher model accuracy on natural image datasets than unguided expansion with SD. This verifies the effectiveness of our criteria in optimizing the informativeness and diversity of the created samples. More ablation analyses of each criterion are given in Appendix F.5. Pixel-wise vs. channel-wise noise. GIF-SD and GIF-MAE inject perturbation along the channel di- mension instead of the spatial dimension. This is attributed to our empirical analysis in Appendix B.2. We empirically find that the generated image based on pixel-level noise variation is analogous to adding pixel-level noise to the original images. This may harm the integrity and smoothness of image content, leading the generated images to be noisy (cf. Figure 10(d) of Appendix B.2). Therefore, we decouple latent features into two dimensions (i.e., token and channel) and particularly conduct channel-level perturbation. As shown in Figure 10(e), optimizing channel-level noise variation can generate more informative data, leading to more effective expansion (cf. Figure 6). Visualization. The samples we created are visualized in Figures 5a and 7. While GridMask obscures some image pixels and RandAugment randomly alters images with pre-set transformations, both fail to generate new image content (cf. Figure 5a). More critically, as shown in Figure 7, RandAugment may crop the lesion location of medical images, leading to less informative and even noisy samples. In contrast, our method can not only generate samples with novel content (e.g., varied postures and backgrounds of water lilies) but also maintains their class semantics, and thus is a more effective way to expand small-scale datasets than traditional augmentations, as evidenced by Table 1. 9Table 6: Consumption costs. Time and costs are calculated from the expansion of 10,000 images. Accuracy improve- ments are compared to the original, small dataset. Method Expansion speed Time Costs Accuracy gains Human collection 121.0s per image 2 weeks $800 - Cutout 0.008s per image 76 seconds $0.46 +12.8GridMask 0.007s per image 72 seconds $0.43 +14.4RandAugment 0.008s per image 82 seconds $0.49 +20.5GIF-MAE (ours) 0.008s per image 80 seconds $0.48 +23.5GIF-SD (ours) 6.6s per image 2 hours $40 +36.9 Table 7: Relation analysis between the domain gap (FID) and model accuracy. Datasets FID Accuracy (%) CIFAR100-S - 35.0 RandAugment 24.3 46.7 Cutout 104.7 44.3 Gridmask 104.8 48.2 GIF-MAE 72.3 52.7 GIF-DALLE 39.5 54.5 GIF-SD 81.7 61.1 Failure cases. Figure 5b visualizes some failure cases by GIF-SD. As we use pre-trained models without fine-tuning on the natural images, the quality of some created samples is limited due to domain shifts. For example, the face of the generated cat in Figure 5b seems like a lion face with a long beard. However, despite seeming less realistic, those samples are created following our guidance, so they can still maintain class consistency and bring new information, thus benefiting model training. Computational efficiency and time costs . Compared to human data collection, our GIF offers substantial savings in time and costs for small dataset expansion. As shown in Table 6, GIF-MAE achieves a 5× expansion per sample in just 0.04 seconds, while GIF-SD accomplishes the same in 33 seconds. To illustrate, according to rates from Masterpiece Group2, manually annotating 10,000 images takes two weeks and costs around $800. In contrast, GIF-SD generates the same amount of labeled data in a mere two hours, costing roughly $40 for renting 8 V100 GPUs3. Moreover, with a slight model performance drop, GIF-MAE can create 10,000 labeled data in just 6 seconds, at a cost of about $0.48 for renting 8 V100 GPUs for 80 seconds. Specifically, GIF-MAE has a time cost within the same magnitude as data augmentation, but it delivers much better performance gains. The slight time overhead introduced by MAE is offset by GPU acceleration, resulting in competitive time costs. For those prioritizing performance, GIF-SD becomes a more attractive option. Although it involves a longer time due to its iterative diffusion process, it provides more significant performance gains. Note that our method only requires one-time expansion: the resultant dataset can be directly used to train various models (cf. Table 3), without the need for regeneration for each model. Relation analysis between the domain gap and model performance . We further compute the Fréchet Inception Distance (FID) between the synthetic data generated by different methods and the original data of CIFAR100-S. Interestingly, while one might initially assume that a lower FID implies better quality for the expanded data, the actual performance does not consistently follow this notion. As shown in Table 7, even though GIF-SD has a worse FID than GIF-DALLE, it achieves better performance. Likewise, despite having nearly identical FIDs, Cutout and Gridmask lead to different performance. These results suggest that the effectiveness of dataset expansion methods depends on how much additional information and class consistency the generated data can provide to the original dataset, rather than the distribution similarity between those samples and the original data. This discussion may spark further research into the relationship between expansion effectiveness and data fidelity (as measured by metrics like FID), potentially guiding the development of even more effective dataset expansion techniques in the future. 6 Conclusion This paper has explored a novel task, dataset expansion, towards resolving the data scarcity issue in DNN training. Inspired by human learning with imagination, we presented a novel guided imagination framework for dataset expansion. Promising results on small-scale natural and medical image datasets have verified its effectiveness. Despite its encouraging results, there is still room to improve. That is, using only the generated samples for model training is still worse than using real samples of equivalent size, suggesting huge potential for algorithmic data generation to improve. We expect that our work can inspire further exploration of dataset expansion so that it can even outperform a human-collected dataset of the same size. Please refer to Appendix D.5 for a more detailed discussion on limitations and broader impact of our work. 2https://mpg-myanmar.com/annotation 3https://cloud.google.com/compute/gpus-pricing#gpu-pricing 10Acknowledgments This work was partially supported by the National Research Foundation Singapore under its AI Singapore Programme (Award Number: [AISG2-TC-2021-002]). References [1] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in Brief, 28:104863, 2020. [2] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023. [3] S Bao, Y Tang, HH Lee, R Gao, S Chiron, I Lyu, LA Coburn, KT Wilson, JT Roland, BA Landman, et al. Random multi-channel image synthesis for multiplexed immunofluores- cence imaging. Proceedings of Machine Learning Research, 156:36–46, 2021. [4] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022. [5] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbal- anced datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing Systems, 2019. [6] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Generalizing dataset distillation via deep generative prior. In Computer Vision and Pattern Recognition, 2023. [7] Akshay Chawla, Hongxu Yin, Pavlo Molchanov, and Jose Alvarez. Data-free knowledge distillation for object detection. In Winter Conference on Applications of Computer Vision, pages 3289–3298, 2021. [8] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Gridmask data augmentation. arXiv preprint arXiv:2001.04086, 2020. [9] Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Interna- tional Conference on Computer Vision, pages 4794–4802, 2019. [10] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. InComputer Vision and Pattern Recognition, pages 3606–3613, 2014. [11] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Computer Vision and Pattern Recognition, pages 113–123, 2019. [12] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, volume 33, 2020. [13] Gang Dai, Yifan Zhang, Qingfeng Wang, Qing Du, Zhuliang Yu, Zhuoman Liu, and Shuang- ping Huang. Disentangling writer and character styles for handwriting generation. InComputer Vision and Pattern Recognition, pages 5977–5986, 2023. [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, pages 248–255, 2009. [15] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pages 8780–8794, 2021. [17] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Computer Vision and Pattern Recognition, pages 12873–12883, 2021. [18] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In Computer Vision and Pattern Recognition Workshop, 2004. 11[19] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, pages 1180–1189, 2015. [20] Yunhao Ge, Harkirat Behl, Jiashu Xu, Suriya Gunasekar, Neel Joshi, Yale Song, Xin Wang, Laurent Itti, and Vibhav Vineet. Neural-sim: Learning to generate training data with nerf. In European Conference on Computer Vision, 2022. [21] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789–1819, 2021. [22] Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, and Sangdoo Yun. Compodiff: Versatile composed image retrieval with latent diffusion. arXiv preprint arXiv:2303.11916, 2023. [23] Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised contrastive learn- ing for pre-trained language model fine-tuning. In International Conference on Learning Representations, 2021. [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Computer Vision and Pattern Recognition, pages 16000–16009, 2022. [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Computer Vision and Pattern Recognition, pages 770–778, 2016. [26] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and XIAOJUAN QI. Is synthetic data from generative models ready for image recognition? In International Conference on Learning Representations, 2023. [27] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. International Conference on Learning Representations, 2019. [28] Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In International Conference on Learning Representations, 2019. [29] Ralf Herbrich. Learning kernel classifiers: theory and algorithms. MIT press, 2001. [30] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840–6851, 2020. [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In International Conference on Computer Vision, pages 1501–1510, 2017. [33] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Computer Vision and Pattern Recognition, pages 1125–1134, 2017. [34] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola. Generative models as a data source for multiview representation learning. In International Conference on Learning Repre- sentations, 2022. [35] Ren Jiawei, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi, et al. Balanced meta-softmax for long-tailed visual recognition. In Advances in Neural Information Processing Systems, 2020. [36] Jakob Nikolas Kather, Johannes Krisam, Pornpimol Charoentong, Tom Luedde, Esther Herpel, Cleo-Aron Weis, Timo Gaiser, Alexander Marx, Nektarios A Valous, Dyke Ferber, et al. Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multicenter study. PLoS Medicine, 16(1):e1002730, 2019. [37] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Computer Vision and Pattern Recognition, pages 2426–2435, 2022. [38] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Founda- tions and Trends in Machine Learning, 12(4):307–392, 2019. [39] Quan Kong, Bin Tong, Martin Klinkigt, Yuki Watanabe, Naoto Akira, and Tomokazu Mu- rakami. Active generative adversarial network for image classification. In AAAI Conference on Artificial Intelligence, pages 4090–4097, 2019. 12[40] Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of fine-grained cars. In Workshop on Fine-Grained Visual Categorization, 2013. [41] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Toronto, ON, Canada, 2009. [42] Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja Fidler, and Antonio Torralba. Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations. In Computer Vision and Pattern Recognition, pages 21330–21340, 2022. [43] Pu Li, Xiangyang Li, and Xiang Long. Fencemask: A data augmentation approach for pre-extracted image features. arXiv preprint arXiv:2006.07877, 2020. [44] Xingjian Li, Haoyi Xiong, et al. Delta: Deep learning transfer using feature map with attention for convolutional networks. In International Conference on Learning Representations, 2019. [45] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. In Advances in Neural Information Processing Systems, volume 32, 2019. [46] Hongbin Lin, Yifan Zhang, Zhen Qiu, Shuaicheng Niu, Chuang Gan, Yanxia Liu, and Mingkui Tan. Prototype-guided continual adaptation for class-incremental unsupervised domain adapta- tion. In European Conference on Computer Vision, pages 351–368, 2022. [47] B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ramamoorthi, and R Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, 2020. [48] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mc- Grew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, 2022. [49] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics & Image Processing, 2008. [50] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In Computer Vision and Pattern Recognition, 2012. [51] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In International Conference on Computer Vision, pages 2085–2094, 2021. [52] Guo-Jun Qi and Jiebo Luo. Small data challenges in big data era: A survey of recent progress on unsupervised and semi-supervised methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. [53] Zhen Qiu, Yifan Zhang, Hongbin Lin, Shuaicheng Niu, Yanxia Liu, Qing Du, and Mingkui Tan. Source-free domain adaptation via avatar prototype generation and adaptation. InInternational Joint Conference on Artificial Intelligence, pages 2921–2927, 2021. [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748–8763. PMLR, 2021. [55] Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding transfer learning for medical imaging. Advances in Neural Information Processing Systems, 32, 2019. [56] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [57] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831, 2021. [58] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. In Neural Information Processing Systems Datasets and Benchmarks Track, 2021. 13[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Computer Vision and Pattern Recognition, pages 10684–10695, 2022. [60] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aber- man. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. [61] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022. [62] Claude Sammut and Geoffrey I Webb. Encyclopedia of machine learning and data mining. Springer Publishing Company, Incorporated, 2017. [63] Veit Sandfort, Ke Yan, Perry J Pickhardt, and Ronald M Summers. Data augmentation using generative adversarial networks (cyclegan) to improve generalizability in ct segmentation tasks. Scientific Reports, 9(1):16884, 2019. [64] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Computer Vision and Pattern Recognition, pages 4510–4520, 2018. [65] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you make it: Learning transferable representations from synthetic imagenet clones. In Computer Vision and Pattern Recognition, 2023. [66] Christoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta, Richard Vencu, Romain Beaumont, Jenia Jitsev, Theo Coombes, and Clayton Mullis. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. In NeurIPS Workshop Datacentric AI, 2021. [67] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations, 2017. [68] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of big data, 6(1):1–48, 2019. [69] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. [70] Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A Alemi, and Andrew G Wilson. Does knowledge distillation really work? In Advances in Neural Information Processing Systems, volume 34, pages 6906–6919, 2021. [71] Changyao Tian, Wenhai Wang, Xizhou Zhu, Xiaogang Wang, Jifeng Dai, and Yu Qiao. Vl-ltr: Learning class-wise visual-linguistic representation for long-tailed visual recognition. In European Conference on Computer Vision, 2022. [72] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic images from text-to-image models make strong visual representation learners. arXiv preprint arXiv:2306.00984, 2023. [73] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Computer Vision and Pattern Recognition, pages 7167–7176, 2017. [74] Andrey Vyshedskiy. Neuroscience of imagination and implications for human evolution. Current Neurobiology, 2019. [75] Can Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text- and-image driven manipulation of neural radiance fields. In Computer Vision and Pattern Recognition, pages 3835–3844, 2022. [76] Pei Wang, Yijun Li, Krishna Kumar Singh, Jingwan Lu, and Nuno Vasconcelos. Imagine: Im- age synthesis by image-guided model inversion. In Computer Vision and Pattern Recognition, pages 3681–3690, 2021. [77] Sihan Wang, Fuping Wu, Lei Li, Zheyao Gao, Byung-Woo Hong, and Xiahai Zhuang. Un- supervised cardiac segmentation utilizing synthesized images from anatomical labels. In International Workshop on Statistical Atlases and Computational Models of the Heart, pages 349–358, 2022. 14[78] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018. [79] Mary Warnock and Jean-Paul Sartre. The psychology of the imagination. Routledge, 2013. [80] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Computer Vision and Pattern Recognition, pages 7959–7971, 2022. [81] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. In International Conference on Computer Vision, 2023. [82] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [83] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In International Conference on Machine Learning, pages 478–487, 2016. [84] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Computer Vision and Pattern Recognition, 2017. [85] Austin Xu, Mariya I Vasileva, and Arjun Seshadri. Handsoff: Labeled dataset generation with no additional human annotations. In NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research, 2022. [86] Haohang Xu, Shuangrui Ding, Xiaopeng Zhang, Hongkai Xiong, and Qi Tian. Masked autoencoders are robust data augmentors. arXiv preprint arXiv:2206.04846, 2022. [87] Xuanang Xu, Fugen Zhou, Bo Liu, Dongshan Fu, and Xiangzhi Bai. Efficient multiple organ localization in ct image using 3d region proposal network. IEEE Transactions on Medical Imaging, 38(8):1885–1898, 2019. [88] Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis. In IEEE International Symposium on Biomedi- cal Imaging, pages 191–195, 2021. [89] Suorong Yang, Weikang Xiao, Mengcheng Zhang, Suhan Guo, Jian Zhao, and Furao Shen. Image data augmentation for deep learning: A survey. arXiv preprint arXiv:2204.08610, 2022. [90] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Computer Vision and Pattern Recognition, pages 8715–8724, 2020. [91] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Computer Vision and Pattern Recognition, pages 4578–4587, 2021. [92] Biting Yu, Luping Zhou, Lei Wang, Yinghuan Shi, Jurgen Fripp, and Pierrick Bourgeat. Ea- gans: edge-aware generative adversarial networks for cross-modality mr image synthesis. IEEE Transactions on Medical Imaging, 38(7):1750–1762, 2019. [93] Longhui Yu, Yifan Zhang, Lanqing Hong, Fei Chen, and Zhenguo Li. Dual-curriculum teacher for domain-inconsistent object detection in autonomous driving. In British Machine Vision Conference, 2022. [94] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In International Conference on Computer Vision, pages 6023–6032, 2019. [95] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference, 2016. [96] Manzil Zaheer, Ankit Singh Rawat, Seungyeon Kim, Chong You, Himanshu Jain, Andreas Veit, Rob Fergus, and Sanjiv Kumar. Teacher guided training: An efficient framework for knowledge transfer. arXiv preprint arXiv:2208.06825, 2022. [97] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018. 15[98] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? In International Conference on Learning Representations, 2021. [99] Yifan Zhang, Bryan Hooi, Dapeng Hu, Jian Liang, and Jiashi Feng. Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning. In Advances in Neural Information Processing Systems, volume 34, pages 29848–29860, 2021. [100] Yifan Zhang, Bryan Hooi, HONG Lanqing, and Jiashi Feng. Self-supervised aggregation of diverse experts for test-agnostic long-tailed recognition. In Advances in Neural Information Processing Systems, 2022. [101] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. Deep long-tailed learning: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [102] Yifan Zhang, Ying Wei, Qingyao Wu, Peilin Zhao, Shuaicheng Niu, Junzhou Huang, and Mingkui Tan. Collaborative unsupervised domain adaptation for medical image diagnosis. IEEE Transactions on Image Processing, 29:7834–7844, 2020. [103] Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. Datasetgan: Efficient labeled data factory with minimal human effort. In Computer Vision and Pattern Recognition, pages 10145–10155, 2021. [104] Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In International Conference on Machine Learning, pages 12674–12685, 2021. [105] Bo Zhao and Hakan Bilen. Synthesizing informative training samples with gan. arXiv preprint arXiv:2204.07513, 2022. [106] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. In International Conference on Learning Representations, 2021. [107] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In AAAI Conference on Artificial Intelligence, 2020. [108] Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng, Dongze Lian, Yifan Zhang, Yang You, and Jiashi Feng. Dataset quantization. In International Conference on Computer Vision, pages 17205–17216, 2023. [109] Yongchao Zhou, Hshmat Sahak, and Jimmy Ba. Training on thin air: Improve image classifi- cation with generated data. arXiv preprint arXiv:2305.15316, 2023. [110] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European Conference on Computer Vision, pages 592–608, 2020. 16A More Related Studies Image synthesis. Over the past decade, image synthesis [2, 13, 65, 72, 109] has been extensively explored, with four main approaches leading the way: generative adversarial networks (GANs) [17, 33], auto-regressive models [38, 57], diffusion models [16, 31], and neural radiance fields [20, 47, 91]. Recently, diffusion techniques, such as DALL-E2 [ 56], Imagen [ 61], and Stable Diffusion [ 59], have demonstrated exceptional capabilities in producing photo-realistic images. In practice, these techniques can serve as prior models in our GIF framework for dataset expansion. Additionally, CLIP [54], thanks to its text-image matching ability, has been used to guide image generation [37, 48, 51, 75]. In these approaches, CLIP matches a generated image with a given text. In contrast, our work uses CLIP to align the latent features of category-agnostic generative models with the label space of the target dataset. This alignment enables GIF to perform guided data expansion, generating informative new samples specific to target classes. Furthermore, model inversion [ 82, 85] is another technique that has been investigated for image generation by inverting a trained classification network [76, 90] or a GAN model [110]. Although we currently apply only two advanced generative models (DALL-E2 and Stable Diffusion) and a reconstruction model (MAE) within the GIF framework in this study, model inversion methods could also be incorporated into our framework for dataset expansion. This opens up exciting avenues for future research. More discussion on data augmentation. Image data augmentation has become a staple in enhanc- ing the generalization of DNNs during model training [68, 89]. Based on technical characteristics, image data augmentation can be categorized into four main types: image manipulation, image erasing, image mix, and auto augmentation. Image manipulation augments data through image transformations like random flipping, rotation, scaling, cropping, sharpening, and translation [ 89]. Image erasing, on the other hand, substitutes pixel values in certain image regions with constant or random values, as seen in Cutout [15], Random Erasing [107], GridMask [8], and Fenchmask [ 43]. Image mix combines two or more images or sub-regions into a single image, as exemplified by Mixup [ 98], CutMix [ 94], and AugMix [ 28]. Lastly, Auto Augmentation utilizes a search algorithm or random selection to determine augmentation operations from a set of random augmentations, such as AutoAugment [11], Fast AutoAugment [45], and RandAugment [12]. While these methods have shown effectiveness in certain applications, they primarily augment data by applying pre-defined transformations on each image. This results in only local variations in pixel values and does not generate images with significantly diversified content. Furthermore, as most methods employ random operations, they cannot ensure that the augmented samples are informative for model training and may even introduce noisy augmented samples. Consequently, the new information brought about is often insufficient for expanding small datasets, leading to low expansion efficiency (cf. Figure 4). In contrast, our proposed GIF framework utilizes powerful generative models (such as DALL-E2 and Stable Diffusion) trained on large-scale image datasets, guiding them to optimize latent features in accordance with our established criteria (i.e., class-maintained information boosting and sample diversity promotion). This results in the creation of images that are both more informative and diversified than those from simple image augmentation, thereby leading to more efficient and effective dataset expansion. We note that the work [86] also explores MAE for image augmentation based on its reconstruction capability. It first masks some sub-regions of images and then feeds the masked images into MAE for reconstruction. The recovered images with slightly different sub-regions are then used as augmented samples. Like other random augmentation methods, this approach only varies pixel values locally and cannot ensure that the reconstructed images are informative and useful. In contrast, our GIF-MAE guides MAE to create informative new samples with diverse styles through our guided latent feature optimization strategy. Therefore, GIF-MAE is capable of generating more useful synthetic samples, effectively expanding the dataset. 17Contrasting with dataset distillation. Dataset distillation, also known as dataset condensation, is a task that seeks to condense a large dataset into a smaller set of synthetic samples that are comparably effective [6, 78, 104, 105, 106, 108]. The goal of this task is to train models to achieve performance comparable to the original dataset while using significantly fewer resources. Such a task is diametrically opposed to our work on dataset expansion, which strives to expand a smaller dataset into a larger, richer, and more informative one. We achieve this by intelligently generating new samples that are both informative and diverse. Hence, dataset distillation focuses on large-data applications, whereas our focus lies on expanding dataset diversity and information richness for more effective deep model training in small-data applications. Contrasting with transfer learning. Numerous studies have focused on model transfer learning techniques using publicly available large datasets like ImageNet [14, 58]. These approaches include model fine-tuning [23, 44, 99], knowledge distillation [21, 30], and domain adaptation [19, 46, 53, 73, 93, 102]. Despite effectiveness in certain applications, these model transfer learning paradigms also suffer from key limitations. For instance, the study [55] found that pre-training and fine-tuning schemes do not significantly enhance model performance when the pre-trained datasets differ substantially from the target datasets, such as when transferring from natural images to medical images. Moreover, model domain adaptation often necessitates that the source dataset and the target dataset share the same or highly similar label spaces, a requirement that is often unmet in small-data application scenarios due to the inaccessibility of a large-scale and labeled source domain with a matching label space. In addition, the work [70] found that knowledge distillation does not necessarily work if the issue of model mismatch exists [9], i.e., large discrepancy between the predictive distributions of the teacher model and the student model. The above limitations of model transfer learning underscore the importance of the dataset expansion paradigm: if a small dataset is successfully expanded, it can be directly used to train various model architectures. We note that some data-free knowledge distillation studies [7, 90, 96] also synthesize images, but their goal is particularly to enable knowledge distillation in the setting without data. In contrast, our task is independent of model knowledge distillation. The expanded datasets are not method-dependent or model-dependent, and, thus, can train various model architectures to perform better than the original small ones. 18B More Preliminary Studies B.1 Sample-wise expansion or sample-agnostic expansion? When we design the selective expansion strategy in Section 3.2, another question appears: should we ensure that each sample is expanded by the same ratio? To determine this, we empirically compare RandAugment expansion with sample-wise selection and sample-agnostic selection according to one expansion criteria, i.e., class-maintained information boosting. Figure 8 shows that sample-wise expansion performs much better than sample-agnostic expansion. To find out the reason for this phenomenon, we visualize how many times a sample is expanded by sample-agnostic expansion. As shown in Figure 9, the expansion numbers of different samples by sample-agnostic expansion present a long-tailed distribution [101], with many image samples not expanded at all. The main reason for this is that, due to the randomness of RandAugment and the differences among images, not all created samples are informative and it is easier for some samples to be augmented more frequently than others. Therefore, given a fixed expansion ratio, the sample-agnostic expansion strategy, as it ignores the differences in images, tends to select more expanded samples for more easily augmented images. This property leads sample-agnostic expansion to waste valuable original samples for expansion (i.e., loss of information) and also incurs a class-imbalance problem, thus resulting in worse performance in Figure 8. In contrast, sample-wise expansion can fully take advantage of all the samples in the target dataset and thus is more effective than sample-agnostic expansion, which should be considered when designing dataset expansion approaches. Random expansion Data-agnostic  guidance Data-wise guidance 45.0 45.5 46.0 46.5 47.0 47.5 48.0Model performance 46.7 45.8 48.0 Figure 8: Comparison of model performance between samples-wise selection and sample- agnostic selection for RandAugment expansion on CIFAR100-Subset. 0 2000 4000 6000 8000 10000 Sample index 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0Expansion number per sample  (sorted) These samples are not expanded Figure 9: Statistics of the expansion numbers of different data in CIFAR100-Subset by sample- agnostic selective expansion with RandAugment, which presents a long-tailed distribution. B.2 Pixel-level noise or channel-level noise? In our preliminary studies exploring the MAE expansion strategy, we initially used pixel-level noise to modify latent features. However, this approach did not perform well. To understand why, we analyze the reconstructed images. An example of this is presented in Figure 10(d). We find that the generated image based on pixel-level noise variation is analogous to adding pixel-level noise to the original images. This may harm the integrity and smoothness of image content, leading the reconstructed images to be noisy and less informative. In comparison, as shown in Figure 10(b), a more robust augmentation method like RandAugment primarily alters the style and geometric positioning of images but only slightly modifies the content semantics. As a result, it better preserves content consistency. This difference inspires us to factorize the influences on images into two dimensions: image styles and image content. In light of the findings in [32], we know that the channel-level latent features encode more subtle style information, whereas the token-level latent features convey more content information. We thus decouple the latent features of MAE into two dimensions (i.e., a token dimension and a channel dimension), and plot the latent feature distribution change between the generated image and the original image in these two dimensions. 19(a) original image  (b) RandAugment  (c) MAE reconstruction  (d) noised-added MAE  (e) our Guided MAE Figure 10: An illustrated visualization of the generated images by (b) RandAugment, (c) MAE recon- struction, (d) random pixel-level variation over latent features, and (e) our guided MAE expansion. We find our guided MAE can generate content-consistent images of diverse styles. Figure 11 shows the visualization of this latent feature distribution change. The added pixel-level noise changes the token-level latent feature distribution more significantly than RandAugment (cf. Figure 11(a)). However, it only slightly changes the channel-level latent feature distribution (cf. Figure 11(b)). This implies that pixel-level noise mainly alters the content of images but slightly changes their styles, whereas RandAugment mainly influences the style of images while maintaining their content semantics. In light of this observation and the effectiveness of RandAugment, we are motivated to disentangle latent features into the two dimensions, and particularly conduct channel- level noise to optimize the latent features in our method. As shown in Figure 11, the newly explored channel-level noise variation varies the channel-level latent feature more significantly than the token- level latent feature. It thus can diversify the style of images while maintaining the integrity of image content. This innovation enables the explored MAE expansion strategy to generate more informative samples compared to pixel-level noise variation (cf. Figure 10(d) vs. Figure 10(e)), leading to more effective dataset expansion, as shown in Figure 6. In light of this finding, we also conduct channel-level noise variation for GIF-SD. 0 50 100 150 200 Token index -0.10 -0.05 0.00 0.05 0.10 0.15 0.20Difference of token-level features (a) Difference of token-level feature distribution Pixel-level random noise RandAugment Our guided channel-level noise 0 200 400 600 800 1000 Channel index -4 -2 0 2 4Difference of channel-level  distribution (b) Difference of channel-level feature distribution Pixel-level random noise RandAugment Our guided channel-level noise Figure 11: Changes of the latent feature distributions along the token dimension and the channel dimension, between the latent feature of the generated image and that of the original image. 20B.3 How to design prompts for Stable Diffusion? Text prompts play an important role in image generation of Stable Diffusion. The key goal of prompts in dataset expansion is to further diversify the generated image without changing its class semantics. We find that domain labels, class labels, and adjective words are necessary to make the prompts semantically effective. The class label is straightforward since we need to ensure the created samples have the correct class labels. Here, we show the influence of different domain labels and adjective words on image generation of Stable Diffusion. Domain labels. We first visualize the influence of different domain prompts on image generation. As shown in Figure 12, domain labels help to generate images with different styles. We note that similar domain prompts, like \"a sketch of\" and \"a pencil sketch of\", tend to generate images with similar styles. Therefore, it is sufficient to choose just one domain label from a set of similar domain prompts, which does not influence the effectiveness of dataset expansion but helps to reduce the redundancy of domain prompts. In light of this preliminary study, we design the domain label set by [\"an image of\", \"a real-world photo of\", \"a cartoon image of\", \"an oil painting of\", \"a sketch of\"]. Input Images generated by pre-trained Stable Diffusion “An image of samoyed” “An oil painting of samoyed” “A cartoon image of samoyed” “A sketch of samoyed” Prompts “A pencil sketch of samoyed” Figure 12: The influence of the domain prompts on image generation of pre-trained Stable Diffusion. The input image is selected from the Pets dataset. Here, the strength hyper-parameter is set to 0.9, and the scale is set to 20. 21Adjective words. We next show the influence of different adjective words on image generation of Stable Diffusion. As shown in Figure 13, different adjectives help diversify the content of the generated images further, although some adjectives may lead to similar effects on image generation. Based on the visualization exploration, we design the adjective set by [\" \", \"colorful\", \"stylized\", \"high-contrast\", \"low-contrast\", \"posterized\", \"solarized\", \"sheared\", \"bright\", \"dark\"]. Input Images generated by pre-trained Stable Diffusion “An image of samoyed” “An image of colorful samoyed” “An image of stylized samoyed” “An image of bright samoyed” Prompts “An image of sheared samoyed” “An image of solarized samoyed” “An image of posterized samoyed” “An image of high- contrast samoyed” Figure 13: The influence of the adjective prompts on image generation of pre-trained Stable Diffusion. The input image is selected from the Pets dataset. Here, the strength hyper-parameter is set to 0.9, and the scale is set to 20. 22B.4 How to set hyper-parameters for Stable Diffusion? B.4.1 Hyper-parameter of strength The hyper-parameter of the nosing strength controls to what degree the initial image is destructed. Setting strength to 1 corresponds to the full destruction of information in the input image while setting strength to 0 corresponds to no destruction of the input image. The higher the strength value is, the more different the generated images would be from the input image. In dataset expansion, the choice of strength depends on the target dataset, but we empirically find that selecting the strength value from [0.5, 0.9] performs better than other values. A too-small value of strength (like 0.1 or 0.3) brings too little new information into the generated images compared to the seed image. At the same time, a too-large value (like 0.99) may degrade the class consistency between the generated images and the seed image when the hyper-parameter of scale is large. Input Images generated by pre-trained Stable Diffusion 0 0.1 0.3 0.5 Strength 0.7 0.9 0.99 Figure 14: The influence of the \"strength\" hyper-parameter on image generation of pre-trained Stable Diffusion. The input image is selected from the Pets dataset. The prompt is \"an image of colorful samoyed\", while the scale is set to 20. 23B.4.2 Hyper-parameter of scale The hyper-parameter of scale controls the importance of the text prompt guidance on image generation of Stable Diffusion. The higher the scale value, the more influence the text prompt has on the generated images. In dataset expansion, the choice of strength depends on the target dataset, but we empirically find that selecting the strength value from [5, 50] performs better than other values. A too-small value of scale (like 1) brings too little new information into the generated images, while a too-large value (like 100) may degrade the class information of the generated images. InputImagesgeneratedbypre-trainedStableDiffusion 1 5 10 20 Scale 50 100 0 Figure 15: The influence of the \"scale\" hyper-parameter on image generation of pre-trained Stable Diffusion. The input image is selected from the Pets dataset. The prompt is \"an image of colorful samoyed\", while the strength is set to 0.9. 24B.5 More discussions on the effectiveness of zero-shot CLIP In GIF, we exploit the zero-shot discriminability of the pre-trained CLIP to guide dataset expansion. In Table 1, we have found that the zero-shot performance of CLIP is not significantly good, particularly on medical image datasets. It is interesting to know whether further fine-tuning CLIP on the target medical dataset can bring further improvement. To determine this, we further compare the results of GIF-MAE with fine-tuned CLIP and with zero-shot CLIP based on OrganSMNIST. To be specific, we add a linear classifier on the top of the CLIP image encoder and fine-tune the CLIP model. As shown in Table 8, GIF-MAE with fine-tuned CLIP performs only comparably to that with zero-shot CLIP, which reflects that the CLIP’s zero-shot classifier is enough to provide sound guidance. The reason is that, although the zero-shot performance is not that good, CLIP still plays an important anchor effect in maintaining the class semantics of the generated samples and helps to bring new information . Let’s first recall the class-maintained informativeness score: Sinf = s′ j + (s log(s) − s′ log(s′)). Specifically, no matter whether CLIP zero-shot classifier is accurate or not, maximizing s′ j essentially uses the prediction of the seed data as an anchor in the CLIP semantic space to regularize the class semantics of the perturbed features. This ensures the created data maintain the correct class, which is highly important for effective dataset expansion. In addition, maximizing the entropy difference, i.e., s log(s) − s′ log(s′), encourages the perturbed feature to have higher entropy regarding CLIP zero-shot prediction. When CLIP zero-shot classifier is accurate, the entropy increment enables the created data to become more difficult to classify regarding CLIP zero-shot discrimination and thus brings more information for classification model training. When CLIP zero-shot classifier is not that accurate, the entropy increment introduces variations into the created data and makes them different from the seed data. Under the condition that the true class is maintained, this optimization is beneficial to boosting the diversity of the expanded dataset, which is helpful for model training. Hence, CLIP’s zero-shot abilities are useful for guided imagination in various image domains. Afterwards, given that zero-shot CLIP can provide valuable guidance despite its limited accuracy, one may wonder whether a random-initialized deep model could serve a similar function. However, as shown in Table 8, using a random-initialized ResNet50 as the guidance model for dataset expansion performs much worse than zero-shot CLIP (i.e., 79.0 vs. 80.6). This could be attributed to the fact that, although the classifiers of both random ResNet50 and zero-shot CLIP struggle with the target medical classes, the CLIP’s pre-training results in a feature space that is more semantically meaningful and representative than a randomly-initialized ResNet50. This distinction allows zero-shot CLIP to better anchor the class semantics of synthetic samples, thereby leading to more effective dataset expansion. These empirical observations further verify the effectiveness of using zero-shot CLIP in guiding dataset expansion. Table 8: Comparison between the model performance by GIF-MAE expansion with zero-shot CLIP guidance and fine-tuned CLIP guidance, as well as random-initialized ResNet-50 guidance, based on the OrganSMNIST medical image dataset. All results are averaged over three runs. OrganSMNIST Guidance model Guidance model accuracy Model accuracy Original dataset - - 76.3 5×-expanded by GIF-MAE Random-initialized ResNet50 7.1 ±0.8 79.0 (+2.7) Fine-tuned CLIP 75.6 ±1.2 80.7 (+4.4) Zero-shot CLIP (ours) 7.7 ±0.0 80.6 (+4.3) 25B.6 Do we need to fine-tune generative models on medical image datasets? Stable Diffusion (SD) and DALL-E2 are trained on large-scale datasets consisting of natural image and text pairs, showing powerful capabilities in natural image generation and variation. However, when we directly apply them to expand medical image datasets, we find the performance improvement is limited, compared to MAE as shown in Table 9. Table 9: Accuracy of ResNet-50 trained on the 5×-expanded medical image datasets by GIF based on SD and DALLE w/o and w/ fine-tuning. All results are averaged over three runs. Dataset PathMNIST BreastMNIST OrganSMNIST Average Original 72.4±0.7 55.8±1.3 76.3±0.4 68.2 GIF-MAE 82.0 ±0.7 73.3±1.3 80.6±0.5 78.6 GIF-DALLE (w/o tuning) 78.4 ±1.0 59.3±2.5 76.4±0.3 71.4 GIF-DALLE (w/ tuning) 84.4 ±0.3 76.6±1.4 80.5±0.2 80.5 GIF-SD (w/o tuning) 80.8 ±1.6 59.4±2.2 79.5±0.4 73.2 GIF-SD (w/ tuning) 86.9 ±0.6 77.4±1.8 80.7±0.2 81.7 To pinpoint the reason, we visualize the generated images by SD on PathMNIST. As shown in Figure 16(top), we find that SD fails to generate photo-realistic medical images, particularly when the hyper-parameter of strength is high. For example, the generated colon pathological images by pre-trained SD look more like a natural sketch and lack medical nidus areas found in the input image. This implies that directly applying SD suffers from significant domain shifts between natural and medical images, preventing the generation of photo-realistic and informative medical samples using its image variation abilities. This issue also happens when applying DALL-E2 for medical dataset expansion. In contrast, MAE is a reconstruction model and does not need to generate new content for the target images, so it has much less negative impact by domain shifts. To address the issue, when applying SD and DALL-E2 to medical domains, we first fine-tune them on target medical datasets, followed by dataset expansion. Specifically, DALL-E2 is fine-tuned based on image reconstruction, while SD is fine-tuned based on Dreambooth [60]. As shown in Figure 16(bottom), the fine-tuned SD is able to generate medical images that are more domain-similar to the input colon pathological image. Thanks to the fine-tuned SD and DALL-E2, GIF is able to bring more significant performance gains over GIF-MAE (cf. Table 9), and thus expands medical image datasets better. Input  GeneratedimagesPre-trainedStableDiffusion Fined-tunedStableDiffusion Figure 16: Visualization of the synthetic medical colon pathological images by Stable Diffusion (SD) with or without fine-tuning. Here, the prompt of SD is \"a colon pathological sketch of colorful debris\", while the strength is set to 0.5. We find that SD suffers from severe domain shifts between natural and medical images and cannot generate photo-realistic and informative medical samples. In contrast, the generated medical images by the fine-tuned SD are more domain-similar to the input colon pathological image. 26B.7 Visualization of created medical images In the main paper, we visualize the created medical samples by GIF-SD. Here, we further visualize the created medical samples by GIF-MAE and discuss them. As shown in Figure 17, RandAugment randomly varies the medical images based on a set of pre-defined transformations. However, due to its randomness, RandAugment may crop the lesion location of medical images and cannot guarantee the created samples to be informative, even leading to noise samples. In contrast, our GIF-MAE can generate content-consistent images with diverse styles, so it can enrich the medical images while maintaining their lesion location unchanged. Therefore, GIF-MAE is able to expand medical image datasets better than RandAugment, leading to higher model performance improvement (cf. Table 1). However, GIF-MAE is unable to generate images with diverse content, which limits its effectiveness. In comparison, SD, after fine-tuning, is able to generate class-maintained samples with more diverse content and styles, and thus achieves better expansion effectiveness (cf. Table 1). To summarize, our methods can expand medical image datasets more effectively than data augmentation. Input RandAugment OurGIF-MAE OurGIF-SD Figure 17: Examples of the created samples for PathMNIST by RandAugment and GIF. 27C Theoretical Analysis In this appendix, we seek to analyze the benefits of our dataset expansion to model generalization performance. Inspired by [ 67], we resort to the concept of δ-cover [62, 29] to analyze how data diversity influences the generalization error bound. Specifically, \"a dataset E is a δ-cover of a dataset S\" means a set of balls with radius δ centered at each sample of the dataset E can cover the entire dataset S. Definition C.1. (δ-cover [62]) Let (M, ρ) be a metric space, let S ⊆ Mand let µ > 0. A set E ⊆ Mis a δ-cover for S, if for every s ∈ S, there is an e ∈ E such that ρ(s, e) ≤ δ. The minimal δ regarding S and E is denoted by δmin. In this work, we follow the assumptions of the work [67] and extend its Theorem 1 to the version of the generalization error bound. Let A be a learning algorithm that outputs a set of parameters, given a training dataset D = {xi, yi}i∈[n] with n i.i.d. samples drawn from the data distribution PZ. Assume that the hypothesis function is λη-Lipschitz continuous, the loss function ℓ(x, y) is λℓ-Lipschitz continuous for all y and bounded by L, and ℓ(xi, yi; A) = 0 for ∀i ∈ [n]. If the training set D is a δ-cover of PZ, with probability at least 1 − γ, the generalization error bound satisfies: |Ex,y∼PZ [ℓ(x, y; A)] − 1 n X i∈[n] ℓ(xi, yi; A)| c ≤ δmin(λℓ + ληLC), (3) where C is a constant, and the symbol c ≤ indicates \"smaller than\" up to an additive constant. According to the property of the δ-cover, we then define the dataset diversity, calledδ-diversity, by the inverse of the minimal δmin: Definition C.2. (δ-diversity) If a dataset E is a δ-cover of the full dataset S, then the δ-diversity of the set E regarding the full set S is δdiv = 1 δmin . The δ-diversity is easy to understand: given a training set D = {xi, yi}i∈[n] that is a δ-cover of the data distribution PZ, if the radius δmin is high, the diversity of this dataset must be low. Then, we have: Theorem C.1. Let A denote a learning algorithm that outputs a set of parameters given a dataset D = {xi, yi}i∈[n] with n i.i.d. samples drawn from distribution PZ. Assume the hypothesis function is λη-Lipschitz continuous, the loss function ℓ(x, y) is λℓ-Lipschitz continuous for all y, and is bounded by L, with ℓ(xi, yi; A) = 0 for all i ∈ [n]. If D constitutes a δ-cover of PZ, then with probability at least 1 − γ, the generalization error bound satisfies: |Ex,y∼PZ [ℓ(x, y; A)] − 1 n X i∈[n] ℓ(xi, yi; A)| c ≤ λℓ + ληLC δdiv , (4) where C is a constant, and the symbol c ≤ indicates \"smaller than\" up to an additive constant. This theorem shows that the generalization error is bounded by the inverse of δ-diversity. That is, the more diverse samples are created by a dataset expansion method, the more improvement of generalization performance would be made in model training. In real small-data applications, the data limitation issue leads the covering radius δ to be very large and thus the δ-diversity is low, which severely affects the generalization performance of the trained model. More critically, simply increasing the data number (e.g., via data repeating) does not help the generalization since it does not increase δ-diversity. Instead of simply increasing the number of samples, our proposed GIF framework adopts two key imagination criteria (i.e., \"class-maintained informativeness boosting\" and \"sample diversity promotion\") to guide advanced generative models (e.g., DALL-E2 and Stable Diffusion) to synthesize informative and diversified new samples. Therefore, the expanded dataset would have higher data diversity than random augmentation, which helps to increase δ-diversity and thus improves model generalization performance. 28D More Method and Implementation Details D.1 Method details of GIF-DALLE Thanks to strong image generation abilities, GIF-DALLE applies DALL-E2 [56] as its prior model which follows the pipeline described in Section 4. Its pseudo-code is provided in Algorithm 1, where the image embedding obtained by fCLIP-I serves as diffusion guidance to help the diffusion decoder to generate new images. GIF-DALLE conducts guided imagination on the CLIP embedding space. We further clarify the implementation of the proposed guidance. Specifically, class-maintained informativeness Sinf encourages the consistency between the predicted classification scores s and s′, and improves the information entropy for the predicted score of the generated sample s′: Sinf = s′ j + (s log(s) − s′ log(s′)), s.t., j = arg max(s). (5) Here, j = argmax(s) is the predicted class label of the original latent feature. Such a criterion helps to keep the class semantics of the optimized feature the same as that of the original one in the CLIP embedding space while encouraging the perturbed feature to have higher information entropy regarding CLIP zero-shot predictions. This enables the generated samples to be more informative for follow-up model training. To promote sample diversity, the diversity Sdiv is computed by the Kullback–Leibler (KL) divergence among all perturbed latent features of a seed sample as follows: Sdiv = DKL(f′∥ ¯f) = σ(f′) log(σ(f′)/σ( ¯f)), (6) where f′ denotes the current perturbed latent feature and ¯f indicates the mean over the K perturbed latent features of this seed sample. In implementing diversity promotion Sdiv, we measure the dissimilarity of two feature vectors by applying the softmax function σ(·) to the latent features, and then measuring the KL divergence between the resulting probability vectors. Algorithm 1: GIF-DALLE Algorithm Input: Original small dataset Do; CLIP image encoder fCLIP-I(·); DALL-E2 diffusion decoder G(·); CLIP zero-shot classifier w(·); Expansion ratio K; Perturbation constraint ε. Initialize: Synthetic data set Ds = ∅; for x ∈ Do do Sinf = 0; f = fCLIP-I(x) ; // latent feature encoding for seed sample s = w(f) ; // CLIP zero-shot prediction for seed sample for i=1,...,K do Initialize noise zi ∼ U(0, 1) and bias bi ∼ N(0, 1); f′ i = Pf,ϵ((1 + zi)f + bi) ; // noise perturbation (Eq.(1)) s′ = w(f′ i) ; // CLIP zero-shot prediction Sinf += s′ j + (s log(s) − s′ log(s′)), s.t. j = arg max(s) ; // class-maintained informativeness (Eq.(5)) end ¯f = mean({f′ i}K i=1); Sdiv = P i{DKL(σ(f′ i)∥σ( ¯f))}K i=1 = P i σ(f′ i) log(σ(f′ i)/σ( ¯f)) ; // diversity (Eq.(6)) {z′ i, b′ i}K i=1 ← −arg maxz,b Sinf + Sdiv ; // guided latent optimization (Eq.(2)) for i=1,...,K do f′′ i = Pf,ϵ((1 + z′ i)f + b′ i) ; // guided noise perturbation (Eq.(1)) x′′ i = G(f′′ i ) ; // sample creation Add x′′ i − → Ds. end end Output: Expanded dataset Do ∪ Ds. More implementation details. In our experiment, DALL-E2 is pre-trained on Laion-400M [66] and then used for dataset expansion. The resolution of the created images by GIF-DALLE is 64×64 for model training without further super-resolution. Only when visualizing the created images, we use super-resolution to up-sample the generated images to 256×256 for clarification. Moreover, we set ε = 0.1 in the guided latent feature optimization. During the diffusion process, we set the guidance scale as 4 and adopt the DDIM sampler [69] for 100-step diffusion. For expanding medical image datasets, it is necessary to fine-tune the prior model for alleviating domain shifts. 29D.2 Method details of GIF-SD GIF-SD applies Stable Diffusion (SD) [59] as its prior model. As its encoder differs from the CLIP image encoder, we slightly modify the pipeline of GIF-SD. Pipeline. As shown in Algorithm 2, GIF-SD first generates a latent feature for the seed image via its image encoder. Following that, GIF-SD conducts prompt-based diffusion for the latent feature, where the generation rule of prompts will be elaborated in Eq. (7). Please note that, with a suitable prompt design, the prompt-based diffusion helps to create more diversified samples. Afterward, GIF-SD conducts channel-wise noise perturbation. Here, the latent feature of SD has three dimensions: two spatial dimensions and one channel dimension. As discussed in our preliminary (cf. Appendix B.2), the channel-level latent feature encodes more subtle style information, whereas the spatial-level latent features encode more content information. In light of the findings in this preliminary study, we particularly conduct channel-level noise to optimize the latent features in GIF-SD for further diversifying the style of the generated images while maintaining the content semantics of the latent features (after prompt-guided diffusion) unchanged. Based on the randomly perturbed feature, GIF- SD generates an intermediate image via its image decoder and applies CLIP to conduct zero-shot prediction for both the seed and the intermediate image to compute the guidance. With the guidance, GIF-SD optimizes the latent features for creating more style-diverse samples. Here, GIF-SD conducts guided imagination on its own latent space. Algorithm 2: GIF-SD Algorithm Input: Original small dataset Do; SD image encoder f(·) and image decoder G(·); SD diffusion module fdiff(·; [prompt]); CLIP image encoder fCLIP-I(·); CLIP zero-shot classifier w(·); Expansion ratio K; Perturbation constraint ε. Initialize: Synthetic data set Ds = ∅; for x ∈ Do do Sinf = 0; f = f(x) ; // latent feature encoding for seed sample Randomly sample a [prompt] ; // Prompt generation (Eq.(7)) f = fdiff(f; [prompt]) ; // SD latent diffusion s = w(fCLIP-I(x)) ; // CLIP zero-shot prediction for seed sample for i=1,...,K do Initialize noise zi ∼ U(0, 1) and bias bi ∼ N(0, 1); f′ i = Pf,ϵ((1 + zi)f + bi) ; // noise perturbation (Eq.(1)) s′ = w(f′ i) ; // CLIP zero-shot prediction Sinf += s′ j + (s log(s) − s′ log(s′)), s.t. j = arg max(s) ; // class-maintained informativeness (Eq.(5)) end ¯f = mean({f′ i}K i=1); Sdiv = P i{DKL(σ(f′ i)∥σ( ¯f))}K i=1 = P i σ(f′ i) log(σ(f′ i)/σ( ¯f)) ; // diversity (Eq.(6)) {z′ i, b′ i}K i=1 ← −arg maxz,b Sinf + Sdiv ; // guided latent optimization (Eq.(2)) for i=1,...,K do f′′ i = Pf,ϵ((1 + z′ i)f + b′ i) ; // guided noise perturbation (Eq.(1)) x′′ i = G(f′′ i ) ; // sample creation Add x′′ i − → Ds. end end Output: Expanded dataset Do ∪ Ds. Rule of prompt design. In our preliminary studies in Appendix B.3, we find that domain labels, class labels, and adjective words are necessary to make the prompts semantically effective. Therefore, we design the prompts using the following rule: Prompt := [domain] of a(n) [adj] [class]. (7) For example, \"an oil painting of a colorful fox\". To enable the prompts to be diversified, inspired by our preliminary studies, we design a set of domain labels and adjective words for natural image datasets as follows. - Domain label set: [\"an image of\", \"a real-world photo of\", \"a cartoon image of\", \"an oil painting of\", \"a sketch of\"] 30- Adjective word set: [\" \", \"colorful\", \"stylized\", \"high-contrast\", \"low-contrast\", \"posterized\", \"solarized\", \"sheared\", \"bright\", \"dark\"] For a seed sample, we randomly sample a domain label and an adjective word from the above sets to construct a prompt. Note that, for medical image datasets, we cancel the domain label set and replace it as the modality of the medical images, e.g., [\"Abdominal CT image of\"], [\"Colon pathological image of\"]. Implementation details. In our experiment, we implement GIF-SD based on CLIP VIT-B/32 and Stable Diffusion v1-4, which are pre-trained on large datasets and then used for dataset expansion. Here, we use the official checkpoints of CLIP VIT-B/32 and Stable Diffusion v1-4. The resolution of the created images by GIF-SD is 512×512 for all datasets. Moreover, for guided latent feature optimization in GIF-SD, we set ε = 0.8 for natural image datasets and ε = 0.1 for medical image datasets. Here, we further adjust ε = 4 for Caltech101 to increase image diversity for better performance. During the diffusion process, we adopt the DDIM sampler [ 69] for 50-step latent diffusion. Moreover, the hyper-parameters of strength and scale in SD depend on datasets, while more analysis is provided in Appendix B.4. Note that, for expanding medical image datasets, it is necessary to fine-tune the prior model for alleviating domain shifts. 31D.3 Method details of GIF-MAE Thanks to strong image reconstruction abilities, our GIF-MAE applies the MAE-trained model [24] as its prior model. As its encoder is different from the CLIP image encoder, we slightly modify the pipeline of GIF-MAE. Pipeline. As shown in Algorithm 3, GIF-MAE first generates a latent feature for the seed image via its encoder, and conducts channel-wise noise perturbation. Here, the latent feature of MAE has two dimensions: spatial dimension and channel dimension. As discussed in our preliminary (cf. Appendix B.2), the channel-level latent feature encodes more subtle style information, whereas the token-level latent feature encodes more content information. Motivated by the findings in this preliminary study, we particularly conduct channel-level noise to optimize the latent features in our GIF-MAE method for maintaining the content semantics of images unchanged. Based on the perturbed feature, GIF-MAE generates an intermediate image via its decoder and applies CLIP to conduct zero-shot prediction for both the seed and the intermediate image to compute the guidance. With the guidance, GIF-MAE optimizes the latent features for creating content-consistent samples of diverse styles. Here, GIF-MAE conducts guided imagination on its own latent space. Algorithm 3: GIF-MAE Algorithm Input: Original small dataset Do; MAE image encoder f(·) and image decoder G(·); CLIP image encoder fCLIP-I(·); CLIP zero-shot classifier w(·); Expansion ratio K; Perturbation constraint ε. Initialize: Synthetic data set Ds = ∅; for x ∈ Do do Sinf = 0; f = f(x) ; // latent feature encoding for seed sample s = w(fCLIP-I(x)) ; // CLIP zero-shot prediction for seed sample for i=1,...,K do Initialize noise zi ∼ U(0, 1) and bias bi ∼ N(0, 1); f′ i = Pf,ϵ((1 + zi)f + bi) ; // channel-level noise perturbation (Eq.(1)) x′ i = G(f′ i) ; // intermediate image generation s′ = w(fCLIP-I(x′ i)); Sinf += s′ j + (s log(s) − s′ log(s′)), s.t. j = arg max(s) ; // class-maintained informativeness (Eq.(5)) end ¯f = mean({f′ i}K i=1); Sdiv = P i{DKL(σ(f′ i)∥σ( ¯f))}K i=1 = P i σ(f′ i) log(σ(f′ i)/σ( ¯f)) ; // diversity (Eq.(6)) {z′ i, b′ i}K i=1 ← −arg maxz,b Sinf + Sdiv ; // guided latent optimization (Eq.(2)) for i=1,...,K do f′′ i = Pf,ϵ((1 + z′ i)f + b′ i) ; // guided channel-wise noise perturbation (Eq.(1)) x′′ i = G(f′′ i ) ; // sample creation Add x′′ i − → Ds. end end Output: Expanded dataset Do ∪ Ds. Implementation details. In our experiment, we implement GIF-MAE based on CLIP VIT-B/32 and MAE VIT-L/16, which are pre-trained on large datasets and then fixed for dataset expansion. Here, we use the official checkpoints of CLIP VIT-B/32 and MAE VIT-L/16. The resolution of the created images by GIF-MAE is 224×224 for all datasets. Moreover, we set ε = 5 for guided latent feature optimization in GIF-MAE. 32D.4 Implementation details of model training We implement GIF in PyTorch based on CLIP VIT-B/32, DALL-E2, MAE VIT-L/16, and Stable Diffusion (SD) V1-4, which are pre-trained on large datasets and then fixed for dataset expansion. We use the official checkpoints of CLIP VIT-B/32, MAE VIT-L/16, and SD v1-4, and use the DALL-E2 pre-trained on Laion-400M [66]. On medical datasets, since DALL-E2 and SD were initially trained on natural images and suffer from domain shifts to medical domains (please see the discussion in Appendix B.6), we fine-tune them on the target dataset before dataset expansion. To fairly evaluate the expansion effectiveness of different methods, we use them to expand the original small datasets by the same ratios, followed by training models from scratch on the expanded dataset with the same number of epochs and the same data pre-processing. In this way, the models are trained with the same number of update steps, so that all expansion methods are fairly compared. The expansion ratio depends on the actual demand of real applications. In the experiment of Table 1, CIFAR100-Subset is expanded by 5×, Pets is expanded by 30×, and all other datasets are expanded by 20×. Moreover, all medical image datasets are expanded by 5×. In addition, all augmentation baselines expand datasets with the same expansion ratio for fair comparisons. After expansion, we train ResNet-50 [25] from scratch for 100 epochs based on the expanded datasets. During model training, we process images via random resize to 224×224 through bicubic sampling, random rotation, and random flips. If not specified, we use the SGD optimizer with a momentum of 0.9. We set the initial learning rate (LR) to 0.01 with cosine LR decay, except the initial LR of CIFAR100-Subset and OrganSMNIST is 0.1. The model performance is averaged over three runs in terms of micro accuracy on natural image datasets and macro accuracy on medical image datasets. D.5 Discussions on limitations and broader impact Limitations. We next discuss the limitations of our method. 1. Performance of generated samples . The expanded samples are still less informative than real samples. For example, a ResNet-50 trained from scratch on our 5x-expanded CIFAR100-Subset achieves an accuracy of 61.1%, which lags behind the 71.0% accuracy on the original CIFAR100. This gap signals the potential for advancing algorithmic dataset expansion. Please see Appendix F.6 for detailed discussions. We expect that this pioneering work can inspire more studies to explore dataset expansion so that it can even outperform a human-collected dataset of the same size. 2. Quality of generated samples. Some samples might have noise, as exemplified in Figure 5b. Despite seeming less realistic, those samples are created following our guidance (e.g., class-maintained informativeness boosting). This ensures the class consistency of these samples, mitigating potential negative effects on model training. Nonetheless, refining the expansion method to address these noisy cases can further enhance the effectiveness of dataset expansion. 3. Scope of work. Our current focus is predominantly on image classification. Exploring the adaptability of our method to other tasks, such as object detection and semantic segmentation, is an intriguing next step. Broader impact. We further summarize our broader impact. Our method can offer a notable reduction in the time and cost associated with manual data collection and annotation for dataset expansion, as discussed in Section 5.1. This can revolutionize how small datasets are expanded, making deep learning more accessible to scenarios with limited data availability (cf. Table 1). 33E Dataset Statistics The statistics of natural image datasets. We evaluate our method on six small-scale natural image datasets, including Caltech-101 [ 18], CIFAR100-Subset [41], Standard Cars [ 40], Oxford 102 Flowers [49], Oxford-IIIT Pets [ 50] and DTD [ 10]. Here, CIFAR100-Subset is an artificial dataset for simulating small-scale datasets by randomly sampling 100 instances per class from the original CIFAR100 dataset, and the total sample number is 10,000. These datasets cover a wide range of classification tasks, including coarse-grained object classification ( i.e., CIFAR100-Subset and Caltech-101), fine-grained object classification (i.e., Cars, Flowers and Pets) and texture classification (i.e., DTD). The data statistics of these natural image datasets are given in Table 10. Note that the higher number of classes or the lower number of average samples per class a dataset has, the more challenging the dataset is. Table 10: Statistics of small-scale natural image datasets. Datasets Tasks #Classes #Samples#Average samples per class Caltech101 Coarse-grained object classification 102 3,060 30 CIFAR100-Subset Coarse-grained object classification 100 10,000 100 Standard Cars Fine-grained object classification 196 8,144 42 Oxford 102 Flowers Fine-grained object classification 102 6,552 64 Oxford-IIIT Pets Fine-grained object classification 37 3,842 104 Describable Textures (DTD) Texture classification 47 3,760 80 The statistics of medical image datasets. To evaluate the effect of dataset expansion on medical images, we conduct experiments on three small-scale medical image datasets. These datasets cover a wide range of medical image modalities, including breast ultrasound ( i.e., BreastMNIST [1]), colon pathology (i.e., PathMNIST [36]), and Abdominal CT (i.e., OrganSMNIST [87]). We provide detailed statistics for these datasets in Table 11. Table 11: Statistics of small-scale medical image datasets. To better simulate the scenario of small medical datasets, we use the validation sets of BreastMNIST and PathMNIST for experiments instead of training sets, whereas OrganSMNIST is based on its training set. Datasets Data Modality #Classes #Samples #Average samples per class BreastMNIST [1, 88] Breast Ultrasound 2 78 39 PathMNIST [36, 88] Colon Pathology 9 10,004 1,112 OrganSMNIST [87, 88] Abdominal CT 11 13,940 1,267 34F More Experimental Results and Discussion F.1 More comparisons to expansion with augmentations F.1.1 More results on expansion efficiency In Figure 4, we have demonstrated the expansion efficiency of our proposed GIF over Cutout, GridMask and RandAugment on the Cars, DTD and Pets datasets. Here, we further report the results on Caltech101, Flowers, and CIFAR100-Subset datasets. As shown in Figure 18, 5× expansion by GIF-SD and GIF-DALLE has already performed comparably to 20× expansion of these augmentation methods, while 10× expansion by GIF-SD and GIF-DALLE outperforms 20× expansion by these data augmentation methods a lot. This result further demonstrates the effectiveness and efficiency of our GIF, and also reflects the importance of automatically creating informative synthetic samples for model training. +5x +10x +20x Expansion size 45 50 55 60 65Model performance 48.9 49.6 51.551.0 54.0 57.8 52.6 55.2 58.4 57.3 61.3 63.0 54.4 59.3 65.1 Caltech101 Cutout GridMask RandAugment SimDEX-MAE (ours) SimDEX-DALLE (ours) GIF-SD (ours) +5x +10x +20x Expansion size 76 78 80 82 84 86 88 90Model performance 77.1 77.7 77.8 78.1 79.3 80.7 81.1 83.4 83.8 81.8 83.8 84.4 82.1 86.4 88.3 Oxford 102 Flowers +1x +2x +5x Expansion size 35 40 45 50 55 60Model performance 38.5 41.1 44.341.3 43.3 46.7 42.7 47.8 52.7 45.6 49.1 54.5 48.4 53.4 61.1CIFAR100-subset Figure 18: Accuracy of ResNet-50 trained from scratch on the expanded datasets with different expansion ratios based on Caltech101, Flowers, and CIFAR100-Subset datasets. F.1.2 Comparison to Mixup and CutMix We further compare our method to more advanced augmentation methods. Specifically, we apply Mixup-based methods, i.e., Mixup [97] and CutMix [94], to expand CIFAR100-Subset by 5 × and use the expanded dataset to train the model from scratch. As shown in Table 12, GIF-SD performs much better than Mixup and CutMix, further demonstrating the superiority of our method over augmentation-based expansion methods. Table 12: Comparison between GIF and Mixup methods for expanding CIFAR100-Subset by 5×. CIFAR100-Subset Accuracy Original dataset 35.0±1.7 Expanded dataset 5×-expandedby Mixup [97] 45.6 ±1.2 5×-expandedby CutMix [94] 50.7 ±0.2 5×-expandedby GIF-SD 61.1±0.8 35F.1.3 Comparison with an advanced generative method We further compare our method with an advanced generative method [26] for dataset expansion. This method includes strategies like language enhancement (LE) [26] and CLIP Filter (CF) [26]. We use this method to expand the CIFAR100-S dataset based on Stable Diffusion (SD). As shown in the following table, SD combined with the method [26] is still noticeably inferior to our GIF-SD for both training from scratch and CLIP tuning. This further demonstrates the superiority of our method. Table 13: Comparison between GIF and the method [26] for expanding CIFAR100-Subset by 5×. CIFAR100-S Training from scratch CLIP fine-tuning Original dataset 35.0 75.2 5x-expanded dataset by SD+method [26] 55.1 (+20.1) 77.0 (+1.8) 5x-expanded dataset by GIF-SD (ours) 61.1 (+26.1) 79.4 (+4.2) F.1.4 Comparison to infinite data augmentation The training time varies based on the specific datasets. However, it is pivotal to note that all dataset expansion methods were compared based on the same expansion ratio, thus ensuring consistent training time/cost and fair comparisons. We acknowledge that training on an expanded dataset will inevitably take longer than training on the original dataset. However, as shown in Table 1 (cf. Section 5.1), the significant improvement in model performance (i.e., by 36.9% on average over six natural image datasets and by 13.5% on average over three medical datasets) makes the increased investment in training time worthwhile. Despite this, one may wonder how the explored dataset expansion would perform compared to training with infinite data augmentation. Therefore, in this appendix, we further evaluate the performance of infinite data augmentation on the CIFAR100-Subset. Specifically, based on RandAugment, we train ResNet-50 using infinite online augmentation for varying numbers of epochs from 100 to 700. As shown in Table 14, using RandAugment to train models for more epochs leads to better performance, but gradually converges (around 51% accuracy at 500 epochs) and keeps fluctuating afterward. By contrast, our proposed method proves advantageous with the same training consumption costs: training the model on the original CIFAR100-S dataset for 5x more epochs performs much worse than the model trained on our 5x-expanded dataset. This comparison further underscores the effectiveness of our method in achieving higher accuracy without inflating training costs. Table 14: Comparison between GIF-SD and infinite data augmentation on CIFAR100-Subset. Here, consumption costs equal data number × training epoch. Methods Epochs Consumption Accuracy Original Standard training 100 1 million 35.0 ±1.7 Training with RandAugment 100 1 million 39.6 ±2.5 Training with RandAugment 200 2 million 46.9 ±0.9 Training with RandAugment 300 3 million 48.1 ±0.6 Training with RandAugment 400 4 million 49.6 ±0.4 Training with RandAugment 500 5 million 51.3 ±0.3 Training with RandAugment 600 6 million 51.1 ±0.3 Training with RandAugment 700 7 million 50.6 ±1.1 Expanded 5×-expanded by GIF-SD 100 6 million 61.1±0.8 36F.1.5 Discussion of picking related samples from larger datasets Picking and labeling data from larger image datasets with CLIP is an interesting idea for dataset expansion. However, such a solution is limited in real applications, since a large-scale related dataset may be unavailable in many image domains ( e.g., medical image domains). Moreover, selecting data from different image domains (e.g., from natural images to medical images) is unhelpful for dataset expansion. Despite the above limitations in real applications, we also evaluate this idea on CIFAR100-Subset and investigate whether it helps dataset expansion when there is a larger dataset of the same image nature, e.g., ImageNet. Here, we use CLIP to select and annotate related images from ImageNet to expand CIFAR100-Subset. Specifically, we scan over all ImageNet images and use CLIP to predict them to the class of CIFAR100-Subset. We select the samples with the highest prediction probability higher than 0.1 and expand each class by 5×. As shown in Table 15, the idea of picking related images from ImageNet makes sense, but performs worse than our proposed method. This result further demonstrates the effectiveness and superiority of our method. In addition, how to better transfer large-scale datasets to expand small datasets is an interesting open question, and we expect to explore it in the future. Table 15: Comparison between GIF and picking related data from ImageNet for expanding CIFAR100- Subset by 5×. CIFAR100-Subset Accuracy Original dataset 35.0±1.7 Expanded dataset 5×-expandedby picking data from ImageNet with CLIP 50.9±1.1 5×-expandedby GIF-SD 61.1±0.8 37F.2 More results of benefits to model generalization In CIFAR100-C [27], there are 15 types of OOD corruption (as shown in Table 16),i.e., Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. Each corruption type has 5 different severity levels: the larger severity level means more severe distribution shifts between CIFAR100 and CIFAR100-C. In Table 2 of the main paper, we have shown the empirical benefit of our method to model out-of-distribution (OOD) generalization based on CIFAR100-C with the severity level 3. Here, we further report its performance on CIFAR100-C with other severity levels. As shown in Table 16, our method is able to achieve consistent performance gains across all severity levels, which further verifies the benefits of GIF to model OOD generalization. Table 16: Corruption Accuracy of ResNet-50 trained from scratch on CIFAR100-S and our 5 × expanded dataset, under 15 types of corruption in CIFAR100-C with various severity levels. (a) CIFAR100-C with the severity level 1 Noise Blur Weather Digital Dataset Gauss. Shot Impul.Defoc. Glass Motion ZoomSnow Frost Fog Brit.Contr. Elastic Pixel JPEGAverage Original 25.6 29.3 25.0 34.2 32.2 31.7 30.9 32.3 28.3 31.8 33.729.2 31.7 34.1 30.9 30.75×-expandedby GIF-SD50.3 54.6 50.8 59.2 29.4 53.7 51.9 53.1 54.0 58.7 59.557.1 52.5 57.9 54.753.2 (+22.5)20×-expandedby GIF-SD55.0 60.5 54.8 66.1 30.2 56.0 58.0 61.1 62.2 65.1 66.264.3 59.2 63.8 60.858.9(+27.2) (b) CIFAR100-C with the severity level 2 Noise Blur Weather Digital Dataset Gauss. Shot Impul.Defoc. Glass Motion ZoomSnow Frost Fog Brit.Contr. Elastic Pixel JPEGAverage Original 18.6 24.4 17.4 32.5 31.9 28.3 29.8 28.4 22.9 23.6 31.116.3 30.8 33.7 29.2 26.65×-expandedby GIF-SD39.5 48.8 41.7 56.3 29.6 46.4 49.7 45.2 46.4 52.8 57.645.5 52.1 54.2 51.147.8 (+21.2)20×-expandedby GIF-SD42.7 53.7 43.9 63.1 31.2 51.8 56.1 52.0 54.9 60.4 65.254.3 59.2 60.0 55.652.3(+25.7) (c) CIFAR100-C with the severity level 3 Noise Blur Weather Digital Dataset Gauss. Shot Impul.Defoc. Glass Motion ZoomSnow Frost Fog Brit.Contr. Elastic Pixel JPEGAverage Original 12.8 17.0 12.5 30.5 31.7 25.2 28.6 26.5 19.0 18.6 28.311.5 29.5 33.6 28.8 23.65×-expandedby GIF-SD29.7 36.4 32.7 51.9 32.4 39.2 46.0 45.3 38.1 47.1 55.737.3 48.6 53.2 49.443.3 (+19.3)20×-expandedby GIF-SD31.8 39.2 34.7 58.4 33.4 43.1 51.9 51.7 47.4 55.0 63.346.5 54.9 58.0 53.648.2(+24.6) (d) CIFAR100-C with the severity level 4 Noise Blur Weather Digital Dataset Gauss. Shot Impul.Defoc. Glass Motion ZoomSnow Frost Fog Brit.Contr. Elastic Pixel JPEGAverage Original 10.8 14.3 7.7 28.5 29.3 25.2 27.8 23.3 19.5 14.1 24.97.4 29.0 33.0 28.1 21.55×-expandedby GIF-SD25.3 31.2 18.0 45.1 21.4 39.6 42.5 41.7 37.7 40.2 52.126.1 44.2 47.8 48.237.4 (+15.9)20×-expandedby GIF-SD27.4 33.7 20.2 50.7 21.7 43.9 47.8 48.8 46.7 47.6 60.735.3 47.9 49.3 51.242.2(+20.7) (e) CIFAR100-C with the severity level 5 Noise Blur Weather Digital Dataset Gauss. Shot Impul.Defoc. Glass Motion ZoomSnow Frost Fog Brit.Contr. Elastic Pixel JPEGAverage Original 9.4 10.7 5.5 24.9 28.9 22.3 25.9 19.4 16.6 8.2 18.32.7 29.0 31.8 27.3 18.75×-expandedby GIF-SD21.4 23.8 10.8 31.8 22.8 33.1 37.6 38.1 31.1 24.7 43.78.6 38.6 36.045.629.8 (+11.1)20×-expandedby GIF-SD22.9 25.5 11.1 33.5 24.1 36.2 41.8 46.4 38.4 32.1 53.513.9 40.4 32.048.833.4(+14.7) 38F.3 More results of applicability to various model architectures In Table 3, we have demonstrated the generalizability of our expanded Cars dataset to various model architectures. Here, we further apply the expanded Caltech101, Flowers, DTD, CIFAR100- S, and Pets datasets (5× expansion ratio) by GIF-SD and GIF-DALLE to train ResNeXt-50 [ 84], WideResNet-50 [95] and MobileNet V2 [64] from scratch. Table 17 shows that our expanded datasets bring consistent performance gains for all the architectures on all datasets. This further affirms the versatility of our expanded datasets, which, once expanded, are readily suited for training various model architectures. Table 17: Model performance of various model architectures trained on 5× expanded natural image datasets by GIF. Dataset Caltech101 [18] ResNet-50 ResNeXt-50 WideResNet-50 MobilteNet-v2 Avg. Original dataset 26.3±1.0 32.6±0.5 34.7±0.8 33.8±1.1 31.9 5×-expandedby GIF-DALLE 57.3±0.4 55.2±0.1 61.8±0.5 59.4±0.7 58.4(+26.5) 5×-expandedby GIF-SD 54.4 ±0.7 52.8±1.1 60.7±0.3 55.6±0.5 55.9 (+24.0) Dataset Cars [40] ResNet-50 ResNeXt-50 WideResNet-50 MobilteNet-v2 Avg. Original dataset 19.8±0.9 18.4±0.5 32.0±0.8 26.2±4.2 24.1 5×-expandedby GIF-DALLE 53.1±0.2 43.7±0.2 60.0±0.6 47.8±0.6 51.2 (+27.1) 5×-expandedby GIF-SD 60.6±1.9 64.1±1.3 75.1±0.4 60.2±1.6 65.0(+40.9) Dataset Flowers [49] ResNet-50 ResNeXt-50 WideResNet-50 MobilteNet-v2 Avg. Original dataset 74.1±0.1 75.8±1.2 79.3±1.6 85.5±1.0 78.7 5×-expandedby GIF-DALLE 82.8±0.5 81.6±0.4 84.6±0.2 88.8±0.5 84.4 (+5.7) 5×-expandedby GIF-SD 82.1 ±1.7 82.0±1.2 85.0±0.6 89.0±0.1 84.5(+5.8) Dataset DTD [10] ResNet-50 ResNeXt-50 WideResNet-50 MobilteNet-v2 Avg. Original dataset 23.1±0.2 25.4±0.6 26.1±0.6 28.1±0.9 25.7 5×-expandedby GIF-DALLE 31.2±0.9 30.6±0.1 35.3±0.9 37.4±0.8 33.6 (+7.9) 5×-expandedby GIF-SD 33.9±0.9 33.3±1.6 40.6±1.7 40.8±1.1 37.2(+11.5) Dataset CIFAR100-S [41] ResNet-50 ResNeXt-50 WideResNet-50 MobilteNet-v2 Avg. Original dataset 35.0±3.2 36.3±2.1 42.0±0.3 50.9±0.2 41.1 5×-expandedby GIF-DALLE 54.5±1.1 52.4±0.7 55.3±0.3 56.2±0.2 54.6 (+13.5) 5×-expandedby GIF-SD 61.1±0.8 59.0±0.7 64.4±0.2 62.4±0.1 61.4(+20.3) Dataset Pets [50] ResNet-50 ResNeXt-50 WideResNet-50 MobilteNet-v2 Avg. Original dataset 6.8±1.8 19.0±1.6 22.1±0.5 37.5±0.4 21.4 5×-expandedby GIF-DALLE 46.2±0.1 52.3±1.5 66.2±0.1 60.3±0.3 56.3 (+34.9) 5×-expandedby GIF-SD 65.8±0.6 56.5±0.6 70.9±0.4 60.6±0.5 63.5(+42.1) 39F.4 More discussions on CLIP In the following subsections, we provide more discussions on the comparisons with CLIP. F.4.1 Why not directly transfer CLIP models to target datasets? In our proposed GIF framework, we leverage the pre-trained CLIP model to guide dataset expansion. An inevitable question might be: why not directly use or transfer the CLIP model to the target dataset, especially given its proven effectiveness on many natural image datasets? Before delving into that, it is important to note that we aim to tackle small-data scenarios, where only a limited-size dataset is available and there are no large-scale external datasets with a similar nature to the target dataset. Consequently, training a new CLIP model on the target dataset (e.g., in the medical image domains) is not feasible. Therefore, we rely on publicly available CLIP models for dataset expansion. Compared to directly using or transferring CLIP models, our dataset expansion introduces a necessary new paradigm for two primary reasons as follows. First, our GIF method has better applicability to scenarios across various image domains. While CLIP demonstrates good transfer performance on certain natural image datasets, it struggles to achieve this performance on other domains, such as medical image datasets. To illustrate this, we test the linear- probing and fine-tuning performance of the CLIP-trained ResNet-50 model on three medical datasets. As shown in Table 18, directly employing or transferring the CLIP model yielded unsatisfactory results or only marginally improved performance—significantly underperforming compared to our dataset expansion approach. The limited transfer performance is attributed to the fact that, when the pre-trained datasets are highly different from the target datasets, the pre-training weights do not significantly bolster performance compared to training from scratch [55]. Such an issue cannot be resolved by conducting CLIP pre-training on these domains, since there is no large-scale dataset of similar data nature to the target dataset in real scenarios. In contrast, our GIF framework is capable of generating images of similar nature as the target data for dataset expansion, enhancing its applicability to real-world scenarios across diverse image domains. Second, our dataset expansion can provide expanded datasets suitable for training various network architectures. In certain practical scenarios, such as mobile terminals, the permissible model size is severely limited due to hardware constraints. Nonetheless, the publicly available CLIP checkpoints are restricted to ResNet-50, ViT-B/32, or even larger models, which may not be viable in these constrained settings. In contrast, the expanded dataset by our method can be readily employed to train a various range of model architectures (cf. Table 3), making it more applicable to scenarios with hardware limitations. One might suggest using CLIP in these situations by conducting knowledge distillation from large CLIP models to facilitate the training of smaller model architectures. However, as indicated in Table 1 and Table 18, although knowledge distillation of CLIP does enhance model performance on most datasets, the gains are limited. This arises from two key limitations of CLIP knowledge distillation. First, distillation can only yield marginal improvements when the performance of CLIP on the target dataset ( e.g., medical image domains) is not good. Second, distillation tends to be ineffective when there is a mismatch between the architectures of student and teacher models [9, 70]. This comparison further underscores the advantages of our method for training various network architectures, while the CLIP model architectures are fixed and not editable. Table 18: Comparison between our methods and directly fine-tuning CLIP models on three medical image datasets. All results are averaged over three runs. Dataset PathMNIST BreastMNIST OrganSMNIST Originaldataset 72.4 ±0.7 55.8±1.3 76.3±0.4 CLIP linear probing 74.3 ±0.1 60.0±2.9 64.9±0.2 CLIP fine-tuning 78.4 ±0.9 67.2±2.4 78.9±0.1 CLIP knowledge distillation 77.3 ±1.7 60.2±1.3 77.4±0.8 5×-expanded by GIF-SD 86.9±0.3 77.4±1.8 80.7±0.2 40F.4.2 Discussion on when to use GIF over zero-shot CLIP models In Table 1, it is noted that while zero-shot CLIP performs well on datasets like Caltech 101 and Pets, it struggles with medical image datasets. This poses the question: when should we prefer GIF over pre-trained CLIP models? Although zero-shot CLIP outperforms our GIF-SD on the Caltech 101 and Pets datasets, our method demonstrates superior overall performance across six natural image datasets, as well as medical image datasets (see Table 1). Thus, we recommend using our method as the primary option. Meanwhile, if the target dataset has a high distributional similarity with the CLIP training dataset, it may also be beneficial to consider CLIP as an alternative and see whether it can achieve better performance. Nevertheless, it is important to note that, as discussed in Section 5.1 and Appendix F.4.1, CLIP is less effective in some specific application scenarios. For instance, its performance on non- natural image domains like medical images is limited (as shown in Table 4). Additionally, publicly available CLIP checkpoints are restricted to larger models like ResNet-50 and ViT-B/32, making them unsuitable for scenarios with hardware constraints (e.g., mobile terminals) where smaller model sizes are necessary. In these scenarios, our proposed method exhibits promising performance (as shown in Tables 3 and 4), offering a more versatile solution. 41F.5 More ablation studies F.5.1 The effectiveness of guidance in GIF-DALLE GIF optimizes data latent features for informative sample creation by maximizing the designed objective functions of guidance (i.e., class-maintained informativeness Sinf and sample diversity Sdiv), which are essential for effective dataset expansion. With these essential guidance criteria, as shown in Table 19, our guided expansion framework obtains consistent performance gains compared to unguided expansion with SD, DALL-E2, or MAE, respectively. This verifies the effectiveness of our criteria in optimizing the informativeness and diversity of the created samples. Table 19: Accuracy of ResNet-50 trained from scratch on small datasets and their expanded datasets by various methods. Here, CIFAR100-Subset is expanded by 5×, Pets is expanded by 30×, and all other natural image datasets are expanded by 20×. All medical image datasets are expanded by 5×. Moreover, MAE, DALL-E2 and SD (Stable Diffusion) are the baselines of directly using them to expand datasets without our GIF. All results are averaged over three runs. Dataset Natural image datasets Medical image datasets Caltech101 Cars Flowers DTD CIFAR100-S Pets Average PathMNIST BreastMNIST OrganSMNIST Average Original 26.3 19.8 74.1 23.1 35.0 6.8 30.9 72.4 55.8 76.3 68.2 Expandedby MAE 50.6 25.9 76.3 27.6 44.3 39.9 44.1 (+13.2) 81.7 63.4 78.6 74.6 (+6.4)Expandedby GIF-MAE (ours) 58.4 44.5 84.4 34.2 52.7 52.4 54.4 (+23.5) 82.0 73.3 80.6 78.6 (+10.4) Expandedby DALL-E2 61.3 48.3 84.1 34.5 52.1 61.7 57.0 (+26.1) 82.8 70.8 79.3 77.6 (+9.4)Expandedby GIF-DALLE (ours) 63.0 53.1 88.2 39.5 54.5 66.4 60.8 (+29.9) 84.4 76.6 80.5 80.5 (+12.3) Expandedby SD 51.1 51.7 78.8 33.2 52.9 57.9 54.3 (+23.4) 85.1 73.8 78.9 79.3 (+11.1)Expandedby GIF-SD (ours)65.1 75.7 88.3 43.4 61.1 73.4 67.8 (+36.9) 86.9 77.4 80.7 81.7 (+13.5) In this appendix, we further explore the individual influence of these criteria on GIF-DALLE. Specifically, as mentioned in Appendix D.1, GIF-DALLE conducts guided imagination on the CLIP embedding space, which directly determines the content of the created samples. With the aforementioned essential criteria, as shown in Figure 2b, our GIF-DALLE is able to create motorbike images with more diverse angles of view and even a new driver compared to unguided DALLE expansion. Here, we further dig into how different criteria influence the expansion effectiveness of GIF-DALLE. As shown in Table 20, boosting the class-maintained informativeness Sinf is the foundation of effective expansion, since it makes sure that the created samples have correct labels and bring new information. Without it, only Sdiv cannot guarantee the created samples to be meaningful, although the sample diversity is improved, even leading to worse performance. In contrast, with Sinf , diversity promotion Sdiv can further bring more diverse information to boost data informativeness and thus achieve better performance (cf. Table 20). Note that contrastive entropy increment s log(s) − s′ log(s′) in class-maintained informativeness plays different roles from diversity promotion Sdiv. Contrastive entropy increment promotes the informativeness of each generated image by increasing the prediction difficulty over the corresponding seed image, but this guidance cannot diversify different latent features obtained from the same image. By contrast, the guidance of diversity promotion encourages the diversity of various latent features of the same seed image, but it cannot increase the informativeness of generated samples regarding prediction difficulty. Therefore, using the two guidance together leads the generated images to be more informative and diversified, thus bringing higher performance improvement (cf. Table 20). As a result, as shown in Table 19, with these two essential criteria as guidance, the model accuracy by GIF-DALLE is 3.3% accuracy higher than unguided data generation with DALL-E2. Table 20: Ablation of guidance in GIF-DALLE for expanding CIFAR100-Subset by 5×. Method Sinf Sdiv CIFAR100-Subset GIF-DALLE 52.1±0.9 53.1±0.3 51.8±1.3 54.5±1.1 42F.5.2 The effectiveness of guidance in GIF-SD We next analyze GIF-SD. As mentioned in Appendix D.2, we conductchannel-wise noise perturbation for latent optimization in GIF-SD. As analyzed in Appendix B.2, the channel-level latent feature encodes more subtle style information, and conducting channel-level noise perturbation diversifies the style of images while maintaining its content integrity. Therefore, our guided optimization particularly diversifies the style of the created images, without changing the content semantics of the latent features after diffusion (cf. Figure 19). Moreover, the prompt-guided diffusion with our explored prompts helps to enrich image styles further (e.g., cartoon or oil painting). Hence, combining both of them enables GIF-SD to create new samples with much higher diversity (cf. Figure 19). Input Generated images Stable Diffusion + our guided optimization + our designed prompts + our guided optimization + designed prompts Methods Figure 19: Visualization of the generated images by SD with our explored guided optimization and designed prompts. We then investigate the individual influence of our guidance criteria on GIF-SD on the basis of our prompt-guided diffusion. As shown in Table 21, both the class-maintained informativeness guidance Sinf and the diversity promotion guidance Sdiv contribute to model performance. One interesting thing is that, unlike GIF-DALLE that does not work without Sinf , GIF-SD can work well using only the diversity promotion guidance Sdiv. The key reason is that GIF-SD conducts channel-level noise perturbation over latent features and particularly diversifies the style of the created images without changing the content semantics of the latent features after diffusion. Therefore, the class semantics can be maintained well when only promoting sample diversity. Moreover, combining both guidance criteria enables GIF-SD to achieve the best expansion effectiveness (cf. Table 21), leading to promising performance gains (i.e., 13.5% accuracy improvement on average over six natural image datasets) compared to unguided expansion with SD (cf. Table 19). Table 21: Ablation of guidance and prompts in GIF-SD for expanding CIFAR100-Subset by 5×. Method Designed prompts Sinf Sdiv CIFAR100-Subset GIF-SD 52.9±0.8 56.2±1.0 59.6±1.1 59.4±1.2 61.1±0.8 43F.5.3 Discussions on the constraint of the perturbed feature in GIF The hyper-parameter ε is used to ensure that the perturbed feature does not deviate from the input feature significantly, and its value depends on the prior model and target dataset. As described in Appendix D, for GIF-SD, we set ε = 0.8 for most natural image datasets and further adjust ε = 4 for Caltech101 to increase its dataset diversity for better performance. Once determined for a given prior model and target dataset, ε remains fixed for various expansion ratios. As shown in the following table, there is no need to increase when the expansion ratio becomes larger. Table 22: Ablation of hyper-parameter ε on Caltech101 for GIF-SD. ε on Caltech101 2 4 8 5×-expanded by GIF-SD 53.0 54.4 53.6 10×-expanded by GIF-SD 59.2 59.3 58.2 44F.6 Discussion of training models with only expanded images It is interesting to know how the model performs when trained with only the created images by our method. To this end, we train ResNet-50 from scratch using only images generated by GIF-DALLE on the CIFAR100-Subset and compare the result with a model trained on the real images of the CIFAR100-Subset. We report the results regarding 1× expansion in Table 23. We find that the model trained with only 1× synthetic images performs worse than the model trained with the original dataset, indicating that the quality of synthetic data still lags behind that of real images. Please note that this does not degrade our contribution, since our work aims to expand small datasets rather than replace them entirely. Moreover, mixing the original images with the created images to the same size as the original dataset can lead to better performance than using only the original dataset. This suggests that the created images are not a simple repetition of the original dataset but offer new information that is useful for model training. Lastly, the model trained on the complete 1x-expanded dataset significantly outperforms the models trained either only on the original dataset or solely on the generated images, underscoring the potential of synthetic images in expanding small-scale datasets for model training. Table 23: Performance of the model trained with only the expanded data of the 5 ×-expanded CIFAR100-Subset dataset by GIF-DALLE. CIFAR100-Subset Data amount Accuracy Training with real images in original dataset 10,000 35.0 ±1.7 Training with only the 1×-created databy GIF-DALLE 10,000 21.0 ±0.7 Training with mixing original data and 1×-created databy GIF-DALLE 10,000 37.2 ±0.8 Training with 1×-expanded datasetby GIF-DALLE 20,000 45.6 ±1.1 We next report the results regarding 5× expansion in Table 24. The model trained with 5× synthetic images has already performed comparably to the model trained with real images. This result further verifies the effectiveness of our explored dataset expansion method. Moreover, the model trained with the full 5×-expanded dataset performs much better than that trained with only the original dataset or with only the generated images. This further shows that using synthetic images for model training is a promising direction. We expect that our innovative work on dataset expansion can inspire more studies to explore this direction in the future. Table 24: Performance of the model trained with only the expanded data of the 5 ×-expanded CIFAR100-Subset dataset by GIF-DALLE. CIFAR100-Subset Data amount Accuracy Training with real images in original dataset 10,000 35.0 ±1.7 Training with only the 5×-created databy GIF-DALLE 50,000 35.2 ±1.3 Training with 5×-expanded datasetby GIF-DALLE 60,000 54.5 ±1.1 45F.7 Effectiveness on long-tailed classification dataset In previous experiments, we have demonstrated the effectiveness of our proposed method on relatively balanced small-scale datasets. However, real-world classification datasets are usually class imbalanced and even follow a long-tailed class distribution. Therefore, we further apply GIF-SD to expand a long-tailed dataset, i.e., CIFAR100-LT [5] (with the imbalance ratio of 100), to see whether it is also beneficial to long-tailed learning. Here, we train ResNet-50 from scratch with the cross-entropy loss or the balanced softmax loss [35] for 200 epochs, where Balanced Softmax [35, 100] is a class re-balancing loss designed for long-tailed learning. As shown in Table 25, compared to training with cross-entropy directly on the original CIFAR100-LT dataset, 20× expansion by our GIF-SD leads to a 13.5% model accuracy gain. This demonstrates the effectiveness of our proposed method in long-tailed learning. More encouragingly, our GIF expansion boosts the performance of few-shot classes more than many-shot classes, which means that GIF helps to address the issue of class imbalance. Besides the cross-entropy loss, our dataset expansion is also beneficial to model training with long- tailed losses, such as Balanced Softmax. As shown in Table 25, 20× expansion by GIF-SD boosts the accuracy of the Balanced Softmax trained model by 14.8%, and significantly improves its tail-class performance by 26.8%. These results further demonstrate the applicability of our GIF to long-tailed learning applications. We expect that this work can inspire more long-tailed learning studies to explore dataset expansion since information lacking is an important challenge in long-tailed learning [101]. Table 25: Effectiveness of GIF-SD for expanding CIFAR100-LT (imbalance ratio 100) by 10×, where all models are trained for 200 epochs. Here, Balanced Softmax [ 35, 100] is a class re-balancing losses designed for long-tailed learning. CIFAR100-LT Training losses Many-shot classes Medium-shot classes Few-shot classes Overall Original Cross-entropy 70.5 41.1 8.1 41.4 20×-expandedby GIF-SD Cross-entropy 79.5 (+9.0) 54.9 (+13.8) 26.4 (+18.3) 54.9 (+13.5) Original Balanced Softmax 67.9 45.8 17.7 45.1 20×-expandedby GIF-SD Balanced Softmax 73.7 (+5.8) 59.2 (+13.4) 44.5 (+26.8) 59.9 (+14.8) F.8 Effectiveness on larger-scale dataset In previous experiments, we have demonstrated the effectiveness of our proposed method on small- scale natural and medical image datasets. In addition to that, one may also wonder whether our method can be applied to larger-scale datasets. Although expanding larger-scale datasets is not the goal of this paper, we also explore our method to expand the full CIFAR100 by 5× for model training. As shown in Table 26, compared to direct training on the original CIFAR100 dataset, our GIF-SD leads to a 9.4% accuracy gain and GIF-DALLE leads to an 8.7% accuracy gain. Such encouraging results verify the effectiveness of our methods on larger-scale datasets. Table 26: Effectiveness of GIF for expanding the full CIFAR100. Dataset CIFAR100 Original 70.9±0.6 Expanded 5×-expanded by GIF-DALLE 79.6 ±0.3 5×-expanded by GIF-SD 80.3±0.3 46F.9 Safety check Ethical considerations, especially in AI research and data generation, are indeed paramount. Our approach is constructed with care to avoid negative implications, as evidenced in the following points: • Controlled generation: In our approach, the generation of synthetic data is driven by our expansion guidances, which ensure that new data is derived directly and meaningfully from the original dataset. This controlled mechanism minimizes the risks of creating unrelated or potentially harmful images. • No personal or sensitive data: It is also worth noting that our method primarily focuses on publicly available datasets like CIFAR, Stanford Cars, and similar, which do not con- tain personal or sensitive information. As such, the risks related to privacy breaches or misrepresentations are substantially diminished. Following this, we further employ the Google Cloud Vision API 4 to perform a safety check on the 50,000 images generated during 5x-expansion of CIFAR100-S by GIF-SD. The Google Cloud Vision API is a tool from Google that uses deep learning to analyze and categorize content in images, commonly used for safety checks. It evaluates the likelihood of the image containing adult themes such as nudity or sexual activities, alterations made for humor or offensiveness ( spoof), medical relevance, violent content, and racy elements which could include suggestive clothing or poses. This assessment aids in ensuring that images adhere to content standards and are appropriate for their target audiences. As evidenced by Table 27, the synthetic images by our method are safe and harmless. To be specific, the majority of our generated images are categorized as either \"Very unlikely\" or \"Unlikely\" across all five metrics. Moreover, for categories like \"Adult\" and \"Medical\", the likelihood is almost negligible. Moreover, the visualized images in Appendix G also highlight the benign nature of the images produced by our method. Table 27: Safety check of the generated images of CIFAR100-S by our GIF-SD, in terms of different metrics of Google Cloud Vision API. Metrics Very unlikely Unlikely Neutral Likely Very likely Adult 96% 4% 0% 0% 0% Spoof 82% 15% 3% 0% 0% Medical 86% 14% 0% 0% 0% Violence 69% 31% 0% 0% 0% Racy 66% 25% 9% 0% 0% 4https://cloud.google.com/vision/docs/detecting-safe-search 47G More Visualization Results This appendix provides more visualized results for the created samples by our methods on various natural image datasets. Specifically, we report the synthetic images by GIF-SD on Caltech101 in Figure 20 , those by GIF-DALLE in Figure 21 and those by GIF-MAE in Figure 22. The visualized results show that our GIF-SD and GIF-DALLE can create semantic-consistent yet content-diversified images well, while GIF-MAE can generate content-consistent yet highly style-diversified images. The visualization of GIF-SD and GIF-DALLE on other natural image datasets are shown in Figures 23-32. G.1 Visualization of the expanded images on Caltech101 G.1.1 Visualization of the expanded images by GIF-SD on Caltech101 Input  Our GIF-SD expansion Figure 20: Visualization of the created samples on Caltech101 by GIF-SD. 48G.1.2 Visualization of the expanded images by GIF-DALLE on Caltech101 Input  OurGIF-DALLEexpansion Figure 21: Visualization of the created samples on Caltech101 by GIF-DALLE. 49G.1.3 Visualization of the expanded images by GIF-MAE on Caltech101 Input  Our GIF-MAE expansion Figure 22: Visualization of the created samples on Caltech101 by GIF-MAE. 50G.2 Visualization of the expanded images on Cars G.2.1 Visualization of the expanded images by GIF-SD on Cars Input Our GIF-SD expansion Figure 23: More visualization of the synthetic samples on Cars by GIF-SD. 51G.2.2 Visualization of the expanded images by GIF-DALLE on Cars Input OurGIF-DALLEexpansion Figure 24: More visualization of the synthetic samples on Cars by GIF-DALLE. 52G.3 Visualization of the expanded images on Flowers G.3.1 Visualization of the expanded images by GIF-SD on Flowers Input  Our GIF-SD expansion Figure 25: More visualization of the synthetic samples on Flowers by GIF-SD. 53G.3.2 Visualization of the expanded images by GIF-DALLE on Flowers Input  OurGIF-DALLEexpansion Figure 26: More visualization of the synthetic samples on Flowers by GIF-DALLE. 54G.4 Visualization of the expanded images on Pets G.4.1 Visualization of the expanded images by GIF-SD on Pets Input  Our GIF-SD expansion Figure 27: More visualization of the synthetic samples on Pets by GIF-SD. 55G.4.2 Visualization of the expanded images by GIF-DALLE on Pets Input  OurGIF-DALLEexpansion Figure 28: More visualization of the synthetic samples on Pets by GIF-DALLE. 56G.5 Visualization of the expanded images on CIFAR100-Subset G.5.1 Visualization of the expanded images by GIF-SD on CIFAR100-Subset Input Our GIF-SD expansion Figure 29: More visualization of the synthetic samples on CIFAR100-Subset by GIF-SD. 57G.5.2 Visualization of the expanded images by GIF-DALLE on CIFAR100-Subset Input OurGIF-DALLEexpansion Figure 30: More visualization of the synthetic samples on CIFAR100-Subset by GIF-DALLE. Note that the resolution of the input CIFAR100 images is small (i.e., 32×32), so their visualization is a little unclear. 58G.6 Visualization of the expanded images on DTD G.6.1 Visualization of the expanded images by GIF-SD on DTD Input  Our GIF-SD expansion Figure 31: More visualization of the synthetic samples on DTD by GIF-SD. 59G.6.2 Visualization of the expanded images by GIF-DALLE on DTD Input  OurGIF-DALLEexpansion Figure 32: More visualization of the synthetic samples on DTD by GIF-DALLE. 60",
      "references": [
        "Dataset of breast ultrasound images.",
        "Synthetic data from diffusion models improves imagenet classification.",
        "Random multi-channel image synthesis for multiplexed immunofluores- cence imaging.",
        "Coyo-700m: Image-text pair dataset, 2022.",
        "Learning imbal- anced datasets with label-distribution-aware margin loss.",
        "Generalizing dataset distillation via deep generative prior.",
        "Data-free knowledge distillation for object detection.",
        "Gridmask data augmentation.",
        "On the efficacy of knowledge distillation.",
        "Describing textures in the wild.",
        "Autoaugment: Learning augmentation strategies from data.",
        "Randaugment: Practical automated data augmentation with a reduced search space.",
        "Disentangling writer and character styles for handwriting generation.",
        "Imagenet: A large-scale hierarchical image database.",
        "Improved regularization of convolutional neural networks with cutout.",
        "Diffusion models beat gans on image synthesis.",
        "Taming transformers for high-resolution image synthesis.",
        "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.",
        "Unsupervised domain adaptation by backpropagation.",
        "Neural-sim: Learning to generate training data with nerf.",
        "Knowledge distillation: A survey.",
        "Compodiff: Versatile composed image retrieval with latent diffusion.",
        "Supervised contrastive learn- ing for pre-trained language model fine-tuning.",
        "Masked autoencoders are scalable vision learners.",
        "Deep residual learning for image recognition.",
        "Is synthetic data from generative models ready for image recognition?",
        "Benchmarking neural network robustness to common corruptions and perturbations.",
        "Augmix: A simple data processing method to improve robustness and uncertainty.",
        "Learning kernel classifiers: theory and algorithms.",
        "Distilling the knowledge in a neural network.",
        "Denoising diffusion probabilistic models.",
        "Arbitrary style transfer in real-time with adaptive instance normalization.",
        "Image-to-image translation with conditional adversarial networks.",
        "Generative models as a data source for multiview representation learning.",
        "Balanced meta-softmax for long-tailed visual recognition.",
        "Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multicenter study.",
        "Diffusionclip: Text-guided diffusion models for robust image manipulation.",
        "An introduction to variational autoencoders.",
        "Active generative adversarial network for image classification.",
        "Collecting a large-scale dataset of fine-grained cars.",
        "Learning multiple layers of features from tiny images.",
        "Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations.",
        "Fencemask: A data augmentation approach for pre-extracted image features.",
        "Delta: Deep learning transfer using feature map with attention for convolutional networks.",
        "Fast autoaugment.",
        "Prototype-guided continual adaptation for class-incremental unsupervised domain adapta- tion.",
        "Nerf: Representing scenes as neural radiance fields for view synthesis.",
        "Glide: Towards photorealistic image generation and editing with text-guided diffusion models.",
        "Automated flower classification over a large number of classes.",
        "Cats and dogs.",
        "Styleclip: Text-driven manipulation of stylegan imagery.",
        "Small data challenges in big data era: A survey of recent progress on unsupervised and semi-supervised methods.",
        "Source-free domain adaptation via avatar prototype generation and adaptation.",
        "Learning transferable visual models from natural language supervision.",
        "Transfusion: Understanding transfer learning for medical imaging.",
        "Hierarchical text-conditional image generation with clip latents.",
        "Zero-shot text-to-image generation.",
        "Imagenet-21k pretraining for the masses.",
        "High-resolution image synthesis with latent diffusion models.",
        "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.",
        "Photorealistic text-to-image diffusion models with deep language understanding.",
        "Encyclopedia of machine learning and data mining.",
        "Data augmentation using generative adversarial networks (cyclegan) to improve generalizability in ct segmentation tasks.",
        "Mobilenetv2: Inverted residuals and linear bottlenecks.",
        "Fake it till you make it: Learning transferable representations from synthetic imagenet clones.",
        "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.",
        "Active learning for convolutional neural networks: A core-set approach.",
        "A survey on image data augmentation for deep learning.",
        "Denoising diffusion implicit models.",
        "Does knowledge distillation really work?",
        "Vl-ltr: Learning class-wise visual-linguistic representation for long-tailed visual recognition.",
        "Stablerep: Synthetic images from text-to-image models make strong visual representation learners.",
        "Adversarial discriminative domain adaptation.",
        "Neuroscience of imagination and implications for human evolution.",
        "Clip-nerf: Text- and-image driven manipulation of neural radiance fields.",
        "Imagine: Im- age synthesis by image-guided model inversion.",
        "Un- supervised cardiac segmentation utilizing synthesized images from anatomical labels.",
        "Ea- gans: edge-aware generative adversarial networks for cross-modality mr image synthesis.",
        "The psychology of the imagination.",
        "Robust fine-tuning of zero-shot models.",
        "Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models.",
        "Gan inversion: A survey.",
        "Unsupervised deep embedding for clustering analysis.",
        "Aggregated residual transformations for deep neural networks.",
        "Handsoff: Labeled dataset generation with no additional human annotations.",
        "Masked autoencoders are robust data augmentors.",
        "Efficient multiple organ localization in ct image using 3d region proposal network.",
        "Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis.",
        "Image data augmentation for deep learning: A survey.",
        "Dreaming to distill: Data-free knowledge transfer via deepinversion.",
        "Pixelnerf: Neural radiance fields from one or few images.",
        "Dataset distillation.",
        "Training on thin air: Improve image classification with generated data.",
        "Dual-curriculum teacher for domain-inconsistent object detection in autonomous driving.",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features.",
        "Wide residual networks.",
        "Teacher guided training: An efficient framework for knowledge transfer.",
        "Mixup: Beyond empirical risk minimization.",
        "How does mixup help with robustness and generalization?",
        "Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning.",
        "Self-supervised aggregation of diverse experts for test-agnostic long-tailed recognition.",
        "Deep long-tailed learning: A survey.",
        "Collaborative unsupervised domain adaptation for medical image diagnosis.",
        "Datasetgan: Efficient labeled data factory with minimal human effort.",
        "Dataset condensation with differentiable siamese augmentation.",
        "Synthesizing informative training samples with gan.",
        "Dataset condensation with gradient matching.",
        "Random erasing data augmentation.",
        "Dataset quantization.",
        "In-domain gan inversion for real image editing."
      ],
      "meta_data": {
        "arxiv_id": "2211.13976v6",
        "authors": [
          "Yifan Zhang",
          "Daquan Zhou",
          "Bryan Hooi",
          "Kai Wang",
          "Jiashi Feng"
        ],
        "published_date": "2022-11-25T09:38:22Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces the new task of dataset expansion—automatically enlarging a small labelled dataset with new, informative and correctly-labelled samples—and proposes the Guided Imagination Framework (GIF). GIF leverages powerful generative priors (DALL-E2, Stable Diffusion, MAE) and two novel guidance objectives—class-maintained information boosting and sample diversity promotion—to optimise latent perturbations that yield photo-realistic, label-consistent and diverse images. The method achieves large accuracy gains (up to +36.9% on six natural-image datasets and +13.5% on three medical datasets) over classical augmentations and unguided generation, and is theoretically analysed to improve generalisation via increased data diversity.",
        "methodology": "1) Encode each seed image with the prior model’s encoder to obtain latent feature f. 2) Repeat f K times and apply residual multiplicative perturbations (noise z, bias b) constrained within an ε-ball: f′ = P((1+z)f+b). 3) Optimise z,b by maximising Sinf + Sdiv, where Sinf enforces class consistency and higher entropy using CLIP zero-shot scores, and Sdiv is the KL-divergence among K perturbed features to encourage diversity. 4) Decode optimised latents with the generator (diffusion decoder for DALL-E2, Stable Diffusion with prompt guidance, or MAE reconstructor) to obtain synthetic images. Implemented variants: GIF-DALLE (operates in CLIP latent space), GIF-SD (prompt-guided diffusion + channel-wise noise), GIF-MAE (channel-wise noise on MAE latents). A theoretical bound links improved δ-diversity to lower generalisation error.",
        "experimental_setup": "Datasets: Natural—Caltech-101, CIFAR100-Subset (100×100), Stanford Cars, Oxford 102 Flowers, Oxford-IIIT Pets, DTD; Medical—PathMNIST, BreastMNIST, OrganSMNIST. Expansion ratios: CIFAR100-S 5×, Pets 30×, others 20× (medical 5×). Baselines: data augmentations (Cutout, GridMask, RandAugment), direct use of MAE/DALL-E2/SD, CLIP zero-shot and distillation. Classifiers: ResNet-50 trained from scratch; additional tests with ResNeXt-50, WideResNet-50, MobileNet-V2. Metrics: top-1 accuracy, CIFAR100-C corruption accuracy (15 corruptions, severities 1-5) for OOD robustness, training speed/cost analysis. Implementation: DDIM sampler 50-100 steps, guidance scale 4-20, ε tuned per dataset; medical experiments fine-tune SD/DALL-E2 with DreamBooth or reconstruction.",
        "limitations": "• Synthetic images still inferior to real data; models trained on only generated samples underperform real-data models.\n• Some outputs contain visual artefacts or label noise; quality depends on prompt design and ε tuning.\n• Generators trained on natural images require additional fine-tuning for specialised domains (e.g., medical), adding complexity.\n• Method evaluated only on image classification; effectiveness for detection, segmentation, etc. untouched.\n• Diffusion-based variants incur higher computation/time costs versus simple augmentations.",
        "future_research_directions": "1) Improve guidance or generation quality so synthetic data can fully match or exceed real samples; explore adversarial or reinforcement learning for latent search.\n2) Extend GIF to other vision tasks (object detection, segmentation, OCR) and non-visual modalities.\n3) Automate prompt engineering and reduce reliance on domain-specific fine-tuning to handle unseen domains.\n4) Integrate additional semantics (text, attributes) or curriculum strategies to control difficulty and informativeness of generated data.\n5) Investigate theoretical links between diversity metrics and downstream performance, and develop metrics to predict expansion benefit.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Efficient Training of Visual Transformers with Small Datasets",
      "full_text": "Efﬁcient Training of Visual Transformers with Small Datasets Yahui Liu University of Trento Fondazione Bruno Kessler yahui.liu@unitn.it Enver Sangineto University of Trento enver.sangineto@unitn.it Wei Bi Tencent AI Lab victoriabi@tencent.com Nicu Sebe University of Trento niculae.sebe@unitn.it Bruno Lepri Fondazione Bruno Kessler lepri@fbk.eu Marco De Nadai Fondazione Bruno Kessler work@marcodena.it Abstract Visual Transformers (VTs) are emerging as an architectural paradigm alternative to Convolutional networks (CNNs). Differently from CNNs, VTs can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of the typical convolutional inductive bias makes these models more data hungry than common CNNs. In fact, some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. In this paper, we empirically analyse different VTs, comparing their robustness in a small training set regime, and we show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, we propose an auxiliary self- supervised task which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn spatial relations within an image and makes the VT training much more robust when training data is scarce. Our task is used jointly with the standard (supervised) training and it does not depend on speciﬁc architectural choices, thus it can be easily plugged in the existing VTs. Using an extensive evaluation with different VTs and datasets, we show that our method can improve (sometimes dramatically) the ﬁnal accuracy of the VTs. Our code is available at: https://github.com/ yhlleo/VTs-Drloc. 1 Introduction Visual Transformers (VTs) are progressively emerging architectures in computer vision as an alter- native to standard Convolutional Neural Networks (CNNs), and they have already been applied to many tasks, such as image classiﬁcation [19, 61, 70, 40, 66, 69, 37, 68], object detection [5, 76, 16], segmentation [57], tracking [42], image generation [34, 32] and 3D data processing [74], to mention a few. These architectures are inspired by the well known Transformer [63], which is the de facto standard in Natural Language Processing (NLP) [17, 52], and one of their appealing properties is the possibility to develop a uniﬁed information-processing paradigm for both visual and textual domains. A pioneering work in this direction is ViT [ 19], in which an image is split using a grid of non-overlapping patches, and each patch is linearly projected in the input embedding space, so obtaining a \"token\". After that, all the tokens are processed by a series of multi-head attention and feed-forward layers, similarly to how (word) tokens are processed in NLP Transformers. Preprint. Under review. arXiv:2106.03746v2  [cs.CV]  14 Nov 2021A clear advantage of VTs is the possibility for the network to use the attention layers to model global relations between tokens, and this is the main difference with respect to CNNs, where the receptive ﬁeld of the convolutional kernels locally limits the type of relations which can be learned. However, this increased representation capacity comes at a price, which is the lack of the typical CNN inductive biases, based on exploiting the locality, the translation invariance and the hierarchical structure of visual information [40, 66, 69]. As a result, VTs need a lot of data for training, usually more than what is necessary to standard CNNs [19]. For instance, ViT is trained with JFT-300M [19], a (proprietary) huge dataset of 303 million (weakly) labeled high-resolution images, and performs worse than ResNets [28] with similar capacity when trained on ImageNet-1K (∼1.3 million samples [55]). This is likely due to the fact that ViT needs to learn some local proprieties of the visual data using more samples than a CNN, while the latter embeds these properties in its architectural design [54]. To alleviate this problem, a second generation of VTs has very recently been independently proposed by different groups [ 70, 40, 66, 69, 68, 37, 32]. A common idea behind these works is to mix convolutional layers with attention layers, in such a way providing a local inductive bias to the VT. These hybrid architectures enjoy the advantages of both paradigms: attention layers model long-range dependencies, while convolutional operations can emphasize the local properties of the image content. The empirical results shown in most of these works demonstrate that these second-generation VTs can be trained on ImageNet outperforming similar-size ResNets on this dataset [70, 40, 66, 69, 68, 37]. However, it is still not clear what is the behaviour of these networks when trained on medium-small datasets. In fact, from an application point of view, most of the computer vision tasks cannot rely on (supervised) datasets whose size is comparable with (or larger than) ImageNet. In this paper, we compare to each other different second-generation VTs by either training them from scratch or ﬁne-tuning them on medium-small datasets, and we empirically show that, despite their ImageNet results are basically on par with each other, their classiﬁcation accuracy with smaller datasets largely varies. We also compare VTs with same capacity ResNets, and we show that, in most cases, VTs can match the ResNet accuracy when trained with small datasets. Moreover, we propose to use an auxiliary self-supervisedpretext task and a corresponding loss function to regularize training in a small training set or few epochs regime. Speciﬁcally, the proposed task is based on (unsupervised) learning the spatial relations between the output token embeddings. Given an image, we densely sample random pairs from the ﬁnal embedding grid, and, for each pair, we ask the network to guess the corresponding geometric distance. To solve this task, the network needs to encode both local and contextual information in each embedding. In fact, without local information, embeddings representing different input image patches cannot be distinguished the one from the others, while, without contextual information (aggregated using the attention layers), the task may be ambiguous. Our task is inspired by ELECTRA [13], in which the (NLP) pretext task is densely deﬁned for each output embedding (Section 2). Clark et al. [ 13] show that their task is more sample-efﬁcient than commonly used NLP pretext tasks, and this gain is particularly strong with small-capacity models or relatively smaller training sets. Similarly, we exploit the fact that an image is represented by a VT using multiple token embeddings, and we use their relative distances to deﬁne a localization task over a subset of all the possible embedding pairs. This way, for a single image forward pass, we can compare many embedding pairs with each other, and average our localization loss over all of them. Thus, our task is drastically different from those multi-crop strategies proposed, for instance, in SwA V [7], which need to independently forward each input patch through the network. Moreover, differently from \"ordering\" based tasks [49], we can deﬁne pairwise distances on a large grid without modeling all the possible permutations (more details in Section 2). Since our auxiliary task is self-supervised, our dense relative localization loss (Ldrloc) does not require additional annotation, and we use it jointly with the standard (supervised) cross-entropy as a regularization of the VT training. Ldrloc is very easy-to-be-reproduced and, despite this simplicity, it can largely boost the accuracy of the VTs, especially when the VT is either trained from scratch on a small dataset, or ﬁne-tuned on a dataset with a large domain-shift with respect to the pretraining ImageNet dataset. In our empirical analysis, based on different training scenarios, a variable amount of training data and different VT architectures, Ldrloc has always improved the results of the tested baselines, sometimes boosting the ﬁnal accuracy of tens of points (and up to 45 points). In summary, our main contributions are: 1. We empirically compare to each other different VTs, showing that their behaviour largely differs when trained with small datasets or few training epochs. 22. We propose a relative localization auxiliary task for VT training regularization. 3. Using an extensive empirical analysis, we show that this task is beneﬁcial to speed-up training and improve the generalization ability of different VTs, independently of their speciﬁc architectural design or application task. 2 Related work In this section, we brieﬂy review previous work related to both VTs and self-supervised learning. Visual Transformers.Despite some previous work in which attention is used inside the convolutional layers of a CNN [65, 30], the ﬁrst fully-transformer architectures for vision are iGPT [ 9] and ViT [19]. The former is trained using a \"masked-pixel\" self-supervised approach, similar in spirit to the common masked-word task used, for instance, in BERT [17] and in GPT [52] (see below). On the other hand, ViT is trained in a supervised way, using a special \"class token\" and a classiﬁcation head attached to the ﬁnal embedding of this token. Both methods are computationally expensive and, despite their very good results when trained on huge datasets, they underperform ResNet architectures when trained from scratch using only ImageNet-1K [19, 9]. VideoBERT [58] is conceptually similar to iGPT, but, rather than using pixels as tokens, each frame of a video is holistically represented by a feature vector, which is quantized using an off-the-shelf pretrained video classiﬁcation model. DeiT [61] trains ViT using distillation information provided by a pretrained CNN. The success of ViT has attracted a lot of interest in the computer vision community, and different variants of this architecture have been recently used in many tasks [ 61, 57, 34, 12]. However, as mentioned in Section 1, the lack of the typical CNN inductive biases in ViT, makes this model difﬁcult to train without using (very) large datasets. For this reason, very recently, a second-generation of VTs has focused on hybrid architectures, in which convolutions are used jointly with long-range attention layers [70, 40, 66, 69, 68, 37, 32]. The common idea behind all these works is that the sequence of the individual token embeddings can be shaped/reshaped in a geometric grid, in which the position of each embedding vector corresponds to a ﬁxed location in the input image. Given this geometric layout of the embeddings, convolutional layers can be applied to neighboring embeddings, so encouraging the network to focus on local properties of the image. The main difference among these works concerns where the convolutional operation is applied (e.g., only in the initial representations [70] or in all the layers [ 40, 66, 69, 68, 37], in the token to query/key/value projections [ 66] or in the forward-layers [69, 37, 32], etc.). In most of the experiments of this paper, we use three state-of- the-art second-generation VTs for which there is a public implementation: T2T [70], Swin [40] and CvT [66]). For each of them, we select the model whose number of parameters is comparable with a ResNet-50 [28] (more details in Section 3 and Section 5). We do not modify the native architectures because the goal of this work is to propose a pretext task and a loss function which can be easily plugged in existing VTs. Similarly to the original Transformer [63], in ViT, an (absolute) positional embedding is added to the representation of the input tokens. In Transformer networks, positional embedding is used to provide information about the token order, since both the attention and the (individual token based) feed-forward layers are permutation invariant. In [ 40, 68], relative positional embedding [56] is used, where the position of each token is represented relatively to the others. Generally speaking, positional embedding is a representation of the token position which is provided as input to the network. Conversely, our relative localization loss exploits the relative positions (of the ﬁnal VT embeddings) as a pretext task to extract additional information without manual supervision. Self-supervised learning. Reviewing the vast self-supervised learning literature is out of the scope of this paper. However, we brieﬂy mention that self-supervised learning was ﬁrst successfully applied in NLP, as a means to get supervision from text by replacing costly manual annotations with pretext tasks [43, 44]. A typical NLP pretext task consists in masking a word in an input sentence and asking the network to guess which is the masked token [43, 44, 17, 52]. ELECTRA [13] is a sample-efﬁcient language model in which the masked-token pretext task is replaced by a discriminative task deﬁned over all the tokens of the input sentence. Our work is inspired by this method, since we propose a pretext task which can be efﬁciently computed by densely sampling the ﬁnal VT embeddings. However, while the densely supervised ELECTRA task is obtained by randomly replacing (word) tokens and using a pre-trained BERT model to generate plausible replacements, we do not need a pre-trained model and we do not replace input tokens, being our task based on predicting the 3inter-token geometric distances. In fact, in NLP tasks, tokens are discrete and limited (e.g., the set of words of a speciﬁc-language dictionary), while image patches are “continuous” and highly variable, hence a replacement-based task is hard to use in a vision scenario. In computer vision, common pretext tasks with still images are based on extracting two different views from the same image (e.g., two different crops) and then considering these as a pair of positive images, likely sharing the same semantic content [ 10]. Most current self-supervised computer vision approaches can be categorised in contrastive learning [62, 29, 10, 26, 60, 64, 21], clustering methods [3, 77, 33, 6, 1, 23, 7, 8], asymmetric networks [25, 11] and feature-decorrelation methods [22, 72, 2, 31]. While the aforementioned approaches are all based on ResNets, very recently, both [12] and [8] have empirically tested some of these methods with a ViT architecture [19]. One important difference of our proposal with respect to previous work, is that we do not propose a fully-self-supervised method, but we rather use self-supervision jointly with standard supervision (i.e., image labels) in order to regularize VT training, hence our framework is a multi-task learning approach [15]. Moreover, our dense relative localization loss is not based on positive pairs, and we do not use multiple views of the same image in the current batch, thus our method can be used with standard (supervised) data-augmentation techniques. Speciﬁcally, our pretext task is based on predicting the relative positions of pairs of tokens extracted from the same image. Previous work using localization for self-supervision is based on predicting the input image rotation [24] or the relative position of adjacent patches extracted from the same image [18, 49, 50, 45]. For instance, in [49], the network should predict the correct permutation of a grid of 3 ×3 patches (in NLP, a similar, permutation based pretext task, is deshufﬂing [53]). In contrast, we do not need to extract multiple patches from the same input image, since we can efﬁciently use the ﬁnal token embeddings (thus, we need a single forward and backward pass per image). Moreover, differently from previous work based on localization pretext tasks, our loss is densely computed between many random pairs of (non necessarily adjacent) token embeddings. For instance, a trivial extension of the ordering task proposed in [49] using a grid of 7 ×7 patches would lead to 49! possible permutations, which becomes intractable if modeled as a classiﬁcation task. Finally, in [ 16], the position of a random query patch is used for the self-supervised training of a transformer-based object detector [5]. However, the localization loss used in [16] is speciﬁc for the ﬁnal task (object localization) and the speciﬁc DETR architecture [5], while our loss is generic and can be plugged in any VT. 1 1 2 3 4 5 k 2 3 4 5 k Visual Transformer Input image … … … … … … … … … … … … … … … … … … … … … … Localization MLP (f ) k k MLP MLP Pooling/Cls token du dv Class Bird Ball Car … b)a) Figure 1: A schematic representation of the VT architecture. (a) A typical second-generation VT. (b) Our localization MLP which takes as input (concatenated) pairs of ﬁnal token embeddings. 3 Preliminaries A typical VT network takes as input an image split in a grid of (possibly overlapping)K×Kpatches. Each patch is projected in the input embedding space, obtaining a set ofK×Kinput tokens. A VT is based on the typical Transformer multi-attention layers [63], which model pairwise relations over the token intermediate representations. Differently from a pure Transformer [63], the hybrid architectures mentioned in Section 1-2 usually shape or reshape the sequence of these token embeddings in a spatial grid, which makes it possible to apply convolutional operations over a small set of neighboring token embeddings. Using convolutions with a stride greater than 1 and/or pooling operations, the resolution of the initial K×Ktoken grid can possibly be reduced, thus simulating the hierarchical 4structure of a CNN. We assume that the ﬁnal embedding grid has a resolution ofk×k(where, usually, k≤K), see Fig. 1 (a). The ﬁnal k×kgrid of embeddings represents the input image and it is used for the discriminative task. For instance, some methods include an additional \"class token\" which collects contextual information over the whole grid [19, 70, 66, 69, 68, 37], while others [40] apply an average global pooling over the ﬁnal grid to get a compact representation of the whole image. Finally, a standard, small MLP head takes as input the whole image representation and it outputs a posterior distribution over the set of the target classes (Fig. 1 (a)). The VT is trained using a standard cross-entropy loss ( Lce), computed using these posteriors and the image ground-truth labels. When we plug our relative localization loss (Section 4) in an existing VT, we always use the native VT architecture of each tested method, without any change apart from the dedicated localization MLP (see Section 4). For instance, we use the class token when available, or the average pooling layer when it is not, and on top of these we use the cross-entropy loss. We also keep the positional embedding (Section 2) for those VTs which use it (see Section 4.1 for a discussion about this choice). The only architectural change we do is to downsample the ﬁnal embedding grid of T2T [ 70] and CvT [66] to make them of the same size as that used in Swin [ 7]. Speciﬁcally, in Swin, the ﬁnal grid has a resolution of 7 ×7 (k = 7), while, in T2T and in CvT, it is 14 ×14. Thus, in T2T and in CvT, we use a 2 ×2 average pooling (without learnable parameters) and we get a ﬁnal 7 ×7 grid for all the three tested architectures. This pooling operation is motivated in Section 4.1, and it is used only together with our localization task (it does not affect the posterior computed by the classiﬁcation MLP). Finally, note that T2T uses convolutional operations only in the input stage, and it outputs a sequence of 14 ×14 = 196embeddings, corresponding to its 14 ×14 input grid. In this case, we ﬁrst reshape the sequence and then we use pooling. In the Supplementary Material, we show additional experiments with a ViT architecture [19], in which we adopt the same reshaping and pooling strategy. 4 Dense relative localization task The goal of our regularization task is to encourage the VT to learn spatial information without using additional manual annotations. We achieve this by densely sampling multiple embedding pairs for each image and asking the network to guess their relative distances. In more detail, given an image x, we denote its corresponding k×kgrid of ﬁnal embeddings (Section 3), as Gx = {ei,j}1≤i,j≤k, where ei,j ∈Rd, and dis the dimension of the embedding space. For each Gx, we randomly sample multiple pairs of embeddings and, for each pair (ei,j,ep,h), we compute the 2D normalized target translation offset (tu,tv)T , where: tu = |i−p| k , t v = |j−h| k , (tu,tv)T ∈[0,1]2. (1) The selected embedding vectors ei,j and ep,h are concatenated and input to a small MLP ( f), with two hidden layers and two output neurons, one per spatial dimension (Fig. 1 (b)), which predicts the relative distance between position (i,j) and position (p,h) on the grid. Let (du,dv)T = f(ei,j,ep,h)T . Given a mini-batch Bof nimages, our dense relative localization loss is: Ldrloc = ∑ x∈B E(ei,j,ep,h)∼Gx[|(tu,tv)T −(du,dv)T |1]. (2) In Eq. 2, for each image x, the expectation is computed by sampling uniformly at random mpairs (ei,j,ep,h) in Gx, and averaging the L1 loss between the corresponding (tu,tv)T and (du,dv)T . Ldrloc is added to the standard cross-entropy loss ( Lce) of each native VT (Section 3). The ﬁnal loss is: Ltot = Lce + λLdrloc. We use λ= 0.1 in all the experiments with both T2T and CvT, and λ= 0.5 in case of Swin. Note that the same pairwise localization task can be associated with slightly different loss formulations. In the Supplementary Material we present some of these variants and we compare them empirically with each other. 4.1 Discussion Intuitively, Ldrloc transforms the relative positional embedding (Section 2), used, for instance, in Swin [40], in a pretext task, asking the network to guess which is the relative distance of a random 5subset of all the possible token pairs. Thus a question arises: is the relative positional embedding used in some VTs sufﬁcient for the localization MLP (f) to solve the localization task? The experiments presented in Section 5.2-5.3 show that, when we plug Ldrloc on CvT, in which no kind of positional embedding is used [66], the relative accuracy boost is usuallysmaller than in case of Swin, conﬁrming that the relative positional embedding, used in the latter, is not sufﬁcient to make our task trivial. We further analyze this point in the Supplementary Material. In Section 3, we mentioned that, in case of T2T and CvT, we average-pool the ﬁnal grid and we obtain a 7 ×7 grid Gx. In fact, in preliminary experiments with both T2T and CvT at their original 14 ×14 resolution, we observed a very slow convergence ofLdrloc. We presume this is due to the fact that, with a ﬁner grid, the localization task is harder. This slows down the convergence of f, and it likely generates noisy gradients which are backpropagated through the whole VT (see also the Supplementary Material). We leave this for future investigation and, in the rest of this article, we always assume that our pretext task is computed with a 7 ×7 grid Gx. Table 1: The size of the datasets used in our empirical analysis. Dataset Train size Test size Classes ImageNet-1K [55] 1,281,167 100,000 1000 ImageNet-100 [60] 126,689 5,000 100 CIFAR-10 [35] 50,000 10,000 10 CIFAR-100 [35] 50,000 10,000 100 Oxford Flowers102 [48] 2,040 6,149 102 SVHN [47] 73,257 26,032 10DomainNet ClipArt 33,525 14,604 345 Infograph 36,023 15,582 Painting 50,416 21,850 Quickdraw 120,750 51,750 Real 120,906 52,041 Sketch 48,212 20,916 5 Experiments All the experiments presented in this section are based on image classiﬁcation tasks, while in the Supplementary Material we also show object detection, instance segmentation and semantic segmentation tasks. In this section we use 11 different datasets: ImageNet-100 (IN-100) [ 60, 64], which is a subset of 100 classes of ImageNet-1K [ 55]; CIFAR-10 and CIFAR-100 [ 35], Oxford Flowers102 [48] and SVHN [47], which are four widely used computer vision datasets; and the six datasets of DomainNet [51], a benchmark commonly used for domain adaptation tasks. We chose the latter because of the large domain-shift between some of its datasets and ImageNet-1K, which makes the ﬁne-tuning experiments non-trivial. Tab. 1 shows the size of each dataset. We used, when available, the ofﬁcial VT code (for T2T [70] and Swin [40]) and a publicly available implementation of CvT [66]1. In the ﬁne-tuning experiments (Section 5.3), we use only T2T and Swin because of the lack of publicly available ImageNet pre-trained CvT networks. For each of the three baselines, we chose a model of comparable size to ResNet-50 (25M parameters): see Tab. 3 for more details. In the Supplementary Material, we show additional results obtained with larger models (ViT-B [19]), larger datasets (e.g., ImageNet-1K) and more training epochs. When we plug our loss on one of the adopted baselines, we follow Section 4, keeping unchanged the VT architecture apart from our localization MLP (f). Moreover, in all the experiments, we train the baselines, both with and without our localization loss, using the same data-augmentation protocol for all the models, and we use the VT-speciﬁc hyper-parameter conﬁguration suggested by the authors of each VT. We do not tune the VT-speciﬁc hyperparameters when we use our loss and we keep ﬁxed the values ofm and λ(Section 5.1) in all the experiments. We train each model using 8 V100 32GB GPUs. 1https://github.com/lucidrains/vit-pytorch 65.1 Ablation study In Tab. 2 (a) we analyze the impact on the accuracy of different values of m(the total number of embedding pairs used per image, see Section 4). Since we use the same grid resolution for all the VTs (i.e., 7 ×7, Section 3), also the maximum number of possible embeddings per image is the same for all the VTs (k2 = 49). Using the results of Tab. 2 (a) (based on CIFAR-100 and Swin), we chose m = 64for all the VTs and all the datasets. Moreover, Tab. 2 (b) shows the inﬂuence of the loss weight λ(Section 4) for each of the three baselines, which motivates our choice of using λ= 0.1 for both CvT and T2T and λ= 0.5 for Swin. These values of mand λare kept ﬁxed in all the other experiments of this paper, independently of the dataset, the main task (e.g., classiﬁcation, detection, segmentation, etc.), and the training protocol (from scratch or ﬁne-tuning). This is done to emphasise the ease of use of our loss. Finally, in the Supplementary Material, we analyze the inﬂuence of the size of the localization MLP (f). Table 2: CIFAR-100, 100 training epochs: (a) the inﬂuence on the accuracy of the number of pair samples (m) in Ldrloc using Swin, and (b) the inﬂuence of the λvalue using all the 3 VT baselines. (a) Model Top-1 Acc. A: Swin-T 53.28 B: A + Ldrloc, m=32 63.70 C: A + Ldrloc, m=64 66.23 D: A + Ldrloc, m=128 65.16 E: A + Ldrloc, m=256 64.87 (b) Model λ=0.0 λ=0.1 λ=0.5 λ=1.0 CvT-13 73.50 74.51 74.07 72.84 Swin-T 53.28 58.15 66.23 64.28 T2T-ViT-14 65.16 68.03 67.03 66.53 Table 3: Top-1 accuracy on IN-100 using either 100 or 300 epochs. In the former case, we show the average and the standard deviation values obtained by repeating each single experiment 5 times with 5 different random seeds. Model # Params ImageNet-100 (M) 100 epochs 300 epochs CvT CvT-13 20 85.62 ±0.05 90.16 CvT-13+Ldrloc 20 86.09 ±0.12 (+0.47) 90.28 (+0.12) Swin Swin-T 29 82.66 ±0.10 89.68 Swin-T+Ldrloc 29 83.95 ±0.05 (+1.29) 90.32 (+0.64) T2T T2T-ViT-14 22 82.67 ±0.01 87.76 T2T-ViT-14+Ldrloc 22 83.74 ±0.08 (+1.07) 88.16 (+0.40) 5.2 Training from scratch In this section, we analyze the performance of both the VT baselines and our regularization loss using small-medium datasets and different number of training epochs, simulating a scenario with limited computational resources and/or limited training data. In fact, while ﬁne-tuning a model pre-trained on ImageNet-1K is the most common protocol when dealing with small training datasets, this is not possible when, e.g., the network input is not an RGB image (e.g., in case of 3D point cloud data [74]) or when using a task-speciﬁc backbone architecture [36, 38]. In these cases, the network needs to be trained from scratch on the target dataset, thus, investigating the robustness of the VTs when trained from scratch with relatively small datasets, is useful for those application domains in which a ﬁne-tuning protocol cannot be adopted. We start by analyzing the impact on the accuracy of the number of training epochs on IN-100. Tab. 3 shows that, using Ldrloc, all the tested VTs show an accuracy improvement, and this boost is larger with fewer epochs. As expected, our loss acts as a regularizer, whose effects are more pronounced in a shorter training regime. We believe this result is particularly signiﬁcant considering the larger computational times which are necessary to train typical VTs with respect to ResNets. 7In Tab. 4, we use all the other datasets and we train from scratch with 100 epochs (see the Supple- mentary Material for longer training protocols). First, we note that the accuracy of the VT baselines varies a lot depending on the dataset (which is expected), but also depending on the speciﬁc VT architecture. This is largely in contrast with the ImageNet-1K results, where the difference between the three baselines is much smaller. As a reference, when these VTs are trained on ImageNet-1K (for 300 epochs), the differences of their respective top-1 accuracy is much smaller: Swin-T, 81.3 [40]; T2T-ViT-14, 81.5 [70]; CvT-13, 81.6 [66]. Conversely, Tab. 4 shows that, for instance, the accuracy difference between CvT and Swin is about 45-46 points in Quickdraw and Sketch, 30 points on CIFAR-10, and about 20 points on many other datasets. Analogously, the difference between CvT and T2T is between 20 and 25 points in Sketch, Painting and Flowers102, and quite signiﬁcant in the other datasets. This comparison shows that CvT is usually much more robust in a small training set regime with respect to the other two VTs, a behaviour which is completely hidden when the training/evaluation protocol is based on large datasets only. In the same table, we also show the accuracy of these three VTs when training is done using Ldrloc as a regularizer. Similarly to the IN-100 results, also in this case our loss improves the accuracy of all the tested VTs in all the datasets. Most of the time, this improvement is quite signiﬁcant (e.g., almost 4 points on SVHN with CvT), and sometimes dramatic (e.g., more than 45 points on Quickdraw with Swin). These results show that a self-supervised auxiliary task can provide a signiﬁcant \"signal\" to the VT when the training set is limited, and, speciﬁcally, that our loss can be very effective in boosting the accuracy of a VT trained from scratch in this scenario. In Tab. 4 we also report the results we obtained using a ResNet-50, trained with 100 epochs and the standard ResNet training protocol (e.g., using Mixup [73] and CutMix [71] data-augmentations, etc.). These results show that the best performing VT (CvT) is usually comparable with a same size ResNet, and demonstrate that VTs can potentially be trained from scratch with darasets smaller than InageNet-1K. Finally, in the last row of the same table, we train the ResNet-50 baseline jointly with our pretext task. In more detail, we replace the VT token embedding grid ( Gx in Eq. 2) with the last convolutional feature map of the ResNet, and we apply our loss (Eq. 2) on top of this map. A comparison between the results of the last 2 rows of Tab. 4 shows that our loss is useful also when used with a ResNet (see the Supplementary Material for longer training protocols). When using ResNets, the improvement obtained with our loss is marginal, but it is consistent in 9 out of 10 datasets. The smaller improvement with respect to the analogous VT results may probably be explained by the fact that ResNets already embed local inductive biases in their architecture, thus a localization auxiliary task is less helpful (Section 1). Table 4: Top-1 accuracy of VTs and ResNets, trained from scratch on different datasets (100 epochs). CIFAR-10 CIFAR-100 Flowers102 SVHN ClipArt Infograph Painting Quickdraw Real Sketch CvT CvT-13 89.02 73.50 54.29 91.47 60.34 19.39 54.79 70.10 76.33 56.98 90.30 74.51 56.29 95.36 60.64 20.05 55.26 70.36 77.05 57.56CvT-13+Ldrloc (+1.28) (+1.01) (+2.00) (+3.89) (+0.30) (+0.67) (+0.47) (+0.26) (+0.68) (+0.58) Swin Swin-T 59.47 53.28 34.51 71.60 38.05 8.20 35.92 24.08 73.47 11.97 83.89 66.23 39.37 94.23 47.47 10.16 41.86 69.41 75.59 38.55Swin-T+Ldrloc (+24.42) (+12.95) (+4.86) (+22.63) (+9.42) (+1.96) (+5.94) (+45.33) (+2.12) (+26.58) T2T T2T-ViT-14 84.19 65.16 31.73 95.36 43.55 6.89 34.24 69.83 73.93 31.51 87.56 68.03 34.35 96.49 52.36 9.51 42.78 70.16 74.63 51.95T2T-ViT-14+Ldrloc (+3.37) (+2.87) (+2.62) (+1.13) (+8.81) (+2.62) (+8.54) (+0.33) (+0.70) (+20.44) ResNet ResNet-50 91.78 72.80 46.92 96.45 63.73 19.81 53.22 71.38 75.28 60.08 92.03 72.94 47.65 96.53 63.93 20.79 53.52 71.57 75.56 59.62ResNet-50+Ldrloc (+0.25) (+0.14) (+0.73) (+0.08) (+0.20) (+0.98) (+0.30) (+0.19) (+0.28) (-0.46) 8Table 5: Pre-training on ImageNet-1K and then ﬁne-tuning on the target dataset (top-1 accuracy, 100 ﬁne-tuning epochs). CIFAR-10 CIFAR-100 Flowers102 SVHN ClipArt Infograph Painting Quickdraw Real Sketch Swin Swin-T 97.95 88.22 98.03 96.10 73.51 41.07 72.99 75.81 85.48 72.37 98.37 88.40 98.21 97.87 79.51 46.10 73.28 76.01 85.61 72.86Swin-T+Ldrloc (+0.42) (+0.18) (+0.18) (+1.77) (+6.00) (+5.03) (+0.29) (+0.20) (+0.13) (+0.49) T2T T2T-ViT-14 98.37 87.33 97.98 97.03 74.59 38.53 72.29 74.16 84.56 72.18 98.52 87.65 98.08 98.20 78.22 45.69 72.42 74.27 84.57 72.29T2T-ViT-14+Ldrloc (+0.15) (+0.32) (+0.10) (+1.17) (+3.63) (+7.16) (+0.13) (+0.11) (+0.01) (+0.11) ResNet ResNet-50 97.65 85.44 96.59 96.60 75.22 44.30 66.58 72.12 80.40 67.77 97.74 85.65 96.72 96.71 75.51 44.39 69.03 72.21 80.54 68.14ResNet-50+Ldrloc (+0.09) (+0.21) (+0.13) (+0.11) (+0.29) (+0.09) (+2.45) (+0.09) (+0.14) (+0.37) 5.3 Fine-tuning In this section, we analyze a typical ﬁne-tuning scenario, in which a model is pre-trained on a big dataset (e.g., ImageNet), and then ﬁne-tuned on the target domain. Speciﬁcally, inall the experiments, we use VT models pre-trained by the corresponding VT authors on ImageNet-1K without our localization loss. The difference between the baselines and ours concerns only the ﬁne-tuning stage, which is done in the standard way for the former and using our Ldrloc regularizer for the latter. Starting from standard pre-trained models and using our loss only in the ﬁne-tuning stage, emphasises the easy to use of our proposal in practical scenarios, in which ﬁne-tuning can be done without re-training the model on ImageNet. As mentioned in Section 5, in this analysis we do not include CvT because of the lack of publicly available ImageNet-1K pre-trained models for this architecture. The results are presented in Tab. 5. Differently from the results shown in Section 5.2, the accuracy difference between the T2T and Swin baselines is much less pronounced, and the latter outperforms the former in most of the datasets. Moreover, analogously to all the other experiments, also in this case, using Ldrloc leads to an accuracy improvement with all the tested VTs and in all the datasets. For instance, on Infograph, Swin with Ldrloc improves of more than 5 points, and T2T more than 7 points. In the last two rows of Tab. 5, we show the ResNet based results. The comparison between ResNet and the VT baselines shows that the latter are very competitive in this ﬁne-tuning scenario, even more than with a training-from-scratch protocol (Tab. 4). For instance, the two VT baselines (without our loss) are outperformed by ResNet only in 2 out of 10 datasets. This conﬁrms that VTs are likely to be widely adopted in computer vision applications in the near future, independently of the training set size. Finally, analogously to the experiments in Section 5.2, Tab. 5 shows that our loss is (marginally) helpful also in ResNet ﬁne-tuning. 6 Conclusion In this paper, we have empirically analyzed different VTs, showing that their performance largely varies when trained with small-medium datasets, and that CvT is usually much more effective in generalizing with less data. Moreover, we proposed a self-supervised auxiliary task to regularize VT training. Our localization task, inspired by [13], is densely deﬁned for a random subset of ﬁnal token embedding pairs, and it encourages the VT to learn spatial information. In our extensive empirical analysis, with 11 datasets, different training scenarios and three VTs, our dense localization loss has always improved the corresponding baseline accuracy, usually by a signiﬁcant margin, and sometimes dramatically (up to +45 points). We believe that this shows that our proposal is an easy-to-reproduce, yet very effective tool to boost the performance of VTs, especially in training regimes with a limited amount of data/training time. It also paves the way to investigating other forms of self-supervised/multi-task learning which are speciﬁc for VTs, and can help VT training without resorting to the use of huge annotated datasets. 9Limitations. A deeper analysis on why ﬁne-grained embedding grids have a negative impact on our auxiliary task (Section 4.1) was left as a future work. Moreover, despite in the Supplementary Material we show a few experiments with ViT-B, which conﬁrm the usefulness ofLdrloc when used with bigger VT models, in our analysis we mainly focused on VTs of approximately the same size as a ResNet-50. In fact, the goal of this paper is investigating the VT behaviour with medium-small datasets, thus, high-capacity models most likely are not the best choice in a training scenario with scarcity of data. Acknowledgements This work was partially supported by the EU H2020 AI4Media No. 951911 project and by the EUREGIO project OLIVER. References [1] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020. [2] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regular- ization for self-supervised learning. arXiv:2105.04906, 2021. [3] Miguel A Bautista, Artsiom Sanakoyeu, Ekaterina Tikhoncheva, and Bjorn Ommer. CliqueCNN: deep unsupervised exemplar learning. In NeurIPS, 2016. [4] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into high quality object detection. In CVPR, 2018. [5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. [6] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018. [7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv:2104.14294, 2021. [9] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. [11] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021. [12] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. ICCV, 2021. [13] Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR, 2020. [14] MMCV Contributors. Openmmlab foundational library for computer vision research, 2020. [15] Michael Crawshaw. Multi-task learning with deep neural networks: A survey.arXiv:2009.09796, 2020. [16] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. UP-DETR: unsupervised pre-training for object detection with transformers. In CVPR, 2021. [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL,, 2019. [18] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. 10[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [20] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal cycle-consistency learning. In CVPR, 2019. [21] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisser- man. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In ICCV, 2021. [22] Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-supervised representation learning. In ICML, 2021. [23] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. SCAN: learning to classify images without labels. In ECCV, 2020. [24] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. [25] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020. [26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. [27] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In ICCV, 2017. [28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [29] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In ICLR, 2019. [30] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In ICCV, 2019. [31] Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, and Hang Zhao. On feature decorrelation in self-supervised learning. arXiv:2105.00470, 2021. [32] Drew A. Hudson and C. Lawrence Zitnick. Generative Adversarial Transformers. In ICML, 2021. [33] Xu Ji, João F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classiﬁcation and segmentation. In ICCV, 2019. [34] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. TransGAN: Two transformers can make one strong GAN. arXiv:2102.07074, 2021. [35] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [36] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. Int. J. Comput. Vis., 128(3):642–656, 2020. [37] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. LocalViT: Bringing locality to vision transformers. arXiv:2104.05707, 2021. [38] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, and Jian Sun. Detnet: Design backbone for object detection. In ECCV, 2018. [39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. [40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv:2103.14030, 2021. 11[41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101, 2017. [42] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixé, and Christoph Feichtenhofer. Track- Former: Multi-object tracking with transformers. arXiv:2101.02702, 2021. [43] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations in vector space. arXiv:1301.3781, 2013. [44] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In NeurIPS, 2013. [45] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant repre- sentations. In CVPR, 2020. [46] Muzammal Naseer, Kanchana Ranasinghe, Salman H. Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. arXiv:2105.10497, 2021. [47] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. [48] Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In Indian Conference on Computer Vision, Graphics & Image Processing, 2008. [49] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. [50] Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to count. In ICCV, 2017. [51] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In CVPR, 2019. [52] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. [53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:140:1–140:67, 2020. [54] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? arXiv:2108.08810, 2021. [55] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015. [56] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position repre- sentations. In NAACL, 2018. [57] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In ICCV, 2021. [58] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT: A joint model for video and language representation learning. In ICCV, 2019. [59] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse R-CNN: End-to-end object detection with learnable proposals. arXiv:2011.12450, 2020. [60] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV, 2020. [61] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv:2012.12877, 2020. [62] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv:1807.03748, 2018. 12[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [64] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML, 2020. [65] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018. [66] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. CvT: Introducing convolutions to vision transformers. arXiv:2103.15808, 2021. [67] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing for scene understanding. In ECCV, 2018. [68] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. arXiv:2104.06399, 2021. [69] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. arXiv:2103.11816, 2021. [70] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token ViT: Training vision transformers from scratch on ImageNet. In ICCV, 2021. [71] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. CutMix: Regularization strategy to train strong classiﬁers with localizable features. In ICCV, 2019. [72] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self- supervised learning via redundancy reduction. In ICML, 2021. [73] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018. [74] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip H. S. Torr, and Vladlen Koltun. Point transformer. arXiv:2012.09164, 2020. [75] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302–321, 2019. [76] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. In ICLR, 2021. [77] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning of visual embeddings. In ICCV, 2019. 13A Pseudocode of the dense relative localization task In order to emphasise the simplicity and the ease of reproduction of our proposed method, in Figure 2 we show a PyTorch-like pseudocode of our auxiliary task with the associated Ldrloc loss. # n : batch size # m : number of pairs # k X k : resolution of the embedding grid # D : dimension of each token embedding # x : a tensor of n embedding grids, shape=[n, D, k, k] def position_sampling(k, m, n): pos_1 = torch.randint(k, size=(n, m, 2)) pos_2 = torch.randint(k, size=(n, m, 2)) return pos_1, pos_2 def collect_samples(x, pos, n): _, c, h, w = x.size() x = x.view(n, c, -1).permute(1, 0, 2).reshape(c, -1) pos = ((torch.arange(n).long().to(pos.device) * h * w).view(n, 1) + pos[:, :, 0] * h + pos[:, :, 1]).view(-1) return (x[:, pos]).view(c, n, -1).permute(1, 0, 2) def dense_relative_localization_loss(x): n, D, k, k = x.size() pos_1, pos_2 = position_sampling(k, m, n) deltaxy = abs((pos_1 - pos_2).float()) # [n, m, 2] deltaxy /= k pts_1 = collect_samples(x, pos_1, n).transpose(1, 2) # [n, m, D] pts_2 = collect_samples(x, pos_2, n).transpose(1, 2) # [n, m, D] predxy = MLP(torch.cat([pts_1, pts_2], dim=2)) return L1Loss(predxy, deltaxy) Figure 2: A PyTorch-like pseudocode of our dense relative localization task and the corresponding Ldrloc loss. B Loss Variants In this section, we present different loss function variants associated with our relative localization task, which are empirically evaluated in Sec. B.1. The goal is to show that the auxiliary task proposed in the main paper can be implemented in different ways and to analyze the differences between these implementations. The ﬁrst variant consists in including negative target offsets: t′ u = i−p k , t ′ v = j−h k , (t′ u,t′ v)T ∈[−1,1]2. (3) Replacing (tu,tv)T in Eq. 2 in the main paper with (t′ u,t′ v)T computed as in Eq. 3, and keeping all the rest unchanged, we obtain the ﬁrst variant, which we call L∗ drloc. In the second variant, we transform the regression task in Eq. 2 in the main paper in a classiﬁcation task, and we replace the L1 loss with the cross-entropy loss. In more detail, we use as target offsets: cu = i−p, c v = j−h, (cu,cv)T ∈{−k,...,k }2, (4) and we associate each of the2k+1 discrete elements in C = {−k,...,k }with a \"class\". Accordingly, the localization MLP f is modiﬁed by replacing the 2 output neurons with 2 different sets of neurons, one per spatial dimension (uand v). Each set of neurons represents a discrete offset prediction over the 2k+ 1\"classes\" in C. Softmax is applied separately to each set of 2k+ 1neurons, and the output of f is composed of two posterior distributions over C: (pu,pv)T = f(ei,j,ep,h)T , where 14pu,pv ∈[0,1]2k+1. Eq. 2 in the main paper is then replaced by: Lce drloc = − ∑ x∈B E(ei,j,ep,h)∼Gx[log(pu[cu]) +log(pv[cv])], (5) where pu[cu] indicates the cu-th element of pu (and similarly for pv[cv]). Note that, using the cross-entropy loss in Eq. 5, corresponds to considering C an unordered set of \"categories\". This implies that prediction errors in pu (and pv) are independent of the \"distance\" with respect to the ground-truth cu (respectively, cv). In order to alleviate this problem, and inspired by [20], in the third variant we propose, we impose a Gaussian prior on pu and pv, and we minimize the normalized squared distance between the expectation of pu and the ground-truth cu (respectively, pv and cv). In more detail, let µu = ∑ c∈C pu[c] ∗cand σ2 u = ∑ c∈C pu[c] ∗(c−µu)2 (and similarly for µv and σ2 v). Then, Eq. 5 is replaced by: Lreg drloc = ∑ x∈B E(ei,j,ep,h)∼Gx [(cu −µu)2 σ2u + αlog(σu) +(cv −µv)2 σ2v + αlog(σv) ] , (6) where the terms log(σu) and log(σv) are used for variance regularization and αweights the impor- tance of the Gaussian prior [ 20]. In preliminary experiments in which we tuned the αparameter using Swin, we found that the default value of α= 0.001, as suggested in [20], works well in our scenario, thus we adopted it for all the experiments involving Lreg drloc. The fourth variant we propose is based on a \"very-dense\" localization loss, where Ldrloc is computed for every transformer block of VT. Speciﬁcally, let Gl x be the kl ×kl grid of token embeddings produced by the l-th block of the VT, and let Lbe the total number of these blocks. Then, Eq. 2 in the main paper is replaced by: Lall drloc = ∑ x∈B L∑ l=1 E(ei,j,ep,h)∼Glx [|(tl u,tl v)T −(dl u,dl v)T |1], (7) where (tl u,tl v)T and (dl u,dl v)T are, respectively, the target (see main paper Eq. 1) and the prediction offsets computed at block lusing the randomly sampled pair (ei,j,ep,h) ∈Gl x. For each block, we use a block-speciﬁc MLP fl to compute (dl u,dl v)T . Note that, using Eq. 7, the initial layers of VT receive more \"signal\", because each block laccumulates the gradients produced by all the blocks l′≥l. Apart from Lall drloc, all the other proposed variants are very computationally efﬁcient, because they involve only one forward and one backward pass per image, and mforward passes through f. B.1 Empirical comparison of the loss variants In Tab. 6, we compare the loss variants with each other, where the baseline model is Swin [40] (row (A)). For these experiments, we use IN-100, we train all the models for 100 epochs, and, as usual, we show the top-1 classiﬁcation accuracy on the test set. When we plug Ldrloc on top of Swin (main paper, Sec. 4), the ﬁnal accuracy increases by 1.26 points (B). All the other dense localization loss variants underperform Ldrloc (C-F). A bit surprisingly, the very-dense localization loss Lall drloc is signiﬁcantly outperformed by the much simpler (and computationally more efﬁcient) Ldrloc. Moreover, Lall drloc is the only variant which underperforms the baseline. We presume that this is due to the fact that most of the Swin intermediate blocks have resolution grids Gl x ﬁner than the last grid GL x (l < L, kl > kL, Sec. B), and this makes the localization task harder, slowing down the convergence offl, and likely providing noisy gradients to the VT (see the discussion in the main paper, Sec. 4.1). In all the other experiments (both in the main paper and in this Supplementary Material), we always use Ldrloc as the relative localization loss. B.2 Relative positional embedding All the loss variants presented in this section have been plugged on Swin, in which relative positional embedding is used (see the main paper, Sec. 3 and Sec. 4.1). However, the results reported in Tab. 6 show that almost all of these losses can boost the accuracy of the Swin baseline. Below, we intuitively 15Table 6: IN-100, 100 epoch training: a comparison between different loss variants. Model Top-1 Acc. A: Swin-T 82.76 B: A + Ldrloc 84.02 (+1.26) C: A + L∗ drloc 83.14 (+0.38) D: A + Lce drloc 83.86 (+1.10) E: A + Lreg drloc 83.24 (+0.48) F: A + Lall drloc 81.88 (-0.88) explain why the relative positional embedding is not sufﬁcient to allow the network to solve our localization task. The relative positional embedding (called Bin [40]) used in Swin, is added to the query/key product before the softmax operation (Eq. 4 in [ 40]). The result of this softmax is then used to weight the importance of each \"value\", and the new embedding representation of each query (i.e., ei,j, in our terminology) is given by this weighted sum of values. Thus, the content of B is not directly represented in ei,j, but only used to weight the values forming ei,j (note that there is also a skip connection). For this reason, Bmay be useful for the task for which it is designed, i.e., computing the importance (attention) of each key with respect to the current query. However, in order to solve our auxiliary task (i.e., to predict tu and tv in Eq. 1 in the main paper), the VT should be able to recover and extract from a given embedding pair(ei,j,ep,h) the speciﬁc offset information originally contained in B(i,j),(p,h) and then blended in the value weights. Probably this is a task (much) harder than exploiting appearance information contained in (ei,j,ep,h). This is somehow in line with different previous work showing the marginal importance of positional embedding in VTs. For instance, Naseer et al. [ 46] show that the (absolute) positional embedding used in ViT [19] is not necessary for the transformer to solve very challenging occlusion or patch permutation tasks, and they conclude that these tasks are solved by ViT thank to its “dynamic receptive ﬁeld” (i.e., the context represented in each individual token embedding). C Experiments with a larger training budget Although the focus of this work is on increasing the VT training efﬁciency in a scenario with a limited training budget, in this section we instead investigate the effect of using our auxiliary task on scenarios with a larger training budget. Speciﬁcally, we test Ldrloc with a larger number of training epochs, using higher-capacity VT models and training the VTs on ImageNet-1K. In Tab. 7 we train both Swin and T2T on ImageNet-1K following the standard protocol (e.g., 300 epochs) and using the publicly available code of each VT baseline. When we use Ldrloc, we get a slight improvement with both the baselines, which shows that our loss is beneﬁcial also with larger datasets and longer training schedules (although the margin is smaller with respect to IN-100, see Tab. 3). Table 7: Top-1 accuracy on ImageNet-1K. (*) Results obtained in our run of the publicly available code with the default hyperparameters of each corresponding VT baseline. Model Top-1 Acc. Swin Swin-T 81.2 (*) Swin-T+Ldrloc 81.33 (+0.13) T2T T2T-ViT-14 80.7 (*) T2T-ViT-14+Ldrloc 80.85 (+0.15) In Tab. 8, we use the Infograph dataset and we train all the networks for 300 epochs. The results conﬁrm that Ldrloc can improve the ﬁnal accuracy even when a longer training schedule is adopted. For instance, comparing the results of T2T in Tab. 8 with the T2T results in Tab. 4 (100 epochs), the relative margin has signiﬁcantly increased (+8.06 versus +2.62). Finally, in Tab. 9, we use three datasets and we train from scratch ViT-B/16 [19], which has 86.4 million parameters (about 4×the number of parameters of the other tested VTs and ResNets). Note 16Table 8: Infograph, training from scratch with 300 epochs. Model Top-1 Acc. CvT-13 29.76 CvT-13 + Ldrloc 30.31 (+0.55) Swin-T 17.17 Swin-T + Ldrloc 20.72 (+3.55) T2T-ViT-14 12.62 T2T-ViT-14 +Ldrloc 20.68 (+8.06) ResNet-50 29.34 ResNet-50 + Ldrloc 30.00 (+0.66) that \"16\" in ViT-B/16 stands for16 ×16 resolution patches, used as input without patch overlapping. For a fair comparison, we used for ViT-B/16 the same image resolution (224 ×224) adopted for all the other VTs (see Sec. F), thus we get a ﬁnal ViT-B/16 embedding grid of 14 ×14, which is pooled to get our 7 ×7 grid as explained in the main paper (Sec. 3). For ViT-B/16, we use λ= 0.01. Tab. 9 shows that our loss is effective also with VT models bigger than the three baselines used in the rest of the paper. Table 9: Training from scratch ViT-B/16 with 100 epochs. Model CIFAR-10 CIFAR-100 Infograph ViT-B/16 71.70 59.67 11.79 ViT-B/16 +Ldrloc 73.91 (+2.21) 61.42 (+1.75) 12.22 (+0.43) D Transfer to object detection and image segmentation tasks In this section, we provide additional ﬁne-tuning experiments using tasks different from classiﬁcation (i.e., object detection, instance segmentation and semantic segmentation). Moreover, we use a different training protocol from the one used in the main paper (Sec. 5.3). Speciﬁcally, the ﬁne- tuning stage is standard (without our loss), while in the pre-training stage we either use the standard cross-entropy (only), or we pre-train the VT jointly using the cross-entropy and Ldrloc. We adopt the framework proposed in [40], where a pre-trained Swin VT is used as the backbone for detection and segmentation tasks. In fact, note that Swin is based on a hierarchy of embedding grids, which can be used by the speciﬁc object detection/image segmentation architectures as they were convolutional feature maps [40]. The pre-training dataset is either ImageNet-1K or IN-100, and in both cases we pre-train Swin using 300 epochs. Hence, in case of ImageNet-1K pre-training, the baseline model is ﬁne-tuned starting from the Swin-T model corresponding to Tab. 7 (ﬁnal accuracy : 81.2), while Swin-T + Ldrloc refers to the model trained with our loss in the same table (ﬁnal accuracy: 81.33). Similarly, in case of IN-100 pre-training, the baseline model is ﬁne-tuned starting from the Swin-T model corresponding to Tab. 3 (ﬁnal accuracy : 89.68), while Swin-T + Ldrloc refers to the model trained with our loss in the same table (ﬁnal accuracy: 90.32). The goal of these experiments is to show that the image representation obtained using Ldrloc for pre-training, can be usefully transferred to other tasks without modifying the task-speciﬁc architecture or the ﬁne-tuning protocol. D.1 Object detection and instance segmentation Setup. We strictly follow the experimental settings used in Swin [40]. Speciﬁcally, we use COCO 2017 [39], which contains 118K training, 5K validation and 20K test-dev images. We use two popular object detection architectures: Cascade Mask R-CNN [4] and Mask R-CNN [27], in which the backbone is replaced with the pre-trained Swin model. Moreover, we use the standard mmcv [14] 17framework to train and evaluate the models. We adopt multi-scale training [5, 59] (i.e., we resize the input image such that the shortest side is between 480 and 800 pixels, while the longest side is at most 1333 pixels), the AdamW [41] optimizer (initial learning rate 0.0001, weight decay 0.05, and batch size 16), and a 1x schedule (12 epochs with the learning rate decayed by 0.1 at epochs 8 and 11). Results. Tab. 10 shows that Swin-T, pre-trained on ImageNet-1K with our Ldrloc loss, achieves both a higher detection and a higher instance segmentation accuracy with respect to the baselines. Speciﬁcally, with both Mask RCNN and Cascade Mask RCNN, our pre-trained model outperforms the baselines with respect to nearly all detection/segmentation metrics. When pre-training with a smaller dataset (IN-100), the relative improvement is even higher (Tab. 11). Table 10: ImageNet-1K pre-training. Results on the COCO object detection and instance segmentation tasks. APbox x and APmask x are the standard object detection and segmentation Average Precision metrics, respectively [39]. Architecture Pre-trained backbone APbox APbox 50 APbox 75 APmask APmask 50 APmask 75 Mask RCNN Swin-T 43.4 66.2 47.4 39.6 63.0 42.6 43.8 66.5 48.0 39.7 63.1 42.5Swin-T + Ldrloc (+0.4) (+0.3) (+0.6) (+0.1) (+0.1) (-0.1) Cascade Mask RCNN Swin-T 48.0 67.1 51.7 41.5 64.3 44.8 48.2 67.4 52.1 41.7 64.7 44.8Swin-T + Ldrloc (+0.2) (+0.3) (+0.4) (+0.2) (+0.4) (+0.0) Table 11: IN-100 pre-training. Results on the COCO object detection and instance segmentation tasks. Architecture Pre-trained backbone APbox APbox 50 APbox 75 APmask APmask 50 APmask 75 Mask RCNN Swin-T 41.8 60.3 45.1 36.7 57.4 39.4 42.7 61.3 45.9 37.2 58.4 40.0Swin-T + Ldrloc (+0.9) (+1.0) (+0.8) (+1.0) (+1.0) (+0.6) Cascade Mask RCNN Swin-T 36.0 58.2 38.6 33.8 55.2 35.9 37.2 59.4 40.3 34.5 56.2 36.6Swin-T + Ldrloc (+1.2) (+1.2) (+1.7) (+0.7) (+1.0) (+0.7) D.2 Semantic segmentation Setup. We again follow the experimental settings adopted in Swin [40]. Speciﬁcally, for the semantic segmentation experiments, we use the ADE20K dataset [75], which is composed of 150 semantic categories, and contains 20K training, 2K validation and 3K testing images. Following [40], we use the popular UperNet [67] architecture with a Swin backbone pre-trained either on ImageNet-1K or on IN-100 (see above). We use the implementation released by mmcv [14] to train and evaluate all the models. When ﬁne-tuning, we used the AdamW [ 41] optimizer with an initial learning rate of 6 ×10−5, a weight decay of 0.01, a scheduler with linear learning-rate decay, and a linear warmup of 1,500 iterations. We ﬁne-tuned all the models on 8 Nvidia V100 32GB GPUs with 2 images per GPU for 160K iterations. We adopt the default data augumentation techniques used for segmentation, namely random horizontal ﬂipping, random re-scaling with a [0.5, 2.0] ratio range and random photometric distortion. We use stochastic depth with ratio 0.2 for all the models, which are trained with an input of 512×512 pixels. At inference time, we use a multi-scale testing, with image resolutions which are {0.5,0.75,1.0,1.25,1.5,1.75}×of the training resolution. Results. The results reported in Tab. 12 show that the models pre-trained on ImageNet-1K with the proposed loss always outperform the baselines with respect to all the segmentation metrics. Similarly to Sec. D.1, when a smaller dataset is used for pre-training (IN-100), the observed relative boost is even higher (Tab. 13). 18Table 12: ImageNet-1K pre-training. Semantic segmentation on the ADE20K dataset (testing on the validation set). mIoU and mAcc refer to the mean Intersection over Union and the mean class Accuracy, respectively. The base architecture is UperNet [67]. Pre-trained backbone mIoU mAcc Swin-T 43.87 55.22 44.33 55.74Swin-T + Ldrloc (+0.46) (+0.52) Table 13: IN-100 pre-training. Semantic segmentation on the ADE20K dataset (testing on the validation set) with a UperNet architecture [67]. Pre-trained backbone mIoU mAcc Swin-T 36.93 47.76 37.83 48.69Swin-T + Ldrloc (+0.90) (+0.93) E Training efﬁciency In Fig. 3 we show the training curves corresponding to the top-1 accuracy of CvT, Swin and T2T, trained from scratch on CIFAR-100, with or without our loss. These graphs show that our auxiliary task is beneﬁcial over the whole training stage, and it can speed-up the overall training. For instance, in case of Swin, after 60 training epochs, or method is already signiﬁcantly better than the baseline full-trained with 100 epochs (55.01 versus 53.28). 0 25 50 75 100 Epochs 0 20 40 60T op-1 Acc. CvT-13 0 25 50 75 100 Epochs Swin-T 0 25 50 75 100 Epochs T2T-ViT-14 Baseline Baseline + drloc Figure 3: CIFAR-100, training from scratch, top-1 accuracy measured every 10 epochs. Finally, we compute the overhead of Ldrloc at training time. The results reported in Tab. 14 refer to seconds per batch (with a batch size equal to 1024), and show that, overall, the overhead due to our auxiliary task is negligible with respect to the whole training time. F Implementation details and an additional ablation study on the localization MLP Our localization MLP (f) is a simple feed-forward network composed of three fully connected layers. The ﬁrst layer projects the concatenation of the two input token embeddings ei,j and ep,h into a 512-dimensional vector and then it applies aRelu activation. Next, we use a linear layer of dimension 512 followed by a Relu activation. Finally, we use a linear layer dedicated to the prediction, which depends on the speciﬁc loss variant, see Sec. B. For instance, in Ldrloc, the last layer is composed of 19Table 14: Training time comparison on CIFAR-100. The values are averaged over all training batches and jointly reported the corresponding standard deviations. Model Seconds per batch CvT-13 0.6037 ±0.0040 CvT-13 + Ldrloc 0.6184 ±0.0070 (+2.43%) Swin-T 0.6684 ±0.0031 Swin-T + Ldrloc 0.6842 ±0.0033 (+2.36%) T2T-ViT-14 0.5941 ±0.0053 T2T-ViT-14 +Ldrloc 0.6046 ±0.0058 (+1.77%) two neurons which predict du and dv. The details of the MLP head are shown in Tab. 15, while in Tab. 16 we show the inﬂuence of the number of neurons in the hidden layers of f. Table 15: The details of the localization MLP head. dis the dimension of a token embedding. The number of outputs oand the ﬁnal nonlinearity (if used) depend on the speciﬁc loss. In Ldrloc, L∗ drloc and Lall drloc, we use o= 2without any nonlinearity. Converesely, in bothLce drloc and Lreg drloc, the last layer is split in two branches of 2k+ 1neurons each, and, on each branch, we separately apply a SoftMax layer. Layer Activation Output dimension Input - d* 2 Linear ReLU 512 Linear ReLU 512 Linear - / SoftMax o Table 16: CIFAR-100, 100 epochs, training from scratch: the inﬂuence of the number of neurons used in each of the two hidden layers of the localization MLP. Model Number of neurons 256 512 1024 CvT-13 + Ldrloc 74.19 74.51 73.80 Swin-T + Ldrloc 65.06 66.23 64.33 T2T-ViT-14 +Ldrloc 66.49 68.03 67.83 In our experiments, we used the ofﬁcially released framework of Swin [40]2, which also provides all the necessary code to train and test VT networks (including the object detection and segmentation tasks of Sec. D). For a fair comparison, we use the ofﬁcial code of T2T-ViT [ 70]3 and a publicly released code of CvT [66]4 and we insert them in the training framework released by the authors of Swin. At submission time of this paper, the ofﬁcial code of CvT [66] was not publicly available. Finally, the ViT-B/16 model used in Sec. C is based on a public code4. When we train the networks from scratch (100 epochs), we use the AdamW [ 41] optimizer with a cosine decay learning-rate scheduler and 20 epochs of linear warm-up. We use a batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05. When we ﬁne-tune the networks (100 epochs), we use the AdamW [41] optimizer with a cosine decay learning-rate scheduler and 10 epochs of linear warm-up. We use a batch size of 1024, an initial learning rate of 0.0005, and a weight decay of 0.05. In all the experiments, the images of all the datasets are resized to the same ﬁxed resolution (224 ×224). 2https://github.com/microsoft/Swin-Transformer 3https://github.com/yitu-opensource/T2T-ViT 4https://github.com/lucidrains/vit-pytorch 20",
      "references": [
        "Self-labelling via simultaneous clustering and representation learning",
        "VICReg: Variance-invariance-covariance regularization for self-supervised learning",
        "CliqueCNN: deep unsupervised exemplar learning",
        "Cascade R-CNN: Delving into high quality object detection",
        "End-to-end object detection with transformers",
        "Deep clustering for unsupervised learning of visual features",
        "Unsupervised learning of visual features by contrasting cluster assignments",
        "Emerging properties in self-supervised vision transformers",
        "Generative pretraining from pixels",
        "A simple framework for contrastive learning of visual representations",
        "Exploring simple siamese representation learning",
        "An empirical study of training self-supervised vision transformers",
        "ELECTRA: Pre-training text encoders as discriminators rather than generators",
        "Openmmlab foundational library for computer vision research",
        "Multi-task learning with deep neural networks: A survey",
        "UP-DETR: unsupervised pre-training for object detection with transformers",
        "BERT: Pre-training of deep bidirectional transformers for language understanding",
        "Unsupervised visual representation learning by context prediction",
        "An image is worth 16x16 words: Transformers for image recognition at scale",
        "Temporal cycle-consistency learning",
        "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations",
        "Whitening for self-supervised representation learning",
        "SCAN: learning to classify images without labels",
        "Unsupervised representation learning by predicting image rotations",
        "Bootstrap your own latent: A new approach to self-supervised learning",
        "Momentum contrast for unsupervised visual representation learning",
        "Mask R-CNN",
        "Deep residual learning for image recognition",
        "Learning deep representations by mutual information estimation and maximization",
        "Local relation networks for image recognition",
        "On feature decorrelation in self-supervised learning",
        "Generative Adversarial Transformers",
        "Invariant information clustering for unsupervised image classification and segmentation",
        "TransGAN: Two transformers can make one strong GAN",
        "Learning multiple layers of features from tiny images",
        "Cornernet: Detecting objects as paired keypoints",
        "LocalViT: Bringing locality to vision transformers",
        "Detnet: Design backbone for object detection",
        "Microsoft COCO: Common objects in context",
        "Swin transformer: Hierarchical vision transformer using shifted windows",
        "Decoupled weight decay regularization",
        "TrackFormer: Multi-object tracking with transformers",
        "Efficient estimation of word representations in vector space",
        "Distributed representations of words and phrases and their compositionality",
        "Self-supervised learning of pretext-invariant representations",
        "Intriguing properties of vision transformers",
        "Reading digits in natural images with unsupervised feature learning",
        "Automated flower classification over a large number of classes",
        "Unsupervised learning of visual representations by solving jigsaw puzzles",
        "Representation learning by learning to count",
        "Moment matching for multi-source domain adaptation",
        "Improving language understanding by generative pre-training",
        "Exploring the limits of transfer learning with a unified text-to-text transformer",
        "Do vision transformers see like convolutional neural networks?",
        "Imagenet large scale visual recognition challenge",
        "Self-attention with relative position representations",
        "Segmenter: Transformer for semantic segmentation",
        "VideoBERT: A joint model for video and language representation learning",
        "Sparse R-CNN: End-to-end object detection with learnable proposals",
        "Contrastive multiview coding",
        "Training data-efficient image transformers & distillation through attention",
        "Representation learning with contrastive predictive coding",
        "Attention is all you need",
        "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
        "Non-local neural networks",
        "CvT: Introducing convolutions to vision transformers",
        "Unified perceptual parsing for scene understanding",
        "Co-scale conv-attentional image transformers",
        "Incorporating convolution designs into visual transformers",
        "Tokens-to-token ViT: Training vision transformers from scratch on ImageNet",
        "CutMix: Regularization strategy to train strong classifiers with localizable features",
        "Barlow twins: Self-supervised learning via redundancy reduction",
        "mixup: Beyond empirical risk minimization",
        "Point transformer",
        "Semantic understanding of scenes through the ade20k dataset",
        "Deformable DETR: Deformable transformers for end-to-end object detection",
        "Local aggregation for unsupervised learning of visual embeddings"
      ],
      "meta_data": {
        "arxiv_id": "2106.03746v2",
        "authors": [
          "Yahui Liu",
          "Enver Sangineto",
          "Wei Bi",
          "Nicu Sebe",
          "Bruno Lepri",
          "Marco De Nadai"
        ],
        "published_date": "2021-06-07T16:14:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "1) Provides the first systematic study of how three second-generation visual transformers (T2T-ViT, Swin, CvT) behave when training data are scarce, revealing large accuracy gaps that are hidden on ImageNet-1K. 2) Introduces a lightweight, architecture-agnostic self-supervised auxiliary loss – Dense Relative Localization (Ldrloc) – that forces a VT to predict the relative spatial offsets between randomly chosen pairs of token embeddings. 3) Demonstrates that adding Ldrloc consistently and sometimes dramatically improves accuracy (up to +45 pp) when training from scratch or fine-tuning, with negligible computational overhead, and that it can transfer to detection and segmentation tasks.",
        "methodology": "For each input image the final k×k token grid is obtained from the VT backbone (pooled to 7×7 when necessary). m random pairs of embeddings are concatenated and passed through a small 3-layer MLP that regresses the normalized horizontal/vertical distances between their grid positions. The L1 regression loss Ldrloc is averaged over the pairs and weighted (λ≈0.1–0.5) before being added to the standard cross-entropy. No change to the original VT architecture is required. Hyper-parameters are fixed across models and datasets. Several loss variants and ablations on m, λ, and MLP size are reported.",
        "experimental_setup": "Training-from-scratch: 100/300 epochs on ImageNet-100, CIFAR-10/100, SVHN, Oxford Flowers102, and six DomainNet subsets; comparison with ResNet-50. Fine-tuning: pretrained on ImageNet-1K then fine-tuned (100 epochs) on the same target datasets. Additional transfer: object detection and instance/semantic segmentation on COCO (Mask RCNN, Cascade Mask RCNN) and ADE20K (UperNet) using a Swin backbone. All images resized to 224×224; batch size 1024; AdamW optimizer with cosine decay; experiments run on 8×V100 32 GB GPUs. Metrics: top-1 accuracy for classification, APbox/APmask for detection, mIoU/mAcc for segmentation.",
        "limitations": "• Auxiliary loss convergence degrades on finer (>7×7) token grids; underlying cause not fully analyzed. • Study focuses on medium-size models (~25 M parameters); evidence on large-scale VTs is limited. • Evaluation centred on 2-D natural images; effectiveness on other modalities (video, 3-D, medical) untested. • Hyper-parameters (m, λ, grid pooling) are fixed empirically; no automated tuning or theoretical guidance is provided. • While performance gains are large, the method adds a secondary MLP and extra memory during training.",
        "future_research_directions": "1) Investigate ways to make dense localization effective on high-resolution embedding grids and earlier transformer layers. 2) Extend and test the approach on larger/backbone models, multi-scale ViTs, and multimodal or video transformers. 3) Combine Ldrloc with other self-supervised objectives (contrastive, masked patch) in a unified multi-task framework. 4) Develop theoretical analysis of why spatial-relation prediction improves data-efficiency in transformers. 5) Explore adaptive or learned weighting of the auxiliary loss and automatic selection of pair sampling strategies.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Label-invariant Augmentation for Semi-Supervised Graph Classification",
      "full_text": "LABEL -INVARIANT AUGMENTATION FOR SEMI -SUPERVISED GRAPH CLASSIFICATION Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu Brandeis University {hanyue, chunhuizhang, chuxuzhang, hongfuliu}@brandeis.edu ABSTRACT Recently, contrastiveness-based augmentation surges a new climax in the computer vision domain, where some operations, including rotation, crop, and ﬂip, combined with dedicated algorithms, dramatically increase the model generalization and robustness. Following this trend, some pioneering attempts employ the similar idea to graph data. Nevertheless, unlike images, it is much more difﬁcult to design reasonable augmentations without changing the nature of graphs. Although exciting, the current graph contrastive learning does not achieve as promising performance as visual contrastive learning. We conjecture the current performance of graph contrastive learning might be limited by the violation of the label-invariant augmentation assumption. In light of this, we propose a label-invariant augmentation for graph-structured data to address this challenge. Different from the node/edge modiﬁcation and subgraph extraction, we conduct the augmentation in the representation space and generate the augmented samples in the most difﬁcult direction while keeping the label of augmented data the same as the original samples. In the semi-supervised scenario, we demonstrate our proposed method outperforms the classical graph neural network based methods and recent graph contrastive learning on eight benchmark graph-structured data, followed by several in-depth experiments to further explore the label-invariant augmentation in several aspects. Keywords Graph Contrastive Learning ·Semi-supervised Classiﬁcation 1 Introduction Contrastive augmentation aims to expand training data in both volume and diversity in a self-supervised fashion to increase model robustness and generalization. Common sense and domain knowledge are employed to design the contrastive augmentation operations. Denoising auto-encoder [ 1, 2] is one of the pioneering studies to apply perturbations to generate contrastive samples for tablet data, which takes a corrupted input and recovers the original undistorted input. For visual data, some operations, including rotation, crop, and ﬂip, combined with dedicated algorithms, signiﬁcantly improve the learning performance in diverse tasks [ 3, 4, 5, 6, 7]. Treating the augmented and original samples as positive pairs and the augmented samples from different source samples as negative pairs, contrastive learning aims to learn the augment-invariant representations by increasing the similarity of positive pairs and the dissimilarity of negative pairs [8]. These positive pairs increase the model robustness due to the assumption that the augmented operations preserve the nature of images and make the augmented samples have consistent labels with the original ones. The negative pairs work as the instance-level discrimination, which is expected to enhance the model generalization, but might deteriorate the downstream task since the negative pairs contain the augmented samples from different source samples but with the same category. The recent BYOL [9] and SimSiam [10] demonstrate the negative effect of the negative pairs and conclude that the current performance of contrastive learning can be further boosted even without negative pairs. Following this trend, some pioneering attempts employ contrastive augmentation to graph data. GraphCL [11] is the ﬁrst work to address the graph contrastive learning problem with four types of augmentations, including node dropping, edge perturbation, attribute masking, and subgraph extraction. Later, JOAO [12] extends GraphCL by automatically selecting one type of graph augmentation from the above four types plus non-augmentation. GRACE [13] treats the original graph data and the novel-level augmented data as two views and learns the graph representation by maximizing arXiv:2205.09802v1  [cs.CV]  19 May 2022Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation the agreement between the two views. Similarly, MVGRL [ 14] conducts the contrastive multi-view representation learning on both node and graph levels. Beyond the above studies to augment graphs, simGRACE [15] perturbs the model parameters for contrastive learning, which can be regarded as an ensemble of model perturbation or a robust regularization. Although exciting, the above studies point out that the effectiveness of graph contrastive learning heavily hinges on ad-hoc data augmentations, which need to be carefully designed or selected per dataset and request more domain knowledge. Contributions. We conjecture these hand-crafted graph augmentations might change the nature of the original graph and violate the label-invariant assumption in the downstream tasks. Different from treating graph contrastive learning in a pre-trained perspective, we aim to incorporate the downstream classiﬁcation task into the representation learning, where the label information is fully used for both decision boundary learning and graph augmentation. Speciﬁcally, we propose Graph Label-invariant Augmentation (GLA), which conducts augmentation in the representation space and augments the most difﬁcult sample while keeping the label of the augmented sample the same as the original sample. Our major contributions are summarized as follows: • We propose a label-invariant augmentation strategy for graph contrastive learning, which involves labels in the downstream task to guide the contrastive augmentation. It is worthy to note that we do not generate any graph data. Instead, we directly generate the label-consistent representations as the augmented graphs during the training phase. • In the rich representation space, we aim to generate the most difﬁcult sample for the model and increase the model generalization. Rather than formulating it as an expensive bi-level optimization problem, we choose a lightweight technique by randomly generating a set of qualiﬁed candidates and selecting the most difﬁcult one, i.e., minimizing the maximum loss or worst case loss over the augmented candidates. • We conduct a series of semi-supervised experiments on eight graph benchmark datasets in a fair setting and compare our label-invariant augmentation with classical graph neural network based methods and recent graph contrastive methods by running the codes provided by the original authors. Extensive results demonstrate our label-invariant augmentation can achieve better performance in general cases without generating real augmented graphs and any speciﬁc domain knowledge. Besides algorithmic performance, we also provide rich and in-depth experiments to explore label-invariant augmentation in several aspects. 2 Related Work Here we introduce the related work of graph neural networks and graph contrastive learning for graph classiﬁcation. Node classiﬁcation, although related, is not covered here due to its different setting. Graph Neural Network. Graph Neural Networks (GNNs) have been employed on various graph learning tasks and achieved promising performance [16]. To extract the representation of each node, GNNs pass node embeddings from its connected neighbor nodes and apply feedforward neural networks to transform the aggregated features. As a pioneer study in GNNs, graph convolutional network (GCN) ﬁrstly aims to generalize the convolution mechanism from image to graph [16, 17, 18]. Based on GCN, instead of simply summing and averaging connected neighboring node’s embedding, graph attention networks [19, 20, 21] adopt an attention mechanism that builds self-attention to score each connected neighboring nodes’ embedding to identify the more important nodes and enhance the effectiveness of message passing. Then in order to break prior GNN’s limitations on message passing over long distances on large graphs, graph recurrent neural networks [22, 23] apply the gating mechanism from RNNs to propagation on graph topology. Simultaneously, for dealing with the noise introduced from more than 3 layers of graph convolution, DeepGCN [ 24, 25] uses skip connections and enables GCN to achieve better results with deeper layers. Recently, GAE [ 26] and Infomax [ 27] achieve state-of-the-art performance on several benchmark datasets. GAE extends the variational auto-encoder to graph neural networks for unsupervised learning, while Infomax learns the unsupervised representation on graphs to enlarge mutual information between local (node-level) and global (graph-level) representations in one graph. Graph Contrastive Learning. Recently, many studies has been devoted to the graph contrastive learning area in diverse angles, including graph augmentation, negative sample selection, and view fusion. GraphCL [11] summarizes four types of graph augmentations to learn the invariant representation across different augmented views. Built on GraphCL, JOAO [12] proposes a learnable module to automatically select augmentation for different datasets to alleviate the human labor in combinations of these augmentations. Differently, MVGRL [14] contrasts node and graph encodings across views which enriches more negative samples for contrastive learning. Later, InfoGCL [ 28] diminishes the mutual information between contrastive parts among views while preserving the task-relevant representation. Beyond augmenting graphs, SimGRACE [15] disturbs the model weights and then learns the invariant high-level representation at the output end to alleviate the design of graph augmentation. 2Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation Different from the above methods that separate the pre-train and ﬁne-tuning phases, we aim to employ the label information in downstream tasks to guide the augmentation process. Speciﬁcally, in this study, we propose a label- invariant augmentation strategy for graph-level representation learning. 3 Methodology A graph can be represented by G= (V,X,A ), where V = {v1,v2,...,v n}is the set of vertexes, X ∈Rn×d denotes the features of each vertex, and A ∈ {0,1}n×n represents the adjacency matrix. Given a set of labeled graphs S= {(G1,y1),(G2,y2),..., (GM,yM)}where M is the number of labeled graphs, and yi ∈Y is the corresponding categorical label of graph Gi ∈G (1≤i≤M), and another set of unlabeled graphs T = {GM+1,...,G N}, where N is the number of all graphs, M<N, the semi-supervised graph classiﬁcation problem can be deﬁned as learning a mapping function from graphs to categorical labels f : G→Y to predict the labels of T. In this section, we ﬁrst illustrate our motivation supported by empirical evidence, then we elaborate on our Graph Label-invariant Augmentation (GLA) method for semi-supervised graph classiﬁcation. 3.1 Motivation Figure 1: Performance gains (%) of GraphCL and JOAOv2 under different augmentation settings on MUTAG [29] dataset compared to none augmentation setting. Augmentation plays an important role in neural network training. It not only improves the robustness of learned representation but also introduces rich data for training. For graph-structured data, GraphCL [11] proposes four types of augmentations: node dropping, edge perturba- tion, attribute masking, and subgraph sampling. How- ever, it is widely noticed that the effectiveness of graph contrastive learning highly depends on the chosen types of augmentations for speciﬁc graph data [ 12, 12]. To illustrate the difference between various augmentation combinations, we conduct experiments on MUTAG [29] in a semi-supervised graph classiﬁcation task, where the label rate is set to 50%. Figure 1 shows the performance gains (classiﬁcation accuracy %) of different augmenta- tion combinations compared to none augmentation. We can see that different augmentation combinations result in different performances, and JOAOv2 automatically selects data augmentations but cannot guarantee to outperform GraphCL with all augmentation combinations. Moreover, some augmentation combinations work worse than none augmentation, which demonstrates that some augmentations hurt the model training. We further ﬁne-tune a model with 100% labeled graphs from MUTAG, and then feed the augmented graphs (randomly selected from the four augmentation types with an augmentation ratio of 0.2) to this model, ﬁnding that about 80% augmented graphs have the same labels with their corresponding original graphs. It indicates that most augmentations are reasonable, which is one of the reasons that GraphCL works well. While on the other hand, there are still about 20% augmented graphs getting different labels from the original ones. Motivated by this, we design a label-invariant augmentation strategy for graph contrastive learning. 3.2 Label-invariant Augmentation Framework Overview. Figure 2 illustrates the framework of our proposed Graph Label-invariant Augmentation (GLA) for semi-supervised graph classiﬁcation, which mainly consists of four components: Graph Neural Network Encoder, Classiﬁer, Label-invariant Augmentation, and Projection Head. We ﬁrst use GNN Encoder to get graph-level original representation for the input graph. Then Label-invariant Augmentation, together with Classiﬁer, is utilized to generate augmented representation from original representation under a label-invariant constraint. For an unlabeled graph, we expect that the labels represented by original prediction and augmented prediction are the same. For a labeled graph, we expect that the label represented by augmented prediction is the same as the ground truth label. A cross-entropy loss is used to keep reﬁning the classiﬁer with labeled graphs. Finally, a Projection Head is adopted to generate projections for contrastive loss. We use Θ = {θG,θC,θP }to denote the trainable parameters set, where θG, θC, and θP denote the parameters of GNN Encoder, Classiﬁer and Projection Head, respectively. Details of each component are as follows. Graph Neural Network Encoder. Graph Neural Network Encoder aims to get graph-level representations for graph- structured data. It is ﬂexible to adopt various GNNs for this part. We follow GraphCL [11] and utilize ResGCN [30], which takes Graph Convolutional Network (GCN) [31] as the backbone, to extract node-level representations from the input graph, and then a global sum pooling layer is used to obtain its graph-level representation. The computation of the 3Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation Projection  Head GNN  Encoder Classifier Maximizing Agreement Label-invariant Augmentation Input Graph … Augmented Representations Figure 2: Framework of our Graph Label-invariant Augmentation (GLA) for semi-supervised graph classiﬁcation. Given an input graph, a Graph Neural Network (GNN) Encoder is employed to encode the input graph into a graph- level representation (original representation). Then we perturb the original representation to get multiple augmented representations. A classiﬁer is adopted to verify whether the augmentations are label-invariant or not. We select the “hardest” augmented representation, i.e., the one that has the least probability of belonging to the same class as original representation, from all augmented representations that satisfy the label-invariant constraint. On top of GNN Encoder, we build a projection head to get projections for both original representation and label-invariant augmented representation. We maximize the agreement between projections via a contrastive loss for all graphs and reﬁne the classiﬁer via a cross-entropy loss with labeled graphs. GCN layer with the parameter θG is described as follows: G(l+1) = σ( ˜D−1 2 ˜A˜D−1 2 G(l)θ(l) G ), (1) where ˜A= A+ In is the adjacency matrix Awith added self-connections, In ∈Rn×n is the identity matrix, ˜Dis the degree matrix of ˜A, and θ(l) G is a layer-speciﬁc trainable weight matrix. G(l) denotes the matrix in the l-th layer, and G(0) = X. We employ σ(·) = ReLU(·) as the activation function. Then on top of the ResGCN, we use a global sum pooling layer to get graph-level representations from node-level representations as follows: H = Pooling(G). (2) Here we use HO to denote the original representation of the input graph andHA to denote the augmented representation of the augmented graph. The augmentation method will be described in the Label-invariant Augmentation part. Classiﬁer. Based on the graph-level representations, we employ fully-connected layers with the parameter θC for prediction: C(l+1) = Softmax(σ(C(l) ·θ(l) C )), (3) where C(l) denotes the embeddings in the l-th layer, and the input layer C(0) = HO or C(0) = HA for the original representation and augmented representation, respectively. In our experiments, we adopt a 2-layer multilayer perceptron and obtain predictions CO and CA for original representation HO and augmented representation HA. Label-invariant Augmentation. Instead of augmenting graph data by node dropping, edge perturbation, attribute masking, or subgraph sampling as recent graph contrastive learning methods [11, 12], we conduct the augmentation in the representation space by adding a perturbation to the original representation HO so that we do not need to generate any graph data. In our experiment, we ﬁrst calculate the centroid of original representations for all graphs and get the average value of euclidean distances between each original representation and the centroid as d, that is: d= 1 N N∑ i=1 ∥HO i − 1 N N∑ j=1 HO j ∥. (4) Then the augmented representation HA is calculated by: HA = HO + ηd∆, (5) where ηscales the magnitude of the perturbation, and ∆ is a random unit vector. To achieve the label-invariant augmentation, each time, we randomly generate multiple perturbations and select the qualiﬁed augmentation candidates that obey the label-invariant property. Among these qualiﬁed candidates, we choose the most difﬁcult one, i.e., the one closest to the decision boundary of the classiﬁer, to increase the model generalization ability. 4Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation Table 1: Statistics of datasets for semi-supervised graph classiﬁcation Datasets Category #Class #Graph Avg. #Node Avg. #Edge MUTAG Biochemical Molecules 2 188 17.93 19.79 PROTEINS Biochemical Molecules 2 1113 39.06 72.82 DD Biochemical Molecules 2 1178 284.32 715.66 NCI1 Biochemical Molecules 2 4110 29.87 32.30 COLLAB Social Networks 3 5000 74.49 2457.78 RDT-B Social Networks 2 2000 429.63 497.75 RDT-M5K Social Networks 5 4999 508.52 594.87 GITHUB Social Networks 2 12725 113.79 234.64 Projection Head. We employ fully-connected layers with the parameter θP to get projections for contrastive learning from graph-level representations, which is shown as: P(l+1) = σ(P(l) ·θ(l) P ). (6) We adopt a 2-layer multilayer perceptron and get projections PO and PA from original representation HO and augmented representation HA. Objective Function. Our objective function consists of contrastive loss and classiﬁcation loss. For contrastive loss, we utilize the normalized temperature-scaled cross-entropy loss (NT-Xent) [11] but only keep the positive-pair part as follows: LP = −(PO)⊤PA ∥PO∥∥PA∥. (7) Maximizing the agreement between original projection and augmented projection would increase the robustness of the model. For classiﬁcation loss, we adopt cross-entropy, which is deﬁned as: LC = − c∑ i=1 (YO i log PO i + YO i log PA i ), (8) where YO is the label of the input graph, and cis the number of graph categories. We only calculate LC for labeled graphs. The improvement of the classiﬁer would help with the label-invariant augmentation, which in turn beneﬁts the training of the classiﬁer. Combining Eq. (7) and (8), our overall objective function can be written as follows: minΘ LP + αLC, (9) where αis a trade-off hyperparameter to balance the contrastive loss and classiﬁcation loss. Discussion. From the perspective of information usage for model training, our proposed method is the same as the semi-supervised learning task by recent graph contrastive learning methods [ 11, 12, 14, 15], which use structure information of all graphs and label information of a subset of all graphs for model training. From the perspective of training strategy, the previous methods ﬁrst pre-train a model via a contrastive loss and then ﬁne-tune the model for downstream tasks. While our proposed method focuses on semi-supervised classiﬁcation, we merge the pre-train and ﬁne-tuning phases into one integrated phase. During our training phase, the augmented samples increase the model robustness and generalization ability, and the classiﬁer helps to generate better augmented samples, which in turn beneﬁts classiﬁcation performance. 4 Experiments In this section, we ﬁrst describe our semi-supervised settings of experiments and baseline methods for comparison. Then we show the algorithmic performance of these methods on eight graph benchmark datasets in a fair setting. Finally, we provide some insightful experiments to demonstrate the effectiveness of the proposed Graph Label-invariant Augmentation (GLA) method. 4.1 Experiment Settings Datasets. We select eight public graph classiﬁcation benchmark datasets from TUDataset [32] for evaluation, including MUTAG [29], PROTEINS [33], DD [34], NCI1 [35], COLLAB [36], RDT-B [36], RDT-M5K [36], and GITHUB [37]. 5Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation Table 2: Semi-supervised graph classiﬁcation results (Accuracy % ±Standard Deviation %) on eight benchmark datasets. The best and second-best results are highlighted in red and blue, respectively. Label Methods MUTAG PROTEINS DD NCI1 COLLAB RDT-B RDT-M5K GITHUB Avg. Rank GAE 83.63±0.81 74.31±0.33 77.33±0.36 77.20±0.22 77.46±0.11 90.75±0.17 54.81±0.18 65.22±0.11 75.09 5.00 Infomax 84.68±1.12 74.84±0.28 77.07±0.45 79.49±0.17 77.30±0.19 90.65±0.17 55.37±0.20 66.45±0.06 75.73 4.25 MVGRL 83.16±0.98 75.56±0.44 77.08±0.56 72.41±0.18 75.28±0.12 88.20±0.16 53.16±0.06 64.71±0.04 73.70 6.12 30% SimGRACE 83.68±0.84 74.38±0.30 76.27±0.38 78.52±0.17 78.66±0.24 90.60±0.17 55.54±0.16 66.81±0.14 75.56 4.50 GraphCL 85.20±0.98 74.12±0.30 78.60±0.37 79.22±0.09 77.90±0.20 90.35±0.18 56.07±0.15 67.63±0.13 76.14 3.38 JOAOv2 85.67±0.91 75.02±0.30 77.16±0.30 78.69±0.18 79.88±0.17 91.65±0.15 55.23±0.14 67.96±0.10 76.41 2.62 GLA (Ours) 86.32±1.25 75.65±0.37 77.49±0.40 79.71±0.13 78.78±0.12 91.05±0.25 55.85±0.22 65.16±0.19 76.25 2.12 GAE 84.12±0.90 74.75±0.38 78.35±0.31 79.56±0.16 80.47±0.14 90.95±0.19 55.69±0.16 67.09±0.13 76.37 5.88 Infomax 87.37±1.11 75.38±0.38 78.26±0.38 80.80±0.13 79.70±0.11 91.50±0.26 56.51±0.18 67.70±0.09 77.15 3.75 MVGRL 85.79±0.23 76.72±0.34 78.60±0.46 74.09±0.10 76.08±0.05 88.55±0.06 54.04±0.06 64.89±0.05 74.84 5.62 50% SimGRACE 86.32±0.88 75.09±0.35 78.39±0.35 79.78±0.24 80.48±0.15 91.45±0.16 56.50±0.20 67.71±0.16 76.97 4.50 GraphCL 87.28±0.71 75.29±0.29 78.73±0.46 80.17±0.19 80.40±0.16 91.45±0.25 56.83±0.19 68.71±0.09 77.36 3.25 JOAOv2 86.78±0.79 75.74±0.29 78.52±0.45 80.10±0.17 81.50±0.18 92.10±0.18 56.51±0.17 68.97±0.11 77.53 2.75 GLA (Ours) 90.00±0.94 76.19±0.28 80.22±0.37 80.66±0.28 80.84±0.12 91.65±0.22 56.63±0.13 66.59±0.14 77.85 2.25 GAE 87.31±0.66 75.47±0.38 79.37±0.36 79.78±0.17 80.78±0.12 91.50±0.19 56.25±0.16 68.42±0.14 77.36 5.62 Infomax 88.33±0.73 75.92±0.38 79.28±0.33 82.85±0.16 81.04±0.12 92.15±0.13 56.63±0.18 68.88±0.14 78.14 3.62 MVGRL 87.95±0.35 77.81±0.35 79.51±0.34 74.43±0.08 76.42±0.08 88.65±0.23 54.40±0.11 65.00±0.08 75.52 5.25 70% SimGRACE 87.37±0.71 76.52±0.36 78.90±0.29 81.80±0.15 81.88±0.23 92.45±0.13 56.58±0.09 68.19±0.15 77.96 4.12 GraphCL 88.33±0.86 76.36±0.25 79.03±0.29 82.50±0.13 81.08±0.17 91.85±0.14 56.91±0.17 69.19±0.08 78.16 3.62 JOAOv2 87.78±0.76 76.46±0.27 79.11±0.38 81.70±0.26 82.16±0.17 92.20±0.19 56.67±0.16 69.96±0.11 78.26 3.25 GLA (Ours) 91.05±0.86 77.45±0.38 80.71±0.29 83.24±0.14 81.54±0.14 91.70±0.17 57.01±0.14 67.11±0.18 78.73 2.50 Table 1 shows the statistics of these datasets. The ﬁrst four datasets include biochemical molecules and proteins, and the last four datasets are about social networks. The numbers of graphs in these datasets range from 188 to 12725, the average node numbers range from 17.93 to 508.52, and the average edge numbers are from 19.79 to 2457.78, indicating the diversity of these datasets. Compared Methods and Implementation . We choose two heuristic self-supervised methods, GAE [ 26] and In- fomax [27], and four recent graph contrastive learning methods, MVGRL [ 14], GraphCL [11], JOAOv2 [12], and SimGRACE [15], for comparison on semi-supervised graph classiﬁcation task. GAE performs adjacency matrix recon- struction by using a graph convolutional network (GCN) [31] encoder and a simple inner product decoder. Infomax is based on global-local representation consistency enforcement, which maximizes the mutual information between global and local representation. MVGRL proposes to learn node and graph level representations by node diffusion and contrasting encodings. GraphCL presents four types of graph augmentations. Based on GraphCL, JOAOv2 is designed as a uniﬁed bi-level optimization framework to automatically select graph augmentations. SimGRACE perturbs parameters of graph encoder for contrastive learning, which does not require data augmentations. For GAE, Infomax, and GraphCL, we adopt the implementations and default hyperparameter settings provided by the source codes of GraphCL [11]. For other compared methods, we follow the implementations and hyperparameter settings in their corresponding source codes. The compared methods are pre-trained ﬁrst and then are ﬁne-tuned for the semi-supervised graph classiﬁcation task. For our proposed Graph Label-invariant Augmentation (GLA) method, we perform contrastive learning and graph classiﬁer learning synchronously. The implementation details of GLA are as follows. We implement the networks based on GraphCL [11] by PyTorch, set the magnitude of perturbation ηto 1.0, and the weight of classiﬁcation loss αto 1.0, which is the same with GraphCL. We adopt Adam optimizer [38] to minimize the objective function in Eq. (9). Evaluation Protocol. We evaluate the models with 10-fold cross-validation. We randomly shufﬂe a dataset and then evenly split it into 10 parts. Each fold corresponds to one part of data as the test set and another part as the validation set to select the best epoch, where the rest folds are used for training. We select 30%, 50%, 70% graphs from the training set as labeled graphs for each fold, then conduct semi-supervised learning. For a fair comparison, we use the same training/validation/test splits for all compared methods on each dataset. Semi-supervised graph classiﬁcation results are reported by the average accuracy across 10 folds and their standard deviations. 4.2 Algorithmic Performance Table 2 shows the prediction results of two self-supervised and ﬁve graph contrastive learning methods under the semi-supervised graph classiﬁcation setting with 30%, 50%, and 70% label ratios on eight benchmark datasets, where the best and second-best results are highlighted in red and blue, respectively, and the last column is the average rank score across all datasets. Although different algorithms achieve their best performances on different datasets, the contrastiveness-based methods perform better than the non-contrastiveness-based methods in general, which indicates the effectiveness of the graph augmentation. Our proposed GLA achieves the best ranking scores under all 30%, 50%, and 70% label ratios in experiments, the second-best average performance under 30% label ratio, and the best average 6Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation /uni00000016/uni00000013/uni00000008/uni00000010/uni00000021/uni00000018/uni00000013/uni00000008/uni00000016/uni00000013/uni00000008/uni00000010/uni00000021/uni0000001a/uni00000013/uni00000008 /uni00000003 /uni00000014 /uni00000014/uni00000011/uni00000018 /uni00000015 /uni00000015/uni00000011/uni00000018/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000002a/uni00000044/uni0000004c/uni00000051/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002d/uni00000032/uni00000024/uni00000032/uni00000059/uni00000015 (a) performance gain /uni00000030/uni00000039/uni0000002a/uni00000035/uni0000002f/uni00000036/uni0000004c/uni00000050/uni0000002a/uni00000035/uni00000024/uni00000026/uni00000028/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni0000002d/uni00000032/uni00000024/uni00000032/uni00000059/uni00000015/uni0000002a/uni0000002f/uni00000024/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000013/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c  (b) label-invariant rate distribution /uni00000016/uni00000013/uni00000008/uni00000018/uni00000013/uni00000008/uni0000001a/uni00000013/uni00000008 /uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052 /uni0000001b/uni00000018 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000030/uni00000038/uni00000037/uni00000024/uni0000002a /uni00000033/uni00000035/uni00000032/uni00000037/uni00000028/uni0000002c/uni00000031/uni00000036 /uni00000027/uni00000027 /uni00000026/uni00000032/uni0000002f/uni0000002f/uni00000024/uni00000025 (c) GLA’s label-invariant rate Figure 3: Performance gain and label-invariant rates. (a) demonstrates the average performance gains on eight datasets with more labeled samples produced by GraphCL, JOAOv2, and GLA. (b) shows the label-invariant rate distributions of different augmentation methods over eight datasets. (c) shows the label-invariant rates of our GLA over different semi-supervised settings. performance under 50% and 70% label ratios. In our algorithmic design, we employ the decision boundary learned from the labeled samples to verify the label-invariant augmentation. It is worthy to note that the quality of the decision boundary depends on the number of labeled samples. We conjecture that a 30% label ratio is not sufﬁcient enough to learn a high-quality decision boundary, resulting in our GLA performing slightly worse than JOAOv2 on average. With more labeled samples, our GLA delivers the best average performance over other competitive methods. Different from other graph contrastive learning methods, our augmentation method aims to generate label-invariant augmentations, which decreases the possibility of getting “bad” augmentations, thus resulting in better performance. Besides the general comparison in Table 2, we dive into details and discover several interesting ﬁndings. Figure 3(a) demonstrates the average performance gains on eight datasets with more labeled samples produced by GraphCL, JOAOv2, and our GLA, the top three methods in our experiments. In addition to seeing that the increased performance of all three methods well aligns with more labeled samples, our GLA receives more performance gains than GraphCL and JOAOv2. By such comparisons, we can roughly eliminate the effect of more labeled samples and attribute the extra gains to the label-invariant augmentation. It also veriﬁes our aforementioned conjecture that the high-quality decision boundary is beneﬁcial to the label-invariant augmentation, further bringing in the performance boost. Moreover, we further verify our motivation by checking the label-invariant property of different contrastive methods. While we do not have a ground truth classiﬁer, we use ﬁne-tuned classiﬁers in the representation spaces learned by these contrastive methods with a 100% label ratio as the surrogates of the ground truth classiﬁer. Then we use these classiﬁers to assess how many of the augmented representations belong to the same class as their corresponding original representations. Figure 3(b) presents the distributions of label-invariant rates across eight baseline datasets for all graph contrastive methods. As our GLA trained under different label ratios would generate different augmentations, we put the results of GLA’s label-invariant rates under 30%, 50%, and 70% label ratios together for plotting. We can see that GLA has the highest label-invariant rates on average compared to other methods. It is also noticed that the label-invariant rates of different contrastive methods keep the same ranking with the performance in Table 2 (the last column), which veriﬁes our motivation for designing a label-invariant augmentation strategy. Moreover, we further demonstrate our GLA’s label-invariant rates along with different label ratios in Figure 3(c), which accords with our expectation that more labeled samples lead to a high-quality decision boundary and further promote the label-invariant rate in GLA. 4.3 In-depth Exploration Here we further demonstrate several in-depth explorations of our GLA in terms of negative pairs, augmentation space, and strategy. Negative Pairs. The existing graph contrastive learning methods treat the augmented graphs from different source samples as negative pairs and employ the instance-level discrimination on these negative pairs. Since these methods separate the pre-train and ﬁne-tuning phases, the negative pairs contain the augmented samples from different source samples but with the same category in the downstream tasks. Here we explore the effect of negative pairs on our GLA. Figure 4(a) shows the performance of our GLA with and without negative pairs on four datasets. We can see the performance with negative pairs signiﬁcantly drops compared with our default setting without negative pairs, which behaves consistently on all four datasets. Different from the existing graph contrastive methods, our GLA integrates the pre-train and ﬁne-tuning phases, where the negative pairs designed in a self-supervised fashion are not beneﬁcial to the downstream tasks. This ﬁnding is also in accord with the recent studies [10, 9] in the visual contrastive learning area. 7Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation /uni00000030/uni00000038/uni00000037/uni00000024/uni0000002a/uni00000033/uni00000035/uni00000032/uni00000037/uni00000028/uni0000002c/uni00000031/uni00000036/uni00000027/uni00000027/uni00000031/uni00000026/uni0000002c/uni00000014 /uni00000003 /uni0000001a/uni00000013 /uni0000001a/uni00000018 /uni0000001b/uni00000013 /uni0000001b/uni00000018 /uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000003a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000031/uni00000048/uni0000004a/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000033/uni00000044/uni0000004c/uni00000055/uni00000056/uni00000003/uni0000000b/uni0000002a/uni0000002f/uni00000024/uni0000000c /uni0000003a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000031/uni00000048/uni0000004a/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000033/uni00000044/uni0000004c/uni00000055/uni00000056 (a) w/o negative pairs /uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018 /uni0000001b/uni00000016 /uni0000001b/uni0000001a /uni0000001c/uni00000014/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000004c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057 (b) η on MUTAG /uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018 /uni0000001a/uni00000014 /uni0000001a/uni00000018 /uni0000001a/uni0000001c/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000004c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057 (c) η on PROTEINS /uni00000030/uni00000038/uni00000037/uni00000024/uni0000002a/uni00000033/uni00000035/uni00000032/uni00000037/uni00000028/uni0000002c/uni00000031/uni00000036/uni00000027/uni00000027/uni00000031/uni00000026/uni0000002c/uni00000014 /uni00000003 /uni0000001a/uni00000013 /uni0000001a/uni00000018 /uni0000001b/uni00000013 /uni0000001b/uni00000018 /uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002b/uni00000044/uni00000055/uni00000047/uni00000048/uni00000056/uni00000057/uni00000003/uni0000000b/uni0000002a/uni0000002f/uni00000024/uni0000000c /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000028/uni00000044/uni00000056/uni0000004c/uni00000048/uni00000056/uni00000057 (d) augmentation strategy /uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018 /uni0000001a/uni00000017 /uni0000001a/uni0000001b /uni0000001b/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000004c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057 (e) η on DD /uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018 /uni0000001a/uni00000017 /uni0000001a/uni0000001b /uni0000001b/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni0000002f/uni00000024 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000026/uni0000002f/uni00000003/uni0000000e/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000010/uni0000004c/uni00000051/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000057 (f) η on NCI1 Figure 4: In-depth exploration of GLA. (a) contrastive loss with/without negative pairs, (d) performance of different label-invariant augmentation strategies, (b,c,e,f) performance of magnitude of perturbation ηon different datasets under 50% label ratio. Augmentation Space. Different from the most graph contrastive learning methods that directly augment raw graphs, our GLA conducts the augmentation in the representation space, as we believe the raw graphs can be mapped into the representation space, and this space is much easier to augment than the original graph space. In Eq. (5), we design our representation augmentation with a random unit vector scaled by the magnitude of the perturbation η. Figure 4(b,c,e,f) show the performance of our GLA with different values of η on four datasets, where we provide GraphCL and GraphCL+Label-Invariant as references. GraphCL+Label-Invariant takes the augmented graph from GraphCL and ﬁlters the augmented samples that violate the label-invariant property by the downstream classiﬁer. Comparing the two references, we can see that the label-invariant property beneﬁts not only our GLA but also other contrastive methods in most cases. For our GLA, although the ηvalues corresponding to the best performance vary on different datasets, the default setting with η= 1 delivers satisfying performance in general, which outperforms GraphCL+Label-Invariant and indicates the superior of the representation augmentation over the raw graph augmentation. Augmentation Strategy. In the representation space, there might exist multiple qualiﬁed candidates that obey the label-invariant property. Our GLA chooses the most difﬁcult augmentation for the model. Here we demonstrate the performance of different augmentation strategies among qualiﬁed candidates, including the most difﬁcult augmentation, random augmentation, and the easiest augmentation in Figure 4(d), where the random augmentation can be regarded as GraphCL+Label-Invariant. We can see that the most difﬁcult augmentation increases the model generalization and indeed brings in signiﬁcant improvements over the other two ways. This also provides good support for our representation augmentation, where we can ﬁnd the most difﬁcult augmentation in the representation space, but it is difﬁcult to directly generate the raw graphs that are challenging to the downstream classiﬁer. 5 Conclusion In this paper, we consider the graph contrastive learning problem. Different from the existing methods from the pre-train perspective, we propose a novel Graph Label-invariant Augmentation (GLA) algorithm which integrates the pre-train and ﬁne-tuning phases to conduct the label-invariant augmentation in the representation space by perturbations. Speciﬁcally, GLA ﬁrst checks whether the augmented representation obeys the label-invariant property and chooses the most difﬁcult sample from the qualiﬁed samples. By this means, GLA achieves the contrastive augmentation without generating any raw graphs and also increases the model generalization. Extensive experiments in the semi-supervised setting on eight benchmark graph datasets demonstrate the effectiveness of our GLA. Moreover, we also provide extra experiments to verify our motivation and explore the in-depth factors of GLA in the effect of negative pairs, augmentation space, and strategy. 8Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation References [1] Minmin Chen, Kilian Weinberger, Fei Sha, and Yoshua Bengio. Marginalized denoising auto-encoders for nonlinear representations. In International Conference on Machine Learning, 2014. [2] Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei Sha. Marginalized denoising autoencoders for domain adaptation. In International Conference on Machine Learning, 2012. [3] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In European Conference on Computer Vision, 2020. [4] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning. Advances in Neural Information Processing Systems, 33:8765–8775, 2020. [5] Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In AAAI Conference on Artiﬁcial Intelligence, 2021. [6] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661–18673, 2020. [7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020. [9] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271–21284, 2020. [10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. [11] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812–5823, 2020. [12] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In International Conference on Machine Learning, 2021. [13] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep Graph Contrastive Representation Learning. In ICML Workshop on Graph Representation Learning and Beyond, 2020. [14] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In International Conference on Machine Learning, 2020. [15] Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. Simgrace: A simple framework for graph contrastive learning without data augmentation. In ACM Web Conference, 2022. [16] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In International Conference on Learning Representations, 2017. [17] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International Conference on Machine Learning, 2019. [18] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018. [19] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [20] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In The World Wide Web Conference, 2019. [21] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. Kgat: Knowledge graph attention network for recommendation. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019. [22] Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Dufﬁeld, Mingyuan Zhou, and Xiaoning Qian. Variational graph recurrent neural networks. Advances in Neural Information Processing Systems, 32, 2019. [23] Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. Trafﬁc graph convolutional recurrent neural network: A deep learning framework for network-scale trafﬁc learning and forecasting. IEEE Transactions on Intelligent Transportation Systems, 21(11):4883–4894, 2019. 9Label-invariant Augmentation for Semi-Supervised Graph Classiﬁcation [24] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In The IEEE International Conference on Computer Vision, 2019. [25] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2021. [26] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016. [27] Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. International Conference on Learning Representations, 2(3):4, 2019. [28] Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information-aware graph contrastive learning. 34:30414–30425, 2021. [29] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of Medicinal Chemistry, 34(2):786–797, 1991. [30] Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on graph classiﬁcation. arXiv preprint arXiv:1905.04579, 2019. [31] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [32] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond, 2020. [33] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47–i56, 2005. [34] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of Molecular Biology, 330(4):771–783, 2003. [35] Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classiﬁcation. Knowledge and Information Systems, 14(3):347–375, 2008. [36] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015. [37] Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar. Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs. In ACM International Conference on Information and Knowledge Management, 2020. [38] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 10",
      "references": [
        "Marginalized denoising auto-encoders for nonlinear representations.",
        "Marginalized denoising autoencoders for domain adaptation.",
        "Contrastive learning for unpaired image-to-image translation.",
        "Debiased contrastive learning.",
        "Contrastive clustering.",
        "Supervised contrastive learning.",
        "Improved baselines with momentum contrastive learning.",
        "A simple framework for contrastive learning of visual representations.",
        "Bootstrap your own latent-a new approach to self-supervised learning.",
        "Exploring simple siamese representation learning.",
        "Graph contrastive learning with augmentations.",
        "Graph contrastive learning automated.",
        "Deep Graph Contrastive Representation Learning.",
        "Contrastive multi-view representation learning on graphs.",
        "Simgrace: A simple framework for graph contrastive learning without data augmentation.",
        "Semi-supervised classification with graph convolutional networks.",
        "Simplifying graph convolutional networks.",
        "Large-scale learnable graph convolutional networks.",
        "Graph attention networks.",
        "Heterogeneous graph attention network.",
        "Kgat: Knowledge graph attention network for recommendation.",
        "Variational graph recurrent neural networks.",
        "Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting.",
        "Deepgcns: Can gcns go as deep as cnns?",
        "Training graph neural networks with 1000 layers.",
        "Variational graph auto-encoders.",
        "Deep graph infomax.",
        "Infogcl: Information-aware graph contrastive learning.",
        "Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity.",
        "Are powerful graph neural nets necessary? a dissection on graph classification.",
        "Tudataset: A collection of benchmark datasets for learning with graphs.",
        "Protein function prediction via graph kernels.",
        "Distinguishing enzyme structures from non-enzymes without alignments.",
        "Comparison of descriptor spaces for chemical compound retrieval and classification.",
        "Deep graph kernels.",
        "Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs.",
        "Adam: A method for stochastic optimization."
      ],
      "meta_data": {
        "arxiv_id": "2205.09802v1",
        "authors": [
          "Han Yue",
          "Chunhui Zhang",
          "Chuxu Zhang",
          "Hongfu Liu"
        ],
        "published_date": "2022-05-19T18:44:02Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Identifies that existing graph contrastive learning often violates the label-invariant assumption because handcrafted graph augmentations may alter class labels. Introduces Graph Label-invariant Augmentation (GLA), a semi-supervised framework that generates augmentations directly in representation space, guarantees label consistency by filtering with the current classifier, selects the hardest (most confusing) augmentations, and learns with a positive-pair-only contrastive loss jointly with classification. Demonstrates superior accuracy to six strong baselines on eight benchmark datasets.",
        "methodology": "1. Architecture: ResGCN encoder → global sum pooling → graph representation HO; a classifier (2-layer MLP); projection head (2-layer MLP).\n2. Representation-space augmentation: Compute dataset-wide average distance d. For each graph, sample multiple random unit vectors ∆ and form candidates HA = HO + η d ∆.\n3. Label-invariant filter: keep candidates whose predicted label equals the original prediction (unlabeled) or ground truth (labeled).\n4. Hardest sample selection: choose candidate with highest loss / lowest predicted probability for the correct class.\n5. Training objective: (a) Positive-pair NT-Xent loss between HO and HA (no negatives); (b) cross-entropy on labeled graphs; total loss = LP + α LC.\n6. Joint, single-stage optimization with Adam; η and α fixed to 1.0 in experiments.",
        "experimental_setup": "Datasets: MUTAG, PROTEINS, DD, NCI1 (biochemical) and COLLAB, RDT-B, RDT-M5K, GITHUB (social) from TUDataset (188–12 725 graphs, avg. 18–509 nodes).\nProtocol: 10-fold cross-validation; each fold uses 10% test, 10% validation, 80% train; label rates 30%, 50%, 70% of training graphs provided. Metrics: classification accuracy (mean ± std).\nBaselines: GAE, Deep Graph Infomax, MVGRL, GraphCL, JOAOv2, SimGRACE; all re-run with authors’ code.\nImplementation: same ResGCN backbone, default hyper-parameters of GraphCL; GLA uses η=1.0, α=1.0, Adam optimizer. Ablations: varying η, effect of negative pairs, augmentation selection strategy.",
        "limitations": "1. Dependence on a reasonably accurate classifier: with few labels (30%) the decision boundary is weak and performance advantage diminishes.\n2. Perturbation magnitude η and number of candidate vectors require tuning; optimal values differ across datasets.\n3. Augmentation confined to latent space; may miss useful structural variations present in graph domain.\n4. Computational overhead from generating and evaluating multiple candidate perturbations per graph each iteration.\n5. Experiments limited to graph-level classification; generalizability to node classification or other graph tasks remains untested.",
        "future_research_directions": "1. Develop adaptive or adversarial perturbation generators that reduce reliance on sampled random vectors and hyper-parameter tuning.\n2. Explore techniques to maintain label-invariant augmentation under very low-label regimes (e.g., self-training, pseudo-labels, active learning).\n3. Combine latent-space and graph-space augmentations to capture both semantic and structural invariances.\n4. Extend the framework to node-level tasks, link prediction, and heterogeneous or dynamic graphs.\n5. Investigate class-aware negative sampling or hierarchical contrastive objectives to complement the positive-pair-only design.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "On Data-Augmentation and Consistency-Based Semi-Supervised Learning",
      "full_text": "Published as a conference paper at ICLR 2021 ON DATA-AUGMENTATION AND CONSISTENCY - BASED SEMI -SUPERVISED LEARNING Atin Ghosh & Alexandre H. Thiery Department of Statistics and Applied Probability National University of Singapore atin.ghosh@u.nus.edu a.h.thiery@nus.edu.sg ABSTRACT Recently proposed consistency-based Semi-Supervised Learning (SSL) methods such as the Π-model, temporal ensembling, the mean teacher, or the virtual ad- versarial training, have advanced the state of the art in several SSL tasks. These methods can typically reach performances that are comparable to their fully super- vised counterparts while using only a fraction of labelled examples. Despite these methodological advances, the understanding of these methods is still relatively limited. In this text, we analyse (variations of) the Π-model in settings where analytically tractable results can be obtained. We establish links with Manifold Tangent Classiﬁers and demonstrate that the quality of the perturbations is key to obtaining reasonable SSL performances. Importantly, we propose a simple exten- sion of the Hidden Manifold Model that naturally incorporates data-augmentation schemes and offers a framework for understanding and experimenting with SSL methods. 1 I NTRODUCTION Consider a dataset D= DL ∪DU that is comprised of labelled samples DL = {xi,yi}i∈IL as well as unlabelled samples DU = {xi}i∈IU . Semi-Supervised Learning (SSL) is concerned with the use of both the labelled and unlabeled data for training. In many scenarios, collecting labelled data is difﬁcult or time consuming or expensive so that the amount of labelled data can be relatively small when compared to the amount of unlabelled data. The main challenge of SSL is in the design of methods that can exploit the information contained in the distribution of the unlabelled data (Zhu, 2005; Chapelle et al., 2009). In modern high-dimensional settings that are common to computer vision, signal processing, Natural Language Processing (NLP) or genomics, standard graph/distance based methods (Blum & Chawla, 2001; Zhu & Ghahramani, 2002; Zhu et al., 2003; Belkin et al., 2006; Dunlop et al., 2019) that are successful in low-dimensional scenarios are difﬁcult to implement. Indeed, in high-dimensional spaces, it is often difﬁcult to design sensible notions of distances that can be exploited within these methods. We refer the interested reader to the book-length treatments (Zhu, 2005; Chapelle et al., 2009) for discussion of other approaches. The manifold assumption is the fundamental structural property that is exploited in most modern approaches to SSL: high-dimensional data samples lie in a small neighbourhood of a low-dimensional manifold (Turk & Pentland, 1991; Basri & Jacobs, 2003; Peyré, 2009; Cayton, 2005; Rifai et al., 2011a). In computer vision, the presence of this low-dimensional structure is instrumental to the success of (variational) autoencoder and generative adversarial networks: large datasets of images can often be parametrized by a relatively small number of degrees of freedom. Exploiting the unlabelled data to uncover this low-dimensional structure is crucial to the design of efﬁcient SSL methods. A recent and independent evaluation of several modern methods for SSL can be found in (Oliver et al., 2018). It is found there that consistency-based methods (Bachman et al., 2014; Sajjadi et al., 2016; Laine & Aila, 2016; Tarvainen & Valpola, 2017; Miyato et al., 2018; Luo et al., 2018; Grill et al., 2020), the topic of this paper, achieve state-of-the art performances in many realistic scenarios. 1 arXiv:2101.06967v1  [stat.ML]  18 Jan 2021Published as a conference paper at ICLR 2021 Contributions: consistency-based semi-supervised learning methods have recently been shown to achieve state-of-the-art results. Despite these methodological advances, the understanding of these methods is still relatively limited when compared to the fully-supervised setting (Saxe et al., 2013; Advani & Saxe, 2017; Saxe et al., 2018; Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017). In this article, we do not propose a new SSL method. Instead, we analyse consistency-based methods in settings where analytically tractable results can be obtained, when the data-samples lie in the neighbourhood of well-deﬁned and tractable low-dimensional manifolds, and simple and controlled experiments can be carried out. We establish links with Manifold Tangent Classiﬁers and demonstrate that consistency-based SSL methods are in general more powerful since they can better exploit the local geometry of the data-manifold if efﬁcient data-augmentation/perturbation schemes are used. Furthermore, in section 4.1 we show that the popular Mean Teacher method and the conceptually more simple Π-model approach share the same solutions in the regime when the data-augmentations are small; this conﬁrms often reported claim that the data-augmentation schemes leveraged by the recent SSL, as well as fully unsupervised algorithms, are instrumental to their success. Finally, in section 4.3 we propose an extension of the Hidden Manifold Model (Goldt et al., 2019; Gerace et al., 2020). This generative model allows us to investigate the properties of consistency-based SSL methods, taking into account the data-augmentation process and the underlying low-dimensionality of the data, in a simple and principled manner, and without relying on a speciﬁc dataset. For gaining understanding of SSL, as well as self-supervised learning methods, we believe it to be important to develop a framework that(i) can take into account the geometry of the data(ii) allows the study of the inﬂuence of the quality of the data-augmentation schemes (iii) does not rely on any particular dataset. While the understanding of fully-supervised methods have largely been driven by the analysis of simpliﬁed model architectures (eg. linear and two-layered models, large dimension asymptotic such as the Neural Tangent Kernel), these analytical tools alone are unlikely to be enough to explain the mechanisms responsible for the success of SSL and self-supervised learning methods Chen et al. (2020); Grill et al. (2020), since they do not, and cannot easily be extended to, account for the geometry of the data and data-augmentation schemes. Our proposed framework offers a small step in that direction. 2 C ONSISTENCY -BASED SEMI -SUPERVISED LEARNING For concreteness and clarity of exposition, we focus the discussion on classiﬁcation problems. The arguments described in the remaining of this article can be adapted without any difﬁculty to other situations such as regression or image segmentation. Assume that the samples xi ∈X ⊂ RD can be represented as D-dimensional vectors and that the labels belong to C ≥2 possible classes, yi ∈Y≡{ 1,...,C }. Consider a mapping Fθ : RD →RC parametrized by θ ∈Θ ⊂R|Θ|. This can be a neural network, although that is not necessary. For x∈X, the quantity Fθ(x) can represent probabilistic output of the classiﬁer, or , for example, the pre-softmax activations. Empirical risk minimization consists in minimizing the function LL(θ) = 1 |DL| ∑ i∈IL ℓ(Fθ(xi),yi) for a loss function ℓ: RC ×Y↦→ R. Maximum likelihood estimation corresponds to choosing the loss function as the cross entropy. The optimal parameter θ∈Θ is found by a variant of stochastic gradient descent (Robbins & Monro, 1951) with estimated gradient ∇θ { 1 |BL| ∑ i∈BL ℓ(Fθ(xi),yi) } for a mini-batch BL of labelled samples. Consistency-based SSL algorithms regularize the learning by enforcing that the learned function x↦→Fθ(x) respects local derivative and invariance constraints. For simplicity, assume that the mapping x ↦→Fθ(x) is deterministic, although the use of drop- out (Srivastava et al., 2014) and other sources of stochasticity are popular in practice. The Π- model (Laine & Aila, 2016; Sajjadi et al., 2016) makes use of a stochastic mapping S : X× Ω →X that maps a sample x∈X and a source of randomness ω∈Ω ⊂RdΩ to another sample Sω(x) ∈X. The mapping S describes a stochastic data augmentation process. In computer vision, popular data- augmentation schemes include random translations, rotations, dilatations, croppings, ﬂippings, elastic 2Published as a conference paper at ICLR 2021 deformations, color jittering, addition of speckle noise, and many more domain-speciﬁc variants. In NLP, synonym replacements, insertions and deletions, back-translations are often used although it is often more difﬁcult to implement these data-augmentation strategies. In a purely supervised setting, data-augmentation can be used as a regularizer. Instead of directly minimizing LL, one can minimize instead θ↦→ 1 |DL| ∑ i∈IL Eω[ℓ(Fθ[Sω(xi)],yi)]. In practice, data-augmentation regularization, although a simple strategy, is often crucial to obtaining good generalization properties (Perez & Wang, 2017; Cubuk et al., 2018; Lemley et al., 2017; Park et al., 2019). The idea of regularizing by enforcing robustness to the injection of noise can be traced back at least to (Bishop, 1995). In the Π-model, the data-augmentation mapping S is used to deﬁne a consistency regularization term, R(θ) = 1 |D| ∑ i∈IL∪IU Eω {Fθ[Sω(xi)] −Fθ⋆(xi) 2} . (1) The notation θ⋆ designates a copy of the parameter θ, i.e. θ⋆ = θ, and emphasizes that when differentiating the consistency regularization term θ↦→R(θ), one does not differentiate through θ⋆. In practice, a stochastic estimate of ∇R(θ) is obtained as follows. For a mini-batch Bof samples {xi}i∈B, the current value θ⋆ ∈Θ of the parameter and the current predictions fi ≡Fθ⋆(xi), the quantity ∇ { 1 |B| ∑ i∈B Fθ[Sω(xi)] −fi 2 } is an approximation of ∇R(θ). There are indeed many variants (eg. use of different norms, different manners to inject noise), but the general idea is to force the learned function x↦→Fθ(x) to be locally invariant to the data-augmentation schemeS. Several extensions such as the Mean Teacher (Tarvainen & Valpola, 2017) and the V AT (Miyato et al., 2018) schemes have been recently proposed and have been shown to lead to good results in many SSL tasks. The recently proposed and state-of-the-art BYOL approach Grill et al. (2020) is relying on mechanisms that are very close to the consistency regularization methods discussed on this text. If one recalls the manifold assumption, this approach is natural: since the samples corresponding to different classes lie on separate manifolds, the function Fθ : X→ RC should be constant on each one of these manifolds. Since the correct value of Fθ is typically well approximated or known for labelled samples (xi,yi) ∈DL, the consistency regularization term equation 1 helps propagating these known values across these manifolds. This mechanism is indeed similar to standard SSL graph-based approaches such as label propagation (Zhu & Ghahramani, 2002). Graph-based methods are difﬁcult to directly implement in computer vision, or NLP, when a meaningful notion of distance is not available. This interpretation reveals that it is crucial to include the labelled samples in the regularization term equation 1 in order to help propagating the information contained in the labelled samples to the unlabelled samples. Our numerical experiments suggest that, in the standard setting when the number of labelled samples is much lower than the number of unlabeled samples, i.e. |DL|≪|D U|, the formulation equation 1 of the consistency regularization leads to sub-optimal results and convergence issues: the information contained in the labelled data is swamped by the number of unlabelled samples. In all our experiments, we have adopted instead the following regularization term R(θ) = 1 |DL| ∑ i∈IL Eω {Fθ[Sω(xi)] −Fθ⋆(xi) 2} + 1 |DU| ∑ j∈IU Eω {Fθ[Sω(xj)] −Fθ⋆(xj) 2} (2) that balances the labelled and unlabelled data samples more efﬁciently. Furthermore, it is clear that the quality and variety of the data-augmentation scheme S : X× Ω →X is pivotal to the success of consistency-based SSL methods. We argue in this article that it is the dominant factor contributing 3Published as a conference paper at ICLR 2021 to the success of this class of methods. Effort spent on building efﬁcient local data-augmentation schemes will be rewarded in terms of generalization performances. Designing good data-augmentation schemes is an efﬁcient manner of injecting expert/prior knowledge into the learning process. It is done by leveraging the understanding of the local geometry of the data manifold. As usual and not surprisingly (Niyogi et al., 1998; Montavon et al., 2012), in data-scarce settings, any type of domain-knowledge needs to be exploited and we argue that consistency regularization approaches to SSL are instances of this general principle. 3 A PPROXIMATE MANIFOLD TANGENT CLASSIFIER It has long been known (Simard et al., 1998) that exploiting the knowledge of derivatives, or more generally enforcing local invariance properties, can greatly enhance the performance of standard classiﬁers/regressors (Haasdonk & Keysers, 2002; Chapelle & Schölkopf, 2002). In the context of deep-learning, the Manifold Tangent Classiﬁer (Rifai et al., 2011a) is yet another illustration of this idea. Consider the data manifold M⊂X ⊂ RD and assume that the data samples lie on a neighbourhood of it. For x∈M, consider as well the tangent plane Tx to Mat x. Assuming that the manifold Mis of dimension 1 ≤d≤D, the tangent plane Tx is also of dimension dwith an orthonormal basis ex 1,..., ex d ∈RD. This informally means that, for suitably small coefﬁcients ω1,...,ω d ∈R, the transformed sample x∈X deﬁned as x = x+ d∑ j=1 ωjex j also lies, or is very close to, the data manifold M. A possible stochastic data-augmentation scheme can therefore be deﬁned as Sω(x) = x+ Vω where Vω = ∑d j=1 ωjex j. If ω is a multivariate d- dimensional centred Gaussian random vector with suitably small covariance matrix, the perturbation vector Vω is also centred and normally distributed. To enforce that the function x→Fθ(x) is locally approximately constant along the manifold M, one can thus penalize the derivatives of Fθ at xin the directions Vω. Denoting by Jx ∈RC,D the Jacobian with respect to x∈RD of Fθ at x∈M, this can be implemented by adding a penalization term of the type Eω[∥JxVω∥2] = Tr ( Γ ⊗JT xJx ) , where Γ ∈RD,D is the covariance matrix of the random vector ω→Vω. This type of regularization of the Jacobian along the data-manifold is for example used in (Belkin et al., 2006). More generally, if one assumes that for any x,ω ∈X× Ω we have Sεω(x) = x+ εD(x,ω) + O(ε2), for some derivative mapping D : X× Ω →X, it follows that lim ε→0 1 ε2 Eω [ ∥Fθ[Sεω(x)] −Fθ(x)∥2] = Eω [ ∥JxD(x,ω)∥2] = Tr ( Γx,S ⊗JT x Jx ) where Γx,S is the covariance matrix of the X-valued random vector ω↦→D(x,ω) ∈X. This shows that consistency-based methods can be understood as approximated Jacobian regularization methods, as proposed in (Simard et al., 1998; Rifai et al., 2011a). 3.1 L IMITATIONS In practice, even if many local dimension reduction techniques have been proposed, it is still relatively difﬁcult to obtain a good parametrization of the data manifold. The Manifold Tangent Classiﬁer (MTC) (Rifai et al., 2011a) implements this idea by ﬁrst extracting in an unsupervised manner a good representation of the dataset Dby using a Contractive-Auto-Encoder (CAE) (Rifai et al., 2011b). This CAE can subsequently be leveraged to obtain an approximate basis of each tangent plane Txi for xi ∈D, which can then be used for penalizing the Jacobian of the mapping x↦→Fθ(x) in the direction of the tangent plane to Mat x. The above discussion shows that the somewhat simplistic approach consisting in adding an isotropic Gaussian noise to the data samples is unlikely to deliver satisfying results. It is equivalent to penalizing the Frobenius norm ∥Jx∥2 F of the Jacobian of the mapping x↦→Fθ(x); in a linear model, that is equivalent to the standard ridge regularization. This mechanism does not take at all into account the local-geometry of the data-manifold. Nevertheless, in medical imaging applications where scans are often contaminated by speckle noise, this class of approaches which can be thought off as adding artiﬁcial speckle noise, can help mitigate over- ﬁtting (Devalla et al., 2018). 4Published as a conference paper at ICLR 2021   cop I f µ labelledsamplesoooo unlabeledsamples localdataaugmentation Figure 1: Left: Jacobian (i.e. ﬁrst order) Penalization method are short-sighted and do not exploit fully the data-manifold Right: Data-Augmentation respecting the geometry of the data-manifold. There are many situations where, because of data scarcity or the sheer difﬁculty of unsupervised representation learning in general, domain-speciﬁc data-augmentation schemes lead to much better regularization than Jacobian penalization. Furthermore, as schematically illustrated in Figure 1, Jacobian penalization techniques are not efﬁcient at learning highly non-linear manifolds that are common, for example, in computer vision. For example, in “pixel space\", a simple image translation is a highly non-linear transformation only well approximated by a ﬁrst order approximation for very small translations. In other words, if x∈X represents an image and g(x,v) is its translated version by a vector v, the approximation g(x,v) ≈x+∇vg(x), with ∇vg(x) ≡limε→0 (g(x,εv )−g(x)/ε, becomes poor as soon as the translation vector vis not extremely small. In computer vision, translations, rotations and dilatations are often used as sole data-augmentation schemes: this leads to a poor local exploration of the data-manifold since this type transformations only generate a very low dimensional exploration manifold. More precisely, the exploration manifold emanating from a sample x0 ∈X, i.e. {S(x0,ω) : ω∈Ω}, is very low dimensional: its dimension is much lower than the dimension dof the data-manifold M. Enriching the set of data-augmentation degrees of freedom with transformations such as elastic deformation or non-linear pixel intensity shifts is crucial to obtaining a high-dimensional local exploration manifold that can help propagating the information on the data-manifold efﬁciently (Cubuk et al., 2019a; Park et al., 2019). 4 A SYMPTOTIC PROPERTIES 4.1 F LUID LIMIT Consider the standard Π-model trained with a standard Stochastic Gradient Descent (SGD). Denote by θt ∈Θ the current value of the parameter and η >0 the learning rate. We have θk+1 = θk −η∇θ { 1 |BL| ∑ i∈BL ℓ( Fθk (xi), yi ) + λ |BL| ∑ j∈BL Fθk (Sω[xj]) −fj  2 + λ |BU| ∑ k∈BU Fθk (Sω[xk]) −fk  2} (3) for a parameter λ> 0 that controls the trade-off between supervised and consistency losses, as well as subsets BL and BU of labelled and unlabelled data samples, and fj ≡Fθ⋆(xj) for θ⋆ ≡θk as discussed in Section 2. The right-hand-side is an unbiased estimate of η∇θ [ LL(θk) +λR(θk) ] with variance of order O(η2), where the regularization term R(θk) is described in equation 2. It follows from standard ﬂuid limit approximations (Ethier & Kurtz, 2009)[Section 4.8] for Markov processes that, under mild regularity and growth assumptions and as η→0, the appropriately time-rescaled trajectory {θk}k≥0 can be approximated by the trajectory of the Ordinary Differential Equation (ODE). 5Published as a conference paper at ICLR 2021 Proposition 4.1 Let D([0,T],R|Θ|) be the usual space of càdlàg R|Θ|-valued functions on a bounded time interval [0,T] endowed with the standard Skorohod topology. Consider the update equation 3 with learning rate η >0 and deﬁne the continuous time process θ η (t) = θ[t/η]. The sequence of processes θ η ∈D([0,T],R|Θ|) converges weakly in D([0,T],R|Θ|) and as η →0 to the solution of the ordinary differential equation ˙θt = −∇ ( L(θt) + λR(θt) ) . (4) The article (Tarvainen & Valpola, 2017) proposes themean teacher model, an averaging approach related to the standard Polyak-Ruppert averaging scheme (Polyak, 1990; Polyak & Juditsky, 1992), which modiﬁes the consistency regularization term equation 2 by replacing the parameter θ⋆ by an exponential moving average (EMA). In practical terms, this simply means that, instead of deﬁning fj = Fθ⋆(xj), with θ⋆ = θk in equation 3, one sets fj = Fθavg,k (xj) where the EMA process {θavg,k}k≥0 is deﬁned through the recursion θavg,k = (1−αη) θavg,k−1 +αηθ k where the coefﬁcient α >0 controls the time-scale of the averaging process. The use of the EMA process {θavg,k}k≥0 helps smoothing out the stochasticity of the process θk. Similarly to Proposition 4.1, as η→0, the joint process (θ η t,θ η avg,t) ≡(θη [t/η],θη avg,[t/η]) converges as η →0 to the solution of the following ordinary differential equation   ˙θt = −∇ ( L(θt) + λR(θt,θavg,t) ) ˙θavg,t = −α(θavg,t −θt) (5) where the notation R(θt,θavg,t) designates the same quantity as the one described in equation 2, but with an emphasis on the dependency on the EMA process. At convergence (θt,θavg,t) → (θ∞,θavg,∞), one must necessarily have that θ∞= θavg,∞, conﬁrming that, in the regime of small learning rate η→0, the Mean Teachermethod converges, albeit often more rapidly, towards the same solution as the more standard Π-model. This indicates that the improved performances of the Mean Teacher approach sometimes reported in the literature are either not statistically meaningful, or due to poorly executed comparisons, or due to mechanisms not captured by the η→0 asymptotic. Indeed, several recently proposed consistency based SSL algorithms (Berthelot et al., 2019; Sohn et al., 2020; Xie et al., 2019) achieve state-of-the-art performance across diverse datasets without employing any exponential averaging processes. These results are achieved by leveraging more sophisticated data augmentation schemes such as Rand-Augment (Cubuk et al., 2019b) , Back Translation (Artetxe et al., 2017) or Mixup (Zhang et al., 2017). 4.2 M INIMIZERS ARE HARMONIC FUNCTIONS To understand better the properties of the solutions, we consider a simpliﬁed setting further exploited in Section 4.3. Assume that F: X≡ RD →R and Y≡ R and that, for every yi ∈Y≡ R, the loss function f ↦→ℓ(f,yi) is uniquely minimized at f = yi. We further assume that the data-manifold M⊂ RD can be globally parametrized by a smooth and bijective mapping Φ : Rd →M⊂ RD. Similarly to the Section 2, we consider a data-augmentation scheme that can be described as Sεω(x) = Φ(z+ εω) for z = Φ−1(x) and a sample ωfrom a Rd-valued centred and isotropic Gaussian distribution. We consider a ﬁnite set of labelled samples {xi,yi}i∈IL, with xi = Φ(zi) and zi ∈Rd for i ∈IL. We choose to model the large number of unlabelled data samples as a continuum distributed on the data manifold Mas the push-forward measure Φ♯µ(dz) of a probability distribution µ(dz) whose support is Rd through the mapping Φ. This means that an empirical average of the type (1/|DU|) ∑ i∈Iu ϕ(xi) can be replaced by ∫ ϕ[Φ(z)] µ(dz). We investigate the regime ε→0 and, similarly to Section 2, the minimization of the consistency-regularized objective LL(θ) + λ ε2 ∫ Rd Eω {Fθ[Sεω(Φ(z))] −Fθ(Φ(z)) 2} µ(dz). (6) For notational convenience, set fθ ≡ Fθ ◦Φ. Since Sεω[Φ(z)] = Φ( z + εω), as ε → 0 the quantity 1 ε2 Eω {Fθ[Sεω(Φ(z))] −Fθ(Φ(z)) 2} converges to ∥∇zfθ∥2 and the objective function equation 6 approaches the quantity G(fθ) ≡ 1 |DL| ∑ i∈IL ℓ(fθ(zi),yi) + λ ∫ Rd ∥∇zfθ(z)∥2 µ(dz). (7) 6Published as a conference paper at ICLR 2021 A minimizer f : Rd →R of the functional G that is consistent with the labelled data, i.e. f(zi) = yi for i ∈IL, is a minimizer of the energy functional f ↦→ ∫ Rd ∥∇zfθ(z)∥2 µ(dz) subject to the constraints f(zi) = yi. It is the variational formulation of the Poisson equation {∆f(z) = 0 for z∈Rd \\{zi}i∈IL f(zi) = yi for i∈IL. (8) Note that the solution does not depend on the regularization parameter λin the regime of ε →0: this indicates, as will be discussed in Section 4.3 in detail, that the generalization properties of consistency-based SSL methods will typically be insensitive to this parameter, in the regime of small data-augmentation at least. Furthermore, equation 8 shows that consistency-based SSL methods are indeed based on the same principles as more standard graph-based approaches such as Label Propagation (Zhu & Ghahramani, 2002): solutions are gradient/Laplacian penalized interpolating functions. In Figure 2, we consider the case where D= d= 2 with trivial mapping Φ(x) = x. We consider labelled data situated on the right (resp. left) boundary of the unit square and corresponding to the label y = 0 (resp. y = 1). For simplicity, we choose the loss function ℓ(f,y) = 1 2 (f −y)2 and parametrize Fθ ≡fθ with a neural network with a single hidden layer with N = 100 neurons. As expected, the Π-model converges to the solution to the Poisson equation 8 in the unit square with boundary condition f(u,v) = 0 for u= 0 and f(u,v) = 1 for u= 1. Figure 2: Labelled data samples with class y= 0 (green triangle) and y= +1 (red dot) are placed on the Left/Right boundary of the unit square. Unlabelled data samples (blue stars) are uniformly placed within the unit square. We consider a simple regression setting with loss functionℓ(f,y) = 1 2 (f−y)2. Left: Randomly initialized neural network. Middle: labelled/unlabelled data Right: Solution of f obtained by training a standard Π-model. It is the harmonic function f(u,v) = u, as described by equation 8. 4.3 G ENERATIVE MODEL FOR SEMI -SUPERVISED LEARNING As has been made clear throughout this text, SSL methods crucially rely on the dependence structure of the data. The existence and exploitation of a much lower-dimensional manifold Msupporting the data-samples is instrumental to this class of methods. Furthermore, the performance of consistency- based SSL approaches is intimately related to the data-augmentation schemes they are based upon. Consequently, in order to understand the mechanisms that are at play when consistency-based SSL methods are used to uncover the structures present in real datasets, it is important to build simpliﬁed and tractable generative models of data that(1) respect these low-dimensional structures and(2) allow the design of efﬁcient data-augmentation schemes. Several articles have investigated the inﬂuence of the dependence structures that are present in the data on the learning algorithm (Bruna & Mallat, 2013; Mossel, 2016). Here, we follow the Hidden Manifold Model (HMM) framework proposed in (Goldt et al., 2019; Gerace et al., 2020) where the authors describe a model of synthetic data concentrating near low-dimensional structures and analyze the learning curve associated to a class of two-layered neural networks. Low-dimensional structure: Similarly to Section 4.2, assume that the D-dimensional data-samples xi ∈X can be expressed as xi = Φ(zi) ∈RD for a ﬁxed smooth mapping Φ : Rd →RD. In other words, the data-manifold Mis d-dimensional and the mapping Φ can be used to parametrize it. The mapping Φ is chosen to be a neural network with a single hidden layer with H neurons, although 7Published as a conference paper at ICLR 2021 0 20 40 60 80 100 Epoch 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Test NLL = 1.0 = 10.0 = 100.0 Unregularized 0 25 50 75 100 125 150 175 200 Epoch 0.2 0.3 0.4 0.5 0.6Test NLL = 0.03 = 0.10 = 0.30 = 1.00 Figure 3: Left: For a ﬁxed data-augmentation scheme, generalization properties for λspanning two orders of magnitude. Right: Inﬂuence of the quantity of the data-augmentation of the generalization properties. other choices are indeed possible. For z = (z1,...,z d) ∈Rd, set Φ(z) = A1→2 ϕ(A0→1z+ b1) for matrices A0→1 ∈RH,d and A1→2 ∈RD,H, bias vector b1 ∈RH and non-linearity ϕ: R →R applied element-wise. In all our experiments, we use the ELU non-linearity. We adopt the standard normalization A0→1 i,j = w(1) i,j/ √ dand A1→2 i,j = w(2) i,j/ √ Hfor weights w(k) i,j drawn i.i.d from a centred Gaussian distribution with unit variance; this ensures that, if the coordinate of the input vectorz∈Rd are all of order O(1), so are the coordinates of x= Φ(z). Data-augmentation: consider a data sample xi ∈M on the data-manifold. It can also be ex- pressed as xi = Φ(zi). We consider the natural data-augmentation process which consists in setting Sεω(xi) = Φ( zi + εω) for a sample ω ∈Rd from an isotropic Gaussian distribution with unit covariance and ε> 0. Crucially, the data-augmentation scheme respect the low-dimensional structure of the data: the perturbed sample Sεω(xi) belongs to the data-manifold Mfor any perturbation vector εω. Note that, for any value of ε, the data-augmentation preserves the low-dimensional manifold: perturbed samples Sεω(xi) exactly lie on the data-manifold. The larger ε, the more efﬁcient the data-augmentation scheme; this property is important since it allows to study the inﬂuence of the amount of data-augmentation. Classiﬁcation: we consider a balanced binary classiﬁcation problem with |DL|≥ 2 labelled training examples {xi,yi}i∈IL where xi = Φ(zi) and yi ∈Y ≡{−1,+1}. The sample zi ∈Rd corre- sponding to the positive (resp. negative) class are assumed to have been drawn i.i.d from a Gaussian distribution with identity covariance matrix and mean µ+ ∈Rd (resp. mean µ−∈Rd). The distance ∥µ+ −µ−∥quantiﬁes the hardness of the classiﬁcation task. Neural architecture and optimization: Consider ﬁtting a two-layered neural network Fθ : RD → R by minimising the negative log-likelihood LL(θ) ≡(1/|DL|) ∑ iℓ[Fθ(xi),yi] where ℓ(f,y) = log(1 + exp[−yf]). We assume that there are |DL|= 10 labelled data pairs {xi,yi}i=IL, as well as |DU|= 1000 unlabelled data samples, that the ambient space has dimension D= 100 and the data manifold Mhas dimension d= 10. The function Φ uses H = 30 neurons in its hidden layer. In all our experiments, we use a standard Stochastic Gradient Descent (SGD) method with constant learning rate and momentum β = 0.9. For minimizing the consistency-based SSL objective LL(θ) + λR(θ), with regularization R(θ) given in equation 2, we use the standard strategy (Tarvainen & Valpola, 2017) consisting in ﬁrst minimizing the un-regularized objective alone LL for a few epochs in order for the function Fθ to be learned in the neighbourhood of the few labelled data-samples before switching on the consistency-based regularization whose role is to propagate the information contained in the labelled samples along the data manifold M. Insensitivity to λ: Figure 3 (Left) shows that this method is relatively insensitive to the parameter λ, as long as it is within reasonable bounds. This phenomenon can be read from equation 8 that does not depend on λ. Much larger or smaller values (not shown in Figure 3) of λdo lead, unsurprisingly, to convergence and stability issues. Amount of Data-Augmentation: As is reported in many tasks Cubuk et al. (2018); Zoph et al. (2019); Kostrikov et al. (2020), tuning the amount data-augmentation in deep-learning applications is often a delicate exercise that can greatly inﬂuence the resulting performances. Figure 3 (Right) 8Published as a conference paper at ICLR 2021 0 25 50 75 100 125 150 175 200 Epoch 0.2 0.4 0.6 0.8 1.0Test NLL k=5 k=6 k=7 k=8 k=9 k=10 5 6 7 8 9 10 k: Data Augmentation Dimension 0.2 0.4 0.6 0.8 1.0Test NLL Generalization at Epoch 200 Figure 4: Learning curve test (NLL) of the Π-model with λ = 10 for different “quality\" of data- augmentation. The data manifold is of dimension d = 10 in an ambient space of dimension D = 100 . For xi = Φ( zi) and 1 ≤k ≤d, the data-augmentation scheme is implemented as Sεω[k](xi) = Φ(zi + εω[k]) where ω[k] is a sample from a Gaussian distribution whose last (d−k) coordinates are zero. In other words, the data-augmentation scheme only explores kdimensions out of the ddimensions of the data-manifold. We use ε = 0.3 in all the experiments. Left: Learning curves (Test NLL) for data-augmentation dimension k∈[5,10] Right: Test NLL at epoch N = 200 (see left plot) for data-augmentation dimension k∈[5,10]. reports the generalization properties of the method for different amount of data-augmentation. Too low an amount of data-augmentation (i.e. ε= 0.03) and the ﬁnal performance is equivalent to the un-regularized method. Too large an amount of data-augmentation (i.e. ε= 1.0) also leads to poor generalization properties. This is because the choice of ε= 1.0 corresponds to augmented samples that are very different from the distribution of the training dataset (i.e. distributional shift), although these samples are still supported by the data-manifold. 0 50 100 150 200 250 300 Epochs 0.3 0.4 0.5 0.6 0.7 0.8Test NLL MT: MT=0.900 MT: MT=0.950 MT: MT=0.990 MT: MT=0.995 -model Figure 5: Mean-Teacher (MT) learning curves (Test NLL) for different values of the exponential smoothing parameter βMT ∈(0,1). For βMT ∈{0.9,0.95,0.99,0.995}, the ﬁnal test NLL obtained through the MT approach is identical to the test NLL obtained through the Π-model. In all the experiments, we used λ= 10 and used SGD with momentum β = 0.9. Quality of the Data-Augmentation: to study the inﬂuence of the quality of the data-augmentation scheme, we consider a perturbation process implemented asSεω[k](xi) = Φ(zi+ω[k]) for xi = Φ(zi) where the noise term ω[k] is deﬁned as follows. For a data-augmentation dimension parameter 1 ≤k≤dwe have ω[k] = (ξ1,...,ξ k,0,..., 0) for i.i.d standard Gaussian samples ξ1,...,ξ k ∈R. This data-augmentation scheme only explores the ﬁrst k dimensions of the d-dimensional data- manifold: the lower k, the poorer the exploration of the data-manifold. As demonstrated on Figure 4, lower quality data-augmentation schemes (i.e. lower values of k∈[0,d]) hurt the generalization performance of the Π-model. Mean-Teacher versus Π-model: we implemented the Mean-Teacher (MT) approach with an expo- nential moving average (EMA) process θavg,k = βMT θavg,k−1 + (1 −βMT) θk for the MT parameter θavg,k with different scales βMT ∈{0.9,0.95,0.99,0.995}, as well as a Π-model approach, with 9Published as a conference paper at ICLR 2021 λ= 10 and ε= 0.3. Figure 5 shows, in accordance with Section 4.1, that the different EMA schemes lead to generalization performances similar to a standard Π-model. 5 C ONCLUSION Consistency-based SSL methods rely on a subtle trade-off between the exploitation of the labelled samples and the discovery of the low-dimensional data-manifold. The results presented in this article highlight the connections with more standard methods such as Jacobian penalization and graph- based approaches and emphasize the crucial role of the data-augmentation scheme. The analysis of consistency-based SSL methods is still in its infancy and our numerical simulations suggest that the variant of the Hidden Manifold Model described in this text is a natural framework to make progress in this direction. REFERENCES Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv preprint arXiv:1710.03667, 2017. Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine translation. arXiv preprint arXiv:1710.11041, 2017. Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In Advances in Neural Information Processing Systems, pp. 3365–3373, 2014. Ronen Basri and David W Jacobs. Lambertian reﬂectance and linear subspaces. IEEE Transactions on Pattern Analysis & Machine Intelligence, (2):218–233, 2003. Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame- work for learning from labeled and unlabeled examples. Journal of machine learning research, 7 (Nov):2399–2434, 2006. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 5050–5060, 2019. Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation, 7 (1):108–116, 1995. Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts. In Proceedings of the Eighteenth International Conference on Machine Learning, pp. 19–26. Morgan Kaufmann Publishers Inc., 2001. Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. IEEE transactions on pattern analysis and machine intelligence, 35(8):1872–1886, 2013. Lawrence Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep, 12 (1-17):1, 2005. Olivier Chapelle and Bernhard Schölkopf. Incorporating invariances in non-linear support vector machines. In Advances in neural information processing systems, pp. 609–616, 2002. Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542–542, 2009. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 113–123, 2019a. 10Published as a conference paper at ICLR 2021 Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data augmentation with no separate search. arXiv preprint arXiv:1909.13719, 2019b. Sripad Krishna Devalla, Prajwal K Renukanand, Bharathwaj K Sreedhar, Giridhar Subramanian, Liang Zhang, Shamira Perera, Jean-Martial Mari, Khai Sing Chin, Tin A Tun, Nicholas G Strouthidis, et al. Drunet: a dilated-residual U-net deep learning network to segment optic nerve head tissues in optical coherence tomography images. Biomedical optics express, 9(7): 3244–3265, 2018. Matthew M Dunlop, Dejan Slepˇcev, Andrew M Stuart, and Matthew Thorpe. Large data and zero noise limits of graph-based semi-supervised learning algorithms. Applied and Computational Harmonic Analysis, 2019. Stewart N Ethier and Thomas G Kurtz. Markov processes: characterization and convergence, volume 282. John Wiley & Sons, 2009. Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborová. Gener- alisation error in learning with random features and the hidden manifold model. arXiv preprint arXiv:2002.09339, 2020. Sebastian Goldt, Marc Mézard, Florent Krzakala, and Lenka Zdeborová. Modelling the inﬂuence of data structure on learning in neural networks. arXiv preprint arXiv:1909.11500, 2019. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020. Bernard Haasdonk and Daniel Keysers. Tangent distance kernels for support vector machines. In Object recognition supported by user interaction for service robots, volume 2, pp. 864–868. IEEE, 2002. Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020. Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016. Joseph Lemley, Shabab Bazrafkan, and Peter Corcoran. Smart augmentation learning an optimal data augmentation strategy. IEEE Access, 5:5858–5869, 2017. Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8896–8905, 2018. Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 2018. Grégoire Montavon, Katja Hansen, Siamac Fazli, Matthias Rupp, Franziska Biegler, Andreas Ziehe, Alexandre Tkatchenko, Anatole V Lilienfeld, and Klaus-Robert Müller. Learning invariant representations of molecules for atomization energy prediction. In Advances in Neural Information Processing Systems, pp. 440–448, 2012. Elchanan Mossel. Deep learning and hierarchal generative models. arXiv preprint arXiv:1612.09057, 2016. Partha Niyogi, Federico Girosi, and Tomaso Poggio. Incorporating prior information in machine learning by creating virtual examples. Proceedings of the IEEE, 86(11):2196–2209, 1998. Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In Advances in Neural Information Processing Systems, pp. 3235–3246, 2018. 11Published as a conference paper at ICLR 2021 Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. Luis Perez and Jason Wang. The effectiveness of data augmentation in image classiﬁcation using deep learning. arXiv preprint arXiv:1712.04621, 2017. Gabriel Peyré. Manifold models for signals and images. Computer Vision and Image Understanding, 113(2):249–260, 2009. Boris T Polyak. New stochastic approximation type procedures. Automat. i Telemekh, 7(98-107):2, 1990. Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.SIAM journal on control and optimization, 30(4):838–855, 1992. Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent classiﬁer. In Advances in Neural Information Processing Systems, pp. 2294–2302, 2011a. Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto- encoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on International Conference on Machine Learning, pp. 833–840. Omnipress, 2011b. Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pp. 400–407, 1951. Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transforma- tions and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 1163–1171, 2016. Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Bren- dan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning. 2018. Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017. Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance in pattern recognition—tangent distance and tangent propagation. In Neural networks: tricks of the trade, pp. 239–274. Springer, 1998. Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence. arXiv preprint arXiv:2001.07685, 2020. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–1958, 2014. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis- tency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pp. 1195–1204, 2017. Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Information Theory Workshop (ITW), pp. 1–5. IEEE, 2015. Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of cognitive neuroscience, 3 (1):71–86, 1991. Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for consistency training. 2019. 12Published as a conference paper at ICLR 2021 Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propaga- tion. Technical report, Citeseer, 2002. Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pp. 912–919, 2003. Xiaojin Jerry Zhu. Semi-supervised learning literature survey. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 2005. Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le. Learning data augmentation strategies for object detection. arXiv preprint arXiv:1906.11172, 2019. 13",
      "references": [
        "High-dimensional dynamics of generalization error in neural networks",
        "Unsupervised neural machine translation",
        "Learning with pseudo-ensembles",
        "Lambertian reﬂectance and linear subspaces",
        "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
        "Mixmatch: A holistic approach to semi-supervised learning",
        "Training with noise is equivalent to tikhonov regularization",
        "Learning from labeled and unlabeled data using graph mincuts",
        "Invariant scattering convolution networks",
        "Algorithms for manifold learning",
        "Incorporating invariances in non-linear support vector machines",
        "Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]",
        "A simple framework for contrastive learning of visual representations",
        "Autoaugment: Learning augmentation policies from data",
        "Autoaugment: Learning augmentation strategies from data",
        "Randaugment: Practical data augmentation with no separate search",
        "Drunet: a dilated-residual U-net deep learning network to segment optic nerve head tissues in optical coherence tomography images",
        "Large data and zero noise limits of graph-based semi-supervised learning algorithms",
        "Markov processes: characterization and convergence",
        "Generalisation error in learning with random features and the hidden manifold model",
        "Modelling the inﬂuence of data structure on learning in neural networks",
        "Bootstrap your own latent: A new approach to self-supervised learning",
        "Tangent distance kernels for support vector machines",
        "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels",
        "Temporal ensembling for semi-supervised learning",
        "Smart augmentation learning an optimal data augmentation strategy",
        "Smooth neighbors on teacher graphs for semi-supervised learning",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
        "Learning invariant representations of molecules for atomization energy prediction",
        "Deep learning and hierarchal generative models",
        "Incorporating prior information in machine learning by creating virtual examples",
        "Realistic evaluation of deep semi-supervised learning algorithms",
        "Specaugment: A simple data augmentation method for automatic speech recognition",
        "The effectiveness of data augmentation in image classiﬁcation using deep learning",
        "Manifold models for signals and images",
        "New stochastic approximation type procedures",
        "Acceleration of stochastic approximation by averaging",
        "The manifold tangent classiﬁer",
        "Contractive auto-encoders: Explicit invariance during feature extraction",
        "A stochastic approximation method",
        "Regularization with stochastic transformations and perturbations for deep semi-supervised learning",
        "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "On the information bottleneck theory of deep learning",
        "Opening the black box of deep neural networks via information",
        "Transformation invariance in pattern recognition—tangent distance and tangent propagation",
        "Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence",
        "Dropout: a simple way to prevent neural networks from overﬁtting",
        "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
        "Deep learning and the information bottleneck principle",
        "Eigenfaces for recognition",
        "Unsupervised data augmentation for consistency training",
        "mixup: Beyond empirical risk minimization",
        "Learning from labeled and unlabeled data with label propagation",
        "Semi-supervised learning using gaussian ﬁelds and harmonic functions",
        "Semi-supervised learning literature survey",
        "Learning data augmentation strategies for object detection"
      ],
      "meta_data": {
        "arxiv_id": "2101.06967v1",
        "authors": [
          "Atin Ghosh",
          "Alexandre H. Thiery"
        ],
        "published_date": "2021-01-18T10:12:31Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Provides a theoretical and empirical analysis of consistency-based semi-supervised learning (SSL) methods (Π-model, Mean Teacher, VAT, etc.). Establishes their connection to Jacobian/Manifold Tangent regularization, proves that Mean Teacher converges to the same solutions as the simpler Π-model in the small-augmentation regime, and demonstrates that the effectiveness of these methods is dominated by the quality and magnitude of data-augmentation. Introduces an extension of the Hidden Manifold Model (HMM) that explicitly incorporates data augmentation, enabling controlled studies of SSL without real datasets.",
        "methodology": "1) Analytical derivations under a manifold assumption: treats augmentation as small perturbations, links consistency loss to Jacobian regularization and harmonic functions. 2) Fluid-limit ODE analysis of SGD to compare Π-model vs. Mean Teacher. 3) Proposes a generative synthetic benchmark—an HMM where data points lie on a d-dimensional manifold Φ(z) in R^D and augmentation moves along that manifold. 4) Implements simple two-layer neural networks trained with SGD (with or without EMA) to verify theoretical claims, varying augmentation magnitude ε, augmentation dimension k, and consistency weight λ.",
        "experimental_setup": "Synthetic data only. • Ambient dimension D=100, manifold dimension d=10. • Manifold generated by a one-hidden-layer network Φ with H=30 ELU units; class means µ+ , µ− define two Gaussian clusters (|DL|=10 labelled, |DU|=1000 unlabelled). • Classifier: two-layer neural net trained with cross-entropy; momentum 0.9. • Augmentation: S_ε,ω(x)=Φ(z+εω) where ω~N(0,I_k) for different k≤d and ε∈{0.03,0.1,0.3,1.0}. • Evaluate test negative log-likelihood (NLL) across epochs; compare Π-model and Mean Teacher under different λ (1–100) and EMA rates β_MT ∈{0.9,0.95,0.99,0.995}.",
        "limitations": "• Experiments restricted to synthetic HMM data—no real-world benchmarks; applicability to complex datasets unverified. • Analysis assumes small perturbations and known smooth manifolds; may not hold with large augmentations or non-manifold data. • Focuses on binary classification and shallow architectures; results might differ for multi-class or deep networks. • Does not propose new SSL algorithm; insights mainly qualitative for practitioners. • Hyper-parameters (λ, ε) studied in limited ranges; broader settings unexplored.",
        "future_research_directions": "1) Validate theoretical insights on real computer-vision and NLP datasets with state-of-the-art architectures. 2) Automate or learn augmentation policies that align with manifold geometry. 3) Extend the HMM framework to multi-class, regression, and segmentation tasks. 4) Analyze large-augmentation regimes and distributional shift formally. 5) Study consistency-based self-supervised objectives within the same manifold-aware model. 6) Investigate interactions with deeper networks and modern optimizers. 7) Develop metrics to quantify augmentation quality relative to underlying manifolds.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection",
      "full_text": "Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection Chuandong Liu1,2 , Chenqiang Gao 1,2*, Fangcen Liu 1,2 , Pengcheng Li 1,2 , Deyu Meng 3,4 , Xinbo Gao 1 1School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China 2Chongqing Key Laboratory of Signal and Information Processing, Chongqing, China 3Xi’an Jiaotong University, Xi’an, China 4Macau University of Science and Technology, Taipa, Macau Abstract State-of-the-art 3D object detectors are usually trained on large-scale datasets with high-quality 3D annotations. However, such 3D annotations are often expensive and time-consuming, which may not be practical for real ap- plications. A natural remedy is to adopt semi-supervised learning (SSL) by leveraging a limited amount of labeled samples and abundant unlabeled samples. Current pseudo- labeling-based SSL object detection methods mainly adopt a teacher-student framework, with a single fixed threshold strategy to generate supervision signals, which inevitably brings confused supervision when guiding the student net- work training. Besides, the data augmentation of the point cloud in the typical teacher-student framework is too weak, and only contains basic down sampling and flip-and-shift (i.e., rotate and scaling), which hinders the effective learn- ing of feature information. Hence, we address these is- sues by introducing a novel approach of Hierarchical Su- pervision and Shuffle Data Augmentation (HSSDA), which is a simple yet effective teacher-student framework. The teacher network generates more reasonable supervision for the student network by designing a dynamic dual-threshold strategy. Besides, the shuffle data augmentation strategy is designed to strengthen the feature representation ability of the student network. Extensive experiments show that HSSDA consistently outperforms the recent state-of-the-art methods on different datasets. The code will be released at https://github.com/azhuantou/HSSDA. 1. Introduction Kinds of important applications, especially autonomous driving, have been motivating the rapid development of 3D *Corresponding author. Vanilla Teacher Teacher Network Unlabeled scene Pseudo-labeling scene Ours Teacher Vanilla Student Student Network Unlabeled scene Augmented scene Student Network Unlabeled scene Augmented scene ( a) RGB ImageFull labeled scene (a) (b) Ours Teacher High threshold Low threshold Teacher Network Unlabeled scene Pseudo-labeling scene Dual-threshold  strategy Flip&Shift Split&Shuffle OR Vanilla Teacher Teacher Network Unlabeled scene Pseudo-labeling scene Ours Teacher Vanilla Student Student Network Unlabeled scene Augmented scene Student Network Unlabeled scene Augmented scene RGB ImageFull labeled scene (a) (b) Ours Student High threshold Low threshold Teacher Network Unlabeled scene Pseudo-labeling scene Dual-threshold  strategy Flip&Shift Split&Shuffle OR Figure 1. Illustration of (a) the previous teacher compared to our teacher framework and (b) the previous student compared to our student framework. The black dashed box includes the RGB im- age and the corresponding fully annotated 3D point cloud (green box). The left side of the yellow dotted line in (a) represents the pseudo-labeling scene generated by the single threshold of the vanilla teacher network, causing the student network may be severely misled due to missing mined objects (high threshold) or false positive objects (low threshold), while our proposed teacher network generates three groups of pseudo labels(shown as green, red, blue) to provide hierarchical supervision for the student net- work. (b) shows our student network adopts stronger shuffled data augmentation than the vanilla student network to learn the stronger ability of feature representation. object detection by the range sensor data (e.g., LiDAR point cloud). Up to now, many point-based and point-voxel-based methods [11,32,33,53] have been proposed. Despite the im- pressive progress, a large amount of accurate instance-level 3D annotations have to be provided for training, which is more time-consuming and expensive than 2D object annota- tion. This hinders the application and deployment of exist- 1 arXiv:2304.01464v1  [cs.CV]  4 Apr 2023ing advanced detection models. To this end, how to reduce the dependence on huge annotated datasets has achieved growing interest in the object detection community. As one of the important machine learning schemes for reducing data annotation, semi-supervised learning (SSL) aims to improve the generalization ability of model train- ing with a small number of labeled data together with large-scale unlabeled samples. In the semi-supervised ob- ject detection community, most works focus on 2D object detection [14, 18, 36, 42, 45], among which the teacher- student model is the mainstream framework. Specifically, the teacher network with weakly augmented labeled data generates pseudo labels to train the student network with strong data augmentation. This pipeline has been widely verified to be effective, in which the quality of pseudo labels and the data augmentation strategy are the key and many works have been proposed to tackle them [2,7,35,36]. Ben- efiting from 2D semi-supervised object detection, several 3D semi-supervised object detection methods have been proposed [25, 41, 49, 55], which still mainly adopted the teacher-student model. These methods, as well as 2D semi- supervised object detection methods [22,36,58], mainly use a hard way, e.g., a score threshold, to get pseudo labels to train the student network. This kind of strategy is difficult to guarantee the quality of pseudo labels. Taking the score threshold strategy as an example, if the threshold is too low, the pseudo labels will contain many false objects, while if it is too high, the pseudo labels will miss many real ob- jects which will be improperly used as background (see in Fig. 1 (a)). As exhibited in [41], only about 30% of the ob- jects can be mined from the unlabeled scenes even at the end of the network training. Thus, both of those two cases will bring the student network confused supervision, which harms the performance of the teacher-student model. This would inevitably happen for the single threshold strategy, even adopting some optimal threshold search method [41]. Thus, how to produce reasonable pseudo labels from the teacher network output is an important issue to address for better training the student networks. Besides the quality of pseudo labels, data augmentation is also the key to the teacher-student model as mentioned previously. Extensive works in 2D semi-supervised object detection have shown that strong data augmentation is very important to learn the strong feature representation ability of the student network. Thus, kinds of strong data aug- mentation strategies, e.g., Mixup [50], Cutout [7], and Mo- saic [4] have been widely adopted. However, current 3D semi-supervised object detection methods adopt some weak data augmentation strategies, e.g., flip-and-shift. These kinds of data augmentations are not able to well drive the student network to learn strong feature representation abil- ity. Thus, the good effect of data augmentation in 2D semi- supervised object detection does not appear obviously in 3D semi-supervised object detection. To tackle the above issues of the quality of pseudo labels and data augmentation, we propose a Hierarchical Super- vision and Shuffle Data Augmentation (HSSDA) method for 3D semi-supervised object detection. We still adopt the teacher-student model as our mainframe. For obtain- ing more reasonable pseudo labels for the student network, we design a dynamic dual-threshold strategy to divide the output of the teacher network into three groups: (1) high- confidence level pseudo labels, (2) ambiguous level pseudo labels, and (3) low-confidence level pseudo labels, as shown in Fig. 1 (a). This division provides hierarchical supervision signals for the student network. Specifically, the first group is used as the strong labels to learn the student network, while the second join learning through a soft-weight way. The higher the score is, the more it affects learning. The third group is much more likely to tend to be false objects. We directly delete them from the point cloud to avoid con- fusing parts of the object point cloud into the background. For strengthening the feature representation ability of the student network, we design a shuffle data augmentation strategy in this paper. As shown in Fig. 1 (b), we first gener- ate shuffled scenes by splitting and shuffling the point cloud patches in BEV (bird-eye view) and use them as inputs to the student model. Next, the feature maps extracted from the detector backbone are unshuffled back to the original point cloud geometry location. To summarize, our contributions are as follows: • We propose a novel hierarchical supervision gener- ation and learning strategy for the teacher-student model. This strategy can provide the student network hierarchical supervision signal, which can fully utilize the output of the teacher network. • We propose a shuffle data augmentation strategy that can strengthen the feature representation ability of the student network. • Our proposed hierarchical supervision strategy and shuffle data augmentation strategy can be directly ap- plied to the off-the-shelf 3D semi-supervised point cloud object detector and extensive experiments demonstrate that our method has achieved state-of-the- art results. 2. Related Work 2.1. 3D Object Detection 3D object detection is a fundamental task in the au- tonomous driving area. The mainstream 3D object detection methods can be roughly divided into three types: voxel- based methods [9, 15, 21, 46, 56, 57], point-based meth- ods [26, 33, 34, 47, 48, 54], and point-voxel-based methods 2[10, 20, 30–32]. For voxel-based methods, voxelization is a common operation that transforms irregular point clouds into voxel grids for applying traditional 2D or 3D convo- lution. In V oxelnet [59], the voxel-wise encoding layer was adopted for collective feature representation extraction from the voxel-wise LiDAR point cloud. V oxSeT [9] presented a novel transformer-based framework that encoded features from a larger receptive field. Point-based approaches di- rectly use the raw point cloud to capture spatial structure information for feature extraction through networks of the PointNet series [27, 28]. PointRCNN [33] is the represen- tative, which directly generates point-level RoIs and uses the point-level features for further refinement. To acceler- ate the inference speed for applications, IA-SSD [52] pro- posed an efficient downsampling way and a contextual cen- troid perception module to capture geometrical structure. Point-V oxel-based methods combined voxel representations with point representations from the point cloud. Built on the PV-RCNN [31], PV-RCNN++ [32] leveraged a Vector- Pool aggregation to learn structure features and a sector- ized proposal-centric keypoints sampling strategy to obtain more keypoints. All the above fully supervised methods can be easily embedded into our HSSDA framework, e.g., PV- RCNN [31] and V oxel-RCNN [6]. 2.2. Semi-supervised Learning (SSL) SSL can greatly reduce the annotations for model train- ing and most existing works focus on image classifica- tion, which can be broadly divided into two types: con- sistency regularization [2, 3, 23, 43] and pseudo-labeling methods [1, 12, 44]. The former approaches assume the model’s predictions to be consistent under input perturba- tions/augmentations (e.g., different contrast, flip, etc.) and penalize the inconsistency of predictions. Techniques range from simple augmentation to more complex transforma- tions such as MixUp [50], as well as stronger automatic augmentation such as Cutout [7] and CTAugment [2]. The latter methods exploit pseudo-labeling, where the model first is trained trains on labeled data and then iteratively generates the pseudo labels of unlabeled data to add highly confident predictions for training. It is revisited in deep neu- ral networks to learn from large amounts of unlabeled data. Notably, perturbation mechanisms of the above two types of methods play a key role in promoting the model robustness against noise in network parameters or structure but have not been explored in SSL for 3D object detection. 2.3. Semi-supervised Object Detection Inspired by the SSL works in image classification, SSL is also applied to the 2D object detection to alleviate the heavy annotation problem [14, 18, 36, 42]. STAC [36] gen- erated pseudo labels for unlabeled data in an offline manner. To further improve the quality of pseudo labels, Instant- Teachering [58] rectified the false predictions via the co- rectify scheme and experimented with MixUp [50] and Mo- saic [4]. This work aims to tackle a more challenging task, SSL for 3D object detection, where large spaces of 7 Degrees-of-Freedom of 3D objects need to be searched. Recently, several works have been proposed in the 3D SSL domain. SESS [55] and 3DIoUMatch [41] are the pioneer approaches for 3D object detection from indoor and outdoor point cloud data. Similar to 2D SSL meth- ods, SESS [55] leveraged a triple consistency regularization strategy to align the 3D proposal from the teacher and stu- dent network. Following the pseudo-labeling line in SSL, 3DIoUMatch [41] designed a series of filtering strategies such as objectness, classification, and localization threshold to obtain high-quality pseudo labels, and a unique IoU esti- mation branch to further deduplicate the predictions. Differ- ent from 3DIoUMatch, Proficient Teachers [49] developed several necessary modules to improve the recall and preci- sion of pseudo labels and removed the necessity of thresh- old setting. DetMatch [25] generated more precise pseudo labels by matching 2D and 3D detection from each modal- ity. These works employed the EMA weight update strat- egy to train a student network and then gradually update the teacher network. Although achieving impressive perfor- mances with high-quality pseudo labels, the missing-mined objects and heuristic strong augmentation are ignored. By contrast, our HSSDA leverages the hierarchical supervision and shuffle data augmentation to alleviate these issues and further improve performance. 3. Method 3.1. Preliminary Problem Definition We first provide the definition of semi-supervised 3D object detection. In detail, the model is trained with a set of labeled scenes Ds = {ps i, ys i}Ns i=1 and a set of unlabeled scenes Du = {pu i }Nu i=1, where pi ∈ Rn×{3+r} represents a point cloud scene pi which has n points with three-dimensional coordinates and additional r- dimensional information (e.g., color, intensity) that can be treated as extra features, Ns and Nu are the numbers of la- beled and unlabeled point cloud scenes, respectively. Gen- erally speaking, Nu >> Ns. For a scene ps i, the annotation ys i is composed of both 7-dimensional location information which includes center, size, and orientation, and category of the 3D bounding boxes. Teacher-Student Framework Similar to the main- stream researches [16, 41, 55], our learning paradigm also builds up on the teacher-student framework which includes two 3D detectors with the same configurations. Here, we can use any off-the-shelf state-of-the-art 3D object detec- tor, e.g., PV-RCNN [31] and V oxel-RCNN [6]. Following those works, we build the teacher detector via exponential 3Weak  Aug. Detection Head Detection Head High-confidence  level supervision Ambiguous level  supervision Weak Aug. Teacher Network Clean  scene  Detector  Backbone  Split & Shuffle Detector  Backbone  Detection Head Student Network Shuffled Features Unshuffled Features  EMA Update Hierarchical supervision  ̂𝑝𝑝𝑖𝑖 𝑢𝑢 Low-confidence level  supervision ~ Unlabeled  scene Augmented scene Pair of predictions Augmented scene  ... Groundtruth Labeled scenes Labeled  scenes Mined  scenes Confident scene set Labeled  scenes Mined  scenes Detector  Backbone  Dynamic dual-threshold generation  Augmented scene set Weak  Aug. Predictions Predictions Predictions c ir Predictions c ir Pair of predictions u ip u ip u ir u ir IoU consistency  constraint Confidence score  constraint Objectness score  constraint Dual-threshold-based  division Position information (,)high low cls clsττ (,)high low obj objττ (,)high low iou iouττClass 1: Class 2: (,)high low cls clsττ (,)high low obj objττ (,)high low iou iouττ...  : Figure 2. Overview of the proposed HSSDA pipeline. We propose a dual-threshold strategy to help the teacher network to generate hierarchical supervision to train the student network. Besides, we also propose a data augmentation strategy to strengthen the ability of feature representation of the student network. moving average (EMA) [39]: θi+1 t = θi t · α + θi s · (1 − α), (1) where α is the EMA decay rate, θt and θs represent the pa- rameters of the teacher and student networks, respectively, and i denotes the training step. 3.2. Overview The pipeline of our HSSDA framework is illustrated in Fig. 2, which is derived from the basic teacher-student mu- tual learning framework. In the burn-in stage of training, we train the detector in a fully supervised manner follow- ing OpenPCDet [40] with the labeled scenes and keep the same setting as the used detector. Then, both the teacher network and student network are initialized with the same pre-trained weight parameters. In the mutual learning stage, there are three steps in each training epoch. The first step is to generate three kinds of dual-thresholds for each class in a global view, as shown on the left of the pink dotted line in Fig. 2. Specifically, we construct a confident scene set Dc composed of labeled scenes and mined scenes (in the first epoch, it just contains all labeled scenes). Then we sequentially input each scene from Dc and its weak augmentation (rotation and scaling) into the teacher network to produce a pair of predictions. Based on those pairs of predictions and the object informa- tion from Dc, we design a dynamic dual-threshold gener- ation strategy to obtain three kinds of dual-thresholds for each class in terms of confidence score, objectness score, and IoU consistency: (τhigh cls , τlow cls ), (τhigh obj , τlow obj ), (τhigh iou , τlow iou ). The second step (see the right of the pink dotted line in Fig. 2) is mainly to mine the hierarchical pseudo la- bels. Specifically, each unlabeled scene pu i and its weak augmented scene ˜pu i are sequentially input to the teacher de- tector to produce a pair of predictions. Through three mea- sure rules based on the dual-thresholds obtained in the first step, we can generate the hierarchical supervision: (1) high- confidence level pseudo labels, (2) ambiguous level pseudo labels, and (3) low-confidence level pseudo labels. We add all high-confidence pseudo labels into the confident scene set Dc for the next dual-threshold generation. We also add the ground-truth from labeled scenes into the first group for following student network training. In the third step, we use the hierarchical supervision composed of three groups of pseudo labels to train the student network with our designed shuffle data augmentation, and then update the teacher net- work by the EMA strategy according to Eq. (1). After the mutual training step, we use the 3D detector from the student network as our final detector. Through the above procedure, we can see that our designed framework can train any off-the-shelf 3D detector which consists of a backbone and a detection head. 3.3. Dynamic dual-threshold generation The dual threshold generation is dynamically conducted in each training epoch. Algorithm 1 describes the whole process of generating dual-thresholds for one class in one training epoch, given a confidence setDc = {pc i , yc i }Nc i=1, its pairs of predictions rc, ˜rc through the 3D detector, and an IoU matching threshold τpair. Here τpair is determined ex- perientially and is used to judge if two 3D bounding boxes match. Initially, we create three empty sets Pcls, Pobj and Piou to collect the confidence score, objectness score and IoU. These three sets will be used to search three optimal dual-thresholds. Concretely, for the i-th scene, we can fetch the predicted bounding boxes br i and ground truth bounding 4Algorithm 1: Dynamic dual-threshold generation Input: confident scene set Dc, pairs of predictions rc and ˜rc, IoU matching threshold τpair. Output: (τhigh cls , τlow cls ), (τhigh obj , τlow obj ), (τhigh iou , τlow iou ) 1 initialize empty sets Pcls, Pobj, and Piou; 2 for each {pc i , yc i } ∈Dc do 3 fetch bounding boxes bgt i from yc i ; 4 fetch bounding boxes br i from rc i ; 5 fetch bounding boxes ˜br i from ˜rc i ; 6 for bgt ij in bgt i do 7 compute the matrix M ←IoU (bgt ij , br i ); 8 if max(M) > τpair then 9 choose index k with max(M); 10 Pcls = Pcls ∪ sc cls, 11 sc cls is the confidence score of rc ik; 12 Pobj = Pobj ∪ sc obj, 13 sc obj is the objectness score of rc ik; 14 Piou = Piou ∪ vc, 15 vc = max(IoU (br ik,˜br i )); 16 (τhigh cls , τlow cls ), (τhigh obj , τlow obj ), (τhigh iou τlow iou ) ← JNB (Pcls, Pobj, Piou); boxes bgt i from rc i and yc i , respectively. Further, we utilize the common IoU-based strategy to pair a predicted box br ik for each ground truth bgt ij in the i-th scene, aiming to ob- tain the predicted confidence scores and predicted object- ness scores of ground truth objects, and collect these scores into the confidence score set Pcls and objectness score set Pobj, respectively. In this way, we can distinguish predic- tions with different classifications and objectness reliability in a global view. At the same time, we can also get the consistency IoU set Piou based on the rc i and ˜rc i , which fa- cilitates grading predictions with different localization qual- ity in a consistency constraints manner. After handling all scenes, we solve the dual-threshold search problem through a global clustering algorithm. Specifically, we adopt the Jenks Natural Breaks (JNB) [13] algorithm to search the natural turning points or breakpoints based on the three sets. As shown in Fig. 3 (a) and (b), we can automatically obtain the dual-threshold τhigh cls and τlow cls for each class. Similarly, we can automatically obtain the other two dual-thresholds τobj and τiou, which are detector-agnostic and category- aware. As mentioned in Sec. 3.2, in each training epoch, the confident scene Dc will be updated, so all three dual- thresholds will dynamically change during training. 3.4. Hierarchical supervision generation As shown in Fig. 2, with the three dual-thresholds for each class, we divide the mined object pseudo labels from * * 0.99 * 0.99 0.99 0.64 * * 0.94 0.71 * * * * * 0.82 0.94 0.11 0.82 Numbers of (× 102) Score Score Score Score Numbers of (× 103) Numbers of (× 103)  Numbers of (× 102) (a) (b) (d)(c) unlabeled scene Detection Head IoU-based  consistency filtering High-confidence  level supervision Ambiguity-level  supervision Weak Aug. Teacher Confidence-based  score filtering Dynamic dual-threshold  strategy  clean scene  Detector  Backbone  Split & Shuffle Detector  Backbone  Detection Head Student Shuffled Features Unshuffled Features  EMA Update Objectness-based  score filtering Hierarchical supervision  𝑝𝑝𝑖𝑖 𝑢𝑢 𝑝𝑝𝑖𝑖 𝑢𝑢𝐴𝐴𝐴𝐴𝐴𝐴. 𝑝𝑝𝑖𝑖 𝑢𝑢′𝐴𝐴𝐴𝐴𝐴𝐴. 𝑝𝑝𝑖𝑖 𝑢𝑢′ ̂𝑝𝑝𝑖𝑖 𝑢𝑢 𝑝𝑝𝑖𝑖 𝑢𝑢𝑆𝑆𝑆𝑆 Low-confidence level supervision Figure 3. Selection of the dual-threshold based on the Jenks Natu- ral Breaks [13]. In each subplot, the blue line represents the sorted scores from the sequence pool P and the red and green dash lines indicate the high threshold and low threshold, respectively. (a) and (b) show the process of dynamic confidence IoU selection for ‘Car’ and ‘Pedestrian’. Similarly, (c) and (d) show the selection of dynamic consistency IoU threshold. unlabeled scenes into hierarchical supervision which in- cludes (1) high-confidence level pseudo labels, (2) ambigu- ous level pseudo labels and (3) low-confidence level pseudo labels. Specifically, the high-confidence level pseudo la- bel ˜yi is chosen when the predicted result ru i of unlabeled scenes from the teacher network meets all three following inequalities simultaneously,i.e., su cls > τhigh cls , su obj > τhigh obj and vu > τhigh iou , where su cls and su obj are the confidence score and the objectness score of the chosen predicted re- sult, respectively, and vu is the consistency IoU between chosen ru i and ˜ru i . Besides, we group the rest of the predic- tions as ambiguous level pseudo labels, which meet all three following inequalities simultaneously, i.e., su cls > τlow cls , su obj > τlow obj and vu > τlow iou . As for those predictions that neither belong to the high-confidence level nor ambiguous level, we mark them as low-confidence level predictions. As mentioned previously, we also add the ground-truth from labeled scenes into the group of high-confidence level pseudo labels which provide strong object label supervi- sion for the student network, while the ambiguous level group will supervise the student network training through a soft-weight way which will be introduced in Sec. 3.6. Following [17], we leverage the points removal strategy to eliminate noise information based on the group of low- confidence level pseudo labels. 3.5. Shuffle Data Augmentation Weak-strong data augmentation plays a significant role in the teacher-student style framework, which guides the model to learn strong feature representation. As mentioned previously, the data augmentation of the student network of 5current methods is too weak to learn the strong ability of feature representation. However, due to the huge modality difference between 2D images and 3D point cloud, it is not feasible to directly apply the widely-used data augmenta- tion strategies, e.g., color transformation or Mixup [50] to point cloud for object detection. Thus, we propose a shuffle data augmentation strategy for the student network. Specif- ically, as shown in Fig. 2, given a sceneˆpu i from the teacher network, we first clip the range of point cloud scene into [x1, x2] for the X axis, [y1, y2] for the Y axis and compress the Z axis to form BEV (bird-eye-view) grid. Then we split each scene into R × C(e.g., 2 × 2) patches and generate the same shaped R×C position information to shuffle each scene patch. By feeding the shuffled patches to the detector backbone for feature extraction, the teacher network with a weak branch and the student network with a strong branch can achieve obvious differences, which is beneficial for the student network to learn more complex and peculiar infor- mation from hierarchical supervision. Then we leverage the position information to unshuffle the patch features for further regression and classification in the detection head. Although shuffled patches make it difficult to distinguish the edges or parts of objects in scenes, the unshuffle opera- tion before the detection head restores the original location from the feature space. Hence, our student network deliv- ers more effort into learning with weaker features, which would strengthen the ability of feature representation, and especially benefit to detect objects with weak feature due to small sizes, e.g., ‘Pedestrian’, ‘Cyclist’, which will be shown in experiments. 3.6. Training Objective Function Following [18, 38], we freeze the optimization of the teacher detector, and the student detector is trained on both unlabeled scenes with the hierarchical supervision and la- beled scenes with the ground-truth. More specifically, the training objective consists of a supervised loss for labeled and unlabeled scenes. Ls = X i Lcls (ps i , ys i ) +Lreg (ps i , ys i ) , (2) Lu = X i Lcls \u0000 ˆpu i , ˜yu ij \u0001 + Lreg \u0000 ˆpu i , ˜yu ij \u0001 +wijLcls \u0000 ˆpu i , ˆyu ij \u0001 + wijLreg \u0000 ˆpu i , ˆyu ij \u0001 , (3) where Lcls is the classification loss, Lreg is the regression loss, ˆyu ij and ˜yu ij are the j-th ambiguous level pseudo la- bels and high-confidence level pseudo labels generated by the teacher detector in the i-th scene, and wij = rcls ij · robj ij denotes the soft-weight for ambiguous level pseudo labels ˆyu ij, which is determined by a combination of predicted confidence score and predicted objectness reliability score. Thanks to the clean scenes generated by the noise points re- moval operation and further obtaining complex scenes by GT sampling data augmentation [46], we do not force each training batch to contain a mixture of labeled scenes ps i and unlabeled scenes pu i with a certain ratio (e.g., 1 : 1in [41]), but randomly sample each batch from the entire dataset for training. Hence, the training loss is defined as follows: L = Ls + Lu. (4) Thus, we remove the hyper-parameter to trade-off between Ls and Lu as used in the common teacher-student frame- work [18, 36, 41]. 4. Experiments 4.1. Datasets and Evaluation Metrics KITTI Dataset. Following the state-of-the-art meth- ods [25, 41], we evaluate our HSSDA on the KITTI 3D de- tection benchmark [8], and we use the divided train split of 3,712 samples and val split of 3,769 samples as a com- mon practice [31]. Then we sample three different 1% and 2% labeled scenes over train split based on the released 3DIoUMatch [41] splits. The reported results are averaged over model training on three sampled splits and evaluated on the val split. In addition, the KITTI benchmark has three difficulty levels (easy, moderate, and hard) due to the oc- clusion and truncation levels of objects. For fair compar- isons, we report the mAP with 40 recall positions, with a 3D IoU threshold of 0.7, 0.5, and 0.5 for the three classes: car, pedestrian, and cyclist, respectively. Waymo Open Dataset.We also evaluate our HSSDA on the Waymo Open Dataset [37], which is one of the biggest autonomous driving datasets, containing 798 sequences (ap- proximately 158k point cloud scenes) for training and 202 sequences (approximately 40k point cloud scenes) for val- idation, whilst the view of annotations is in full 360 ◦ field. We find that even only 1% of the labeled Waymo scenes contain approximately 3 times as many object annotations as the full KITTItrain split. Thus, we sample 1% of the 798 training sequences (approximately 1.4k point cloud scenes) and report the standard mean average precision (mAP) as well as mAPH, which represent the heading angle factors. In addition, the prediction results are split into LEVEL 1 and LEVEL 2 for 3D objects including more than five Li- DAR points and one LiDAR point, respectively. 4.2. Implementation Details At the training stage, the student network of our HSSDA is end-to-end optimized with the ADAM optimizer and a cosine annealing learning rate [19]. As for the weak augmentation for the teacher network, we randomly flip each scene along X-axis and Y-axis with 0.5 probability, and then scale it with a uniformly sampled factor from [0.91, 1.12]. Finally, we rotate the point cloud around Z- axis with a random angle sampled from \u0002 −π 4 , π 4 \u0003 . For 6Model Modality 1% 2% 20% Car Ped. Cyc. Avg. Car Ped. Cyc. Avg. Car Ped. Cyc. Avg. PV-RCNN [31] LiDAR 73.5 28.7 28.4 43.5 76.6 40.8 45.5 54.3 77.9 47.1 58.9 61.3 3DIoUMatch [41] (PVR.-based) LiDAR 76.0 31.7 36.4 48.0 78.7 48.2 56.2 61.0 - - - - DetMatch [25] (PVR.&FR. [29]-based)LiDAR + RGB77.5 57.3 42.3 59.0 78.2 54.1 64.7 65.6 78.7 57.6 69.6 68.7 Our HSSDA (PVR.-based) LiDAR 80.9 51.9 45.7 59.5 81.9 58.2 65.8 68.6 82.5 59.1 73.2 71.6 Table 1. Experimental results on KITTI dataset compared with recent state-of-the-art methods. For fair comparison, the results are reported with 40 recall positions, under IoU thresholds 0.7, 0.5, and 0.5 for ‘Car’, ‘Pedestrian’, and ‘Cyclist’, respectively. 1% Data (∼1.4k scenes) Modality Veh. (LEVEL 1)Veh. (LEVEL 2)Ped. (LEVEL 1)Ped. (LEVEL 2)Cyc. (LEVEL 1)Cyc. (LEVEL 2) mAP mAPH mAP mAPH mAP mAPH mAP mAPH mAP mAPH mAP mAPH PV-RCNN [31] LiDAR 47.3 45.6 43.6 42.0 28.9 15.6 26.2 14.1 - - - - DetMatch [25] (PVR.& FR. [29]-based)LiDAR+RGB52.2 51.1 48.1 47.2 39.5 18.9 35.8 17.1 - - - - Improvement - +4.9 +5.5 +4.5 +5.2 +10.6 +3.3 +9.6 +3.0 - - - - PV-RCNN [31] (Reproduced by us)LiDAR 48.5 46.2 45.5 43.3 30.1 15.7 27.3 15.9 4.5 3.0 4.3 2.9 Our HSSDA (PVR.-based) LiDAR 56.4 53.8 49.7 47.3 40.1 20.9 33.5 17.5 29.1 20.9 27.9 20.0 Improvement - +7.9 +7.6 +4.2 +4.0 +10.0 +5.2 +6.2 +1.6 +24.6 +17.9 +23.6 +17.1 Table 2. Performance comparison on the Waymo Open Dataset with 202 validation sequences for the 3D detection. 3D Detection (Car)3D Detection (Ped)3D Detection (Cyc)Model DataEasy Mod HardEasy Mod HardEasy Mod HardV oxel-RCNN [6]1% 87.9 74.0 67.123.7 19.0 17.444.8 37.0 25.5Ours (V oxel-RCNN-based)1% 92.5 81.7 77.550.7 43.9 42.465.2 48.3 42.5V oxel-RCNN [6]2% 89.2 76.5 71.544.2 40.2 34.456.7 39.9 37.4Ours (V oxel-RCNN-based)2% 91.6 82.0 77.964.9 58.3 50.988.0 65.7 60.9 Table 3. Experimental results on KITTI dataset based on the V oxel-RCNN detector, where the metrics are the same as Tab. 1. the KITTI Dataset, the X-axis and Y-axis are limited in [0, 70.4]m and [−40, 40]m in the shuffle data augmentation, and our HSSDA (PV-RCNN-based) is trained for 80 epochs with the batch size 50. For the Waymo Open Dataset, the point cloud scene is clipped into [−75.2, 75.2]m for X and Y axes, and training with the batch size 30 for 10 epochs. We set the value ofτpair to 0.5 in all experiments according to the evaluation metric in the public datasets. 4.3. Main Results KITTI Dataset. We first evaluate our proposed model on the popular KITTI dataset. Tab. 1 lists the results of dif- ferent methods. From this table, we can observe that our ap- proach significantly outperforms the state-of-the-art meth- ods. Specifically, our approach has a remarkable boost in the ‘Car’ class for all settings, which has improvements of 7.4, 5.3, and 4.6 points compared to the PV-RCNN baseline for 1%, 2%, and 20%, respectively. Even compared to the recently proposed DetMatch [25] which uses two modalities of LiDAR and RGB, our methods just with LiDAR still have better results for most of the settings. Besides, we replace the point-voxel-based PV-RCNN 3D detector with a repre- sentative voxel-based V oxel-RCNN [6] 3D detector. The similarly impressive experimental results in Tab. 3 demon- strate the effectiveness of our HSSDA. Waymo Dataset. For the more challenging Waymo Dataset, our approach still has a significant improvement in performance compared to the state-of-the-art methods. As shown in Tab. 2, our approach surpasses DetMatch [25]. It is worth mentioning that the proposed method achieves 29.1 mAP for ‘Cyclist’, which far exceeds the baseline. 4.4. Ablation Study In this section, we present a series of ablation studies to analyze the effect of our proposed strategies in HSSDA. All the experiments are conducted based on the V oxel-RCNN detector with the 2% KITTI split and evaluated on val split due to its fast training speed. Tab. 4 summarizes the abla- tion results on our shuffle data augmentation (SDA) and hi- erarchical supervision (HS) of the teacher network, which provides three levels of supervision: high-confidence level (H LEV) pseudo labels, ambiguous level (A LEV) pseudo labels, and low-confidence level (L LEV) pseudo labels. Effect of the hierarchical supervision. It can be found that only considering the high-confidence level pseudo la- bels will perform better than the baseline as shown in Exp.2 in Tab. 4, but the improvements are limited by the con- fused background supervision. The introduction of ambigu- ous level supervision information can lead to further per- formance improvements which can be seen in Exp.3. Fur- thermore, we can observe that from Exp.4 generating clean scenes via low-confidence supervision can significantly im- prove the detection accuracy, which indicates the effective- ness of the points removal operation. Besides, the collab- oration of three different levels of supervisions can greatly improve performance, as shown in Exp.5 of Tab. 4. Those results mean that all hierarchical supervisions have contri- butions to final performance when they work together. Effect of the shuffle data augmentation. Exp.5 in Tab. 4 shows the effect of our shuffle data augmentation strategy. The classes of ‘Pedestrian’ and ‘Cyclist’ have very weak original features due to their small sizes. Both of them usually are very hard to detect. However, our shuffle data augmentation strategy can significantly improve their per- formance. It can be also observed that a slight drop for ‘Car’ may be due to the shuffle data augmentation splitting the original objects, leading to blurred edges for locating. 7Exp. Hierarchical Supervision (HS)SDA 3D Detection mAPHLEV A LEV L LEV Car Ped. Cyc. 1 - - - - 76.5 40.2 39.9 52.2 2 ✓ - - - 75.9 51.8 51.5 59.7 3 ✓ ✓ - - 78.8 51.1 52.2 60.7 4 ✓ ✓ ✓ - 82.4 56.0 61.3 66.5 5 ✓ ✓ ✓ ✓ 82.0 58.3 65.7 68.6 Table 4. Ablation study of different components in HSSDA. Method R C 3D Detection mAPCar Ped. Cyc. HS 1 1 82.4 56.0 61.3 66.5 HS + SDA 1 2 82.5 57.1 64.3 67.9 2 2 82.0 58.3 65.7 68.6 2 4 81.2 56.8 65.5 67.8 4 4 81.1 54.8 65.8 67.2 Table 5. Results of various combinations of R and C. Our shuffle data augmentation has two hyperparameters: R and C, which decide the number of scene patches to shuf- fle. To evaluate the effect of two hyperparameters, we inves- tigate the performance of the proposed HSSDA with differ- ent combinations of R and C in Tab. 5. We can observe that the model achieves the best result when R = C = 2 (i.e., the scene grids are split into 4 patches and perform random shuffle). Hence, we set R and C to 2 in all our experiments. 4.5. Quality Analysis In this section, we analyze the quality of the generated pseudo labels which play a key role during model train- ing. First of all, if the 3D IoU between pseudo labels and ground-truth boxes of labeled scenes is bigger than 0.5 with the same class, we regard the pseudo-label as a correctly mined object. From Tab. 6, we can see the final precision of our high-confidence level pseudo labels for each class on the KITTI dataset is particularly accurate, which indicates the effectiveness of our dual-threshold strategy. Besides, we provide qualitative results of wrong high-confidence level pseudo labels in Fig. 4. For ease of viewing, we only show the object of one failure case in each scene. (a) and (b) in Fig. 4 show that the common failures for ‘Car’ usually occur with similar classes (such as vans and trucks). Interestingly, our method can reliably mine some real objects which were not annotated in the dataset (see Fig. 4 (c) and Fig. 4 (d)). Additionally, due to the small sizes of ‘Pedestrian’, most of the failure examples are caused by inaccurate localization, as shown in Fig. 4 (e) and (f) (the ground truth and pseudo- labeling 3D bounding box are drawn in red and cyan.) 5. Conclusion In this paper, we propose a novel teacher-student-based method for 3D semi-supervised object detection, called HSSDA. Through the dual-threshold strategy in the teacher network, we can provide hierarchical supervision to effec- Category Split setting on KITTI 1% 2% 20% Car 96.73 (4627/4783)98.69 (4476/4535)98.88 (4508/4559) Pedestrian 85.58 (204/239)92.29 (273/296) 93.67 (74/79) Cyclist 95.53 (107/112)95.00 (114/120)96.33 (105/109) Table 6. Final precision of high-confidence level pseudo labels on ‘Car’, ‘Pedestrian’ and ‘Cyclist’ classes with different SSL set- tings. The blue and red numbers represent the total and correctly number of mined pseudo labels, respectively. （a） （b） （c） （d） （e） （f） （a） （b） （c） （d） （e） （f） Figure 4. Qualitative analysis of pseudo labels on KITTI. For a better view, we only show the objects we are interested in and set the pseudo-labeling car, pseudo-labeling pedestrian, and ground truth bounding box in green, cyan, and red, respectively, whilst projecting boxes in point cloud back to RGB images. (a) and (b) show the case of category errors, (c) and (d) show the missing- annotated instance of the dataset, and (e) and (f) show the case of poorly localized pseudo labels. Best viewed in color. tively train the student network, while eliminating the neg- ative impact of missing-mined objects of unlabeled scenes. In addition, the shuffle data augmentation strategy shuffles the input and unshuffles the feature blocks to strengthen the feature representation ability of the student network. Ex- tensive experiments validate the superiority of our method in challenging datasets. Our HSSDA can train any 3D de- tector which consists of a backbone and detection head. Limitations. Our HSSDA designs a dynamic dual- threshold strategy that determines the optimal threshold in a global view. So, we need to use the teacher network for additional validation, which requires more training time. In addition, the shuffle data augmentation may split the com- plete objects resulting in blurred edges and locating. Acknowledgments. This work is supported in part by the National Key R&D Program of China (2022YFA1004100), and in part by the National Natural Science Foundation of China (No.62176035, 62201111, 62036007), the Sci- ence and Technology Research Program of Chongqing Mu- nicipal Education Commission under Grant (No.KJZD- K202100606), the Chongqing graduate Research Innova- tion Project (CYB22249), the CQUPT Ph.D. Innovative Talents Project (BYJS202105), and the Macao Science and Technology Development Fund (061/2020/A2). 8References [1] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In IJCNN, pages 1–8. IEEE, 2020. 3 [2] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Ku- rakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remix- match: Semi-supervised learning with distribution matching and augmentation anchoring. In ICLR, 2019. 2, 3 [3] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. NeurIPS, 32, 2019. 3 [4] Alexey Bochkovskiy, Chien-Yao Wang, and Hong- Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 2, 3 [5] Yunlu Chen, Vincent Tao Hu, Efstratios Gavves, Thomas Mensink, Pascal Mettes, Pengwan Yang, and Cees GM Snoek. Pointmixup: Augmentation for point clouds. In ECCV, pages 330–345. Springer, 2020. 11 [6] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. V oxel r-cnn: Towards high performance voxel-based 3d object detection. In AAAI, pages 1201–1209, 2021. 3, 7, 11 [7] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 2, 3 [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. Int. J. Rob. Res., 32(11):1231–1237, 2013. 6 [9] Chenhang He, Ruihuang Li, Shuai Li, and Lei Zhang. V oxel set transformer: A set-to-set approach to 3d object detection from point clouds. In CVPR, pages 8417–8427, 2022. 2, 3 [10] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Structure aware single-stage 3d object detec- tion from point cloud. In CVPR, pages 11873–11882, 2020. 3 [11] Jordan SK Hu, Tianshu Kuai, and Steven L Waslander. Point density-aware voxels for lidar 3d object detection. In CVPR, pages 8469–8478, 2022. 1 [12] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning. In CVPR, pages 5070–5079, 2019. 3 [13] George F Jenks. Optimal data classification for choropleth maps. Department of Geographiy, University of Kansas Oc- casional Paper, 1977. 5 [14] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-based semi-supervised learning for object de- tection. NeurIPS, 32, 2019. 2, 3 [15] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, pages 12697–12705, 2019. 2 [16] Zhaoqi Leng, Shuyang Cheng, Benjamin Caine, Weiyue Wang, Xiao Zhang, Jonathon Shlens, Mingxing Tan, and Dragomir Anguelov. Pseudoaugment: Learning to use unla- beled data for data augmentation in point clouds. In ECCV, pages 555–572. Springer, 2022. 3 [17] Chuandong Liu, Chenqiang Gao, Fangcen Liu, Jiang Liu, Deyu Meng, and Xinbo Gao. Ss3d: Sparsely-supervised 3d object detection from point cloud. In CVPR, pages 8428– 8437, 2022. 5 [18] Yen-Cheng Liu, Chih-Yao Ma, and Zsolt Kira. Unbiased teacher v2: Semi-supervised object detection for anchor-free and anchor-based detectors. In CVPR, pages 9819–9828, 2022. 2, 3, 6 [19] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In ICLR, 2017. 6 [20] Jiageng Mao, Minzhe Niu, Haoyue Bai, Xiaodan Liang, Hang Xu, and Chunjing Xu. Pyramid r-cnn: Towards bet- ter performance and adaptability for 3d object detection. In ICCV, pages 2723–2732, 2021. 3 [21] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. V oxel transformer for 3d object detection. In ICCV, pages 3164– 3173, 2021. 2 [22] Peng Mi, Jianghang Lin, Yiyi Zhou, Yunhang Shen, Gen Luo, Xiaoshuai Sun, Liujuan Cao, Rongrong Fu, Qiang Xu, and Rongrong Ji. Active teacher for semi-supervised object detection. In CVPR, pages 14482–14491, 2022. 2 [23] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence , 41(8):1979–1993, 2018. 3 [24] Alexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe, and Francis Engelmann. Mix3d: Out-of-context data aug- mentation for 3d scenes. In 3DV, pages 116–125. IEEE, 2021. 11 [25] Jinhyung Park, Chenfeng Xu, Yiyang Zhou, Masayoshi Tomizuka, and Wei Zhan. Detmatch: Two teachers are bet- ter than one for joint 2d and 3d semi-supervised object de- tection. In ECCV, pages 370–389. Springer, 2022. 2, 3, 6, 7 [26] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In ICCV, pages 9277–9286, 2019. 2 [27] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, pages 652–660, 2017. 3 [28] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, pages 5099–5108, 2017. 3 [29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. NeurIPS, 28, 2015. 7 [30] Hualian Sheng, Sijia Cai, Yuan Liu, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, and Min-Jian Zhao. Improving 3d object detection with channel-wise transformer. In ICCV, pages 2743–2752, 2021. 3 9[31] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point- voxel feature set abstraction for 3d object detection. In CVPR, pages 10529–10538, 2020. 3, 6, 7, 11 [32] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv- rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection. International Journal of Computer Vision, 131(2):531–551, 2023. 1, 3 [33] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr- cnn: 3d object proposal generation and detection from point cloud. In CVPR, pages 770–779, 2019. 1, 2, 3, 11 [34] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural net- work for 3d object detection in a point cloud. InCVPR, pages 1711–1719, 2020. 2 [35] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. NeurIPS, 33:596–608, 2020. 2 [36] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020. 2, 3, 6, 11 [37] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, pages 2446–2454, 2020. 6, 11 [38] Yihe Tang, Weifeng Chen, Yijun Luo, and Yuting Zhang. Humble teachers teach better students for semi-supervised object detection. In CVPR, pages 3132–3141, 2021. 6 [39] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, pages 1195–1204, 2017. 4 [40] OpenPCDet Development Team. Openpcdet: An open- source toolbox for 3d object detection from point clouds. https://github.com/open-mmlab/OpenPCDet , 2020. 4 [41] He Wang, Yezhen Cong, Or Litany, Yue Gao, and Leonidas J Guibas. 3dioumatch: Leveraging iou prediction for semi- supervised 3d object detection. In CVPR, pages 14615– 14624, 2021. 2, 3, 6, 7 [42] Zhenyu Wang, Yali Li, Ye Guo, Lu Fang, and Shengjin Wang. Data-uncertainty guided multi-phase learning for semi-supervised object detection. In CVPR, pages 4568– 4577, 2021. 2, 3 [43] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. NeurIPS, 33:6256–6268, 2020. 3 [44] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet clas- sification. In CVPR, pages 10687–10698, 2020. 3 [45] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to- end semi-supervised object detection with soft teacher. In ICCV, pages 3060–3069, 2021. 2 [46] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed- ded convolutional detection. Sensors, 18(10):3337, 2018. 2, 6 [47] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd: Point-based 3d single stage object detector. In CVPR, pages 11040–11048, 2020. 2 [48] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Std: Sparse-to-dense 3d object detector for point cloud. In ICCV, pages 1951–1960, 2019. 2 [49] Junbo Yin, Jin Fang, Dingfu Zhou, Liangjun Zhang, Cheng- Zhong Xu, Jianbing Shen, and Wenguan Wang. Semi- supervised 3d object detection with proficient teachers. In ECCV, pages 727–743. Springer, 2022. 2, 3 [50] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 2, 3, 6, 11 [51] Jinlai Zhang, Lyujie Chen, Bo Ouyang, Binbin Liu, Jihong Zhu, Yujin Chen, Yanmei Meng, and Danfeng Wu. Pointcut- mix: Regularization strategy for point cloud classification. Neurocomputing, 505:58–67, 2022. 11 [52] Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jian- wei Wan, and Yulan Guo. Not all points are equal: Learn- ing highly efficient point-based detectors for 3d lidar point clouds. In CVPR, pages 18953–18962, 2022. 3 [53] Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jian- wei Wan, and Yulan Guo. Not all points are equal: Learn- ing highly efficient point-based detectors for 3d lidar point clouds. In CVPR, pages 18953–18962, 2022. 1 [54] Yanan Zhang, Di Huang, and Yunhong Wang. Pc-rgnn: Point cloud completion and graph neural network for 3d object de- tection. In AAAI, number 4, pages 3430–3437, 2021. 2 [55] Na Zhao, Tat-Seng Chua, and Gim Hee Lee. Sess: Self- ensembling semi-supervised 3d object detection. In CVPR, pages 11079–11087, 2020. 2, 3 [56] Wu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, and Chi- Wing Fu. Cia-ssd: Confident iou-aware single-stage object detector from point cloud. In AAAI, number 4, pages 3555– 3562, 2021. 2 [57] Wu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu. Se- ssd: Self-ensembling single-stage object detector from point cloud. In CVPR, pages 14494–14503, 2021. 2 [58] Qiang Zhou, Chaohui Yu, Zhibin Wang, Qi Qian, and Hao Li. Instant-teaching: An end-to-end semi-supervised object detection framework. In CVPR, pages 4081–4090, 2021. 2, 3 [59] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning for point cloud based 3d object detection. In CVPR, pages 4490–4499, 2018. 3 101% Data(∼1.4k scenes)Veh.(LEVEL 1)Veh.(LEVEL 2)Ped.(LEVEL 1)Ped.(LEVEL 2)Cyc.(LEVEL 1)Cyc.(LEVEL 2)V oxel-RCNN [6]49.02/48.0342.36/41.5041.16/32.8134.73/27.665.84/5.615.62/5.40Ours (V oxel-RCNN-based)54.89/54.0648.28/47.5343.86/37.8436.59/31.5617.47/16.7316.72/16.01 Table 7. Results on the Waymo for the V oxel-RCNN detector. Model Data 3D Detection (Car)3D Detection (Ped.)3D Detection (Cyc.)Easy Mod HardEasy Mod HardEasy Mod HardPV-RCNN [31]100%92.1084.3682.4863.1254.8451.7889.10 70.38 66.01PV-RCNN [31] with SDA100%91.9184.5782.3162.8355.4951.0489.68 71.09 66.71 Table 8. Ablation study of SDA in the fully supervised framework. A. Additional Experimental Results (1) Additional experiments on the Waymo Dataset. We additionally test the V oxel-RCNN [6] on 1% of the Waymo [37] dataset, and the results in Tab. 7 still show the superiority of our method, which validates its generaliza- tion. (2) If the shuffle data augmentation (SDA) strategy is also effective for full supervision training ? To verify the effect of the SDA on fully-supervised 3D object de- tector, we inset the SDA into the PV-RCNN [31] and the results are listed in Tab. 8, which shows that the superior- ity of SDA in the supervised framework is not as obvious as in the semi-supervised framework. This is due to that the design of the strong augmentation in the student branch module has two main purposes: (1) strong enough to make a significant difference with weakly augmented samples of the teacher branch and (2) not too strong to ensure effective supervision information transmission. B. Visualization of Dynamic Dual-Threshold To better understand the dual-threshold hierarchical su- pervision in intuitive, we visualize the dynamic threshold changes during the training process in Fig. 5, where a solid line of a certain color represents a high threshold, and the dotted line of the same color represents a low threshold. C. Discuss on Other Augmentation Methods for Point Cloud The Mixup-based [50] augmentation methods have been extensively studied in the field of image classification and widely applied in 2D semi-supervised object detection [36] task. Following this idea, there have been several explo- rations in point cloud tasks as well. PointMixup [5] first ap- plied the idea of Mixup to point cloud and achieved linear interpolation through the optimal allocation. Mix3D [24] balances global contextual information and local geomet- ric information to achieve high-performance models. In addition, PointCutMix [51] proposes two different ways of replacing points to mix two point clouds. The latest SageMix explores salient regions in two point clouds and Vanilla Teacher Teacher Network Unlabeled scene Pseudo-labeling scene Ours Teacher Vanilla Student Student Network Unlabeled scene Augmented scene Student Network Unlabeled scene Augmented scene ( a) RGB ImageFull labeled scene (a) (b) Ours Teacher Low threshold Teacher Network Unlabeled scene Pseudo-labeling scene Dual-threshold  strategy Flip&Shift Split&Shuffle OR of cyc of car of car of car of car of cyc of cyc of cyc 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 𝑐𝑐𝑙𝑙𝑙𝑙 10 20 30 40 50 60 70 80 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Threshold Epoch  Ratio of label data (%) 8060 10040200 50 60 70 80 Moderate mAP Car Pedestrian Cyclist (a) (b) of cyc of car of car of car of car of cyc of cyc of cyc 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑐𝑐𝑐𝑐𝑐𝑐 𝑐𝑐𝑙𝑙𝑙𝑙 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 ℎ𝑖𝑖𝑖𝑖ℎ 𝜏𝜏𝑖𝑖𝑙𝑙𝑢𝑢 𝑐𝑐𝑙𝑙𝑙𝑙 10 20 30 40 50 60 70 80 0.6 0.7 0.8 0.9 1.0 Threshold Epoch Figure 5. Visualization curve of the dynamic dual-threshold during training. smoothly combines them into a continuous shape. How- ever, these methods mainly focus on point cloud classifica- tion and segmentation tasks. For outdoor 3D object detec- tion task, objects are usually naturally separated [33], and merging two point cloud scenes will cause overlaps between objects (e.g., two vehicles are rarely overlapped in 3D real- ity). Therefore, to the best of our knowledge, the above Mixup-based point cloud augmentation methods cannot be directly applied to detection tasks, which is the direction for our future research. 11",
      "references": [
        "Pseudo-labeling and confirmation bias in deep semi-supervised learning",
        "Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring",
        "Mixmatch: A holistic approach to semi-supervised learning",
        "Yolov4: Optimal speed and accuracy of object detection",
        "Pointmixup: Augmentation for point clouds",
        "Voxel r-cnn: Towards high performance voxel-based 3d object detection",
        "Improved regularization of convolutional neural networks with cutout",
        "Vision meets robotics: The kitti dataset",
        "Voxel set transformer: A set-to-set approach to 3d object detection from point clouds",
        "Structure aware single-stage 3d object detection from point cloud",
        "Point density-aware voxels for lidar 3d object detection",
        "Label propagation for deep semi-supervised learning",
        "Optimal data classification for choropleth maps",
        "Consistency-based semi-supervised learning for object detection",
        "Pointpillars: Fast encoders for object detection from point clouds",
        "Pseudoaugment: Learning to use unlabeled data for data augmentation in point clouds",
        "Ss3d: Sparsely-supervised 3d object detection from point cloud",
        "Unbiased teacher v2: Semi-supervised object detection for anchor-free and anchor-based detectors",
        "SGDR: stochastic gradient descent with warm restarts",
        "Pyramid r-cnn: Towards better performance and adaptability for 3d object detection",
        "Voxel transformer for 3d object detection",
        "Active teacher for semi-supervised object detection",
        "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
        "Mix3d: Out-of-context data augmentation for 3d scenes",
        "Detmatch: Two teachers are better than one for joint 2d and 3d semi-supervised object detection",
        "Deep hough voting for 3d object detection in point clouds",
        "Pointnet: Deep learning on point sets for 3d classification and segmentation",
        "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
        "Faster r-cnn: Towards real-time object detection with region proposal networks",
        "PV-RCNN: Point-voxel feature set abstraction for 3d object detection",
        "PV-RCNN++: Point-voxel feature set abstraction with local vector representation for 3d object detection",
        "PointRCNN: 3d object proposal generation and detection from point cloud",
        "Point-gnn: Graph neural network for 3d object detection in a point cloud",
        "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
        "A simple semi-supervised learning framework for object detection",
        "Scalability in perception for autonomous driving: Waymo open dataset",
        "Humble teachers teach better students for semi-supervised object detection",
        "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
        "OpenPCDet: An open-source toolbox for 3d object detection from point clouds",
        "3dioumatch: Leveraging iou prediction for semi-supervised 3d object detection",
        "Data-uncertainty guided multi-phase learning for semi-supervised object detection",
        "Unsupervised data augmentation for consistency training",
        "Self-training with noisy student improves imagenet classification",
        "End-to-end semi-supervised object detection with soft teacher",
        "Second: Sparsely embedded convolutional detection",
        "3dssd: Point-based 3d single stage object detector",
        "STD: Sparse-to-dense 3d object detector for point cloud",
        "Semi-supervised 3d object detection with proficient teachers",
        "mixup: Beyond empirical risk minimization",
        "Pointcutmix: Regularization strategy for point cloud classification",
        "Not all points are equal: Learning highly efficient point-based detectors for 3d lidar point clouds",
        "PC-RGNN: Point cloud completion and graph neural network for 3d object detection",
        "SESS: Self-ensembling semi-supervised 3d object detection",
        "CIA-SSD: Confident iou-aware single-stage object detector from point cloud",
        "SE-SSD: Self-ensembling single-stage object detector from point cloud",
        "Instant-teaching: An end-to-end semi-supervised object detection framework",
        "Voxelnet: End-to-end learning for point cloud based 3d object detection"
      ],
      "meta_data": {
        "arxiv_id": "2304.01464v1",
        "authors": [
          "Chuandong Liu",
          "Chenqiang Gao",
          "Fangcen Liu",
          "Pengcheng Li",
          "Deyu Meng",
          "Xinbo Gao"
        ],
        "published_date": "2023-04-04T02:09:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces HSSDA (Hierarchical Supervision and Shuffle Data Augmentation), a generic teacher-student framework for 3D semi-supervised object detection that: (1) devises a dynamic dual-threshold scheme to generate three levels of pseudo-labels (high-confidence, ambiguous, low) providing hierarchical supervision, and (2) proposes shuffle data augmentation that splits and shuffles BEV patches to strengthen the student’s feature learning. The approach consistently surpasses state-of-the-art methods on KITTI and Waymo with as little as 1-2% labeled data and can be plugged into off-the-shelf detectors such as PV-RCNN and Voxel-RCNN.",
        "methodology": "• Teacher–student model with EMA-updated teacher.\n• Dynamic dual-threshold generation per class across confidence score, objectness score, and teacher–student IoU consistency using Jenks Natural Breaks clustering on a ‘confident scene’ set each epoch.\n  – Thresholds split predictions into high, ambiguous, and low groups.\n  – High: treated as strong labels; Ambiguous: used with soft weights (product of confidence & objectness); Low: points inside predicted boxes are removed to avoid noise.\n• Shuffle Data Augmentation (SDA): input point cloud is clipped in BEV, divided into R×C grids (default 2×2), shuffled, processed by backbone, then feature maps are unshuffled before the detection head, creating strong augmentation for the student while keeping geometry for regression.\n• Loss = supervised loss on labeled data + hierarchical unsupervised loss on pseudo labels; no extra weighting hyper-parameter.\n• Compatible with various backbones; experiments use PV-RCNN and Voxel-RCNN.",
        "experimental_setup": "Datasets:\n1. KITTI 3D detection – 3,712 train / 3,769 val; SSL settings with 1%, 2%, 20% labeled subsets (three random splits). Metrics: 3D mAP@40 recall with IoU 0.7 (Car), 0.5 (Pedestrian/Cyclist).\n2. Waymo Open Dataset – 798 train / 202 val sequences; 1% labeled subset (~1.4k scenes). Metrics: mAP/mAPH for LEVEL-1 (>5 points) and LEVEL-2 (>1 point).\nBaselines/Comparisons: Fully-supervised PV-RCNN, Voxel-RCNN; semi-supervised 3DIoUMatch, DetMatch, Proficient Teachers, etc.\nImplementation: ADAM optimizer, cosine LR schedule; PV-RCNN trained 80 epochs (batch 50) on KITTI, 10 epochs (batch 30) on Waymo. Weak augmentations for teacher: random flip, scale 0.91-1.12, rotation ±45°. Patch shuffle parameters R=C=2. IoU pairing threshold τ_pair=0.5. Ablations on hierarchical supervision levels and SDA, plus grid size study.",
        "limitations": "1. Dual-threshold computation requires an extra teacher forward pass over the confident scene set each epoch, increasing training time and memory.\n2. Shuffle augmentation can split large objects, slightly degrading localization of classes like ‘Car’.\n3. Effectiveness demonstrated only on LiDAR point clouds; fusion with images not explored.\n4. Hyper-parameters (thresholds, shuffle grid size) still require empirical tuning per dataset.\n5. Approach relies on teacher quality; error accumulation from poor initial models or highly imbalanced classes may still occur.",
        "future_research_directions": "• Develop more efficient or adaptive threshold estimation to cut additional teacher inference cost.\n• Learnable or geometry-aware shuffling/mixing strategies that preserve object integrity while enriching context.\n• Extend HSSDA to multi-modal semi-supervised detection (LiDAR+camera, radar).\n• Investigate curriculum or self-training schedules that adjust soft weights for ambiguous labels dynamically.\n• Combine with self-supervised pre-training or active learning to further reduce labeled data requirements.\n• Explore real-time implementations and deployment on edge hardware.\n• Study robustness to domain shifts (weather, sensor noise) and applicability to dynamic/indoor scenes.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ]
}